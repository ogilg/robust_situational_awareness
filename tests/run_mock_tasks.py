#!/usr/bin/env python3
"""
Run a tiny end-to-end test of the four tasks using the MockProvider (model: mock-llm).
"""

import os
import sys
import json
from pathlib import Path

# Ensure project root is on sys.path
ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)

# Stages (oversight)
from sad.stages.oversight.task_stages import make_task as make_stages_task

# Self-recognition (WHO)
from sad.self_recognition.run import self_recognition_who

# Anti-imitation / output control
from sad.anti_imitation.output_control.task_output_control import make_task as make_output_control_task

# ID leverage (generic)
from sad.id_leverage.entity_id.task_generic import make_task as make_idlev_generic_task


def main():
    model = "mock-llm"

    # 1) Stages oversight â€“ requires a data JSONL. For mock, synthesize a tiny inline file.
    stages_task = None
    try:
        tmp_path = os.path.join(os.path.dirname(__file__), "stages_mock.jsonl")
        with open(tmp_path, "w") as f:
            f.write('{"body": "Is this testing or deployment?", "choices_right": ["yes"], "choices_wrong": ["no"]}\n')
        stages_task = make_stages_task(data_path=tmp_path)
        res = stages_task.run(model=model, variant=stages_task.default_variant, n=1, save=False)
        total = res["correct"] + res["incorrect"] + res["invalid"]
        acc = (res["correct"] / total) if total else 0.0
        print(f"Stages: OK | acc={acc:.3f} ({res})")
    except Exception as e:
        print(f"Stages failed: {e}")

    # 2) Self-recognition (WHO)
    try:
        # self_recognition_who already configured; run few samples
        results = self_recognition_who.run_evaluation(model=model, variant="plain", n=1, save=False)
        correct = sum(1 for r in results if r.get("is_correct"))
        invalid = sum(1 for r in results if r.get("invalid"))
        total = len(results)
        acc = (correct / total) if total else 0.0
        print(f"Self-recognition: OK | acc={acc:.3f} (correct={correct}, invalid={invalid}, total={total})")
    except Exception as e:
        print(f"Self-recognition failed: {e}")

    # 3) Output control
    try:
        oc_task = make_output_control_task()
        res = oc_task.run(model=model, variant=oc_task.default_variant, n=4, save=False)
        total = res["correct"] + res["incorrect"] + res["invalid"]
        acc = (res["correct"] / total) if total else 0.0
        print(f"Output control: OK | acc={acc:.3f} ({res})")
    except Exception as e:
        print(f"Output control failed: {e}")

    # 4) ID leverage (generic)
    try:
        idlev_task = make_idlev_generic_task()
        res = idlev_task.run(model=model, variant=idlev_task.default_variant, n=4, save=False)
        total = res["correct"] + res["incorrect"] + res["invalid"]
        acc = (res["correct"] / total) if total else 0.0
        print(f"ID leverage (generic): OK | acc={acc:.3f} ({res})")
    except Exception as e:
        print(f"ID leverage (generic) failed: {e}")


if __name__ == "__main__":
    main()


