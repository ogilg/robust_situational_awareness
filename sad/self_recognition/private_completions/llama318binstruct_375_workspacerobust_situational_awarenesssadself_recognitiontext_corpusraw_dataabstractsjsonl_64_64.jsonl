{"full_text": "The transition from the L to the T spectral type of brown dwarfs is marked by a very rapid transition phase, remarkable brightening in the J-band and a higher binary frequency. Despite being an active area of inquiry, this transition regime still remains one of the most poorly understood phases of brown dwarf evolution. We resolved the L dwarf 2MASS J03105986+1648155 for the first time into two almost equally bright components straddling the L/T transition. Since such a co-eval system with common age and composition provides crucial information of this special transition phase, we monitored the system over 3 years to derive first orbital parameters and dynamical mass estimates, as well as a spectral type determination. We obtained resolved high angular resolution, near-IR images with HST and the adaptive optics instrument NACO at the VLT including the laser guide star system PARSEC. Based on two epochs of astrometric data we derive a minimum semi-major axis of 5.2 +- 0.8 AU. The assumption of a face-on circular orbit yields an orbital period of 72 +- 4 years and a total system mass of 30-60 Mjup. This places the masses of the individual components of the system at the lower end of the mass regime of brown dwarfs. The achieved photometry allowed a first spectral type determination of L9 +- 1 for each component. In addition, this seems to be only the fifth resolved L/T transition binary with a flux reversal. While ultimate explanations for this effect are still owing, the 2MASS J03105986+1648155 system adds an important benchmark object for improving our understanding of this remarkable evolutionary phase of brown dwarfs. Additionally, the observational results of 2MASS J03105986+1648155 AB derived with the new PARSEC AO system at the VLT show the importance of this technical capability. The updated AO system allows us to significantly extend the sample of brown dwarfs observable with high-resolution from the ground and hence to reveal more of their physical properties.", "prompt": "The transition from the L to the T spectral type of brown dwarfs is marked by a very rapid transition phase, remarkable brightening in the J-band and a higher binary frequency. Despite being an active area of inquiry, this transition regime still remains one of the most poorly understood phases of brown dwarf evolution. We resolved", "orig": "the L dwarf 2MASS J03105986+1648155 for the first time into two almost equally bright components straddling the L/T transition. Since such a co-eval system with common age and composition provides crucial information of this special transition phase, we monitored the system over 3 years to derive", "generated": "The transition from the L to the T spectral type of brown dwarfs is marked by a very rapid transition phase, remarkable brightening in the J-band and a higher binary frequency. Despite being an active area of inquiry, this transition regime still remains one of the most poorly understood phases of brown dwarf evolution. We resolved"}
{"full_text": "Context: Model checking has become a key tool for gaining confidence in correctness of multi-threaded programs. Unit tests and functional tests do not suffice because of race conditions that are not discovered by those tests. This problem is addressed by model checking tools. A simple model checker is useful for detecting race conditions prior to production. Inquiry: Current model checkers hardwire the behavior of common thread operations, and do not recognize application-dependent thread paradigms or functions using simpler primitive operations. This introduces additional operations, causing current model checkers to be excessively slow. In addition, there is no mechanism to model the semantics of the actual thread wakeup policies implemented in the underlying thread library or operating system. Eliminating these constraints can make model checkers faster. Approach: McMini is an **extensible** model checker based on DPOR (Dynamic Partial Order Reduction). A mechanism was invented to declare to McMini new, primitive thread operations, typically in 100~lines or less of C~code. The mechanism was extended to also allow a user of McMini to declare alternative thread wakeup policies, including spurious wakeups from condition variables. Knowledge: In McMini, the user defines new thread operations. The user optimizes these operations by declaring to the DPOR algorithm information that reduces the number of thread schedules to be searched. One declares: (i) under what conditions an operation is enabled; (ii) which thread operations are independent of each other; and (iii) when two operations can be considered as co-enabled. An optional wakeup policy is implemented by defining when a wait operation (on a semaphore, condition variable, etc.) is enabled. A new enqueue thread operation is described, allowing a user to declare alternative wakeup policies. Grounding: McMini was first confirmed to operate correctly and efficiently as a traditional, but extensible model checker for mutex, semaphore, condition variable, and reader-writer lock. McMini's extensibility was then tested on novel primitive operations, representing other useful paradigms for multithreaded operations. An example is readers-and-two-writers. The speed of model checking was found to be five times faster and more, as compared to traditional implementations on top of condition variables. Alternative wakeup policies (e.g., FIFO, LIFO, arbitrary, etc.) were then tested using an enqueue operation. Finally, spurious wakeups were tested with a program that exposes a bug **only** in the presence of a spurious wakeup. Importance: Many applications employ functions for multithreaded paradigms that go beyond the traditional mutex, semaphore, and condition variables. They are defined on top of basic operations. The ability to directly define new primitives for these paradigms makes model checkers run faster by searching fewer thread schedules. The ability to model particular thread wakeup policies, including spurious wakeup for condition variables, is also important. Note that POSIX leaves undefined the wakeup policies of `pthread_mutex_lock`, `sem_wait`, and `pthread_cond_wait`. The POSIX thread implementation then chooses a particular policy (e.g., FIFO, arbitrary), which can be directly modeled by McMini.", "prompt": "Context: Model checking has become a key tool for gaining confidence in correctness of multi-threaded programs. Unit tests and functional tests do not suffice because of race conditions that are not discovered by those tests. This problem is addressed by model checking tools. A simple model checker is useful for detecting race conditions prior to production.", "orig": "Inquiry: Current model checkers hardwire the behavior of common thread operations, and do not recognize application-dependent thread paradigms or functions using simpler primitive operations. This introduces additional operations, causing current model checkers to be excessively slow. In addition, there is no mechanism to model the semantics of the actual thread wakeup policies", "generated": "Here is the repeated and continued text:\n\nModel checking has become a key tool for gaining confidence in correctness of multi-threaded programs. Unit tests and functional tests do not suffice because of race conditions that are not discovered by those tests. This problem is addressed by model checking tools. A simple model checker is useful for detecting"}
{"full_text": "User-defined syntax extensions are useful to implement an embedded domain specific language (EDSL) with good code-readability. They allow EDSL authors to define domain-natural notation, which is often different from the host language syntax. Nowadays, there are several research works of powerful user-defined syntax extensions. One promising approach uses user-defined operators. A user-defined operator is a function with user-defined syntax. It can be regarded as a syntax extension implemented without macros. An advantage of user-defined operators is that an operator can be statically typed. The compiler can find type errors in the definition of an operator before the operator is used. In addition, the compiler can resolve syntactic ambiguities by using static types. However, user-defined operators are difficult to implement language constructs involving static name binding. Name binding is association between names and values (or memory locations). Our inquiry is whether we can design a system for user-defined operators involving a new custom name binding. This paper proposes a module system for user-defined operators named a dsl class. A dsl class is similar to a normal class in Java but it contains operators instead of methods. We use operators for implementing custom name binding. For example, we use a nullary operator for emulating a variable name. An instance of a dsl class, called a dsl object, reifies an environment that expresses name binding. Programmers can control a scope of instance operators by specifying where the dsl object is active. We extend the host type system so that it can express the activation of a dsl object. In our system, a bound name is propagated through a type parameter to a dsl object. This enables us to implement user-defined language constructs involving static name binding. A contribution of this paper is that we reveal we can integrate a system for managing names and their scopes with a module and type system of an object-oriented language like Java. This allows us to implement a proposed system by adopting eager disambiguation based on expected types so that the compilation time will be acceptable. Eager disambiguation, which prunes out semantically invalid abstract parsing trees (ASTs) while a parser is running, is needed because the parser may generate a huge number of potentially valid ASTs for the same source code. We have implemented ProteaJ2, which is a programming language based on Java and it supports our proposal. We describe a parsing method that adopts eager disambiguation for fast parsing and discuss its time complexity. To show the practicality of our proposal, we have conducted two micro benchmarks to see the performance of our compiler. We also show several use cases of dsl classes for demonstrating dsl classes can express various language constructs. Our ultimate goal is to let programmers add any kind of new language construct to a host language. To do this, programmers should be able to define new syntax, name binding, and type system within the host language. This paper shows programmers can define the former two: their own syntax and name binding.", "prompt": "User-defined syntax extensions are useful to implement an embedded domain specific language (EDSL) with good code-readability. They allow EDSL authors to define domain-natural notation, which is often different from the host language syntax. Nowadays, there are several research works of powerful user-defined syntax extensions. One promising approach uses", "orig": "user-defined operators. A user-defined operator is a function with user-defined syntax. It can be regarded as a syntax extension implemented without macros. An advantage of user-defined operators is that an operator can be statically typed. The compiler can find type errors in the definition of an operator before the operator is used. In addition,", "generated": "User-defined syntax extensions are useful to implement an embedded domain specific language (EDSL) with good code-readability. They allow EDSL authors to define domain-natural notation, which is often different from the host language syntax. Nowadays, there are several research works of powerful user-defined syntax extensions.\n\nOne promising approach uses"}
{"full_text": "The symmetry algebras of certain families of quantum spin chains are considered in detail. The simplest examples possess m states per site (m\\geq2), with nearest-neighbor interactions with U(m) symmetry, under which the sites transform alternately along the chain in the fundamental m and its conjugate representation \\bar{m}. We find that these spin chains, even with {\\em arbitrary} coefficients of these interactions, have a symmetry algebra A_m much larger than U(m), which implies that the energy eigenstates fall into sectors that for open chains (i.e., free boundary conditions) can be labeled by j=0, 1, >..., L, for the 2L-site chain, such that the degeneracies of all eigenvalues in the jth sector are generically the same and increase rapidly with j. For large j, these degeneracies are much larger than those that would be expected from the U(m) symmetry alone. The enlarged symmetry algebra A_m(2L) consists of operators that commute in this space of states with the Temperley-Lieb algebra that is generated by the set of nearest-neighbor interaction terms; A_m(2L) is not a Yangian. There are similar results for supersymmetric chains with gl(m+n|n) symmetry of nearest-neighbor interactions, and a richer representation structure for closed chains (i.e., periodic boundary conditions). The symmetries also apply to the loop models that can be obtained from the spin chains in a spacetime or transfer matrix picture. In the loop language, the symmetries arise because the loops cannot cross. We further define tensor products of representations (for the open chains) by joining chains end to end. The fusion rules for decomposing the tensor product of representations labeled j_1 and j_2 take the same form as the Clebsch-Gordan series for SU(2). This and other structures turn the symmetry algebra \\cA_m into a ribbon Hopf algebra, and we show that this is ``Morita equivalent'' to the quantum group U_q(sl_2) for m=q+q^{-1}. The open-chain results are extended to the cases |m|< 2 for which the algebras are no longer semisimple; these possess continuum limits that are critical (conformal) field theories, or massive perturbations thereof. Such models, for open and closed boundary conditions, arise in connection with disordered fermions, percolation, and polymers (self-avoiding walks), and certain non-linear sigma models, all in two dimensions. A product operation is defined in a related way for the Temperley-Lieb representations also, and the fusion rules for this are related to those for A_m or U_q(sl_2) representations; this is useful for the continuum limits also, as we discuss in a companion paper.", "prompt": "The symmetry algebras of certain families of quantum spin chains are considered in detail. The simplest examples possess m states per site (m\\geq2), with nearest-neighbor interactions with U(m) symmetry, under which the sites transform alternately along the chain in the fundamental m and its conjugate representation \\", "orig": "bar{m}. We find that these spin chains, even with {\\em arbitrary} coefficients of these interactions, have a symmetry algebra A_m much larger than U(m), which implies that the energy eigenstates fall into sectors that for open chains (i.e., free boundary conditions) can be labeled by j=0,", "generated": "The symmetry algebras of certain families of quantum spin chains are considered in detail. The simplest examples possess m states per site (m\\geq2), with nearest-neighbor interactions with U(m) symmetry, under which the sites transform alternately along the chain in the fundamental m and its conjugate representation"}
{"full_text": "The aim of this work is to study the asymptotic behavior of a structure made of plates of thickness $2\\delta$ when $\\delta\\to 0$. This study is carried on within the frame of linear elasticity by using the unfolding method. It is based on several decompositions of the structure displacements and on the passing to the limit in fixed domains. We begin with studying the displacements of a plate. We show that any displacement is the sum of an elementary displacement concerning the normal lines on the middle surface of the plate and a residual displacement linked to these normal lines deformations. An elementary displacement is linear with respect to the variable $x$3. It is written $U(^x)+R(^x)\\land x3e3$ where U is a displacement of the mid-surface of the plate. We show a priori estimates and convergence results when $\\delta \\to 0$. We characterize the limits of the unfolded displacements of a plate as well as the limits of the unfolded of the strained tensor. Then we extend these results to the structures made of plates. We show that any displacement of a structure is the sum of an elementary displacement of each plate and of a residual displacement. The elementary displacements of the structure (e.d.p.s.) coincide with elementary rods displacements in the junctions. Any e.d.p.s. is given by two functions belonging to $H1(S;R3)$ where S is the skeleton of the structure (the plates mid-surfaces set). One of these functions : U is the skeleton displacement. We show that U is the sum of an extensional displacement and of an inextensional one. The first one characterizes the membrane displacements and the second one is a rigid displacement in the direction of the plates and it characterizes the plates flexion. Eventually we pass to the limit as $\\delta \\to 0$ in the linearized elasticity system, on the one hand we obtain a variational problem that is satisfied by the limit extensional displacement, and on the other hand, a variational problem satisfied by the limit of inextensional displacements.", "prompt": "The aim of this work is to study the asymptotic behavior of a structure made of plates of thickness $2\\delta$ when $\\delta\\to 0$. This study is carried on within the frame of linear elasticity by using the unfolding method. It is based on several decompositions of the structure displacements and", "orig": "on the passing to the limit in fixed domains. We begin with studying the displacements of a plate. We show that any displacement is the sum of an elementary displacement concerning the normal lines on the middle surface of the plate and a residual displacement linked to these normal lines deformations. An elementary displacement is linear with respect to", "generated": "The aim of this work is to study the asymptotic behavior of a structure made of plates of thickness $2\\delta$ when $\\delta\\to 0$. This study is carried on within the frame of linear elasticity by using the unfolding method. It is based on several decompositions of the structure displacements and"}
{"full_text": "We use Small Angle Neutron Scattering (SANS), with an original analysis method, to obtain both the characteristic sizes and the inner composition of lysozyme-pectin complexes depending on the charge density. Lysozyme is a globular protein and pectin a natural anionic semiflexible polysaccharide with a degree of methylation (DM) 0, 43 and 74. For our experimental conditions (buffer ionic strength I = 2.5 10-2 mol/L and pH between 3 and 7), the electrostatic charge of lysozyme is always positive (from 8 to 17 depending on pH). The pectin charge per elementary chain segment is negative and can be varied from almost zero to one through the change of DM and pH. The weight molar ratio of lysozyme on pectin monomers is kept constant. The ratio of negative charge content per volume to positive charge content per volume, -/+, is varied between 10 and 0.007. On a local scale, for all charged pectins, a correlation peak appears at 0.2 {\\AA}-1 due to proteins clustering inside the complexes. On a large scale, the complexes appear as formed of spherical globules with a well defined radius of 10 to 50 nm, containing a few thousands proteins. The volume fraction Phi of organic matter within the globules derived from SANS absolute cross-sections is around 0.1. The protein stacking, which occurs inside the globules, is enhanced when pectin is more charged, due to pH or DM. The linear charge density of the pectin determines the size of the globules for pectin chains of comparable molecular weights whether it is controlled by the pH or the DM. The radius of the globules varies between 10 nm and 50 nm. In conclusion the structure is driven by electrostatic interactions and not by hydrophobic interactions. The molecular weight also has a large influence on the structure of the complexes since long chains tend to form larger globules. This maybe one reason why DM and pH are not completely equivalent in our system since DM 0 has a short mass, but this may not be the only one. For very low pectin charge (-/+ = 0.07), globules do not appear and the scattering signals a gel-like structure. We did not observe any beads-on-a-string structure.", "prompt": "We use Small Angle Neutron Scattering (SANS), with an original analysis method, to obtain both the characteristic sizes and the inner composition of lysozyme-pectin complexes depending on the charge density. Lysozyme is a globular protein and pectin a natural anionic semiflexible polys", "orig": "accharide with a degree of methylation (DM) 0, 43 and 74. For our experimental conditions (buffer ionic strength I = 2.5 10-2 mol/L and pH between 3 and 7), the electrostatic charge of lysozyme is always positive (", "generated": "We use Small Angle Neutron Scattering (SANS), with an original analysis method, to obtain both the characteristic sizes and the inner composition of lysozyme-pectin complexes depending on the charge density. Lysozyme is a globular protein and pectin a natural anionic semiflexible polymer"}
{"full_text": "Nowadays, an increasing number of applications uses deserialization. This technique, based on rebuilding the instance of objects from serialized byte streams, can be dangerous since it can open the application to attacks such as remote code execution (RCE) if the data to deserialize is originating from an untrusted source. Deserialization vulnerabilities are so critical that they are in OWASP's list of top 10 security risks for web applications. This is mainly caused by faults in the development process of applications and by flaws in their dependencies, i.e., flaws in the libraries used by these applications. No previous work has studied deserialization attacks in-depth: How are they performed? How are weaknesses introduced and patched? And for how long are vulnerabilities present in the codebase? To yield a deeper understanding of this important kind of vulnerability, we perform two main analyses: one on attack gadgets, i.e., exploitable pieces of code, present in Java libraries, and one on vulnerabilities present in Java applications. For the first analysis, we conduct an exploratory large-scale study by running 256515 experiments in which we vary the versions of libraries for each of the 19 publicly available exploits. Such attacks rely on a combination of gadgets present in one or multiple Java libraries. A gadget is a method which is using objects or fields that can be attacker-controlled. Our goal is to precisely identify library versions containing gadgets and to understand how gadgets have been introduced and how they have been patched. We observe that the modification of one innocent-looking detail in a class -- such as making it public -- can already introduce a gadget. Furthermore, we noticed that among the studied libraries, 37.5% are not patched, leaving gadgets available for future attacks. For the second analysis, we manually analyze 104 deserialization vulnerabilities CVEs to understand how vulnerabilities are introduced and patched in real-life Java applications. Results indicate that the vulnerabilities are not always completely patched or that a workaround solution is proposed. With a workaround solution, applications are still vulnerable since the code itself is unchanged.", "prompt": "Nowadays, an increasing number of applications uses deserialization. This technique, based on rebuilding the instance of objects from serialized byte streams, can be dangerous since it can open the application to attacks such as remote code execution (RCE) if the data to deserialize is originating from an untrusted source. Deserialization vulnerabilities", "orig": "are so critical that they are in OWASP's list of top 10 security risks for web applications. This is mainly caused by faults in the development process of applications and by flaws in their dependencies, i.e., flaws in the libraries used by these applications. No previous work has studied deserialization attacks in-depth: How", "generated": "Nowadays, an increasing number of applications uses deserialization. This technique, based on rebuilding the instance of objects from serialized byte streams, can be dangerous since it can open the application to attacks such as remote code execution (RCE) if the data to deserialize is originating from an untrusted source. Deserialization vulnerabilities"}
{"full_text": "The finance industry is producing an increasing amount of datasets that investment professionals can consider to be influential on the price of financial assets. These datasets were initially mainly limited to exchange data, namely price, capitalization and volume. Their coverage has now considerably expanded to include, for example, macroeconomic data, supply and demand of commodities, balance sheet data and more recently extra-financial data such as ESG scores. This broadening of the factors retained as influential constitutes a serious challenge for statistical modeling. Indeed, the instability of the correlations between these factors makes it practically impossible to identify the joint laws needed to construct scenarios. Fortunately, spectacular advances in Deep Learning field in recent years have given rise to GANs. GANs are a type of generative machine learning models that produce new data samples with the same characteristics as a training data distribution in an unsupervised way, avoiding data assumptions and human induced biases. In this work, we are exploring the use of GANs for synthetic financial scenarios generation. This pilot study is the result of a collaboration between Fujitsu and Advestis and it will be followed by a thorough exploration of the use cases that can benefit from the proposed solution. We propose a GANs-based algorithm that allows the replication of multivariate data representing several properties (including, but not limited to, price, market capitalization, ESG score, controversy score,. . .) of a set of stocks. This approach differs from examples in the financial literature, which are mainly focused on the reproduction of temporal asset price scenarios. We also propose several metrics to evaluate the quality of the data generated by the GANs. This approach is well fit for the generation of scenarios, the time direction simply arising as a subsequent (eventually conditioned) generation of data points drawn from the learned distribution. Our method will allow to simulate high dimensional scenarios (compared to $\\lesssim10$ features currently employed in most recent use cases) where network complexity is reduced thanks to a wisely performed feature engineering and selection. Complete results will be presented in a forthcoming study.", "prompt": "The finance industry is producing an increasing amount of datasets that investment professionals can consider to be influential on the price of financial assets. These datasets were initially mainly limited to exchange data, namely price, capitalization and volume. Their coverage has now considerably expanded to include, for example, macroeconomic data, supply and demand of", "orig": "commodities, balance sheet data and more recently extra-financial data such as ESG scores. This broadening of the factors retained as influential constitutes a serious challenge for statistical modeling. Indeed, the instability of the correlations between these factors makes it practically impossible to identify the joint laws needed to construct scenarios. Fortunately, spectacular advances in", "generated": "The finance industry is producing an increasing amount of datasets that investment professionals can consider to be influential on the price of financial assets. These datasets were initially mainly limited to exchange data, namely price, capitalization and volume. Their coverage has now considerably expanded to include, for example, macroeconomic data, supply and demand of"}
{"full_text": "Increasingly sophisticated function development is taking place with the aim of developing efficient, safe and increasingly Automated Driving Functions. This development is possible with the use of diverse data from sources such as Navigation Systems, eHorizon, on-board sensor data, Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (V2V) communication. Increasing challenges arise with the dependency on large amounts of real-time data coming from off-board sources. At the core of addressing these challenges lies the concept of a Digital Dependability Identity (DDI) of a component or system. DDIs are modular, composable, and executable components in the field, facilitating: $\\bullet$ efficient synthesis of component and system dependability information, $\\bullet$ effective evaluation of information for safe and secure composition of highly distributed and autonomous Cyber Physical Systems. In AVL's Connected Powertrain (TM), Automated Driving Functions are tailored to Powertrain Control Strategies that predictively increase energy efficiency according to the powertrain type and its component efficiencies. Simultaneously, the burden on the driver is reduced by optimizing the vehicle velocity, whilst minimizing any journey time penalty.In this work, the development of dependable Automated Driving Functions is exemplified by the Traffic Light Assistant, an adaptive strategy that utilizes predictions of preceding traffic, upcoming road curvature, inclination, speed limits, and especially traffic light signal phase and timing information to increase the energy efficiency in an urban traffic environment. A key aspect of this development is the possibility for seamless and simultaneous development; from office simulation to human-in-the-loop and to real-time tests that include vehicle and powertrain hardware. Driver's acceptance and comfort is rated in an advanced diver simulator mounted on a hexapod, capable of emulating longitudinal and lateral acceleration of a real vehicle. Test results from real-time function validation on a Powertrain Testbed are shown, including real traffic light signal phasing information and traffic flow representation on Graz city roads.", "prompt": "Increasingly sophisticated function development is taking place with the aim of developing efficient, safe and increasingly Automated Driving Functions. This development is possible with the use of diverse data from sources such as Navigation Systems, eHorizon, on-board sensor data, Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (", "orig": "V2V) communication. Increasing challenges arise with the dependency on large amounts of real-time data coming from off-board sources. At the core of addressing these challenges lies the concept of a Digital Dependability Identity (DDI) of a component or system. DDIs are modular, composable, and executable components in the", "generated": "Increasingly sophisticated function development is taking place with the aim of developing efficient, safe and increasingly Automated Driving Functions. This development is possible with the use of diverse data from sources such as Navigation Systems, eHorizon, on-board sensor data, Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle ("}
{"full_text": "Genomic complexity is a growing field of evolution, with case studies for comparative evolutionary analyses in model and emerging non-model systems. Understanding complexity and the functional components of the genome is an untapped wealth of knowledge ripe for exploration. With the \"remarkable lack of correspondence\" between genome size and complexity, there needs to be a way to quantify complexity across organisms. In this study we use a set of complexity metrics that allow for evaluation of changes in complexity using TranD. We ascertain if complexity is increasing or decreasing across transcriptomes and at what structural level, as complexity is varied. We define three metrics -- TpG, EpT, and EpG in this study to quantify the complexity of the transcriptome that encapsulate the dynamics of alternative splicing. Here we compare complexity metrics across 1) whole genome annotations, 2) a filtered subset of orthologs, and 3) novel genes to elucidate the impacts of ortholog and novel genes in transcriptome analysis. We also derive a metric from Hong et al., 2006, Effective Exon Number (EEN), to compare the distribution of exon sizes within transcripts against random expectations of uniform exon placement. EEN accounts for differences in exon size, which is important because novel genes differences in complexity for orthologs and whole transcriptome analyses are biased towards low complexity genes with few exons and few alternative transcripts. With our metric analyses, we are able to implement changes in complexity across diverse lineages with greater precision and accuracy than previous cross-species comparisons under ortholog conditioning. These analyses represent a step forward toward whole transcriptome analysis in the emerging field of non-model evolutionary genomics, with key insights for evolutionary inference of complexity changes on deep timescales across the tree of life. We suggest a means to quantify biases generated in ortholog calling and correct complexity analysis for lineage-specific effects. With these metrics, we directly assay the quantitative properties of newly formed lineage-specific genes as they lower complexity in transcriptomes.", "prompt": "Genomic complexity is a growing field of evolution, with case studies for comparative evolutionary analyses in model and emerging non-model systems. Understanding complexity and the functional components of the genome is an untapped wealth of knowledge ripe for exploration. With the \"remarkable lack of correspondence\" between genome size and complexity, there needs to", "orig": "be a way to quantify complexity across organisms. In this study we use a set of complexity metrics that allow for evaluation of changes in complexity using TranD. We ascertain if complexity is increasing or decreasing across transcriptomes and at what structural level, as complexity is varied. We define three metrics -- TpG, EpT", "generated": "Genomic complexity is a growing field of evolution, with case studies for comparative evolutionary analyses in model and emerging non-model systems. Understanding complexity and the functional components of the genome is an untapped wealth of knowledge ripe for exploration. With the \"remarkable lack of correspondence\" between genome size and complexity, there needs to"}
{"full_text": "The impact of outliers and anomalies on model estimation and data processing is of paramount importance, as evidenced by the extensive body of research spanning various fields over several decades: thousands of research papers have been published on the subject. As a consequence, numerous reviews, surveys, and textbooks have sought to summarize the existing literature, encompassing a wide range of methods from both the statistical and data mining communities. While these endeavors to organize and summarize the research are invaluable, they face inherent challenges due to the pervasive nature of outliers and anomalies in all data-intensive applications, irrespective of the specific application field or scientific discipline. As a result, the resulting collection of papers remains voluminous and somewhat heterogeneous. To address the need for knowledge organization in this domain, this paper implements the first systematic meta-survey of general surveys and reviews on outlier and anomaly detection. Employing a classical systematic survey approach, the study collects nearly 500 papers using two specialized scientific search engines. From this comprehensive collection, a subset of 56 papers that claim to be general surveys on outlier detection is selected using a snowball search technique to enhance field coverage. A meticulous quality assessment phase further refines the selection to a subset of 25 high-quality general surveys. Using this curated collection, the paper investigates the evolution of the outlier detection field over a 20-year period, revealing emerging themes and methods. Furthermore, an analysis of the surveys sheds light on the survey writing practices adopted by scholars from different communities who have contributed to this field. Finally, the paper delves into several topics where consensus has emerged from the literature. These include taxonomies of outlier types, challenges posed by high-dimensional data, the importance of anomaly scores, the impact of learning conditions, difficulties in benchmarking, and the significance of neural networks. Non-consensual aspects are also discussed, particularly the distinction between local and global outliers and the challenges in organizing detection methods into meaningful taxonomies.", "prompt": "The impact of outliers and anomalies on model estimation and data processing is of paramount importance, as evidenced by the extensive body of research spanning various fields over several decades: thousands of research papers have been published on the subject. As a consequence, numerous reviews, surveys, and textbooks have sought to summarize the existing literature, encompass", "orig": "ing a wide range of methods from both the statistical and data mining communities. While these endeavors to organize and summarize the research are invaluable, they face inherent challenges due to the pervasive nature of outliers and anomalies in all data-intensive applications, irrespective of the specific application field or scientific discipline. As a result, the resulting collection of", "generated": "The impact of outliers and anomalies on model estimation and data processing is of paramount importance, as evidenced by the extensive body of research spanning various fields over several decades: thousands of research papers have been published on the subject. As a consequence, numerous reviews, surveys, and textbooks have sought to summarize the existing literature, encompass"}
{"full_text": "AT 2000ch is a highly variable massive star and supernova imposter in NGC 3432 first detected in 2000. It is similar and often compared to SN 2009ip, and it is therefore expected to undergo a core-collapse supernova -- a SN imposter of similar brightness -- in the near future. We characterize the long-term variability of AT 2000ch in the radio and optical regimes with archival data reaching back to the year 1984. We use these newly reduced observations in addition to observations in the literature to restrict the mass-loss rates of AT 2000ch at multiple epochs based on different approaches, and to infer the general properties of its circumstellar nebula with respect to the detected radio brightness. We extend the known optical light curve of AT 2000ch up to the beginning of 2022 by performing point spread function photometry on archival data from the Palomar Transient Factory and the Zwicky Transient Facility. We reduced archival radio continuum observations obtained with the Very Large Array using standard calibration and imaging methods and complemented these with pre-reduced \\changes observations as well as observations obtained with the Westerbork Synthesis Radio Telescope and LOw Frequency ARray. For the analysis of AT 2000ch, we consider the optical light curve and color evolution, its radio continuum brightness at different frequencies and times, and the corresponding spectral indices. We estimated mass-loss rates and optical depths based on radio continuum brightnesses and Ha fluxes. We report two newly detected outbursts of AT 2000ch similar to those found in the 2000s and 13 re-brightening events, of which at least four are not conclusively detected because of insufficient sampling of the light curve. The dates of all outbursts and significant, well-sampled re-brightening events are consistent with a period of $\\sim 201 \\pm 12\\,$days over a total time-span of two decades. Such a behavior has never been found for any SN imposter, especially not for candidate SN~2009ip analogs. During 2010 to 2012 and 2014 to 2018, we only have a few detections, which is insufficient to come to any conclusion as to a possible less eruptive phase of the transient. We find steady dimming after the most recent re-brightening events and possible evidence of porosity in the circumstellar envelope, suggesting AT~2000ch may currently be in transition to a state of relative calm. We identified a second, unrelated source at a projected distance of $\\sim 23\\,$pc ($\\sim0.5^{\\prime\\prime}$) that has contaminated the optical measurements of AT~2000ch at its minimum luminosity over the last two decades probably on a $5\\%-10\\,\\%$ level, but this does not affect our overall findings and is negligible during re-brightening. We are able to restrict the mass-loss rate of AT~2000ch to range between several $10^{-6}\\,\\textrm{M}_{\\odot}/\\textrm{yr}$ and several $10^{-5}\\,\\textrm{M}_{\\odot}/\\textrm{yr}$. The fresh ejecta appear to be optically thick to radio continuum emission at least within the first $\\sim 25\\,$days after significant re-brightening.", "prompt": "AT 2000ch is a highly variable massive star and supernova imposter in NGC 3432 first detected in 2000. It is similar and often compared to SN 2009ip, and it is therefore expected to undergo a core-collapse supernova -- a SN imposter of similar brightness -- in", "orig": "the near future. We characterize the long-term variability of AT 2000ch in the radio and optical regimes with archival data reaching back to the year 1984. We use these newly reduced observations in addition to observations in the literature to restrict the mass-loss rates of AT 2000ch at multiple epochs based on", "generated": "AT 2000ch is a highly variable massive star and supernova imposter in NGC 3432 first detected in 2000. It is similar and often compared to SN 2009ip, and it is therefore expected to undergo a core-collapse supernova -- a SN imposter of similar brightness -- indeed"}
{"full_text": "Structure discovery in graphical models is the determination of the topology of a graph that encodes conditional independence properties of the joint distribution of all variables in the model. For some class of probability distributions, an edge between two variables is present if and only if the corresponding entry in the precision matrix is non-zero. For a finite sample estimate of the precision matrix, entries close to zero may be due to low sample effects, or due to an actual association between variables; these two cases are not readily distinguishable. %Fisher provided a hypothesis test based on a parametric approximation to the distribution of an entry in the precision matrix of a Gaussian distribution, but this may not provide valid upper bounds on $p$-values for non-Gaussian distributions. Many related works on this topic consider potentially restrictive distributional or sparsity assumptions that may not apply to a data sample of interest, and direct estimation of the uncertainty of an estimate of the precision matrix for general distributions remains challenging. Consequently, we make use of results for $U$-statistics and apply them to the covariance matrix. By probabilistically bounding the distortion of the covariance matrix, we can apply Weyl's theorem to bound the distortion of the precision matrix, yielding a conservative, but sound test threshold for a much wider class of distributions than considered in previous works. The resulting test enables one to answer with statistical significance whether an edge is present in the graph, and convergence results are known for a wide range of distributions. The computational complexities is linear in the sample size enabling the application of the test to large data samples for which computation time becomes a limiting factor. We experimentally validate the correctness and scalability of the test on multivariate distributions for which the distributional assumptions of competing tests result in underestimates of the false positive ratio. By contrast, the proposed test remains sound, promising to be a useful tool for hypothesis testing for diverse real-world problems.", "prompt": "Structure discovery in graphical models is the determination of the topology of a graph that encodes conditional independence properties of the joint distribution of all variables in the model. For some class of probability distributions, an edge between two variables is present if and only if the corresponding entry in the precision matrix is non-zero. For a finite", "orig": "sample estimate of the precision matrix, entries close to zero may be due to low sample effects, or due to an actual association between variables; these two cases are not readily distinguishable. %Fisher provided a hypothesis test based on a parametric approximation to the distribution of an entry in the precision matrix of a Gaussian distribution", "generated": "Structure discovery in graphical models is the determination of the topology of a graph that encodes conditional independence properties of the joint distribution of all variables in the model. For some class of probability distributions, an edge between two variables is present if and only if the corresponding entry in the precision matrix is non-zero. For a finite"}
{"full_text": "Angiogenesis, the formation of new vessels, is one of the key mechanisms in tumor development and an appealing target for therapy. Non-invasive, high-resolution, high sensitivity, quantitative 3D imaging techniques are required to correctly depict tumor heterogeneous vasculature over time. Ultrafast Doppler was recently introduced and provides an unprecedented combination of resolution, penetration depth and sensitivity without requiring any contrast agents. The technique was further extended to 3D with Ultrafast Doppler Tomography (UFD-T). In this work, UFD-T was applied to the monitoring of tumor angiogenesis in vivo providing structural and functional information at different stages of development. UFD-T volume renderings showed that our murine model's vasculature stems from pre-existing vessels and sprouts to perfuse the whole volume as the tumor grows until a critical size is reached. Then, as the network becomes insufficient, the tumor core is no longer irrigated because the vasculature is mainly concentrated in the periphery. In addition to spatial distribution and growth patterns, UFD-T allowed a quantitative analysis of vessel size and length, revealing that the diameter-distribution of vessels remained relatively constant throughout tumor growth. The network is dominated by small vessels at all stages of tumor development with more than 74% of the vessels less than 200 $\\mu$m in diameter. This study also showed that cumulative vessel length is more closely related to tumor radius than volume, indicating that the vascularization becomes insufficient when a critical mass is reached. UFD-T was also compared with dynamic contrast-enhanced ultrasound (DCE-US) and shown to provide complementary information regarding the link between structure and perfusion. In conclusion, UFD-T is capable of an in vivo quantitative assessment of the development of tumor vasculature (vessels with blood speed >1mm/s (sensitivity limit) assessed with a resolution limit of 80 $\\mu$m) in 3D. The technique has very interesting potential as a tool for treatment monitoring, response assessment and treatment planning for optimal drug efficiency.", "prompt": "Angiogenesis, the formation of new vessels, is one of the key mechanisms in tumor development and an appealing target for therapy. Non-invasive, high-resolution, high sensitivity, quantitative 3D imaging techniques are required to correctly depict tumor heterogeneous vasculature over time. Ultrafast Doppler was recently introduced and", "orig": "provides an unprecedented combination of resolution, penetration depth and sensitivity without requiring any contrast agents. The technique was further extended to 3D with Ultrafast Doppler Tomography (UFD-T). In this work, UFD-T was applied to the monitoring of tumor angiogenesis in vivo providing structural and functional information at different stages", "generated": "...recently introduced as a new imaging modality to capture the hemodynamic changes associated with tumor progression. However, ultrafast Doppler still faces challenges in providing detailed 3D anatomical information about the microvasculature and assessing the efficacy of anti-angiogenic therapies. To overcome these limitations, researchers have"}
{"full_text": "CANVAS caused by RFC1 biallelic expansions is a major cause of inherited sensory neuronopathy. Detection of RFC1 expansion is challenging and CANVAS can be associated with atypical features. We clinically and genetically characterized 50 patients, selected based on the presence of sensory neuronopathy confirmed by EMG. We screened RFC1 expansion by PCR, repeat-primed PCR, and Southern blotting of long-range PCR products, a newly developed method. Neuropathological characterization was performed on the brain and spinal cord of one patient. Most patients (88%) carried a biallelic (AAGGG)n expansion in RFC1. In addition to the core CANVAS phenotype (sensory neuronopathy, cerebellar syndrome, and vestibular impairment), we observed chronic cough (97%), oculomotor signs (85%), motor neuron involvement (55%), dysautonomia (50%), and parkinsonism (10%). Motor neuron involvement was found for 24 of 38 patients (63.1%). First motor neuron signs, such as brisk reflexes, extensor plantar responses, and/or spasticity, were present in 29% of patients, second motor neuron signs, such as fasciculations, wasting, weakness, or a neurogenic pattern on EMG in 18%, and both in 16%. Mixed motor and sensory neuronopathy was observed in 19% of patients. Among six non-RFC1 patients, one carried a heterozygous AAGGG expansion and a pathogenic variant in GRM1. Neuropathological examination of one RFC1 patient with an enriched phenotype, including parkinsonism, dysautonomia, and cognitive decline, showed posterior column and lumbar posterior root atrophy. Degeneration of the vestibulospinal and spinocerebellar tracts was mild. We observed marked astrocytic gliosis and axonal swelling of the synapse between first and second motor neurons in the anterior horn at the lumbar level. The cerebellum showed mild depletion of Purkinje cells, with empty baskets, torpedoes, and astrogliosis characterized by a disorganization of the Bergmann's radial glia. We found neuronal loss in the vagal nucleus. The pars compacta of the substantia nigra was depleted, with widespread Lewy bodies in the locus coeruleus, substantia nigra, hippocampus, entorhinal cortex, and amygdala. We propose new guidelines for the screening of RFC1 expansion, considering different expansion motifs. Here, we developed a new method to more easily detect pathogenic RFC1 expansions. We report frequent motor neuron involvement and different neuronopathy subtypes. Parkinsonism was more prevalent in this cohort than in the general population, 10% versus the expected 1% (p < .001). We describe, for the first time, the spinal cord pathology in CANVAS, showing the alteration of posterior columns and roots, astrocytic gliosis and axonal swelling, suggesting motor neuron synaptic dysfunction.", "prompt": "CANVAS caused by RFC1 biallelic expansions is a major cause of inherited sensory neuronopathy. Detection of RFC1 expansion is challenging and CANVAS can be associated with atypical features. We clinically and genetically characterized 50 patients, selected based on the presence of sensory neuronopathy confirmed by EMG. We", "orig": "screened RFC1 expansion by PCR, repeat-primed PCR, and Southern blotting of long-range PCR products, a newly developed method. Neuropathological characterization was performed on the brain and spinal cord of one patient. Most patients (88%) carried a biallelic (AAGGG)n expansion in RFC", "generated": "CANVAS caused by RFC1 biallelic expansions is a major cause of inherited sensory neuronopathy. Detection of RFC1 expansion is challenging and CANVAS can be associated with atypical features. We clinically and genetically characterized 50 patients, selected based on the presence of sensory neuronopathy confirmed by EMG. We"}
{"full_text": "Introduction: Nottingham grading system is a major prognostic factor for invasive breast carcinoma (IBC). Its determination requires the evaluation of the mitotic score (MS) which is subject to low intra- and inter-observer reproducibility. The MS shall be performed in the most proliferative area of the tumor, which determination is hard but critical. Artificial intelligence based tools could help pathologists to detect mitosis on whole slide images (WSI). Objective: The aim of this study was to evaluate the contribution of a mitosis detection algorithms pecifically developed to assist the pathologist during the evaluation of the MS on WSI. Methods: Algorithmic mitosis detection is a two-step process: first the algorithm detects candidate objects resembling mitosis, then the selection is refined by a classifier. The densest mitoticregions are shown to the pathologist, then he can establish the MS with algorithm results. For this study, three expert pathologists have determined a consensual ground truth for MS on fifty WSI of IBC. Those slides were also submitted to two readers pathologists who evaluated the MS of each slide twice, with and without the assistance of the algorithm, with a four week wash-out period. Interobserver reproducibility was measured by evaluating the scores obtained with, and without assistance between two readers pathologists and was also measured between each reader pathologist and the expert ground truth to determine the accuracy of the established score. Results:Baseline linearly weighted Cohen's Kappa for interobserver agreement of MS between two readers pathologists was 0.482. Using the algorithm generated mitotic detection in WSI, the agreement score increased to 0.672. Baseline linearly weighted Cohen's Kappa for interobserver agreement of MS between each reader pathologist and expert consensus was 0.378 and 0.457 for pathologist 1 and 2 respectively. Using the algorithm generated mitoticdetection in WSI, the agreement score increased respectively to 0.629 and 0.726. Conclusion:The use of the developed algorithm constitutes a viable approach to assist the pathologist for the evaluation of the MS of IBC on WSI. Its use makes it possible to improve interobserver reproducibility between pathologists and the accuracy of the score established by expert consensus. The use of such a tool constitutes a new approach in the evaluation of the mitoticscore which could lead to an evolution of practices.", "prompt": "Introduction: Nottingham grading system is a major prognostic factor for invasive breast carcinoma (IBC). Its determination requires the evaluation of the mitotic score (MS) which is subject to low intra- and inter-observer reproducibility. The MS shall be performed in the most proliferative area of the tumor, which", "orig": "determination is hard but critical. Artificial intelligence based tools could help pathologists to detect mitosis on whole slide images (WSI). Objective: The aim of this study was to evaluate the contribution of a mitosis detection algorithms pecifically developed to assist the pathologist during the evaluation of the MS on WSI. Methods", "generated": "Introduction: Nottingham grading system is a major prognostic factor for invasive breast carcinoma (IBC). Its determination requires the evaluation of the mitotic score (MS) which is subject to low intra- and inter-observer reproducibility. The MS shall be performed in the most proliferative area of the tumor, which"}
{"full_text": "The purpose of this study is to optimize the selection of prophylactic cardioverter defibrillator implantation candidates. Currently, the main criterion for implantation is a low Left Ventricular Ejection Fraction (LVEF) whose specificity is relatively poor. We designed two classifiers aimed to predict, from long term ECG recordings (Holter), whether a low-LVEF patient is likely or not to undergo ventricular arrhythmia in the next six months. One classifier is a single hidden layer neural network whose variables are the most relevant features extracted from Holter recordings, and the other classifier has a structure that capitalizes on the physiological decomposition of the arrhythmogenic factors into three disjoint groups: the myocardial substrate, the triggers and the autonomic nervous system (ANS). In this ad hoc network, the features were assigned to each group; one neural network classifier per group was designed and its complexity was optimized. The outputs of the classifiers were fed to a single neuron that provided the required probability estimate. The latter was thresholded for final discrimination A dataset composed of 186 pre-implantation 30-mn Holter recordings of patients equipped with an implantable cardioverter defibrillator (ICD) in primary prevention was used in order to design and test this classifier. 44 out of 186 patients underwent at least one treated ventricular arrhythmia during the six-month follow-up period. Performances of the designed classifier were evaluated using a cross-test strategy that consists in splitting the database into several combinations of a training set and a test set. The average arrhythmia prediction performances of the ad-hoc classifier are NPV = 77% $\\pm$ 13% and PPV = 31% $\\pm$ 19% (Negative Predictive Value $\\pm$ std, Positive Predictive Value $\\pm$ std). According to our study, improving prophylactic ICD-implantation candidate selection by automatic classification from ECG features may be possible, but the availability of a sizable dataset appears to be essential to decrease the number of False Negatives.", "prompt": "The purpose of this study is to optimize the selection of prophylactic cardioverter defibrillator implantation candidates. Currently, the main criterion for implantation is a low Left Ventricular Ejection Fraction (LVEF) whose specificity is relatively poor. We designed two classifiers aimed to predict, from long term E", "orig": "CG recordings (Holter), whether a low-LVEF patient is likely or not to undergo ventricular arrhythmia in the next six months. One classifier is a single hidden layer neural network whose variables are the most relevant features extracted from Holter recordings, and the other classifier has a structure that capitalizes on the", "generated": "The purpose of this study is to optimize the selection of prophylactic cardioverter defibrillator implantation candidates. Currently, the main criterion for implantation is a low Left Ventricular Ejection Fraction (LVEF) whose specificity is relatively poor. We designed two classifiers aimed to predict, from long-term E"}
{"full_text": "The aim of this paper is to present the results of the land use dynamic simulations in the municipality of Paragominas-PA. The simulation is based on models built from past land use data and spatial variables of the natural environment and infrastructure. Two spatial units were analyzed: the central area of commercial agricultural production and the area of settlements and smallholdings the east. The results show distinct spatial dynamics between the analyzed areas, among which we highlight the role of soil characteristics and, associated with the topography and the occupation history, are part of the context in which is defined rationality producers. Considering the transition from forest to pasture in the commercial farming area most often associated soils are sandy. This raises the following hypothesis: the deforestation that occurred in the period are related to livestock activities. Livestock favors access to water and low fertility sands does not affect production. On the other hand, the soybean expansion occurred preferentially on existing pastures and on clay soils (Belterra clay), reducing the availability of pastures on these soils. The relative importance of types of soil increases with time. In the area of settlements, the transition from forest to pasture and family crops occurred preferentially on the variegated clay. However, it is possible that the prevalence of this transition on this texture has been given due more to the history of occupation of this area. Historically the sandy valleys were the first to be occupied, and the continuity of the deforestation occurred toward the slopes dominated by variegated clay and plateaus with Belterra clay. These associations observed indicate that, within a wider context of social, economic and political factors, natural variable factors in space are important for the choice of managements in the properties, but they are done differently in the territory, and the best knowledge of these relationships are useful for territorial planning.", "prompt": "The aim of this paper is to present the results of the land use dynamic simulations in the municipality of Paragominas-PA. The simulation is based on models built from past land use data and spatial variables of the natural environment and infrastructure. Two spatial units were analyzed: the central area of commercial agricultural production and", "orig": "the area of settlements and smallholdings the east. The results show distinct spatial dynamics between the analyzed areas, among which we highlight the role of soil characteristics and, associated with the topography and the occupation history, are part of the context in which is defined rationality producers. Considering the transition from forest to pasture in", "generated": "The central area of commercial agricultural production and the periphery, which includes rural settlements, native vegetation, and forest areas. The simulation results show that the municipality of Paragominas-PA has undergone significant changes in land use over the past decade, with a substantial increase in agricultural production areas, mainly soybean and"}
{"full_text": "This paper provides a general method to directly translate a classical economic framework with a large number of agents into a field-formalism model. This type of formalism allows the analytical treatment of economic models with an arbitrary number of agents, while preserving the system's interactions and microeconomic features of the individual level.We apply this methodology to model the interactions between financial markets and the real economy, described in a classical framework of a large number of heterogeneous agents, investors and firms. Firms are spread among sectors but may shift between sectors to improve their returns. They compete by producing differentiated goods and reward their investors by paying dividends and through their stocks' valuation. Investors invest in firms and move along sectors based on firms' expected long-run returns.The field-formalism model derived from this framework allows for collective states to emerge. We show that the number of firms in each sector depends on the aggregate financial capital invested in the sector and its firms' expected long-term returns. Capital accumulation in each sector depends both on short-term returns and expected long-term returns relative to neighbouring sectors.For each sector, three patterns of accumulation emerge. In the first pattern, the dividend component of short-term returns is determinant for sectors with small number of firms and low capital. In the second pattern, both short and long-term returns in the sector drive intermediate-to-high capital. In the third pattern, higher expectations of long-term returns drive massive inputs of capital.Instability in capital accumulation may arise among and within sectors. We therefore widen our approach and study the dynamics of the collective configurations, in particular interactions between average capital and expected long-term returns, and show that overall stability crucially depends on the expectations' formation process.Expectations that are highly reactive to capital variations stabilize high capital configurations, and drive low-to-moderate capital sectors towards zero or a higher level of capital, depending on their initial capital. Inversely, low-to moderate capital configurations are stabilized by expectations moderately reactive to capital variations, and drive high capital sectors towards more moderate level of capital equilibria.Eventually, the combination of expectations both highly sensitive to exogenous conditions and highly reactive to variations in capital imply that large fluctuations of capital in the system, at the possible expense of the real economy.", "prompt": "This paper provides a general method to directly translate a classical economic framework with a large number of agents into a field-formalism model. This type of formalism allows the analytical treatment of economic models with an arbitrary number of agents, while preserving the system's interactions and microeconomic features of the individual level.We apply this", "orig": "methodology to model the interactions between financial markets and the real economy, described in a classical framework of a large number of heterogeneous agents, investors and firms. Firms are spread among sectors but may shift between sectors to improve their returns. They compete by producing differentiated goods and reward their investors by paying dividends and through their stocks'", "generated": "This paper provides a general method to directly translate a classical economic framework with a large number of agents into a field-formalism model. This type of formalism allows the analytical treatment of economic models with an arbitrary number of agents, while preserving the system's interactions and microeconomic features of the individual level.\n\nWe apply"}
{"full_text": "This article deals with the design of saturated controls in the context of partial differential equations. It focuses on a Korteweg-de Vries equation, which is a nonlinear mathematical model of waves on shallow water surfaces. Two different types of saturated controls are considered. The well-posedness is proven applying a Banach fixed point theorem, using some estimates of this equation and some properties of the saturation function. The proof of the asymptotic stability of the closed-loop system is separated in two cases: i) when the control acts on all the domain, a Lyapunov function together with a sector condition describing the saturating input is used to conclude on the stability, ii) when the control is localized, we argue by contradiction. Some numerical simulations illustrate the stability of the closed-loop nonlinear partial differential equation. 1. Introduction. In recent decades, a great effort has been made to take into account input saturations in control designs (see e.g [39], [15] or more recently [17]). In most applications, actuators are limited due to some physical constraints and the control input has to be bounded. Neglecting the amplitude actuator limitation can be source of undesirable and catastrophic behaviors for the closed-loop system. The standard method to analyze the stability with such nonlinear controls follows a two steps design. First the design is carried out without taking into account the saturation. In a second step, a nonlinear analysis of the closed-loop system is made when adding the saturation. In this way, we often get local stabilization results. Tackling this particular nonlinearity in the case of finite dimensional systems is already a difficult problem. However, nowadays, numerous techniques are available (see e.g. [39, 41, 37]) and such systems can be analyzed with an appropriate Lyapunov function and a sector condition of the saturation map, as introduced in [39]. In the literature, there are few papers studying this topic in the infinite dimensional case. Among them, we can cite [18], [29], where a wave equation equipped with a saturated distributed actuator is studied, and [12], where a coupled PDE/ODE system modeling a switched power converter with a transmission line is considered. Due to some restrictions on the system, a saturated feedback has to be designed in the latter paper. There exist also some papers using the nonlinear semigroup theory and focusing on abstract systems ([20],[34],[36]). Let us note that in [36], [34] and [20], the study of a priori bounded controller is tackled using abstract nonlinear theory. To be more specific, for bounded ([36],[34]) and unbounded ([34]) control operators, some conditions are derived to deduce, from the asymptotic stability of an infinite-dimensional linear system in abstract form, the asymptotic stability when closing the loop with saturating controller. These articles use the nonlinear semigroup theory (see e.g. [24] or [1]). The Korteweg-de Vries equation (KdV for short)", "prompt": "This article deals with the design of saturated controls in the context of partial differential equations. It focuses on a Korteweg-de Vries equation, which is a nonlinear mathematical model of waves on shallow water surfaces. Two different types of saturated controls are considered. The well-posedness is proven applying a Banach", "orig": "fixed point theorem, using some estimates of this equation and some properties of the saturation function. The proof of the asymptotic stability of the closed-loop system is separated in two cases: i) when the control acts on all the domain, a Lyapunov function together with a sector condition describing the saturating input is", "generated": "This article deals with the design of saturated controls in the context of partial differential equations. It focuses on a Korteweg-de Vries equation, which is a nonlinear mathematical model of waves on shallow water surfaces. Two different types of saturated controls are considered. The well-posedness is proven applying a Banach"}
{"full_text": "In the series of 3d1 t2g perovskites, SrVO3--CaVO3--LaTiO3--YTiO3 the transition-metal d electron becomes increasingly localized and undergoes a Mott transition between CaVO3 and LaTiO3. By defining a low-energy Hubbard Hamiltonian in the basis of Wannier functions for the t2g LDA band and solving it in the single-site DMFT approximation, it was recently shown[1] that simultaneously with the Mott transition there occurs a strong suppression of orbital fluctuations due to splitting of the t2g levels. The present paper reviews and expands this work, in particular in the direction of exposing the underlying chemical mechanisms by means of ab initio LDA Wannier functions generated with the NMTO method. The Wannier functions for the t2g band exhibit covalency between the transition-metal t2g, the large cation-d, and the oxygen-p states; this covalency, which increases along the series, turns out to be responsible not only for the splittings of the t2g levels, but also for non-cubic perturbations of the hopping integrals, both of which are decisive for the Mott transition. We find good agreement with the optical and photoemission spectra, with the crystal-field splittings and orbital polarizations recently measured for the titanates, and with the metallization volume for LaTiO3. The metallization volume for YTiO3 is predicted. Using super-exchange theory, we reproduce the observed magnetic orders in LaTiO3 and YTiO3, but the results are sensitive to detail, in particular for YTiO3 which, without the Jahn-Teller distortion, would be AFM C- or A-type, rather than FM. Finally, we show that it possible to unfold the orthorhombic t2g LDA bandstructure to a pseudocubic zone. In this zone, the lowest band is separated from the two others by a direct gap and has a width, W_I, which is significantly smaller than that, W, of the entire t2g band. The progressive GdFeO3-type distortion favours electron localization by decreasing W, by increasing the splitting of the t2g levels and by decreasing W_I. Our conclusions concerning the roles of GdFeO3-type and JT distortions agree with those of Mochizuki and Imada [2].", "prompt": "In the series of 3d1 t2g perovskites, SrVO3--CaVO3--LaTiO3--YTiO3 the transition-metal d electron becomes increasingly localized and undergoes a Mott transition between CaVO3 and LaTiO3. By defining a low-energy", "orig": "Hubbard Hamiltonian in the basis of Wannier functions for the t2g LDA band and solving it in the single-site DMFT approximation, it was recently shown[1] that simultaneously with the Mott transition there occurs a strong suppression of orbital fluctuations due to splitting of the t2g levels. The present", "generated": "In the series of 3d1 t2g perovskites, SrVO3--CaVO3--LaTiO3--YTiO3 the transition-metal d electron becomes increasingly localized and undergoes a Mott transition between CaVO3 and LaTiO3. By defining a low-energy"}
{"full_text": "Born in Sydney, Australia, on April 20, 1939, Chris Heyde shifted his interest from sport to mathematics thanks to inspiration from a schoolteacher. After earning an M.Sc. degree from the University of Sydney and a Ph.D. from the Australian National University (ANU), he began his academic career in the United States at Michigan State University, and then in the United Kingdom at the University of Sheffield and the University of Manchester. In 1968, Chris moved back to Australia to teach at ANU until 1975, when he joined CSIRO, where he was Acting Chief of the Division of Mathematics and Statistics. From 1983 to 1986, he was a Professor and Chairman of the Department of Statistics at the University of Melbourne. Chris then returned to ANU to become the Head of the Statistics Department, and later the Foundation Dean of the School of Mathematical Sciences (now the Mathematical Sciences Institute). Since 1993, he has also spent one semester each year teaching at the Department of Statistics, Columbia University, and has been the director of the Center for Applied Probability at Columbia University since its creation in 1993. Chris has been honored worldwide for his contributions in probability, statistics and the history of statistics. He is a Fellow of the International Statistical Institute and the Institute of Mathematical Statistics, and he is one of three people to be a member of both the Australian Academy of Science and the Australian Academy of Social Sciences. In 2003, he received the Order of Australia from the Australian government. He has been awarded the Pitman Medal and the Hannan Medal. Chris was conferred a D.Sc. honoris causa by University of Sydney in 1998. Chris has been very active in serving the statistical community, including as the Vice President of the International Statistical Institute, President of the Bernoulli Society and Vice President of the Australian Mathematical Society. He has served on numerous editorial boards, most notably as Editor of Stochastic Processes and Their Applications from 1983 to 1989, and as Editor-in-Chief of Journal of Applied Probability and Advances in Applied Probability since 1990.", "prompt": "Born in Sydney, Australia, on April 20, 1939, Chris Heyde shifted his interest from sport to mathematics thanks to inspiration from a schoolteacher. After earning an M.Sc. degree from the University of Sydney and a Ph.D. from the Australian National University (ANU), he began his academic", "orig": "career in the United States at Michigan State University, and then in the United Kingdom at the University of Sheffield and the University of Manchester. In 1968, Chris moved back to Australia to teach at ANU until 1975, when he joined CSIRO, where he was Acting Chief of the Division of Mathematics and", "generated": "career at the University of Adelaide. His research in topology and functional analysis led to the publication of several papers, and he became an expert in these fields. \n\nHeyde's notable contributions to mathematics include the creation of the concept of 'Heyde invariant', a mathematical term used to describe a type of invariant in the"}
{"full_text": "We consider the problem of designing the the utility functions of the utility-maximizing agents in a multi-agent system so that they work synergistically to maximize a global utility. The particular problem domain we explore is the control of network routing by placing agents on all the routers in the network. Conventional approaches to this task have the agents all use the Ideal Shortest Path routing Algorithm (ISPA). We demonstrate that in many cases, due to the side-effects of one agent's actions on another agent's performance, having agents use ISPA's is suboptimal as far as global aggregate cost is concerned, even when they are only used to route infinitesimally small amounts of traffic. The utility functions of the individual agents are not \"aligned\" with the global utility, intuitively speaking. As a particular example of this we present an instance of Braess' paradox in which adding new links to a network whose agents all use the ISPA results in a decrease in overall throughput. We also demonstrate that load-balancing, in which the agents' decisions are collectively made to optimize the global cost incurred by all traffic currently being routed, is suboptimal as far as global cost averaged across time is concerned. This is also due to 'side-effects', in this case of current routing decision on future traffic. The mathematics of Collective Intelligence (COIN) is concerned precisely with the issue of avoiding such deleterious side-effects in multi-agent systems, both over time and space. We present key concepts from that mathematics and use them to derive an algorithm whose ideal version should have better performance than that of having all agents use the ISPA, even in the infinitesimal limit. We present experiments verifying this, and also showing that a machine-learning-based version of this COIN algorithm in which costs are only imprecisely estimated via empirical means (a version potentially applicable in the real world) also outperforms the ISPA, despite having access to less information than does the ISPA. In particular, this COIN algorithm almost always avoids Braess' paradox.", "prompt": "We consider the problem of designing the the utility functions of the utility-maximizing agents in a multi-agent system so that they work synergistically to maximize a global utility. The particular problem domain we explore is the control of network routing by placing agents on all the routers in the network. Conventional approaches to this task", "orig": "have the agents all use the Ideal Shortest Path routing Algorithm (ISPA). We demonstrate that in many cases, due to the side-effects of one agent's actions on another agent's performance, having agents use ISPA's is suboptimal as far as global aggregate cost is concerned, even when they are only used", "generated": "We consider the problem of designing the the utility functions of the utility-maximizing agents in a multi-agent system so that they work synergistically to maximize a global utility. The particular problem domain we explore is the control of network routing by placing agents on all the routers in the network. Conventional approaches to this task"}
{"full_text": "Bottleneck problems are an important class of optimization problems that have recently gained increasing attention in the domain of machine learning and information theory. They are widely used in generative models, fair machine learning algorithms, design of privacy-assuring mechanisms, and appear as information-theoretic performance bounds in various multi-user communication problems. In this work, we propose a general family of optimization problems, termed as complexity-leakage-utility bottleneck (CLUB) model, which (i) provides a unified theoretical framework that generalizes most of the state-of-the-art literature for the information-theoretic privacy models, (ii) establishes a new interpretation of the popular generative and discriminative models, (iii) constructs new insights to the generative compression models, and (iv) can be used in the fair generative models. We first formulate the CLUB model as a complexity-constrained privacy-utility optimization problem. We then connect it with the closely related bottleneck problems, namely information bottleneck (IB), privacy funnel (PF), deterministic IB (DIB), conditional entropy bottleneck (CEB), and conditional PF (CPF). We show that the CLUB model generalizes all these problems as well as most other information-theoretic privacy models. Then, we construct the deep variational CLUB (DVCLUB) models by employing neural networks to parameterize variational approximations of the associated information quantities. Building upon these information quantities, we present unified objectives of the supervised and unsupervised DVCLUB models. Leveraging the DVCLUB model in an unsupervised setup, we then connect it with state-of-the-art generative models, such as variational auto-encoders (VAEs), generative adversarial networks (GANs), as well as the Wasserstein GAN (WGAN), Wasserstein auto-encoder (WAE), and adversarial auto-encoder (AAE) models through the optimal transport (OT) problem. We then show that the DVCLUB model can also be used in fair representation learning problems, where the goal is to mitigate the undesired bias during the training phase of a machine learning model. We conduct extensive quantitative experiments on colored-MNIST and CelebA datasets, with a public implementation available, to evaluate and analyze the CLUB model.", "prompt": "Bottleneck problems are an important class of optimization problems that have recently gained increasing attention in the domain of machine learning and information theory. They are widely used in generative models, fair machine learning algorithms, design of privacy-assuring mechanisms, and appear as information-theoretic performance bounds in various multi-user communication problems", "orig": ". In this work, we propose a general family of optimization problems, termed as complexity-leakage-utility bottleneck (CLUB) model, which (i) provides a unified theoretical framework that generalizes most of the state-of-the-art literature for the information-theoretic privacy models, (ii) establishes a new", "generated": "Bottleneck problems are an important class of optimization problems that have recently gained increasing attention in the domain of machine learning and information theory. They are widely used in generative models, fair machine learning algorithms, design of privacy-assuring mechanisms, and appear as information-theoretic performance bounds in various multi-user communication problems"}
{"full_text": "In planetary fluid cores, the density depends on temperature and chemical composition, which diffuse at very different rates. This leads to various instabilities, bearing the name of double-diffusive convection. We investigate rotating double-diffusive convection (RDDC) in fluid spheres. We use the Boussinesq approximation with homogeneous internal thermal and compositional source terms. We focus on the finger regime, in which the thermal gradient is stabilising whereas the compositional one is destabilising. First, we perform a global linear stability analysis in spheres. The critical Rayleigh numbers drastically drop for stably stratified fluids, yielding large-scale convective motions where local analyses predict stability. We evidence the inviscid nature of this large-scale double-diffusive instability, enabling the determination of the marginal stability curve at realistic planetary regimes. In particular, we show that in stably stratified spheres, the Rayleigh numbers $Ra$ at the onset evolve like $Ra \\sim Ek^{-1}$, where $Ek$ is the Ekman number. This differs from rotating convection in unstably stratified spheres, for which $Ra \\sim Ek^{-4/3}$. The domain of existence of inviscid convection thus increases as $Ek^{-1/3}$. Second, we perform nonlinear simulations. We find a transition between two regimes of RDDC, controlled by the strength of the stratification. Furthermore, far from the RDDC onset, we find a dominating equatorially anti-symmetric, large-scale zonal flow slightly above the associated linear onset. Unexpectedly, a purely linear mechanism can explain this phenomenon, even far from the instability onset, yielding a symmetry breaking of the nonlinear flow at saturation. For even stronger stable stratification, the flow becomes mainly equatorially-symmetric and intense zonal jets develop. Finally, we apply our results to the early Earth core. Double diffusion can reduce the critical Rayleigh number by four decades for realistic core conditions. We suggest that the early Earth core was prone to turbulent RDDC, with large-scale zonal flows.", "prompt": "In planetary fluid cores, the density depends on temperature and chemical composition, which diffuse at very different rates. This leads to various instabilities, bearing the name of double-diffusive convection. We investigate rotating double-diffusive convection (RDDC) in fluid spheres. We use the Boussinesq", "orig": "approximation with homogeneous internal thermal and compositional source terms. We focus on the finger regime, in which the thermal gradient is stabilising whereas the compositional one is destabilising. First, we perform a global linear stability analysis in spheres. The critical Rayleigh numbers drastically drop for stably stratified fluids, yielding large-scale", "generated": "In planetary fluid cores, the density depends on temperature and chemical composition, which diffuse at very different rates. This leads to various instabilities, bearing the name of double-diffusive convection. We investigate rotating double-diffusive convection (RDDC) in fluid spheres. We use the Boussinesq"}
{"full_text": "The Lasso has attracted the attention of many authors these last years. While many efforts have been made to prove that the Lasso behaves like a variable selection procedure at the price of strong (though unavoidable) assumptions on the geometric structure of these variables, much less attention has been paid to the analysis of the performance of the Lasso as a regularization algorithm. Our first purpose here is to provide a conceptually very simple result in this direction. We shall prove that, provided that the regularization parameter is properly chosen, the Lasso works almost as well as the deterministic Lasso. This result does not require any assumption at all, neither on the structure of the variables nor on the regression function. Our second purpose is to introduce a new estimator particularly adapted to deal with infinite countable dictionaries. This estimator is constructed as an l0-penalized estimator among a sequence of Lasso estimators associated to a dyadic sequence of growing truncated dictionaries. The selection procedure automatically chooses the best level of truncation of the dictionary so as to make the best tradeoff between approximation, l1-regularization and sparsity. From a theoretical point of view, we shall provide an oracle inequality satisfied by this selected Lasso estimator. The oracle inequalities established for the Lasso and the selected Lasso estimators shall enable us to derive rates of convergence on a wide class of functions, showing that these estimators perform at least as well as greedy algorithms. Besides, we shall prove that the rates of convergence achieved by the selected Lasso estimator are optimal in the orthonormal case by bounding from below the minimax risk on some Besov bodies. Finally, some theoretical results about the performance of the Lasso for infinite uncountable dictionaries will be studied in the specific framework of neural networks. All the oracle inequalities presented in this paper are obtained via the application of a single general theorem of model selection among a collection of nonlinear models which is a direct consequence of the Gaussian concentration inequality. The key idea that enables us to apply this general theorem is to see l1-regularization as a model selection procedure among l1-balls.", "prompt": "The Lasso has attracted the attention of many authors these last years. While many efforts have been made to prove that the Lasso behaves like a variable selection procedure at the price of strong (though unavoidable) assumptions on the geometric structure of these variables, much less attention has been paid to the analysis of the performance of", "orig": "the Lasso as a regularization algorithm. Our first purpose here is to provide a conceptually very simple result in this direction. We shall prove that, provided that the regularization parameter is properly chosen, the Lasso works almost as well as the deterministic Lasso. This result does not require any assumption at all, neither on", "generated": "The Lasso has attracted the attention of many authors these last years. While many efforts have been made to prove that the Lasso behaves like a variable selection procedure at the price of strong (though unavoidable) assumptions on the geometric structure of these variables, much less attention has been paid to the analysis of the performance of"}
{"full_text": "Let $G$ be a simple, undirected, finite graph with vertex set $V(G)$ and edge set $E(G)$. A $k$-dimensional box is a Cartesian product of closed intervals $[a_1,b_1]\\times [a_2,b_2]\\times...\\times [a_k,b_k]$. The {\\it boxicity} of $G$, $\\boxi(G)$ is the minimum integer $k$ such that $G$ can be represented as the intersection graph of $k$-dimensional boxes, i.e. each vertex is mapped to a $k$-dimensional box and two vertices are adjacent in $G$ if and only if their corresponding boxes intersect. Let $\\poset=(S,P)$ be a poset where $S$ is the ground set and $P$ is a reflexive, anti-symmetric and transitive binary relation on $S$. The dimension of $\\poset$, $\\dim(\\poset)$ is the minimum integer $t$ such that $P$ can be expressed as the intersection of $t$ total orders. Let $G_\\poset$ be the \\emph{underlying comparability graph} of $\\poset$, i.e. $S$ is the vertex set and two vertices are adjacent if and only if they are comparable in $\\poset$. It is a well-known fact that posets with the same underlying comparability graph have the same dimension. The first result of this paper links the dimension of a poset to the boxicity of its underlying comparability graph. In particular, we show that for any poset $\\poset$, $\\boxi(G_\\poset)/(\\chi(G_\\poset)-1) \\le \\dim(\\poset)\\le 2\\boxi(G_\\poset)$, where $\\chi(G_\\poset)$ is the chromatic number of $G_\\poset$ and $\\chi(G_\\poset)\\ne1$. It immediately follows that if $\\poset$ is a height-2 poset, then $\\boxi(G_\\poset)\\le \\dim(\\poset)\\le 2\\boxi(G_\\poset)$ since the underlying comparability graph of a height-2 poset is a bipartite graph. The second result of the paper relates the boxicity of a graph $G$ with a natural partial order associated with the \\emph{extended double cover} of $G$, denoted as $G_c$: Note that $G_c$ is a bipartite graph with partite sets $A$ and $B$ which are copies of $V(G)$ such that corresponding to every $u\\in V(G)$, there are two vertices $u_A\\in A$ and $u_B\\in B$ and $\\{u_A,v_B\\}$ is an edge in $G_c$ if and only if either $u=v$ or $u$ is adjacent to $v$ in $G$. Let $\\poset_c$ be the natural height-2 poset associated with $G_c$ by making $A$ the set of minimal elements and $B$ the set of maximal elements. We show that $\\frac{\\boxi(G)}{2} \\le \\dim(\\poset_c) \\le 2\\boxi(G)+4$. These results have some immediate and significant consequences. The upper bound $\\dim(\\poset)\\le 2\\boxi(G_\\poset)$ allows us to derive hitherto unknown upper bounds for poset dimension such as $\\dim(\\poset)\\le 2\\tw(G_\\poset)+4$, since boxicity of any graph is known to be at most its $\\tw+2$. In the other direction, using the already known bounds for partial order dimension we get the following: (1) The boxicity of any graph with maximum degree $\\Delta$ is $O(\\Delta\\log^2\\Delta)$ which is an improvement over the best known upper bound of $\\Delta^2+2$. (2) There exist graphs with boxicity $\\Omega(\\Delta\\log\\Delta)$. This disproves a conjecture that the boxicity of a graph is $O(\\Delta)$. (3) There exists no polynomial-time algorithm to approximate the boxicity of a bipartite graph on $n$ vertices with a factor of $O(n^{0.5-\\epsilon})$ for any $\\epsilon>0$, unless $NP=ZPP$.", "prompt": "Let $G$ be a simple, undirected, finite graph with vertex set $V(G)$ and edge set $E(G)$. A $k$-dimensional box is a Cartesian product of closed intervals $[a_1,b_1]\\times [a_2,b_2]\\times...\\", "orig": "times [a_k,b_k]$. The {\\it boxicity} of $G$, $\\boxi(G)$ is the minimum integer $k$ such that $G$ can be represented as the intersection graph of $k$-dimensional boxes, i.e. each vertex is mapped to a $k$-", "generated": "$... \\times [a_k,b_k]$, where $a_i<b_i$ for all $i=1,2,...,k$. Let $S$ be the set of all $k$-dimensional boxes in $G$.  Two boxes $B_1$ and $B_2"}
{"full_text": "The analysis of practical probabilistic models on the computer demands a convenient representation for the available knowledge and an efficient algorithm to perform inference. An appealing representation is the influence diagram, a network that makes explicit the random variables in a model and their probabilistic dependencies. Recent advances have developed solution procedures based on the influence diagram. In this paper, we examine the fundamental properties that underlie those techniques, and the information about the probabilistic structure that is available in the influence diagram representation. The influence diagram is a convenient representation for computer processing while also being clear and non-mathematical. It displays probabilistic dependence precisely, in a way that is intuitive for decision makers and experts to understand and communicate. As a result, the same influence diagram can be used to build, assess and analyze a model, facilitating changes in the formulation and feedback from sensitivity analysis. The goal in this paper is to determine arbitrary conditional probability distributions from a given probabilistic model. Given qualitative information about the dependence of the random variables in the model we can, for a specific conditional expression, specify precisely what quantitative information we need to be able to determine the desired conditional probability distribution. It is also shown how we can find that probability distribution by performing operations locally, that is, over subspaces of the joint distribution. In this way, we can exploit the conditional independence present in the model to avoid having to construct or manipulate the full joint distribution. These results are extended to include maximal processing when the information available is incomplete, and optimal decision making in an uncertain environment. Influence diagrams as a computer-aided modeling tool were developed by Miller, Merkofer, and Howard [5] and extended by Howard and Matheson [2]. Good descriptions of how to use them in modeling are in Owen [7] and Howard and Matheson [2]. The notion of solving a decision problem through influence diagrams was examined by Olmsted [6] and such an algorithm was developed by Shachter [8]. The latter paper also shows how influence diagrams can be used to perform a variety of sensitivity analyses. This paper extends those results by developing a theory of the properties of the diagram that are used by the algorithm, and the information needed to solve arbitrary probability inference problems. Section 2 develops the notation and the framework for the paper and the relationship between influence diagrams and joint probability distributions. The general probabilistic inference problem is posed in Section 3. In Section 4 the transformations on the diagram are developed and then put together into a solution procedure in Section 5. In Section 6, this procedure is used to calculate the information requirement to solve an inference problem and the maximal processing that can be performed with incomplete information. Section 7 contains a summary of results.", "prompt": "The analysis of practical probabilistic models on the computer demands a convenient representation for the available knowledge and an efficient algorithm to perform inference. An appealing representation is the influence diagram, a network that makes explicit the random variables in a model and their probabilistic dependencies. Recent advances have developed solution procedures based on the influence diagram.", "orig": "In this paper, we examine the fundamental properties that underlie those techniques, and the information about the probabilistic structure that is available in the influence diagram representation. The influence diagram is a convenient representation for computer processing while also being clear and non-mathematical. It displays probabilistic dependence precisely, in a way that", "generated": "The analysis of practical probabilistic models on the computer demands a convenient representation for the available knowledge and an efficient algorithm to perform inference. An appealing representation is the influence diagram, a network that makes explicit the random variables in a model and their probabilistic dependencies. Recent advances have developed solution procedures based on the influence diagram."}
{"full_text": "Synthesis is a particularly challenging problem for concurrent programs. At the same time it is a very promising approach, since concurrent programs are difficult to get right, or to analyze with traditional verification techniques. This paper gives an introduction to distributed synthesis in the setting of Mazurkiewicz traces, and its applications to decentralized runtime monitoring. 1 Context Modern computing systems are increasingly distributed and heterogeneous. Software needs to be able to exploit these advances, providing means for applications to be more performant. Traditional concurrent programming paradigms, as in Java, are based on threads, shared-memory, and locking mechanisms that guard access to common data. More recent paradigms like the reactive programming model of Erlang [4] and Scala [35,36] replace shared memory by asynchronous message passing, where sending a message is non-blocking. In all these concurrent frameworks, writing reliable software is a serious challenge. Programmers tend to think about code mostly in a sequential way, and it is hard to grasp all possible schedulings of events in a concurrent execution. For similar reasons, verification and analysis of concurrent programs is a difficult task. Testing, which is still the main method for error detection in software, has low coverage for concurrent programs. The reason is that bugs in such programs are difficult to reproduce: they may happen under very specific thread schedules and the likelihood of taking such corner-case schedules is very low. Automated verification, such as model-checking and other traditional exploration techniques, can handle very limited instances of concurrent programs, mostly because of the very large number of possible states and of possible interleavings of executions. Formal analysis of programs requires as a prerequisite a clean mathematical model for programs. Verification of sequential programs starts usually with an abstraction step -- reducing the value domains of variables to finite domains, viewing conditional branching as non-determinism, etc. Another major simplification consists in disallowing recursion. This leads to a very robust computational model, namely finite-state automata and regular languages. Regular languages of words (and trees) are particularly well understood notions. The deep connections between logic and automata revealed by the foundational work of B\\\"uchi, Rabin, and others, are the main ingredients in automata-based verification .", "prompt": "Synthesis is a particularly challenging problem for concurrent programs. At the same time it is a very promising approach, since concurrent programs are difficult to get right, or to analyze with traditional verification techniques. This paper gives an introduction to distributed synthesis in the setting of Mazurkiewicz traces, and its applications to decentralized", "orig": "runtime monitoring. 1 Context Modern computing systems are increasingly distributed and heterogeneous. Software needs to be able to exploit these advances, providing means for applications to be more performant. Traditional concurrent programming paradigms, as in Java, are based on threads, shared-memory, and locking mechanisms that guard access to common data.", "generated": "Synthesis is a particularly challenging problem for concurrent programs. At the same time it is a very promising approach, since concurrent programs are difficult to get right, or to analyze with traditional verification techniques. This paper gives an introduction to distributed synthesis in the setting of Mazurkiewicz traces, and its applications to decentralized"}
{"full_text": "This short book is the result of various master and summer school courses I have taught. The objective is to introduce the readers to mathematical control theory, both in finite and infinite dimension. In the finite-dimensional context, we consider controlled ordinary differential equations (ODEs); in this context, existence and uniqueness issues are easily resolved thanks to the Picard-Lindel\\''of (Cauchy-Lipschitz) theorem. In infinite dimension, in view of dealing with controlled partial differential equations (PDEs), the concept of well-posed system is much more difficult and requires to develop a bunch of functional analysis tools, in particular semigroup theory -- and this, just for the setting in which the control system is written and makes sense. This is why I have splitted the book into two parts, the first being devoted to finite-dimensional control systems, and the second to infinite-dimensional ones. In spite of this splitting, it may be nice to learn basics of control theory for finite-dimensional linear autonomous control systems (e.g., the Kalman condition) and then to see in the second part how some results are extended to infinite dimension, where matrices are replaced by operators, and exponentials of matrices are replaced by semigroups. For instance, the reader will see how the Gramian controllability condition is expressed in infinite dimension, and leads to the celebrated Hilbert Uniqueness Method (HUM). Except the very last section, in the second part I have only considered linear autonomous control systems (the theory is already quite complicated), providing anyway several references to other textbooks for the several techniques existing to treat some particular classes of nonlinear PDEs. In contrast, in the first part on finite-dimensional control theory, there are much less difficulties to treat general nonlinear control systems, and I give here some general results on controllability, optimal control and stabilization. Of course, whether in finite or infinite dimension, there exist much finer results and methods in the literature, established however for specific classes of control systems. Here, my objective is to provide the reader with an introduction to control theory and to the main tools allowing to treat general control systems. I hope this will serve as motivation to go deeper into the theory or numerical aspects that are not covered here.", "prompt": "This short book is the result of various master and summer school courses I have taught. The objective is to introduce the readers to mathematical control theory, both in finite and infinite dimension. In the finite-dimensional context, we consider controlled ordinary differential equations (ODEs); in this context, existence and uniqueness issues are easily resolved", "orig": "thanks to the Picard-Lindel\\''of (Cauchy-Lipschitz) theorem. In infinite dimension, in view of dealing with controlled partial differential equations (PDEs), the concept of well-posed system is much more difficult and requires to develop a bunch of functional analysis tools, in particular", "generated": "This short book is the result of various master and summer school courses I have taught. The objective is to introduce the readers to mathematical control theory, both in finite and infinite dimension. In the finite-dimensional context, we consider controlled ordinary differential equations (ODEs); in this context, existence and uniqueness issues are easily resolved"}
{"full_text": "We consider here one-parameter semigroups ${\\bf T}=(T(t))_{t>0}$ of bounded operators on a Banach space $X$ which are weakly continuous in the sense of Arveson. For such a semigroup ${\\bf T}$ denote by ${\\mathcal M}_{\\omega_{\\bf T}}$ the convolution algebra consisting in those measures $\\mu$ on $(0,+\\infty)$ such that $\\int_0^{+\\infty}\\Vert T(t)\\Vert d\\vert \\mu \\vert (t)<+\\infty.$ The Pettis integral $\\int_0^{+\\infty}T(t)d\\mu(t)$ defines for $\\mu \\in {\\mathcal M}_{\\omega_{\\bf T}}$ a bounded operator $\\phi_{\\bf T}(\\mu)$ on $X.$ Identifying the space $L^1_{\\omega_{\\bf T}}$ of (classes of) measurable functions $f$ satisfying $\\int_0^{+\\infty}\\vert f(t)\\Vert T(t)\\Vert dt< +\\infty$ to a closed subspace ${\\mathcal M}_{\\omega_{\\bf T}}$ in the usual way, we define the Arveson ideal $\\mathcal{I}_{\\bf T}$ of the semigroup to be the closure in ${\\mathcal B}(X)$ of $\\phi_{\\bf T}(L^1_{\\omega_{\\bf T}}).$ Using a variant of a procedure introduced a long time ago by the author we introduce a dense ideal $\\mathcal{U}_{\\bf T}$ of $\\mathcal{I}_{\\bf T},$ which is a Banach algebra with respect to a suitable norm $\\Vert .\\Vert_{\\mathcal{U}_{\\bf T}},$ such that $\\lim \\sup_{t\\to 0^+}\\Vert T(t)\\Vert_{{\\mathcal B}(\\mathcal{U}_{\\bf T})}<+\\infty.$ The normalized Arveson ideal $\\mathcal{J}_{\\bf T}$ is the closure of $\\mathcal{I}_{\\bf T}$ in ${\\mathcal B}(\\mathcal{U}_{\\bf T}).$ The Banach algebra $\\mathcal{J}_{\\bf T}$ has a sequential approximate identity and is isometrically isomorphic to a closed ideal of its multiplier algebra ${\\mathcal M}(\\mathcal{J}_{\\bf T}).$ The Banach algebras $\\mathcal{U}_{\\bf T},$ $\\mathcal{I}_{\\bf T}$ and $\\mathcal{J}_{\\bf T}$ are \"similar\", and the map $S_{u/v}\\to S_{au/av}$ defines when $a$ generates a dense principal ideal of $\\mathcal{U}_{\\bf T}$ a pseudo bounded isomorphism from the algebre $\\mathcal{QM}(\\mathcal{J}_{\\bf T})$ of quasimultipliers on $\\mathcal{J}_{\\bf T}$ onto the quasimultipliers algebras $\\mathcal{QM}(\\mathcal{U}_{\\bf T})$ and $\\mathcal{QM}(\\mathcal{I}_{\\bf T}).$ We define the generator $A_{\\bf T}$ of the semigroup $\\bf T$ to be a quasimultiplier on $\\mathcal{I}_{\\bf T},$ or ,equivalently, on $\\mathcal{J}_{\\bf T}.$ Every character $\\chi$ on $\\mathcal{I}_{\\bf T}$ has an extension $\\tilde \\chi$ to $\\mathcal{QM}(\\mathcal{I}_{\\bf T}).$ Let $Res_{ar} (A_{\\bf T})$ be the complement of the set $\\{ \\tilde \\chi (A_{\\bf T})\\}_{\\chi \\in \\widehat{\\mathcal{I}_{\\bf T}}}.$ The quasimultiplier $A-\\mu I$ has an inverse belonging to $\\mathcal{J}_{\\bf T}$ for $\\mu \\in Res_{ar} (A_{\\bf T}),$ which allows to consider this inverse as a \"regular\" quasimultiplier on the Arveson ideal $\\mathcal{I}_{\\bf T}.$ The usual resolvent formula holds in this context for $Re(\\mu)>\\lim_{t\\to +\\infty}{log \\Vert T(t)\\Vert\\over t}.$ Set $\\Pi_{\\alpha}^+:=\\{ z \\in \\mathbb{C} \\ | \\ Re(z) >\\alpha\\}.$ We revisit the functional calculus associated to the generator $A_{\\bf T}$ by defining $F(-A_{\\bf T})\\in \\mathcal{J}_{\\bf T}$ by a Cauchy integral when $F$ belongs to the Hardy space $H^1(\\Pi_{\\alpha}^+)$ for some $\\alpha < -\\lim_{t\\to +\\infty} {log\\Vert T(t)\\vert\\over t}.$ We then define $F(-A_{\\bf T})$ as a quasimultiplier on $\\mathcal{J}_{\\bf T}$ and $\\mathcal{I}_{\\bf T}$ when $F$ belongs to the Smirnov class on $\\Pi_{\\alpha}^+,$ and $F(-A_{\\bf T})$ is a regular quasimultiplier on $\\mathcal{J}_{\\bf T}$ and $\\mathcal{I}_{\\bf T}$ if $F$ is bounded on $\\Pi_{\\alpha}^+.$ If $F(z)=e^{-zt}$ for some $t>0,$ then $F(-A_{\\bf T})=T(t),$ and if $F(z)=-z,$ we indeed have $F(-A_{\\bf T})=A_{\\bf T}.$", "prompt": "We consider here one-parameter semigroups ${\\bf T}=(T(t))_{t>0}$ of bounded operators on a Banach space $X$ which are weakly continuous in the sense of Arveson. For such a semigroup ${\\bf T}$ denote by ${\\mathcal M", "orig": "}_{\\omega_{\\bf T}}$ the convolution algebra consisting in those measures $\\mu$ on $(0,+\\infty)$ such that $\\int_0^{+\\infty}\\Vert T(t)\\Vert d\\vert \\mu \\vert (t)<+\\infty.$ The Pettis integral $\\int_", "generated": "We consider here one-parameter semigroups ${\\bf T}=(T(t))_{t>0}$ of bounded operators on a Banach space $X$ which are weakly continuous in the sense of Arveson. For such a semigroup ${\\bf T}$ denote by ${\\mathcal M"}
{"full_text": "Objective: Realistic tissue-mimicking phantoms are essential for the development, the investigation and the calibration of medical imaging techniques and protocols. Because it requires taking both mechanical and imaging properties into account, the development of robust, calibrated phantoms is a major challenge in elastography. Soft polyvinyl chloride gels in a liquid plasticizer (plastisol or PVCP) have been proposed as soft tissue-mimicking phantoms (TMP) for elasticity imaging. PVCP phantoms are relatively low-cost and can be easily stored over long time periods without any specific requirements. In this work, the preparation of a PVCP gel phantom for both MR and ultrasoundelastography is proposed and its acoustic, NMR and mechanical properties are studied.Material and methods: The acoustic and magnetic resonance imaging properties of PVCP are measured for different mass ratios between ultrasound speckle particles and PVCP solution, and between resin and plasticizer. The linear mechanical properties of plastisol samples are then investigated over time using not only indentation tests, but also MR and ultrasound-elastography clinical protocols. These properties are compared to typical values reported for biological soft tissues and to the values found in the literature for PVCP gels.Results and conclusions: After a period of two weeks, the mechanical properties of the plastisol samples measured with indentation testing are stable for at least the following 4 weeks (end of follow-up period 43 days after gelation-fusion). Neither the mechanical nor the NMR properties of plastisol gels were found to be affected by the addition of cellulose as acoustic speckle. Mechanical properties of the proposed gels were successfully characterized by clinical, commercially-available MR Elastography and sonoelastography protocols. PVCP with a mass ratio of ultrasound speckle particles of 0.6% to 0.8% and a mass ratio between resin and plasticizer between 50 and 70% appears as a good TMP candidate that can be used with both MR and ultrasound-based elastography methods.", "prompt": "Objective: Realistic tissue-mimicking phantoms are essential for the development, the investigation and the calibration of medical imaging techniques and protocols. Because it requires taking both mechanical and imaging properties into account, the development of robust, calibrated phantoms is a major challenge in elastography. Soft polyvinyl chloride", "orig": "gels in a liquid plasticizer (plastisol or PVCP) have been proposed as soft tissue-mimicking phantoms (TMP) for elasticity imaging. PVCP phantoms are relatively low-cost and can be easily stored over long time periods without any specific requirements. In this work, the preparation of", "generated": "Objective: Realistic tissue-mimicking phantoms are essential for the development, the investigation and the calibration of medical imaging techniques and protocols. Because it requires taking both mechanical and imaging properties into account, the development of robust, calibrated phantoms is a major challenge in elastography. Soft polyvinyl chloride"}
{"full_text": "In the context of unconstraint numerical optimization, this paper investigates the global linear convergence of a simple probabilistic derivative-free optimization algorithm (DFO). The algorithm samples a candidate solution from a standard multivariate normal distribution scaled by a step-size and centered in the current solution. This solution is accepted if it has a better objective function value than the current one. Crucial to the algorithm is the adaptation of the step-size that is done in order to maintain a certain probability of success. The algorithm, already proposed in the 60's, is a generalization of the well-known Rechenberg's $(1+1)$ Evolution Strategy (ES) with one-fifth success rule which was also proposed by Devroye under the name compound random search or by Schumer and Steiglitz under the name step-size adaptive random search. In addition to be derivative-free, the algorithm is function-value-free: it exploits the objective function only through comparisons. It belongs to the class of comparison-based step-size adaptive randomized search (CB-SARS). For the convergence analysis, we follow the methodology developed in a companion paper for investigating linear convergence of CB-SARS: by exploiting invariance properties of the algorithm, we turn the study of global linear convergence on scaling-invariant functions into the study of the stability of an underlying normalized Markov chain (MC). We hence prove global linear convergence by studying the stability (irreducibility, recurrence, positivity, geometric ergodicity) of the normalized MC associated to the $(1+1)$-ES. More precisely, we prove that starting from any initial solution and any step-size, linear convergence with probability one and in expectation occurs. Our proof holds on unimodal functions that are the composite of strictly increasing functions by positively homogeneous functions with degree $\\alpha$ (assumed also to be continuously differentiable). This function class includes composite of norm functions but also non-quasi convex functions. Because of the composition by a strictly increasing function, it includes non continuous functions. We find that a sufficient condition for global linear convergence is the step-size increase on linear functions, a condition typically satisfied for standard parameter choices. While introduced more than 40 years ago, we provide here the first proof of global linear convergence for the $(1+1)$-ES with generalized one-fifth success rule and the first proof of linear convergence for a CB-SARS on such a class of functions that includes non-quasi convex and non-continuous functions. Our proof also holds on functions where linear convergence of some CB-SARS was previously proven, namely convex-quadratic functions (including the well-know sphere function).", "prompt": "In the context of unconstraint numerical optimization, this paper investigates the global linear convergence of a simple probabilistic derivative-free optimization algorithm (DFO). The algorithm samples a candidate solution from a standard multivariate normal distribution scaled by a step-size and centered in the current solution. This solution is accepted if it has a better", "orig": "objective function value than the current one. Crucial to the algorithm is the adaptation of the step-size that is done in order to maintain a certain probability of success. The algorithm, already proposed in the 60's, is a generalization of the well-known Rechenberg's $(1+1)$ Evolution Strategy (", "generated": "In the context of unconstraint numerical optimization, this paper investigates the global linear convergence of a simple probabilistic derivative-free optimization algorithm (DFO). The algorithm samples a candidate solution from a standard multivariate normal distribution scaled by a step-size and centered in the current solution. This solution is accepted if it has better fitness"}
{"full_text": "How to efficiently and reliably spread information in a system is one of the most fundamental problems in distributed computing. Recently, inspired by biological scenarios, several works focused on identifying the minimal communication resources necessary to spread information under faulty conditions. Here we study the self-stabilizing bit-dissemination problem, introduced by Boczkowski, Korman, and Natale in [SODA 2017]. The problem considers a fully-connected network of n agents, with a binary world of opinions, one of which is called correct. At any given time, each agent holds an opinion bit as its public output. The population contains a source agent which knows which opinion is correct. This agent adopts the correct opinion and remains with it throughout the execution. We consider the basic PULL model of communication, in which each agent observes relatively few randomly chosen agents in each round. The goal of the non-source agents is to quickly converge on the correct opinion, despite having an arbitrary initial configuration, i.e., in a self-stabilizing manner. Once the population converges on the correct opinion, it should remain with it forever. Motivated by biological scenarios in which animals observe and react to the behavior of others, we focus on the extremely constrained model of passive communication, which assumes that when observing another agent the only information that can be extracted is the opinion bit of that agent. We prove that this problem can be solved in a poly-logarithmic in n number of rounds with high probability, while sampling a logarithmic number of agents at each round. Previous works solved this problem faster and using fewer samples, but they did that by decoupling the messages sent by agents from their output opinion, and hence do not fit the framework of passive communication. Moreover, these works use complex recursive algorithms with refined clocks that are unlikely to be used by biological entities. In contrast, our proposed algorithm has a natural appeal as it is based on letting agents estimate the current tendency direction of the dynamics, and then adapt to the emerging trend.", "prompt": "How to efficiently and reliably spread information in a system is one of the most fundamental problems in distributed computing. Recently, inspired by biological scenarios, several works focused on identifying the minimal communication resources necessary to spread information under faulty conditions. Here we study the self-stabilizing bit-dissemination problem, introduced by Boc", "orig": "zkowski, Korman, and Natale in [SODA 2017]. The problem considers a fully-connected network of n agents, with a binary world of opinions, one of which is called correct. At any given time, each agent holds an opinion bit as its public output. The population contains a", "generated": "How to efficiently and reliably spread information in a system is one of the most fundamental problems in distributed computing. Recently, inspired by biological scenarios, several works focused on identifying the minimal communication resources necessary to spread information under faulty conditions. Here we study the self-stabilizing bit-dissemination problem, introduced by Boc"}
{"full_text": "The fundamental plane of early-type galaxies is a rather tight three-parameter correlation discovered more than twenty years ago. It has resisted a both global and precise physical interpretation despite a consequent number of works, observational, theoretical or using numerical simulations. It appears that its precise properties depend on the population of galaxies in study. Instead of selecting a priori these populations, we propose to objectively construct homologous populations from multivariate analyses. We have undertaken multivariate cluster and cladistic analyses of a sample of 56 low-redshift galaxy clusters containing 699 early-type galaxies, using four parameters: effective radius, velocity dispersion, surface brightness averaged over effective radius, and Mg2 index. All our analyses are consistent with seven groups that define separate regions on the global fundamental plane, not across its thickness. In fact, each group shows its own fundamental plane, which is more loosely defined for less diversified groups. We conclude that the global fundamental plane is not a bent surface, but made of a collection of several groups characterizing several fundamental planes with different thicknesses and orientations in the parameter space. Our diversification scenario probably indicates that the level of diversity is linked to the number and the nature of transforming events and that the fundamental plane is the result of several transforming events. We also show that our classification, not the fundamental planes, is universal within our redshift range (0.007 - 0.053). We find that the three groups with the thinnest fundamental planes presumably formed through dissipative (wet) mergers. In one of them, this(ese) merger(s) must have been quite ancient because of the relatively low metallicity of its galaxies, Two of these groups have subsequently undergone dry mergers to increase their masses. In the k-space, the third one clearly occupies the region where bulges (of lenticular or spiral galaxies) lie and might also have formed through minor mergers and accretions. The two least diversified groups probably did not form by major mergers and must have been strongly affected by interactions, some of the gas in the objects of one of these groups having possibly been swept out. The interpretation, based on specific assembly histories of galaxies of our seven groups, shows that they are truly homologous. They were obtained directly from several observables, thus independently of any a priori classification. The diversification scenario relating these groups does not depend on models or numerical simulations, but is objectively provided by the cladistic analysis. Consequently, our classification is more easily compared to models and numerical simulations, and our work can be readily repeated with additional observables.", "prompt": "The fundamental plane of early-type galaxies is a rather tight three-parameter correlation discovered more than twenty years ago. It has resisted a both global and precise physical interpretation despite a consequent number of works, observational, theoretical or using numerical simulations. It appears that its precise properties depend on the population of galaxies in study. Instead", "orig": "of selecting a priori these populations, we propose to objectively construct homologous populations from multivariate analyses. We have undertaken multivariate cluster and cladistic analyses of a sample of 56 low-redshift galaxy clusters containing 699 early-type galaxies, using four parameters: effective radius, velocity dispersion, surface brightness averaged over", "generated": "The fundamental plane of early-type galaxies is a rather tight three-parameter correlation discovered more than twenty years ago. It has resisted a both global and precise physical interpretation despite a consequent number of works, observational, theoretical or using numerical simulations. It appears that its precise properties depend on the population of galaxies in study. Instead"}
{"full_text": "Multifunctional nanomaterials combining diagnosis and therapeutic properties have attracted a considerable attention in cancer research. Yet some important challenges are still to be faced, including an optimal coupling between these two types of properties that would be effective within complex biological tissues. To address these points, we have prepared novel nanoplatforms associating controlled drug delivery of doxorubicin and Magnetic Resonance Imaging (MRI) contrast-enhancement that exhibit high specificity towards cancer cells compared to normal cells and evaluated them both in 2D cultures and within 3D tissue-like biomimetic matrices. Methods: Nanoplatforms were prepared from hollow silica nanoparticles coated with MnO2 nanosheets and conjugated with the AS1411 aptamer as a targeting agent. They were fully characterized from a chemical and structural point of view as well as for drug release and MRI signal enhancement. Standard two-dimensional monolayer cultures were performed using HeLa and Normal Human Dermal Fibroblasts (NHDF) cells to testify targeting and cytotoxicity. Cellularized type I collagen-based hydrogels were also used to study nanoparticles behavior in 3D biomimetic environments. Results: The as-established nanoplatforms can enter HeLa cells, leading to the dissociation of the MnO2 nanosheets into Mn 2+ that enhanced T1 magnetic resonance signals and concomitantly release doxorubicin, both effects being markedly more significant than in the presence of NHDFs. Moreover, particles functionality and specificity were preserved when the cells were immobilized within type I collagen-based fibrillar hydrogels. Conclusion: The use of MnO2 nanosheets as glutathione-sensitive coatings of drug loaded nanoparticles together with surface conjugation with a targeting aptamer offers an effective strategy to obtain efficient and specific nanotheranostic systems for cancer research, both in 2D and 3D. The here-described tissue-like models should be easy to implement and could constitute an interesting intermediate validation step for newly-developed theranostic nanoparticles before in vivo evaluation.", "prompt": "Multifunctional nanomaterials combining diagnosis and therapeutic properties have attracted a considerable attention in cancer research. Yet some important challenges are still to be faced, including an optimal coupling between these two types of properties that would be effective within complex biological tissues. To address these points, we have prepared novel nanoplatform", "orig": "s associating controlled drug delivery of doxorubicin and Magnetic Resonance Imaging (MRI) contrast-enhancement that exhibit high specificity towards cancer cells compared to normal cells and evaluated them both in 2D cultures and within 3D tissue-like biomimetic matrices. Methods: Nanoplatforms were prepared from", "generated": "Multifunctional nanomaterials combining diagnosis and therapeutic properties have attracted a considerable attention in cancer research. Yet some important challenges are still to be faced, including an optimal coupling between these two types of properties that would be effective within complex biological tissues. To address these points, we have prepared novel nanoplatform"}
{"full_text": "Graph Neural Networks share with Logic Programming several key relational inference mechanisms. The datasets on which they are trained and evaluated can be seen as database facts containing ground terms. This makes possible modeling their inference mechanisms with equivalent logic programs, to better understand not just how they propagate information between the entities involved in the machine learning process but also to infer limits on what can be learned from a given dataset and how well that might generalize to unseen test data. This leads us to the key idea of this paper: modeling with the help of a logic program the information flows involved in learning to infer from the link structure of a graph and the information content of its nodes properties of new nodes, given their known connections to nodes with possibly similar properties. The problem is known as graph node property prediction and our approach will consist in emulating with help of a Prolog program the key information propagation steps of a Graph Neural Network's training and inference stages. We test our a approach on the ogbn-arxiv node property inference benchmark. To infer class labels for nodes representing papers in a citation network, we distill the dependency trees of the text associated to each node into directed acyclic graphs that we encode as ground Prolog terms. Together with the set of their references to other papers, they become facts in a database on which we reason with help of a Prolog program that mimics the information propagation in graph neural networks predicting node properties. In the process, we invent ground term similarity relations that help infer labels in the test set by propagating node properties from similar nodes in the training set and we evaluate their effectiveness in comparison with that of the graph's link structure. Finally, we implement explanation generators that unveil performance upper bounds inherent to the dataset. As a practical outcome, we obtain a logic program, that, when seen as machine learning algorithm, performs close to the state of the art on the node property prediction benchmark.", "prompt": "Graph Neural Networks share with Logic Programming several key relational inference mechanisms. The datasets on which they are trained and evaluated can be seen as database facts containing ground terms. This makes possible modeling their inference mechanisms with equivalent logic programs, to better understand not just how they propagate information between the entities involved in the machine learning process but", "orig": "also to infer limits on what can be learned from a given dataset and how well that might generalize to unseen test data. This leads us to the key idea of this paper: modeling with the help of a logic program the information flows involved in learning to infer from the link structure of a graph and the information content of its", "generated": "Graph Neural Networks share with Logic Programming several key relational inference mechanisms. The datasets on which they are trained and evaluated can be seen as database facts containing ground terms. This makes possible modeling their inference mechanisms with equivalent logic programs, to better understand not just how they propagate information between the entities involved in the machine learning process but"}
{"full_text": "The practice of unit testing enables programmers to obtain automated feedback on whether a currently edited program is consistent with the expectations specified in test cases. Feedback is most valuable when it happens immediately, as defects can be corrected instantly before they become harder to fix. With growing and longer running test suites, however, feedback is obtained less frequently and lags behind program changes. The objective of test prioritization is to rank tests so that defects, if present, are found as early as possible or with the least costs. While there are numerous static approaches that output a ranking of tests solely based on the current version of a program, we focus on change-based test prioritization, which recommends tests that likely fail in response to the most recent program change. The canonical approach relies on coverage data and prioritizes tests that cover the changed region, but obtaining and updating coverage data is costly. More recently, information retrieval techniques that exploit overlapping vocabulary between change and tests have proven to be powerful, yet lightweight. In this work, we demonstrate the capabilities of information retrieval for prioritizing tests in dynamic programming languages using Python as example. We discuss and measure previously understudied variation points, including how contextual information around a program change can be used, and design alternatives to the widespread \\emph{TF-IDF} retrieval model tailored to retrieving failing tests. To obtain program changes with associated test failures, we designed a tool that generates a large set of faulty changes from version history along with their test results. Using this data set, we compared existing and new lexical prioritization strategies using four open-source Python projects, showing large improvements over untreated and random test orders and results consistent with related work in statically typed languages. We conclude that lightweight IR-based prioritization strategies are effective tools to predict failing tests in the absence of coverage data or when static analysis is intractable like in dynamic languages. This knowledge can benefit both individual programmers that rely on fast feedback, as well as operators of continuous integration infrastructure, where resources can be freed sooner by detecting defects earlier in the build cycle.", "prompt": "The practice of unit testing enables programmers to obtain automated feedback on whether a currently edited program is consistent with the expectations specified in test cases. Feedback is most valuable when it happens immediately, as defects can be corrected instantly before they become harder to fix. With growing and longer running test suites, however, feedback is obtained less", "orig": "frequently and lags behind program changes. The objective of test prioritization is to rank tests so that defects, if present, are found as early as possible or with the least costs. While there are numerous static approaches that output a ranking of tests solely based on the current version of a program, we focus on change-based", "generated": "The practice of unit testing enables programmers to obtain automated feedback on whether a currently edited program is consistent with the expectations specified in test cases. Feedback is most valuable when it happens immediately, as defects can be corrected instantly before they become harder to fix. With growing and longer running test suites, however, feedback is obtained less"}
{"full_text": "We introduce the dependent doors problem as an abstraction for situations in which one must perform a sequence of possibly dependent decisions, without receiving feedback information on the effectiveness of previously made actions. Informally, the problem considers a set of $d$ doors that are initially closed, and the aim is to open all of them as fast as possible. To open a door, the algorithm knocks on it and it might open or not according to some probability distribution. This distribution may depend on which other doors are currently open, as well as on which other doors were open during each of the previous knocks on that door. The algorithm aims to minimize the expected time until all doors open. Crucially, it must act at any time without knowing whether or which other doors have already opened. In this work, we focus on scenarios where dependencies between doors are both positively correlated and acyclic.The fundamental distribution of a door describes the probability it opens in the best of conditions (with respect to other doors being open or closed). We show that if in two configurations of $d$ doors corresponding doors share the same fundamental distribution, then these configurations have the same optimal running time up to a universal constant, no matter what are the dependencies between doors and what are the distributions. We also identify algorithms that are optimal up to a universal constant factor. For the case in which all doors share the same fundamental distribution we additionally provide a simpler algorithm, and a formula to calculate its running time. We furthermore analyse the price of lacking feedback for several configurations governed by standard fundamental distributions. In particular, we show that the price is logarithmic in $d$ for memoryless doors, but can potentially grow to be linear in $d$ for other distributions.We then turn our attention to investigate precise bounds. Even for the case of two doors, identifying the optimal sequence is an intriguing combinatorial question. Here, we study the case of two cascading memoryless doors. That is, the first door opens on each knock independently with probability $p\\_1$. The second door can only open if the first door is open, in which case it will open on each knock independently with probability $p\\_2$. We solve this problem almost completely by identifying algorithms that are optimal up to an additive term of 1.", "prompt": "We introduce the dependent doors problem as an abstraction for situations in which one must perform a sequence of possibly dependent decisions, without receiving feedback information on the effectiveness of previously made actions. Informally, the problem considers a set of $d$ doors that are initially closed, and the aim is to open all of them as", "orig": "fast as possible. To open a door, the algorithm knocks on it and it might open or not according to some probability distribution. This distribution may depend on which other doors are currently open, as well as on which other doors were open during each of the previous knocks on that door. The algorithm aims to minimize the expected", "generated": "We introduce the dependent doors problem as an abstraction for situations in which one must perform a sequence of possibly dependent decisions, without receiving feedback information on the effectiveness of previously made actions. Informally, the problem considers a set of $d$ doors that are initially closed, and the aim is to open all of them and"}
{"full_text": "This version is the last version of our book project on Hamilton-Jacobi Equations and Control Problems with discontinuities. Compared to the third version (online in december 2022), we have improved Part V (Stratified solutions for state-constraints problems) and Part VI on the applications but also the stability results for stratified solutions; we have rewritten a large part of the introduction and added guidelines for the reader. As in the previous versions, we have incorporated new results and examples, changed some points-of-view, detailed some proofs and corrected several mistakes. Version 3 had 550 pages, this one 630.As the third version, it is composed of six parts: Part I is still a toolbox with key results which are used in all the other parts. The study of the simplest case, i.e. the case of a co-dimension 1 discontinuity, is now split in two parts: in Part II, we only consider control problems and the associated Bellman Equations are treated by using only the classical notion of viscosity solutions. In this part, the methods are a combinations of control and pdes techniques. On the contrary, Part III describes purely pdes approaches which are inspired by the literature on Hamilton Jacobi Equations on networks and which can handle the case of non-convex Hamiltonians. In this part, we present two notions of solutions, namely flux-limited and junction viscosity solutions, and we study in detail their properties by providing comparison and stability results. We also show that they are ``almost'' equivalent when both make sense, i.e. for quasi-convex Hamiltonians. Part IV concerns stratified problems in $\\R^N$, i.e. problems with discontinuities of any co-dimensions: the main change compared to the previous version is the introduction of a notion of ``weak'' stratified (sub)solution. In Part V, we address the case of stratified problems in bounded or unbounded domains with state-constraints, allowing very surprising applications as well as singular boundary conditions. Finally, in Part VI we describe some applications to KPP (Kolmogorov-Petrovsky-Piskunov) type problems and we discuss possible extensions to problems with jumps and to ``stratified networks''.Even if we consider this version as being the final one, all comments are welcome!", "prompt": "This version is the last version of our book project on Hamilton-Jacobi Equations and Control Problems with discontinuities. Compared to the third version (online in december 2022), we have improved Part V (Stratified solutions for state-constraints problems) and Part VI on the applications but also the", "orig": "stability results for stratified solutions; we have rewritten a large part of the introduction and added guidelines for the reader. As in the previous versions, we have incorporated new results and examples, changed some points-of-view, detailed some proofs and corrected several mistakes. Version 3 had 550 pages, this one 630.As", "generated": "This version is the last version of our book project on Hamilton-Jacobi Equations and Control Problems with discontinuities. Compared to the third version (online in december 2022), we have improved Part V (Stratified solutions for state-constraints problems) and Part VI on the applications but also the"}
{"full_text": "This study proposes a methodology using OpenAlex (OA) for tracking Open Access publications in the case of Argentina, a country where a self-archiving mandate has been in effect since 2013 ( Law 26.899, 2013). A sample of 167,240 papers by researchers from the National Council for Scientific and Technical Research (CONICET) was created and analyzed using statistical techniques. We estimate that OA is able to capture between 85-93% of authors for all disciplines, with the exception of Social Sciences and Humanities, where it only reaches an estimated 47%. The availability of papers in Open Access was calculated to be 41% for the period 1953-2021 and 46% when considering exclusively the post-law period (2014-2021). In both periods, gold Open Access made up the most common route. When comparing equal periods post and pre-law, we observed that the upward trend of gold Open Access was pre-existing to the legislation and the availability of closed articles in repositories increased by 5% to what is estimated based on existing trends. However, while the green route has had a positive evolution, it has been the publication in gold journals that has boosted access to Argentine production more rapidly. We concluded that the OA-based methodology, piloted here for the first time, is viable for tracking Open Access in Argentina since it yields percentages similar to other national and international studies. En este estudio se propone una metodolog\\'ia utilizando OpenAlex (OA) para monitorear el acceso abierto (AA) a las publicaciones cient\\'ificas para el caso de Argentina, pa\\'is donde rige el mandato de autoarchivo -Ley 26.899 (2013)-. Se conform\\'o una muestra con 167.240 art\\'iculos de investigadores del Consejo Nacional de Investigaciones Cient\\'ificas y T\\'ecnicas (CONICET) que se analizaron con t\\'ecnicas estad\\'isticas. Se estim\\'o que OA puede representar entre 85-93% de los autores para todas las disciplinas, excepto Ciencias Sociales y Humanidades, donde solo alcanza al 47%. Se calcul\\'o que 41% de los art\\'iculos publicados entre 1953-2021 incluidos en la fuente est\\'an en AA, porcentaje que sube a 46% al considerar exclusivamente el periodo post ley (2014-2021). En ambos periodos es la v\\'ia dorada la que representa mayor proporci\\'on. Al comparar periodos iguales post y pre ley, se observ\\'o que la tendencia en alza de la v\\'ia dorada era preexistente a la legislaci\\'on y la disponibilidad de art\\'iculos cerrados en repositorios aument\\'o un 5% a lo que se estima en base a tendencias existentes. Se concluye que si bien la v\\'ia verde ha tenido una evoluci\\'on positiva, ha sido la publicaci\\'on en revistas doradas lo que ha impulsado m\\'as r\\'apidamente el acceso a la producci\\'on argentina. Asimismo, que la metodolog\\'ia basada en OA, piloteada aqu\\'i por primera vez, es viable para monitorear el AA en Argentina ya que arroja porcentajes similares a otros estudios nacionales e internacionales.", "prompt": "This study proposes a methodology using OpenAlex (OA) for tracking Open Access publications in the case of Argentina, a country where a self-archiving mandate has been in effect since 2013 ( Law 26.899, 2013). A sample of 167,240 papers by researchers from the National", "orig": "Council for Scientific and Technical Research (CONICET) was created and analyzed using statistical techniques. We estimate that OA is able to capture between 85-93% of authors for all disciplines, with the exception of Social Sciences and Humanities, where it only reaches an estimated 47%. The availability of papers in Open Access", "generated": "This study proposes a methodology using OpenAlex (OA) for tracking Open Access publications in the case of Argentina, a country where a self-archiving mandate has been in effect since 2013 (Law 26.899, 2013). A sample of 167,240 papers by researchers from the National"}
{"full_text": "Context: Domain-specific languages (DSLs) enable domain experts to specify tasks and problems themselves, while enabling static analysis to elucidate issues in the modelled domain early. Although language workbenches have simplified the design of DSLs and extensions to general purpose languages, static analyses must still be implemented manually. Inquiry: Moreover, static analyses, e.g., complexity metrics, dependency analysis, and declaration-use analysis, are usually domain-dependent and cannot be easily reused. Therefore, transferring existing static analyses to another DSL incurs a huge implementation overhead. However, this overhead is not always intrinsically necessary: in many cases, while the concepts of the DSL on which a static analysis is performed are domain-specific, the underlying algorithm employed in the analysis is actually domain-independent and thus can be reused in principle, depending on how it is specified. While current approaches either implement static analyses internally or with an external Visitor, the implementation is tied to the language's grammar and cannot be reused easily. Thus far, a commonly used approach that achieves reusable static analysis relies on the transformation into an intermediate representation upon which the analysis is performed. This, however, entails a considerable additional implementation effort. Approach: To remedy this, it has been proposed to map the necessary domain-specific concepts to the algorithm's domain-independent data structures, yet without a practical implementation and the demonstration of reuse. Thus, to make static analysis reusable again, we employ relational Reference Attribute Grammars (RAGs) by creating such a mapping to a domain-independent overlay structure using higher-order attributes. Knowledge: We describe how static analysis can be specified on analysis-specific data structures, how relational RAGs can help with the specification, and how a mapping from the domain-specific language can be performed. Furthermore, we demonstrate how a static analysis for a DSL can be externalized and reused in another general purpose language. Grounding: The approach was evaluated using the RAG system JastAdd. To illustrate reusability, we implemented two analyses with two addressed languages each: a cycle detection analysis used in a small state machine DSL and for detecting circular dependencies in Java types and packages, and an analysis of variable shadowing, applied to both Java and the Modelica modelling language. Thereby, we demonstrate the reuse of two analysis algorithms in three completely different domains. Additionally, we use the cycle detection analysis to evaluate the efficiency by comparing our external analysis to an internal reference implementation analysing all Java programs in the Qualitas Corpus and thereby are able to show that an externalized analysis incurs only minimal overhead. Importance: We make static analysis reusable, again, showing the practicality and efficiency of externalizing static analysis for both DSLs and general purpose languages using relational RAGs.", "prompt": "Context: Domain-specific languages (DSLs) enable domain experts to specify tasks and problems themselves, while enabling static analysis to elucidate issues in the modelled domain early. Although language workbenches have simplified the design of DSLs and extensions to general purpose languages, static analyses must still be implemented manually. Inquiry:", "orig": "Moreover, static analyses, e.g., complexity metrics, dependency analysis, and declaration-use analysis, are usually domain-dependent and cannot be easily reused. Therefore, transferring existing static analyses to another DSL incurs a huge implementation overhead. However, this overhead is not always intrinsically necessary: in many cases, while the concepts", "generated": "Domain-specific languages (DSLs) enable domain experts to specify tasks and problems themselves, while enabling static analysis to elucidate issues in the modelled domain early. Although language workbenches have simplified the design of DSLs and extensions to general purpose languages, static analyses must still be implemented manually.\n\nDomain-specific languages ("}
{"full_text": "Monte Carlo (MC) sampling is a popular method for estimating the statistics (e.g. expectation and variance) of a random variable. Its slow convergence has led to the emergence of advanced techniques to reduce the variance of the MC estimator for the outputs of computationally expensive solvers. The control variates (CV) method corrects the MC estimator with a term derived from auxiliary random variables that are highly correlated with the original random variable. These auxiliary variables may come from surrogate models. Such a surrogate-based CV strategy is extended here to the multilevel Monte Carlo (MLMC) framework, which relies on a sequence of levels corresponding to numerical simulators with increasing accuracy and computational cost. MLMC combines output samples obtained across levels, into a telescopic sum of differences between MC estimators for successive fidelities. In this paper, we introduce three multilevel variance reduction strategies that rely on surrogate-based CV and MLMC. MLCV is presented as an extension of CV where the correction terms devised from surrogate models for simulators of different levels add up. MLMC-CV improves the MLMC estimator by using a CV based on a surrogate of the correction term at each level. Further variance reduction is achieved by using the surrogate-based CVs of all the levels in the MLMC-MLCV strategy. Alternative solutions that reduce the subset of surrogates used for the multilevel estimation are also introduced. The proposed methods are tested on a test case from the literature consisting of a spectral discretization of an uncertain 1D heat equation, where the statistic of interest is the expected value of the integrated temperature along the domain at a given time. The results are assessed in terms of the accuracy and computational cost of the multilevel estimators, depending on whether the construction of the surrogates, and the associated computational cost, precede the evaluation of the estimator. It was shown that when the lower fidelity outputs are strongly correlated with the high-fidelity outputs, a significant variance reduction is obtained when using surrogate models for the coarser levels only. It was also shown that taking advantage of pre-existing surrogate models proves to be an even more efficient strategy.", "prompt": "Monte Carlo (MC) sampling is a popular method for estimating the statistics (e.g. expectation and variance) of a random variable. Its slow convergence has led to the emergence of advanced techniques to reduce the variance of the MC estimator for the outputs of computationally expensive solvers. The control variates (CV", "orig": ") method corrects the MC estimator with a term derived from auxiliary random variables that are highly correlated with the original random variable. These auxiliary variables may come from surrogate models. Such a surrogate-based CV strategy is extended here to the multilevel Monte Carlo (MLMC) framework, which relies on a sequence of levels corresponding", "generated": "Monte Carlo (MC) sampling is a popular method for estimating the statistics (e.g. expectation and variance) of a random variable. Its slow convergence has led to the emergence of advanced techniques to reduce the variance of the MC estimator for the outputs of computationally expensive solvers. The control variates (CV"}
{"full_text": "Three-dimensional data have become increasingly present in earth observation over the last decades. However, many 3D surveys are still underexploited due to the lack of accessible and explainable automatic classification methods, for example, new topo-bathymetric lidar data. In this work, we introduce explainable machine learning for 3D data classification using Multiple Attributes, Scales, and Clouds under 3DMASC, a new workflow. This workflow introduces multi-cloud classification through dual-cloud features, encrypting local spectral and geometrical ratios and differences. 3DMASC uses classical multi-scale descriptors adapted to all types of 3D point clouds and new ones based on their spatial variations. In this paper, we present the performances of 3DMASC for multi-class classification of topo-bathymetric lidar data in coastal and fluvial environments. We show how multivariate and embedded feature selection allows the building of optimized predictor sets of reduced complexity, and we identify features particularly relevant for coastal and riverine scene descriptions. Our results show the importance of dual-cloud features, lidar return-based attributes averaged over specific scales, and of statistics of dimensionality-based and spectral features. Additionally, they indicate that small to medium spherical neighbourhood diameters (<7 m) are sufficient to build effective classifiers, namely when combined with distance-to-ground or distance-to-water-surface features. Without using optional RGB information, and with a maximum of 37 descriptors, we obtain classification accuracies between 91 % for complex multi-class tasks and 98 % for lower-level processing using models trained on less than 2000 samples per class. Comparisons with classical point cloud classification methods show that 3DMASC features have a significantly improved descriptive power. Our contributions are made available through a plugin in the CloudCompare software, allowing non-specialist users to create classifiers for any type of 3D data characterized by 1 or 2 point clouds (airborne or terrestrial lidar, structure from motion), and two labelled topo-bathymetric lidar datasets, available on https://opentopography.org/.", "prompt": "Three-dimensional data have become increasingly present in earth observation over the last decades. However, many 3D surveys are still underexploited due to the lack of accessible and explainable automatic classification methods, for example, new topo-bathymetric lidar data. In this work, we introduce explainable machine learning", "orig": "for 3D data classification using Multiple Attributes, Scales, and Clouds under 3DMASC, a new workflow. This workflow introduces multi-cloud classification through dual-cloud features, encrypting local spectral and geometrical ratios and differences. 3DMASC uses classical multi-scale descriptors adapted to all types of 3", "generated": "Three-dimensional data have become increasingly present in earth observation over the last decades. However, many 3D surveys are still underexploited due to the lack of accessible and explainable automatic classification methods, for example, new topo-bathymetric lidar data. In this work, we introduce explainable machine learning"}
{"full_text": "Core helium burning is the dominant source of energy of extreme horizontal branch stars, as the hydrogen envelope is too small to contribute to the nuclear energy output. The evolution of each mass in the HR diagram occurs along vertical tracks that, when the core helium is consumed, evolve to higher Teff and then to the white dwarf stage. The larger is the mass, the smaller is the Teff of the models, so that the zero age horizontal branch (ZAHB) is \"horizontal\". In this paper we show that, if the helium mass fraction (Y) of the envelope is larger than Y~0.5, the shape of the tracks changes completely: the hydrogen burning becomes efficient again also for very small envelope masses, thanks to the higher molecular weight and to the higher temperatures of the hydrogen shell. The larger is Y, the smaller is the envelope mass that provides strong H-shell burning. These tracks have a curled shape, are located at a Teff following the approximate relation Teff=8090+ 32900xY, and become more luminous for larger envelope masses. Consequently, the ZAHB of the very high helium models is \"vertical\" in the HR diagram. Synthetic models based on these tracks nicely reproduce the location and shape of the \"blue hook\" in the globular cluster wCen, best fit by a very high Teff (bluer) sequence with Y=0.80 and a cooler (redder) one with Y=0.65. Although these precise values of Y may depend on the color-Teff conversions, we know that the helium content of the progenitors of the blue hook stars can not be larger than Y~0.38-0.40, if they are descendants of the cluster blue main sequence. Consequently, this interpretation implies that all these objects must in fact be progeny of the blue main sequence, but they have all suffered further deep mixing, that has largely and uniformly increased their surface helium abundance, during the red giant branch evolution. A late helium flash can not be the cause of this deep mixing, as the models we propose have hydrogen rich envelopes much more massive than those required for a late flash. We discuss different models of deep mixing proposed in the literature, and conclude that our interpretation of the blue hook can not be ruled out, but requires a much deeper investigation before it can be accepted.", "prompt": "Core helium burning is the dominant source of energy of extreme horizontal branch stars, as the hydrogen envelope is too small to contribute to the nuclear energy output. The evolution of each mass in the HR diagram occurs along vertical tracks that, when the core helium is consumed, evolve to higher Teff and then to the white dwarf", "orig": "stage. The larger is the mass, the smaller is the Teff of the models, so that the zero age horizontal branch (ZAHB) is \"horizontal\". In this paper we show that, if the helium mass fraction (Y) of the envelope is larger than Y~0.5, the shape of", "generated": "Core helium burning is the dominant source of energy of extreme horizontal branch stars, as the hydrogen envelope is too small to contribute to the nuclear energy output. The evolution of each mass in the HR diagram occurs along vertical tracks that, when the core helium is consumed, evolve to higher Teff and then to the white dwarf"}
{"full_text": "Context: Software development tools that interact with running programs such as debuggers, profilers, and dynamic analysis frameworks are presumed to demand difficult tradeoffs among implementation complexity (cost), functionality, usability, and performance. Among the many consequences, tools are often delivered late (if ever), have limited functionality, require non-standard configurations, and impose serious performance costs on running programs. Inquiry: Can flexible tool support become a practical, first class, intrinsic requirement for a modern highperformance programming language implementation framework? Approach: We extended the Truffle Language Implementation Framework, which together with the GraalVM execution environment makes possible very high performance language implementations. Truffle's new Instrumentation Framework is language-agnostic and designed to derive high performance from the same technologies as do language implementations. Truffle Instrumentation includes: (1) low overhead capture of execution events by dynamically adding \"wrapper\" nodes to executing ASTs; (2) extensions to the Language Implementation Framework that allow per-language specialization, primarily for visual display of values and names, among others; and (3) versatile APIs and support services for implementing many kinds of tools without VM modification. Knowledge: It is now possible for a client in a production environment to insert (dynamically, with thread safety) an instrumentation probe that captures and reports abstractly specified execution events. A probe in fully optimized code imposes very low overhead until actually used to access (or modify) execution state. Event capture has enabled construction of numerous GraalVM services and tools that work for all implemented languages, either singly or in combination. Instrumentation has also proved valuable for implementing some traditionally tricky language features, as well as some GraalVM services such as placing bounds on resources consumed by running programs. Grounding: Tools for debugging (via multiple clients), profiling, statement counting, dynamic analysis, and others are now part of GraalVM or are in active development. Third parties have also used Truffle Instrumentation for innovative tool implementations. Importance: Experience with Truffle Instrumentation validates the notion that addressing developer tools support as a forethought can change expectations about the availability of practical, efficient tools for high-performance languages. Tool development becomes a natural part of language implementation, requiring little additional effort and offering the advantage of early and continuous availability.", "prompt": "Context: Software development tools that interact with running programs such as debuggers, profilers, and dynamic analysis frameworks are presumed to demand difficult tradeoffs among implementation complexity (cost), functionality, usability, and performance. Among the many consequences, tools are often delivered late (if ever), have limited functionality, require non-standard", "orig": "configurations, and impose serious performance costs on running programs. Inquiry: Can flexible tool support become a practical, first class, intrinsic requirement for a modern highperformance programming language implementation framework? Approach: We extended the Truffle Language Implementation Framework, which together with the GraalVM execution environment makes possible very high performance language implementations.", "generated": "Context: Software development tools that interact with running programs such as debuggers, profilers, and dynamic analysis frameworks are presumed to demand difficult tradeoffs among implementation complexity (cost), functionality, usability, and performance. Among the many consequences, tools are often delivered late (if ever), have limited functionality, require non-standard"}
{"full_text": "Ion accelerators have been used by material scientists for decades to investigate radiation damage formation in nuclear materials and thus to emulate neutron-induced changes. The versatility of conditions in terms of particle energy, dose rate, fluence, etc., is a key asset of ion beams allowing for fully instrumented analytical studies. In addition, very short irradiation times and handling of non-radioactive samples dramatically curtail the global cost and duration as compared to in-reactor testing. Coupling of two or more beams, use of heated/cooled sample holders, and implementation of in situ characterization and microscopy pave the way to real time observation of microstructural and property evolution in various extreme radiation conditions more closely mimicking the nuclear environments. For these reasons, multiple ion beam facilities have been commissioned worldwide. In France, under the auspices of the Universit{\\'e} Paris-Saclay, the JANNuS platform for 'Joint Accelerators for Nanosciences and Nuclear Simulation' comprises five ion implanter and electrostatic accelerators with complementary performances. At CSNSM (CNRS \\& Univ Paris-Sud, Orsay), a 200 kV Transmission Electron Microscope is coupled to an accelerator and an implanter for in situ observation of microstructure modifications induced by ion beams in a material, making important contribution to the understanding of physical phenomena at the nanoscale. At CEA Paris-Saclay, the unique triple beam facility in Europe allows the simultaneous irradiation with heavy ions (like Fe, W) for nuclear recoil damage and implantation of a large array of ions including gasses for well-controlled modelling-oriented experiments. Several classes of materials are of interest for the nuclear industry ranging from metals and alloys, to oxides or glasses and carbides. This paper gives selected examples that illustrate the use of JANNuS ion beams in investigating the radiation resistance of structural materials for today's and tomorrow's nuclear reactors.", "prompt": "Ion accelerators have been used by material scientists for decades to investigate radiation damage formation in nuclear materials and thus to emulate neutron-induced changes. The versatility of conditions in terms of particle energy, dose rate, fluence, etc., is a key asset of ion beams allowing for fully instrumented analytical studies. In addition,", "orig": "very short irradiation times and handling of non-radioactive samples dramatically curtail the global cost and duration as compared to in-reactor testing. Coupling of two or more beams, use of heated/cooled sample holders, and implementation of in situ characterization and microscopy pave the way to real time observation of microstructural and property", "generated": "Ion accelerators have been used by material scientists for decades to investigate radiation damage formation in nuclear materials and thus to emulate neutron-induced changes. The versatility of conditions in terms of particle energy, dose rate, fluence, etc., is a key asset of ion beams allowing for fully instrumented analytical studies. In addition,"}
{"full_text": "When dealing with incomplete data in statistical learning, or incomplete observations in probabilistic inference, one needs to distinguish the fact that a certain event is observed from the fact that the observed event has happened. Since the modeling and computational complexities entailed by maintaining this proper distinction are often prohibitive, one asks for conditions under which it can be safely ignored. Such conditions are given by the missing at random (mar) and coarsened at random (car) assumptions. In this paper we provide an in-depth analysis of several questions relating to mar/car assumptions. Main purpose of our study is to provide criteria by which one may evaluate whether a car assumption is reasonable for a particular data collecting or observational process. This question is complicated by the fact that several distinct versions of mar/car assumptions exist. We therefore first provide an overview over these different versions, in which we highlight the distinction between distributional and coarsening variable induced versions. We show that distributional versions are less restrictive and sufficient for most applications. We then address from two different perspectives the question of when the mar/car assumption is warranted. First we provide a static analysis that characterizes the admissibility of the car assumption in terms of the support structure of the joint probability distribution of complete data and incomplete observations. Here we obtain an equivalence characterization that improves and extends a recent result by Grunwald and Halpern. We then turn to a procedural analysis that characterizes the admissibility of the car assumption in terms of procedural models for the actual data (or observation) generating process. The main result of this analysis is that the stronger coarsened completely at random (ccar) condition is arguably the most reasonable assumption, as it alone corresponds to data coarsening procedures that satisfy a natural robustness property.", "prompt": "When dealing with incomplete data in statistical learning, or incomplete observations in probabilistic inference, one needs to distinguish the fact that a certain event is observed from the fact that the observed event has happened. Since the modeling and computational complexities entailed by maintaining this proper distinction are often prohibitive, one asks for conditions under which", "orig": "it can be safely ignored. Such conditions are given by the missing at random (mar) and coarsened at random (car) assumptions. In this paper we provide an in-depth analysis of several questions relating to mar/car assumptions. Main purpose of our study is to provide criteria by which one may evaluate whether a car", "generated": "When dealing with incomplete data in statistical learning, or incomplete observations in probabilistic inference, one needs to distinguish the fact that a certain event is observed from the fact that the observed event has happened. Since the modeling and computational complexities entailed by maintaining this proper distinction are often prohibitive, one asks for conditions under which"}
{"full_text": "In robotic radiation therapy, high-energy photon beams from different directions are directed at a target within the patient. Target motion can be tracked by robotic ultrasound and then compensated by synchronous beam motion. However, moving the beams may result in beams passing through the ultrasound transducer or the robot carrying it. While this can be avoided by pausing the beam delivery, the treatment time would increase. Typically, the beams are delivered in an order which minimizes the robot motion and thereby the overall treatment time. However, this order can be changed, i.e., instead of pausing beams, other feasible beam could be delivered. We address this problem of dynamically ordering the beams by applying a model checking paradigm to select feasible beams. Since breathing patterns are complex and change rapidly, any offline model would be too imprecise. Thus, model checking must be conducted online, predicting the patient's current breathing pattern for a short amount of time and checking which beams can be delivered safely. Monitoring the treatment delivery online provides the option to reschedule beams dynamically in order to avoid pausing and hence to reduce treatment time. While human breathing patterns are complex and may change rapidly, we need a model which can be verified quickly and use approximation by a superposition of sine curves. Further, we simplify the 3D breathing motion into separate 1D models. We compensate the simplification by adding noise inside the model itself. In turn, we synchronize between the multiple models representing the different spatial directions, the treatment simulation, and corresponding verification queries. Our preliminary results show a 16.02 % to 37.21 % mean improvement on the idle time compared to a static beam schedule, depending on an additional safety margin. Note that an additional safety margin around the ultrasound robot can decrease idle times but also compromises plan quality by limiting the range of available beam directions. In contrast, the approach using online model checking maintains the plan quality. Further, we compare to a naive machine learning approach that does not achieve its goals while being harder to reason about.", "prompt": "In robotic radiation therapy, high-energy photon beams from different directions are directed at a target within the patient. Target motion can be tracked by robotic ultrasound and then compensated by synchronous beam motion. However, moving the beams may result in beams passing through the ultrasound transducer or the robot carrying it. While this can be avoided", "orig": "by pausing the beam delivery, the treatment time would increase. Typically, the beams are delivered in an order which minimizes the robot motion and thereby the overall treatment time. However, this order can be changed, i.e., instead of pausing beams, other feasible beam could be delivered. We address this problem of", "generated": "In robotic radiation therapy, high-energy photon beams from different directions are directed at a target within the patient. Target motion can be tracked by robotic ultrasound and then compensated by synchronous beam motion. However, moving the beams may result in beams passing through the ultrasound transducer or the robot carrying it. While this can be avoided"}
{"full_text": "Shadow libraries, also known as ''pirate libraries'', are online collections of copyrighted publications that have been made available for free without the permission of the copyright holders. They have gradually become key players of scientific knowledge dissemination, despite their illegality in most countries of the world. Many publishers and scientist-editors decry such libraries for their copyright infringement and loss of publication usage information, while some scholars and institutions support them, sometimes in a roundabout way, for their role in reducing inequalities of access to knowledge, particularly in low-income countries. Although there is a wealth of literature on shadow libraries, none of this have focused on its potential role in knowledge dissemination, through the open access movement. Here we analyze how shadow libraries can affect researchers' citation practices, highlighting some counter-intuitive findings about their impact on the Open Access Citation Advantage (OACA). Based on a large randomized sample, this study first shows that OA publications, including those in fully OA journals, receive more citations than their subscription-based counterparts do. However, the OACA has slightly decreased over the seven last years. The introduction of a distinction between those accessible or not via the Scihub platform among subscription-based suggest that the generalization of its use cancels the positive effect of OA publishing. The results show that publications in fully OA journals are victims of the success of Sci-hub. Thus, paradoxically, although Sci-hub may seem to facilitate access to scientific knowledge, it negatively affects the OA movement as a whole, by reducing the comparative advantage of OA publications in terms of visibility for researchers. The democratization of the use of Sci-hub may therefore lead to a vicious cycle, hindering efforts to develop full OA strategies without proposing a credible and sustainable alternative model for the dissemination of scientific knowledge.", "prompt": "Shadow libraries, also known as ''pirate libraries'', are online collections of copyrighted publications that have been made available for free without the permission of the copyright holders. They have gradually become key players of scientific knowledge dissemination, despite their illegality in most countries of the world. Many publishers and scientist-editors decry such", "orig": "libraries for their copyright infringement and loss of publication usage information, while some scholars and institutions support them, sometimes in a roundabout way, for their role in reducing inequalities of access to knowledge, particularly in low-income countries. Although there is a wealth of literature on shadow libraries, none of this have focused on its potential role", "generated": "Shadow libraries, also known as ''pirate libraries'', are online collections of copyrighted publications that have been made available for free without the permission of the copyright holders. They have gradually become key players of scientific knowledge dissemination, despite their illegality in most countries of the world. Many publishers and scientist-editors decry such"}
{"full_text": "Fingerprinting-based positioning, one of the promising indoor positioning solutions, has been broadly explored owing to the pervasiveness of sensor-rich mobile devices, the prosperity of opportunistically measurable location-relevant signals and the progress of data-driven algorithms. One critical challenge is to controland improve the quality of the reference fingerprint map (RFM), which is built at the offline stage and applied for online positioning. The key concept concerningthe quality control of the RFM is updating the RFM according to the newly measured data. Though varies methods have been proposed for adapting the RFM, they approach the problem by introducing extra-positioning schemes (e.g. PDR orUGV) and directly adjust the RFM without distinguishing whether critical changes have occurred. This paper aims at proposing an extra-positioning-free solution by making full use of the redundancy of measurable features. Loosely inspired by random sampling consensus (RANSAC), arbitrarily sampled subset of features from the online measurement are used for generating multi-resamples, which areused for estimating the intermediate locations. In the way of resampling, it can mitigate the impact of the changed features on positioning and enables to retrieve accurate location estimation. The users location is robustly computed by identifying the candidate locations from these intermediate ones using modified Jaccardindex (MJI) and the feature-wise change belief is calculated according to the world model of the RFM and the estimated variability of features. In order to validate our proposed approach, two levels of experimental analysis have been carried out. On the simulated dataset, the average change detection accuracy is about 90%. Meanwhile, the improvement of positioning accuracy within 2 m is about 20% by dropping out the features that are detected as changed when performing positioning comparing to that of using all measured features for location estimation. On the long-term collected dataset, the average change detection accuracy is about 85%.", "prompt": "Fingerprinting-based positioning, one of the promising indoor positioning solutions, has been broadly explored owing to the pervasiveness of sensor-rich mobile devices, the prosperity of opportunistically measurable location-relevant signals and the progress of data-driven algorithms. One critical challenge is to controland improve the quality of the reference fingerprint", "orig": "map (RFM), which is built at the offline stage and applied for online positioning. The key concept concerningthe quality control of the RFM is updating the RFM according to the newly measured data. Though varies methods have been proposed for adapting the RFM, they approach the problem by introducing extra-positioning schemes (", "generated": "Fingerprinting-based positioning, one of the promising indoor positioning solutions, has been broadly explored owing to the pervasiveness of sensor-rich mobile devices, the prosperity of opportunistically measurable location-relevant signals and the progress of data-driven algorithms. One critical challenge is to control and improve the quality of the reference fingerprint"}
{"full_text": "Context. Some circumstellar-interacting (CSI) supernovae (SNe) are produced by the explosions of massive stars that have lost mass shortly before the SN explosion. There is evidence that the precursors of some SNe IIn were luminous blue variable (LBV) stars. For a small number of CSI SNe, outbursts have been observed before the SN explosion. Eruptive events of massive stars are named as SN impostors (SN IMs) and whether they herald a forthcoming SN or not is still unclear. The large variety of observational properties of CSI SNe suggests the existence of other progenitors, such as red supergiant (RSG) stars with superwinds. Furthermore, the role of metallicity in the mass loss of CSI SN progenitors is still largely unexplored. Aims. Our goal is to gain insight on the nature of the progenitor stars of CSI SNe by studying their environments, in particular the metallicity at their locations. Methods. We obtain metallicity measurements at the location of 60 transients (including SNe IIn, SNe Ibn, and SN IMs), via emission-line diagnostic on optical spectra obtained at the Nordic Optical Telescope and through public archives. Metallicity values from the literature complement our sample. We compare the metallicity distributions among the different CSI SN subtypes and to those of other core-collapse SN types. We also search for possible correlations between metallicity and CSI SN observational properties. Results. We find that SN IMs tend to occur in environments with lower metallicity than those of SNe IIn. Among SNe IIn, SN IIn-L(1998S-like) SNe show higher metallicities, similar to those of SNe IIL/P, whereas long-lasting SNe IIn (1988Z-like) show lower metallicities, similar to those of SN IMs. The metallicity distribution of SNe IIn can be reproduced by combining the metallicity distributions of SN IMs (that may be produced by major outbursts of massive stars like LBVs) and SNe IIP (produced by RSGs). The same applies to the distributions of the Normalized Cumulative Rank (NCR) values, which quantifies the SN association to H II regions. For SNe IIn, we find larger mass-loss rates and higher CSM velocities at higher metallicities. The luminosity increment in the optical bands during SN IM outbursts tend to be larger at higher metallicity, whereas the SN IM quiescent optical luminosities tend to be lower. Conclusions. The difference in metallicity between SNe IIn and SN IMs suggests that LBVs are only one of the progenitor channels for SNe IIn, with 1988Z-like and 1998S-like SNe possibly arising from LBVs and RSGs, respectively. Finally, even though linedriven winds likely do not primarily drive the late mass-loss of CSI SN progenitors, metallicity has some impact on the observational properties of these transients. Key words. supernovae: general - stars: evolution - galaxies: abundances", "prompt": "Context. Some circumstellar-interacting (CSI) supernovae (SNe) are produced by the explosions of massive stars that have lost mass shortly before the SN explosion. There is evidence that the precursors of some SNe IIn were luminous blue variable (LBV) stars. For a small number", "orig": "of CSI SNe, outbursts have been observed before the SN explosion. Eruptive events of massive stars are named as SN impostors (SN IMs) and whether they herald a forthcoming SN or not is still unclear. The large variety of observational properties of CSI SNe suggests the existence of other progenitors", "generated": "Context. Some circumstellar-interacting (CSI) supernovae (SNe) are produced by the explosions of massive stars that have lost mass shortly before the SN explosion. There is evidence that the precursors of some SNe IIn were luminous blue variable (LBV) stars. For a small number"}
{"full_text": "The electrochemical behaviour of the biomedical and metallic alloys, especially in the orthopaedic implants fields, raises many questions. This study is dedicated for studying the Ti-6Al-4V alloy, by electrochemical impedance spectroscopy, EIS, in various physiological media,: Ringer solution, phosphate buffered solution (PBS), PBS solution and albumin, PBS solution with calf serum and PBS solution with calf serum and an antioxidant (sodium azide). Moreover, the desionised water was considered as the reference solution. The tests reproducibility was investigated. The time-frequency-Module graphs highlighted that the desionised water is the most protective for the Ti-6Al-4V alloy. This biomedical alloy is the less protected in the solution constituted by PBS and albumin. The time-frequency graph allows pointing out the graphic signatures of adsorption for organic and inorganic species (differences between the modules means in studied solution and the modules mean in the reference solution). --- Le comportement \\'electrochimique des alliages m\\'etalliques biom\\'edicaux, notamment dans le domaine des implants orthop\\'ediques, pose encore de nombreuses questions. Ce travail propose d'\\'etudier l'alliage de titane Ti-6Al-4V, par spectroscopie d'imp\\'edance \\'electrochimique, SIE, dans diff\\'erents milieux physiologiques : solution de Ringer, solution \\`a base d'un tampon phosphate (PBS), solution PBS avec de l'albumine, solution PBS avec du s\\'erum bovin et une solution PBS avec du s\\'erum bovin et un antioxydant (azoture de sodium). De plus, une solution d'eau ultra-pure servira de r\\'ef\\'erence. La reproductibilit\\'e des tests a \\'et\\'e \\'etudi\\'ee. Les repr\\'esentations temps-fr\\'equence des modules ont mis en \\'evidence que l'eau d\\'esionis\\'ee est la solution qui pr\\'esente le caract\\`ere le plus protecteur pour le Ti-6Al-4V. Cet alliage de titane est le moins prot\\'eg\\'e dans la solution de PBS contenant de l'albumine. Cette repr\\'esentation permet de mettre en \\'evidence des signatures graphiques d'adsorption des esp\\`eces inorganiques et organiques (diff\\'erences entre les moyennes des modules dans les solutions \\'etudi\\'ees et la moyenne des modules dans la solution de r\\'ef\\'erence).", "prompt": "The electrochemical behaviour of the biomedical and metallic alloys, especially in the orthopaedic implants fields, raises many questions. This study is dedicated for studying the Ti-6Al-4V alloy, by electrochemical impedance spectroscopy, EIS, in various physiological media,: Ringer solution, phosphate buffered solution (", "orig": "PBS), PBS solution and albumin, PBS solution with calf serum and PBS solution with calf serum and an antioxidant (sodium azide). Moreover, the desionised water was considered as the reference solution. The tests reproducibility was investigated. The time-frequency-Module graphs highlighted that the desionised water is", "generated": "The electrochemical behaviour of the biomedical and metallic alloys, especially in the orthopaedic implants fields, raises many questions. This study is dedicated for studying the Ti-6Al-4V alloy, by electrochemical impedance spectroscopy, EIS, in various physiological media, Ringer solution, phosphate buffered solution ("}
{"full_text": "Lattice QCD with background magnetic fields is used to calculate the magnetic moments and magnetic polarizabilities of the nucleons and of light nuclei with $A\\le4$, along with the cross-section for the $M1$ transition $np\\rightarrow d\\gamma$, at the flavor SU(3)-symmetric point where the pion mass is $m_\\pi\\sim 806$ MeV. These magnetic properties are extracted from nucleon and nuclear energies in six uniform magnetic fields of varying strengths. The magnetic moments are presented in a recent Letter. For the charged states, the extraction of the polarizability requires careful treatment of Landau levels, which enter non-trivially in the method that is employed. The nucleon polarizabilities are found to be of similar magnitude to their physical values, with $\\beta_p=5.22(+0.66/-0.45)(0.23) \\times 10^{-4}$ fm$^3$ and $\\beta_n=1.253(+0.056/-0.067)(0.055) \\times 10^{-4}$ fm$^3$, exhibiting a significant isovector component. The dineutron is bound at these heavy quark masses and its magnetic polarizability, $\\beta_{nn}=1.872(+0.121/-0.113)(0.082) \\times 10^{-4}$ fm$^3$ differs significantly from twice that of the neutron. A linear combination of deuteron scalar and tensor polarizabilities is determined by the energies of the $j_z=\\pm 1$ deuteron states, and is found to be $\\beta_{d,\\pm 1}=4.4(+1.6/-1.5)(0.2) \\times 10^{-4}$ fm$^3$. The magnetic polarizabilities of the three-nucleon and four-nucleon systems are found to be positive and similar in size to those of the proton, $\\beta_{^{3}\\rm He}=5.4(+2.2/-2.1)(0.2) \\times 10^{-4}$ fm$^3$, $\\beta_{^{3}\\rm H}=2.6(1.7)(0.1) \\times 10^{-4}$ fm$^3$, $\\beta_{^{4}\\rm He}=3.4(+2.0/-1.9)(0.2) \\times 10^{-4}$ fm$^3$. Mixing between the $j_z=0$ deuteron state and the spin-singlet $np$ state induced by the background magnetic field is used to extract the short-distance two-nucleon counterterm, ${\\bar L}_1$, of the pionless effective theory for $NN$ systems (equivalent to the meson-exchange current contribution in nuclear potential models), that dictates the cross-section for the $np\\to d\\gamma$ process near threshold. Combined with previous determinations of NN scattering parameters, this enables an ab initio determination of the threshold cross-section at these unphysical masses.", "prompt": "Lattice QCD with background magnetic fields is used to calculate the magnetic moments and magnetic polarizabilities of the nucleons and of light nuclei with $A\\le4$, along with the cross-section for the $M1$ transition $np\\rightarrow d\\gamma$, at the flavor SU(3)-sym", "orig": "metric point where the pion mass is $m_\\pi\\sim 806$ MeV. These magnetic properties are extracted from nucleon and nuclear energies in six uniform magnetic fields of varying strengths. The magnetic moments are presented in a recent Letter. For the charged states, the extraction of the polarizability requires careful", "generated": "Lattice QCD with background magnetic fields is used to calculate the magnetic moments and magnetic polarizabilities of the nucleons and of light nuclei with $A\\le4$, along with the cross-section for the $M1$ transition $np\\rightarrow d\\gamma$, at the flavor SU(3)-sym"}
{"full_text": "Degenerate ignition of helium in low-mass stars at the end of the red giant branch phase leads to dynamic convection in their helium cores. One-dimensional (1D) stellar modeling of this intrinsically multi-dimensional dynamic event is likely to be inadequate. Previous hydrodynamic simulations imply that the single convection zone in the helium core of metal-rich Pop I stars grows during the flash on a dynamic timescale. This may lead to hydrogen injection into the core, and a double convection zone structure as known from one-dimensional core helium flash simulations of low-mass Pop III stars. We perform hydrodynamic simulations of the core helium flash in two and three dimensions to better constrain the nature of these events. To this end we study the hydrodynamics of convection within the helium cores of a 1.25 \\Msun metal-rich Pop I star (Z=0.02), and a 0.85 \\Msun metal-free Pop III star (Z=0) near the peak of the flash. These models possess single and double convection zones, respectively. We use 1D stellar models of the core helium flash computed with state-of-the-art stellar evolution codes as initial models for our multidimensional hydrodynamic study, and simulate the evolution of these models with the Riemann solver based hydrodynamics code Herakles which integrates the Euler equations coupled with source terms corresponding to gravity and nuclear burning. The hydrodynamic simulation of the Pop I model involving a single convection zone covers 27 hours of stellar evolution, while the first hydrodynamic simulations of a double convection zone, in the Pop III model, span 1.8 hours of stellar life. We find differences between the predictions of mixing length theory and our hydrodynamic simulations. The simulation of the single convection zone in the Pop I model shows a strong growth of the size of the convection zone due to turbulent entrainment. Hence we predict that for the Pop I model a hydrogen injection phase (i.e. hydrogen injection into the helium core) will commence after about 23 days, which should eventually lead to a double convection zone structure known from 1D stellar modeling of low-mass Pop III stars. Our two and three-dimensional hydrodynamic simulations of the double (Pop III) convection zone model show that the velocity field in the convection zones is different from that predicted by stellar evolutionary calculations. The simulations suggest that the double convection zone decays quickly, the flow eventually being dominated by internal gravity waves.", "prompt": "Degenerate ignition of helium in low-mass stars at the end of the red giant branch phase leads to dynamic convection in their helium cores. One-dimensional (1D) stellar modeling of this intrinsically multi-dimensional dynamic event is likely to be inadequate. Previous hydrodynamic simulations imply that the single convection zone", "orig": "in the helium core of metal-rich Pop I stars grows during the flash on a dynamic timescale. This may lead to hydrogen injection into the core, and a double convection zone structure as known from one-dimensional core helium flash simulations of low-mass Pop III stars. We perform hydrodynamic simulations of the core helium flash", "generated": "Degenerate ignition of helium in low-mass stars at the end of the red giant branch phase leads to dynamic convection in their helium cores. One-dimensional (1D) stellar modeling of this intrinsically multi-dimensional dynamic event is likely to be inadequate. Previous hydrodynamic simulations imply that the single convection zone"}
{"full_text": "We report here the observation of a surprising phenomenon consisting in a oscillating phase transition which appears in a binary mixture, PMMA/3-octanone, when this is enlightened by a strongly focused infrared laser beam. PMMA/3-octanone has a UCST (Upper Critical Solution Temperature) which presents a critical point at temperature Tc = 306.6 K and volume fraction $\\phi$c = 12.8 % [Crauste et al., ArXiv 1310.6720, 2012]. This oscillatory phenomenon appears because of thermophoretic and electrostriction effects and non-linear diffusion. We analyze these oscillations and we propose a simple model which includes the minimal ingredients to produce the oscillatory behavior. Phase transitions in binary mixtures are still a widely studied subject, specifically near the critical point where several interesting and not completely understood phenomena may appear, among them we recall the critical Casimir forces [2],[3], confinement effects [4], [5] and out-of-equilibrium dynamics after a quench. The perturbation of the binary mixtures by mean of external fields is also an important and recent field of investigation [6]. For example, a laser can induce interesting phenomena in demixing binary mixtures because the radiation pressure can deform the interface between the two phases and it can be used to measure the interface tension [7]. Depending on the nature of the binary mixtures, laser illumination can also lead to a mixing or demixing transition. In ref.[8], focused infrared laser light heats the medium initially in the homogeneous phase and causes a separation in the LCST (Low Critical Solution Temperature) system. The radiation pressure gradients in a laser beam also contribute in the aggregation of polymers , thus producing a phase transition. The local heating may induce thermophoretic forces which attract towards the laser beam one of the binary-mixture components [9]. Other forces like electrostriction can also be involved [10]. In this letter, we report a new phenomenon, which consists in an oscillating phase transition induced by a constant illumination from an infrared laser beam in the heterogeneous region of an UCST (Upper Critical Solution Temperature) binary mixture. Oscillation phenomena in phase transition have already been reported in slow cooling UCST [11],[12] but as far as we know, never induced by a stationary laser illumination. After describing our experimental setup , we will present the results. Then we will use a very simplified model which contains the main necessary physical ingredients to induce this oscillation phenomenon.", "prompt": "We report here the observation of a surprising phenomenon consisting in a oscillating phase transition which appears in a binary mixture, PMMA/3-octanone, when this is enlightened by a strongly focused infrared laser beam. PMMA/3-octanone has a UCST (Upper Critical Solution Temperature) which", "orig": "presents a critical point at temperature Tc = 306.6 K and volume fraction $\\phi$c = 12.8 % [Crauste et al., ArXiv 1310.6720, 2012]. This oscillatory phenomenon appears because of thermophoretic and electrostriction effects and non", "generated": "We report here the observation of a surprising phenomenon consisting in an oscillating phase transition which appears in a binary mixture, PMMA/3-octanone, when this is enlightened by a strongly focused infrared laser beam. PMMA/3-octanone has a UCST (Upper Critical Solution Temperature) which"}
{"full_text": "To sustain metabolism, intracellular ATP concentration must be regulated within an appropriate range. This coordination is achieved through the function of the AMP-activated protein kinase (AMPK), a cellular \"fuel gauge\" that is expressed in essentially all eukaryotic cells as heterotrimeric complexes containing catalytic $\\alpha$ subunits and regulatory $\\beta$ and $\\gamma$ subunits. When cellular energy status has been compromised, AMPK is activated by increases in AMP:ATP or ADP:ATP ratios and acts to restore energy homeostasis by stimulating energy production via catabolic pathways while decreasing non-essential energy-consuming pathways. Although the primary function of AMPK is to regulate energy homeostasis at a cell-autonomous level, in multicellular organisms, the AMPK system has evolved to interact with hormones to regulate energy intake and expenditure at the whole body level. Thus, AMPK functions as a signaling hub, coordinating anabolic and catabolic pathways to balance nutrient supply with energy demand at both the cellular and whole-body levels. AMPK is activated by various metabolic stresses such as ischemia or hypoxia or glucose deprivation and has both acute and long-term effects on metabolic pathways and key cellular functions. In addition, AMPK appears to be a major sensor of energy demand in exercising muscle and acts both as a multitask gatekeeper and an energy regulator in skeletal muscle. Acute activation of AMPK has been shown to promote glucose transport and fatty acid oxidation while suppressing glycogen synthase activity and protein synthesis. Chronic activation of AMPK induces a shift in muscle fiber type composition, reduces markers of muscle degeneration and enhances muscle oxidative capacity potentially by stimulating mitochondrial biogenesis. Furthermore, recent evidence demonstrates that AMPK may not only regulate metabolism during exercise but also in the recovery phase. AMPK acts as a molecular transducer between exercise and insulin signaling and is necessary for the ability of prior contraction/exercise to increase muscle insulin sensitivity. Based on these observations, drugs that activate AMPK might be expected to be useful in the treatment of metabolic disorders and insulin resistance in various conditions.", "prompt": "To sustain metabolism, intracellular ATP concentration must be regulated within an appropriate range. This coordination is achieved through the function of the AMP-activated protein kinase (AMPK), a cellular \"fuel gauge\" that is expressed in essentially all eukaryotic cells as heterotrimeric complexes containing catalytic $\\alpha$", "orig": "subunits and regulatory $\\beta$ and $\\gamma$ subunits. When cellular energy status has been compromised, AMPK is activated by increases in AMP:ATP or ADP:ATP ratios and acts to restore energy homeostasis by stimulating energy production via catabolic pathways while decreasing non-essential energy-consuming", "generated": "To sustain metabolism, intracellular ATP concentration must be regulated within an appropriate range. This coordination is achieved through the function of the AMP-activated protein kinase (AMPK), a cellular \"fuel gauge\" that is expressed in essentially all eukaryotic cells as heterotrimeric complexes containing catalytic $\\alpha$"}
{"full_text": "Context. Recent studies show that developers spend most of their programming time testing, verifying and debugging software. As applications become more and more complex, developers demand more advanced debugging support to ease the software development process. Inquiry. Since the 70's many debugging solutions were introduced. Amongst them, online debuggers provide a good insight on the conditions that led to a bug, allowing inspection and interaction with the variables of the program. However, most of the online debugging solutions introduce \\textit{debugging interference} to the execution of the program, i.e. pauses, latency, and evaluation of code containing side-effects. Approach. This paper investigates a novel debugging technique called \\outofplace debugging. The goal is to minimize the debugging interference characteristic of online debugging while allowing online remote capabilities. An \\outofplace debugger transfers the program execution and application state from the debugged application to the debugger application, both running in different processes. Knowledge. On the one hand, \\outofplace debugging allows developers to debug applications remotely, overcoming the need of physical access to the machine where the debugged application is running. On the other hand, debugging happens locally on the remote machine avoiding latency. That makes it suitable to be deployed on a distributed system and handle the debugging of several processes running in parallel. Grounding. We implemented a concrete out-of-place debugger for the Pharo Smalltalk programming language. We show that our approach is practical by performing several benchmarks, comparing our approach with a classic remote online debugger. We show that our prototype debugger outperforms by a 1000 times a traditional remote debugger in several scenarios. Moreover, we show that the presence of our debugger does not impact the overall performance of an application. Importance. This work combines remote debugging with the debugging experience of a local online debugger. Out-of-place debugging is the first online debugging technique that can minimize debugging interference while debugging a remote application. Yet, it still keeps the benefits of online debugging ( e.g. step-by-step execution). This makes the technique suitable for modern applications which are increasingly parallel, distributed and reactive to streams of data from various sources like sensors, UI, network, etc.", "prompt": "Context. Recent studies show that developers spend most of their programming time testing, verifying and debugging software. As applications become more and more complex, developers demand more advanced debugging support to ease the software development process. Inquiry. Since the 70's many debugging solutions were introduced. Amongst them, online debuggers provide a", "orig": "good insight on the conditions that led to a bug, allowing inspection and interaction with the variables of the program. However, most of the online debugging solutions introduce \\textit{debugging interference} to the execution of the program, i.e. pauses, latency, and evaluation of code containing side-effects. Approach. This", "generated": "Context. Recent studies show that developers spend most of their programming time testing, verifying and debugging software. As applications become more and more complex, developers demand more advanced debugging support to ease the software development process. Inquiry. Since the 70's many debugging solutions were introduced. Amongst them, online debuggers provide a"}
{"full_text": "Given a universe of discourse X-a domain of possible outcomes-an experiment may consist of selecting one of its elements, subject to the operation of chance, or of observing the elements, subject to imprecision. A priori uncertainty about the actual result of the experiment may be quantified, representing either the likelihood of the choice of :r_X or the degree to which any such X would be suitable as a description of the outcome. The former case corresponds to a probability distribution, while the latter gives a possibility assignment on X. The study of such assignments and their properties falls within the purview of possibility theory [DP88, Y80, Z783. It, like probability theory, assigns values between 0 and 1 to express likelihoods of outcomes. Here, however, the similarity ends. Possibility theory uses the maximum and minimum functions to combine uncertainties, whereas probability theory uses the plus and times operations. This leads to very dissimilar theories in terms of analytical framework, even though they share several semantic concepts. One of the shared concepts consists of expressing quantitatively the uncertainty associated with a given distribution. In probability theory its value corresponds to the gain of information that would result from conducting an experiment and ascertaining an actual result. This gain of information can equally well be viewed as a decrease in uncertainty about the outcome of an experiment. In this case the standard measure of information, and thus uncertainty, is Shannon entropy [AD75, G77]. It enjoys several advantages-it is characterized uniquely by a few, very natural properties, and it can be conveniently used in decision processes. This application is based on the principle of maximum entropy; it has become a popular method of relating decisions to uncertainty. This paper demonstrates that an equally integrated theory can be built on the foundation of possibility theory. We first show how to define measures of in formation and uncertainty for possibility assignments. Next we construct an information-based metric on the space of all possibility distributions defined on a given domain. It allows us to capture the notion of proximity in information content among the distributions. Lastly, we show that all the above constructions can be carried out for continuous distributions-possibility assignments on arbitrary measurable domains. We consider this step very significant-finite domains of discourse are but approximations of the real-life infinite domains. If possibility theory is to represent real world situations, it must handle continuous distributions both directly and through finite approximations. In the last section we discuss a principle of maximum uncertainty for possibility distributions. We show how such a principle could be formalized as an inference rule. We also suggest it could be derived as a consequence of simple assumptions about combining information. We would like to mention that possibility assignments can be viewed as fuzzy sets and that every fuzzy set gives rise to an assignment of possibilities. This correspondence has far reaching consequences in logic and in control theory. Our treatment here is independent of any special interpretation; in particular we speak of possibility distributions and possibility measures, defining them as measurable mappings into the interval [0, 1]. Our presentation is intended as a self-contained, albeit terse summary. Topics discussed were selected with care, to demonstrate both the completeness and a certain elegance of the theory. Proofs are not included; we only offer illustrative examples.", "prompt": "Given a universe of discourse X-a domain of possible outcomes-an experiment may consist of selecting one of its elements, subject to the operation of chance, or of observing the elements, subject to imprecision. A priori uncertainty about the actual result of the experiment may be quantified, representing either the likelihood of the choice", "orig": "of :r_X or the degree to which any such X would be suitable as a description of the outcome. The former case corresponds to a probability distribution, while the latter gives a possibility assignment on X. The study of such assignments and their properties falls within the purview of possibility theory [DP88, Y80,", "generated": "A universe of discourse X-a domain of possible outcomes-an experiment may consist of selecting one of its elements, subject to the operation of chance, or of observing the elements, subject to imprecision. A priori uncertainty about the actual result of the experiment may be quantified, representing either the likelihood of the choice made"}
{"full_text": "We have performed mm-wave observations with the IRAM 30m telescope of the 12CO J=2-1 and J=1-0, 13CO J=2-1 and J=1-0, and SiO J=5-4 transitions in the symbiotic stars R Aqr, CH Cyg, and HM Sge. The data were analyzed by means of a simple analytical description of the general properties of molecular emission from the inner shells around the cool star. Numerical calculations of the expected line profiles, taking into account the level population and radiative transfer under such conditions, were also performed. Weak emission of 12CO J=2-1 and J=1-0 was detected in R Aqr and CH Cyg; a good line profile of 12CO J=2-1 in R Aqr was obtained. The intensities and profile shapes of the detected lines are compatible with emission coming from a very small shell around the Mira-type star, with a radius comparable to or slightly smaller than the distance to the hot dwarf companion, 10$^{14}$ - 2 10$^{14}$ cm. We argue that other possible explanations are improbable. This region probably shows properties similar to those characteristic of the inner shells around standard AGB stars: outwards expansion at about 5 - 25 km/s, with a significant acceleration of the gas, temperatures decreasing with radius between about 1000 and 500 K, and densities ~ 10$^9$ - 3 10$^8$ cm$^{-3}$. Our model calculations are able to explain the asymmetric line shape observed in 12CO J=2-1 from R Aqr, in which the relatively weaker red part of the profile would result from selfabsorption by the outer layers (in the presence of a velocity increase and a temperature decrease with radius). The mass-loss rates are somewhat larger than in standard AGB stars, as often happens for symbiotic systems. In R Aqr, we find that the total mass of the CO emitting region is ~ 2 - 3 10$^{-5}$ Mo, corresponding to M' ~ 5 10$^{-6}$ - 10$^{-5}$ Mo/yr, and compatible with results obtained from dust emission. Taking into account other existing data on molecular emission, we suggest that the small extent of the molecule-rich gas in symbiotic systems is mainly due to molecule photodissociation by the radiation of the hot dwarf star.", "prompt": "We have performed mm-wave observations with the IRAM 30m telescope of the 12CO J=2-1 and J=1-0, 13CO J=2-1 and J=1-0, and SiO J=5-4 transitions in the symbiotic stars R Aqr", "orig": ", CH Cyg, and HM Sge. The data were analyzed by means of a simple analytical description of the general properties of molecular emission from the inner shells around the cool star. Numerical calculations of the expected line profiles, taking into account the level population and radiative transfer under such conditions, were also performed.", "generated": "We have performed mm-wave observations with the IRAM 30m telescope of the 12CO J=2-1 and J=1-0, 13CO J=2-1 and J=1-0, and SiO J=5-4 transitions in the symbiotic stars R Aqr"}
{"full_text": "This article focuses on liquefaction of saturated granular soils, triggered by earthquakes. Liquefaction is definedhere as the transition from a rigid state, in which the granular soil layer supports structures placed on its surface, toa fluidlike state, in which structures placed initially on the surface sink to their isostatic depth within the granularlayer.We suggest a simple theoretical model for soil liquefaction and show that buoyancy caused by the presence ofwater inside a granular medium has a dramatic influence on the stability of an intruder resting at the surface of themedium.We confirm this hypothesis by comparison with laboratory experiments and discrete-element numericalsimulations. The external excitation representing ground motion during earthquakes is simulated via horizontalsinusoidal oscillations of controlled frequency and amplitude. In the experiments, we use particles only slightlydenser than water, which as predicted theoretically increases the effect of liquefaction and allows clear depth-of-sinkingmeasurements. In the simulations, a micromechanical model simulates grains using molecular dynamicswith friction between neighbors. The effect of the fluid is captured by taking into account buoyancy effects onthe grains when they are immersed. We show that the motion of an intruder inside a granular medium is mainlydependent on the peak acceleration of the ground motion and establish a phase diagram for the conditions underwhich liquefaction happens, depending on the soil bulk density, friction properties, presence of water, and peak acceleration of the imposed large-scale soil vibrations.We establish that in liquefaction conditions, most cases relaxtoward an equilibrium position following an exponential in time.We also show that the equilibrium position itself,for most liquefaction regimes, corresponds to the isostatic equilibrium of the intruder inside a medium of effectivedensity. The characteristic time to relaxation is shown to be essentially a function of the peak ground velocity.", "prompt": "This article focuses on liquefaction of saturated granular soils, triggered by earthquakes. Liquefaction is definedhere as the transition from a rigid state, in which the granular soil layer supports structures placed on its surface, toa fluidlike state, in which structures placed initially on the surface sink to their isost", "orig": "atic depth within the granularlayer.We suggest a simple theoretical model for soil liquefaction and show that buoyancy caused by the presence ofwater inside a granular medium has a dramatic influence on the stability of an intruder resting at the surface of themedium.We confirm this hypothesis by comparison with laboratory experiments and discrete-element numerical", "generated": "This article focuses on liquefaction of saturated granular soils, triggered by earthquakes. Liquefaction is defined here as the transition from a rigid state, in which the granular soil layer supports structures placed on its surface, to a fluidlike state, in which structures placed initially on the surface sink to their initial foundation"}
{"full_text": "We study the circumstellar environment of the M-type AGB star RT Vir using mid-infrared high spatial resolution observations from the ESO-VLTI focal instrument MIDI. The aim of this study is to provide observational constraints on theoretical prediction that the winds of M-type AGB objects can be driven by photon scattering on iron-free silicate grains located in the close environment (about 2 to 3 stellar radii) of the star. We interpreted spectro-interferometric data, first using wavelength-dependent geometric models. We then used a self-consistent dynamic model atmosphere containing a time-dependent description of grain growth for pure forsterite dust particles to reproduce the photometric, spectrometric, and interferometric measurements of RT Vir. Since the hydrodynamic computation needs stellar parameters as input, a considerable effort was first made to determine these parameters. MIDI differential phases reveal the presence of an asymmetry in the stellar vicinity. Results from the geometrical modeling give us clues to the presence of aluminum and silicate dust in the close circumstellar environment (< ~5 stellar radii). Comparison between spectro-interferometric data and a self-consistent dust-driven wind model reveals that silicate dust has to be present in the region between 2 to 3 stellar radii to reproduce the 59 and 63 m baseline visibility measurements around 9.8 micron. This gives additional observational evidence in favor of winds driven by photon scattering on iron-free silicate grains located in the close vicinity of an M-type star. However, other sources of opacity are clearly missing to reproduce the 10-13 micron visibility measurements for all baselines. This study is a first attempt to understand the wind mechanism of M-type AGB stars by comparing photometric, spectrometric, and interferometric measurements with state-of-the-art, self-consistent dust-driven wind models. The agreement of the dynamic model atmosphere with interferometric measurements in the 8-10 micron spectral region gives additional observational evidence that the winds of M-type stars can be driven by photon scattering on iron-free silicate grains. Finally, a larger statistical study and progress in advanced self-consistent 3D modeling are still required to solve the remaining problems.", "prompt": "We study the circumstellar environment of the M-type AGB star RT Vir using mid-infrared high spatial resolution observations from the ESO-VLTI focal instrument MIDI. The aim of this study is to provide observational constraints on theoretical prediction that the winds of M-type AGB objects can be driven by photon scattering on", "orig": "iron-free silicate grains located in the close environment (about 2 to 3 stellar radii) of the star. We interpreted spectro-interferometric data, first using wavelength-dependent geometric models. We then used a self-consistent dynamic model atmosphere containing a time-dependent description of grain growth for pure forsterite dust", "generated": "We study the circumstellar environment of the M-type AGB star RT Vir using mid-infrared high spatial resolution observations from the ESO-VLTI focal instrument MIDI. The aim of this study is to provide observational constraints on theoretical prediction that the winds of M-type AGB objects can be driven by photon scattering off"}
{"full_text": "Deterministic and nondeterministic finite automata with translucent letters were introduced by Nagy and Otto more than a decade ago as Cooperative Distributed systems of a kind of stateless restarting automata with window size one. These finite state machines have a surprisingly large expressive power: all commutative semi-linear languages and all rational trace languages can be accepted by them including various not context-free languages. While the nondeterministic variant defines a language class with nice closure properties, the deterministic variant is weaker, however it contains all regular languages, some non-regular context-free languages, as the Dyck language, and also some languages that are not even context-free. In all those models for each state, the letters of the alphabet could be in one of the following categories: the automaton cannot see the letter (it is translucent), there is a transition defined on the letter (maybe more than one transitions in nondeterministic case) or none of the above categories (the automaton gets stuck by seeing this letter at the given state and this computation is not accepting). State-deterministic automata are recent models, where the next state of the computation determined by the structure of the automata and it is independent of the processed letters. In this paper our aim is twofold, on the one hand, we investigate state-deterministic finite automata with translucent letters. These automata are specially restricted deterministic finite automata with translucent letters. In the other novel model we present, it is allowed that for a state the set of translucent letters and the set of letters for which transition is defined are not disjoint. One can interpret this fact that the automaton has a nondeterministic choice for each occurrence of such letters to see them (and then erase and make the transition) or not to see that occurrence at that time. Based on these semi-translucent letters, the expressive power of the automata increases, i.e., in this way a proper generalization of the previous models is obtained.", "prompt": "Deterministic and nondeterministic finite automata with translucent letters were introduced by Nagy and Otto more than a decade ago as Cooperative Distributed systems of a kind of stateless restarting automata with window size one. These finite state machines have a surprisingly large expressive power: all commutative semi-linear languages and all rational", "orig": "trace languages can be accepted by them including various not context-free languages. While the nondeterministic variant defines a language class with nice closure properties, the deterministic variant is weaker, however it contains all regular languages, some non-regular context-free languages, as the Dyck language, and also some languages that are not even", "generated": "Deterministic and nondeterministic finite automata with translucent letters were introduced by Nagy and Otto more than a decade ago as Cooperative Distributed systems of a kind of stateless restarting automata with window size one. These finite state machines have a surprisingly large expressive power: all commutative semi-linear languages and all rational"}
{"full_text": "Symmetric tensor decomposition is an important problem with applications in several areas for example signal processing, statistics, data analysis and computational neuroscience. It is equivalent to Waring's problem for homogeneous polynomials, that is to write a homogeneous polynomial in n variables of degree D as a sum of D-th powers of linear forms, using the minimal number of summands. This minimal number is called the rank of the polynomial/tensor. We focus on decomposing binary forms, a problem that corresponds to the decomposition of symmetric tensors of dimension 2 and order D. Under this formulation, the problem finds its roots in invariant theory where the decompositions are known as canonical forms. In this context many different algorithms were proposed. We introduce a superfast algorithm that improves the previous approaches with results from structured linear algebra. It achieves a softly linear arithmetic complexity bound. To the best of our knowledge, the previously known algorithms have at least quadratic complexity bounds. Our algorithm computes a symbolic decomposition in $O(M(D) log(D))$ arithmetic operations, where $M(D)$ is the complexity of multiplying two polynomials of degree D. It is deterministic when the decomposition is unique. When the decomposition is not unique, our algorithm is randomized. We present a Monte Carlo version of it and we show how to modify it to a Las Vegas one, within the same complexity. From the symbolic decomposition, we approximate the terms of the decomposition with an error of $2^{--$\\epsilon$}$ , in $O(D log^2(D) (log^2(D) + log($\\epsilon$)))$ arithmetic operations. We use results from Kaltofen and Yagati (1989) to bound the size of the representation of the coefficients involved in the decomposition and we bound the algebraic degree of the problem by min(rank, D -- rank + 1). We show that this bound can be tight. When the input polynomial has integer coefficients, our algorithm performs, up to poly-logarithmic factors, $O\\_{bit} (D{\\ell} + D^4 + D^3 $\\tau$)$ bit operations, where $$\\tau$$ is the maximum bitsize of the coefficients and $2^{--{\\ell}}$ is the relative error of the terms in the decomposition.", "prompt": "Symmetric tensor decomposition is an important problem with applications in several areas for example signal processing, statistics, data analysis and computational neuroscience. It is equivalent to Waring's problem for homogeneous polynomials, that is to write a homogeneous polynomial in n variables of degree D as a sum of D-th powers of linear forms,", "orig": "using the minimal number of summands. This minimal number is called the rank of the polynomial/tensor. We focus on decomposing binary forms, a problem that corresponds to the decomposition of symmetric tensors of dimension 2 and order D. Under this formulation, the problem finds its roots in invariant theory where the decompositions", "generated": "Symmetric tensor decomposition is an important problem with applications in several areas for example signal processing, statistics, data analysis and computational neuroscience. It is equivalent to Waring's problem for homogeneous polynomials, that is to write a homogeneous polynomial in n variables of degree D as a sum of D-th powers of linear forms."}
{"full_text": "Acute neuroprotection in numerous human clinical trials has been an abject failure. Major systemic-and procedural-based issues have subsequently been identified in both clinical trials and preclinical animal model experimentation. As well, issues related to the neuroprotective moiety itself have contributed to clinical trial failures, including late delivery, mono-targeting, low potency and poor tolerability. Conditioning (pre-or post-) strategies can potentially address these issues and are therefore gaining increasing attention as approaches to protect the brain from cerebral ischemia. In principle, conditioning can address concerns of timing (preconditioning could be pre-emptively applied in high-risk patients, and post-conditioning after patients experience an unannounced brain infarction) and signaling (multi-modal). However, acute neuroprotection and conditioning strategies face a common translational issue: a myriad of possibilities exist, but with no strategy to select optimal candidates. In this review, we argue that what is required is a neuroprotective framework to identify the \"best\" agent(s), at the earliest investigational stage possible. This may require switching mindsets from identifying how neuroprotection can be achieved to determining how neuroprotection can fail, for the vast majority of candidates. Understanding the basis for failure can in turn guide supplementary treatment, thereby forming an evidence-based rationale for selecting combinations of therapies. An appropriately designed in vitro (neuron culture, brain slices) approach, based on increasing the harshness of the ischemic-like insult, can be useful in identifying the \"best\" conditioner or acute neuroprotective therapy, as well as how the two modalities can be combined to overcome individual limitations. This would serve as a base from which to launch further investigation into therapies required to protect the neurovascular unit in in vivo animal models of cerebral ischemia. Based on these respective approaches, our laboratories suggest that there is merit in examining synaptic activity-and nutraceutical-based preconditioning / acute neuroprotection.", "prompt": "Acute neuroprotection in numerous human clinical trials has been an abject failure. Major systemic-and procedural-based issues have subsequently been identified in both clinical trials and preclinical animal model experimentation. As well, issues related to the neuroprotective moiety itself have contributed to clinical trial failures, including late delivery, mono", "orig": "-targeting, low potency and poor tolerability. Conditioning (pre-or post-) strategies can potentially address these issues and are therefore gaining increasing attention as approaches to protect the brain from cerebral ischemia. In principle, conditioning can address concerns of timing (preconditioning could be pre-emptively applied in high-risk patients,", "generated": "Acute neuroprotection in numerous human clinical trials has been an abject failure. Major systemic-and procedural-based issues have subsequently been identified in both clinical trials and preclinical animal model experimentation. As well, issues related to the neuroprotective moiety itself have contributed to clinical trial failures, including late delivery, mono"}
{"full_text": "Simplified silica (Zeosil 1165 MP) and SBR (140k carrying silanol end-groups) nanocomposites have been formulated by mixing of a reduced number of ingredients with respect to industrial applications. The thermo-mechanical history of the samples during the mixing process was monitored and adjusted to identical final temperatures. The filler structure on large scales up to micrometers was studied by transmission electron microscopy (TEM) and very small-angle X-ray scattering (SAXS). A complete quantitative model extending from the primary silica nanoparticle (of radius \\approx 10 nm), to nanoparticles aggregates, up to micrometer-sized branches with typical lateral dimension of 150 nm is proposed. Image analysis of the TEM-pictures yields the fraction of zones of pure polymer, which extend between the branches of a large-scale filler network. This network is compatible with a fractal of average dimension 2.4 as measured by scattering. On smaller length scales, inside the branches, small silica aggregates are present. Their average radius has been deduced from a Kratky analysis, and it ranges between 35 and 40 nm for all silica fractions investigated here (\\phi_si = 8-21% vol.). A central piece of our analysis is the description of the interaggregate interaction by a simulated structure factor for polydisperse spheres representing aggregates. A polydispersity of 30% in aggregate size is assumed, and interactions between these aggregates are described with a hard core repulsive potential. The same distribution in size is used to evaluate the polydisperse form factor. Comparison with the experimental intensity leads to the determination of the average aggregate compacity (assumed identical for all aggregates in the distribution, between 31% and 38% depending on \\phi_si), and thus aggregation number (ca. 45, with a large spread). Because of the effect of aggregate compacity and of pure polymer zones, the volume fraction of aggregates is higher in the branches than \\phi_si. The repulsion between aggregates has a strong effect on the apparent isothermal compressibility: it leads to a characteristic low-q depression, which cannot be interpreted as aggregate mass decrease in our data. In addition, the reinforcement effect of these silica structures in the SBR-matrix is characterized with oscillatory shear and described with a model based on the same aggregate compacity. Finally, our results show that it is possible to analyze the complex structure of interacting aggregates in nanocomposites of industrial origin in a self-consistent and quantitative manner.", "prompt": "Simplified silica (Zeosil 1165 MP) and SBR (140k carrying silanol end-groups) nanocomposites have been formulated by mixing of a reduced number of ingredients with respect to industrial applications. The thermo-mechanical history of the samples during the mixing process was monitored and adjusted to identical", "orig": "final temperatures. The filler structure on large scales up to micrometers was studied by transmission electron microscopy (TEM) and very small-angle X-ray scattering (SAXS). A complete quantitative model extending from the primary silica nanoparticle (of radius \\approx 10 nm), to nanoparticles aggregates, up to micrometer-sized", "generated": "Simplified silica (Zeosil 1165 MP) and SBR (140k carrying silanol end-groups) nanocomposites have been formulated by mixing of a reduced number of ingredients with respect to industrial applications. The thermo-mechanical history of the samples during the mixing process was monitored and adjusted to identical"}
{"full_text": "This volume contains the proceedings of the Second International Workshop Hybrid Systems and Biology (HSB 2013) held in Taormina (Italy), on September 2th, 2013. The workshop is affiliated to the 12th European Conference on Artificial Life (ECAL 2013). Systems biology aims at providing a system-level understanding of biological systems by unveiling their structure, dynamics and control methods. Due to the intrinsic multi-scale nature of these systems in space, in organization levels and in time, it is extremely difficult to model them in a uniform way, e.g., by means of differential equations or discrete stochastic processes. Furthermore, such models are often not easily amenable to formal analysis, and their simulations at the organ or even at the cell levels are frequently impractical. Indeed, an important open problem is finding appropriate computational models that scale well for both simulation and formal analysis of biological processes. Hybrid modeling techniques, combining discrete and continuous processes, are gaining more and more attention in such a context, and they have been successfully applied to capture the behavior of many biological complex systems, ranging from genetic networks, biochemical reactions, signaling pathways, cardiac tissues electro-physiology, and tumor genesis. This workshop aims at bringing together researchers in computer science, mathematics, and life sciences, interested in the opportunities and the challenges of hybrid modeling applied to systems biology. The workshop programme included the keynote presentation of Alessandro Astolfi (Imperial College of London, UK) on Immune response enhancement via hybrid control. Furthermore, 8 papers were selected out of 13 submissions by the Program Committee of HSB 2013. The papers in this volume address the hybrid modeling of a number important biological processes (iron homeostasis network, mammalian cell cycle, vascular endothelial growth factor (VEGF), genetic regulatory network in mammalian sclera) and, the formalisms and techniques for specifying and validating properties of biological systems (such as, robustness, oscillations).", "prompt": "This volume contains the proceedings of the Second International Workshop Hybrid Systems and Biology (HSB 2013) held in Taormina (Italy), on September 2th, 2013. The workshop is affiliated to the 12th European Conference on Artificial Life (ECAL 2013). Systems biology aims", "orig": "at providing a system-level understanding of biological systems by unveiling their structure, dynamics and control methods. Due to the intrinsic multi-scale nature of these systems in space, in organization levels and in time, it is extremely difficult to model them in a uniform way, e.g., by means of differential equations or discrete stochastic processes.", "generated": "This volume contains the proceedings of the Second International Workshop Hybrid Systems and Biology (HSB 2013) held in Taormina (Italy), on September 2th, 2013. The workshop is affiliated to the 12th European Conference on Artificial Life (ECAL 2013). Systems biology aims"}
{"full_text": "Multiple sequence alignment (MSA) is a ubiquitous problem in computational biology. Although it is NP-hard to find an optimal solution for an arbitrary number of sequences, due to the importance of this problem researchers are trying to push the limits of exact algorithms further. Since MSA can be cast as a classical path finding problem, it is attracting a growing number of AI researchers interested in heuristic search algorithms as a challenge with actual practical relevance. In this paper, we first review two previous, complementary lines of research. Based on Hirschbergs algorithm, Dynamic Programming needs O(kN^(k-1)) space to store both the search frontier and the nodes needed to reconstruct the solution path, for k sequences of length N. Best first search, on the other hand, has the advantage of bounding the search space that has to be explored using a heuristic. However, it is necessary to maintain all explored nodes up to the final solution in order to prevent the search from re-expanding them at higher cost. Earlier approaches to reduce the Closed list are either incompatible with pruning methods for the Open list, or must retain at least the boundary of the Closed list. In this article, we present an algorithm that attempts at combining the respective advantages; like A* it uses a heuristic for pruning the search space, but reduces both the maximum Open and Closed size to O(kN^(k-1)), as in Dynamic Programming. The underlying idea is to conduct a series of searches with successively increasing upper bounds, but using the DP ordering as the key for the Open priority queue. With a suitable choice of thresholds, in practice, a running time below four times that of A* can be expected. In our experiments we show that our algorithm outperforms one of the currently most successful algorithms for optimal multiple sequence alignments, Partial Expansion A*, both in time and memory. Moreover, we apply a refined heuristic based on optimal alignments not only of pairs of sequences, but of larger subsets. This idea is not new; however, to make it practically relevant we show that it is equally important to bound the heuristic computation appropriately, or the overhead can obliterate any possible gain. Furthermore, we discuss a number of improvements in time and space efficiency with regard to practical implementations. Our algorithm, used in conjunction with higher-dimensional heuristics, is able to calculate for the first time the optimal alignment for almost all of the problems in Reference 1 of the benchmark database BAliBASE.", "prompt": "Multiple sequence alignment (MSA) is a ubiquitous problem in computational biology. Although it is NP-hard to find an optimal solution for an arbitrary number of sequences, due to the importance of this problem researchers are trying to push the limits of exact algorithms further. Since MSA can be cast as a classical path finding problem", "orig": ", it is attracting a growing number of AI researchers interested in heuristic search algorithms as a challenge with actual practical relevance. In this paper, we first review two previous, complementary lines of research. Based on Hirschbergs algorithm, Dynamic Programming needs O(kN^(k-1)) space to store both the search frontier", "generated": "Multiple sequence alignment (MSA) is a ubiquitous problem in computational biology. Although it is NP-hard to find an optimal solution for an arbitrary number of sequences, due to the importance of this problem researchers are trying to push the limits of exact algorithms further. Since MSA can be cast as a classical path finding problem"}
{"full_text": "Context: This paper presents the concept of open programming language interpreters and the implementation of a framework-level metaobject protocol (MOP) to support them. Inquiry: We address the problem of dynamic interpreter adaptation to tailor the interpreter's behavior on the task to be solved and to introduce new features to fulfill unforeseen requirements. Many languages provide a MOP that to some degree supports reflection. However, MOPs are typically language-specific, their reflective functionality is often restricted, and the adaptation and application logic are often mixed which hardens the understanding and maintenance of the source code. Our system overcomes these limitations. Approach: We designed and implemented a system to support open programming language interpreters. The prototype implementation is integrated in the Neverlang framework. The system exposes the structure, behavior and the runtime state of any Neverlang-based interpreter with the ability to modify it. Knowledge: Our system provides a complete control over interpreter's structure, behavior and its runtime state. The approach is applicable to every Neverlang-based interpreter. Adaptation code can potentially be reused across different language implementations. Grounding: Having a prototype implementation we focused on feasibility evaluation. The paper shows that our approach well addresses problems commonly found in the research literature. We have a demonstrative video and examples that illustrate our approach on dynamic software adaptation, aspect-oriented programming, debugging and context-aware interpreters. Importance: To our knowledge, our paper presents the first reflective approach targeting a general framework for language development. Our system provides full reflective support for free to any Neverlang-based interpreter. We are not aware of any prior application of open implementations to programming language interpreters in the sense defined in this paper. Rather than substituting other approaches, we believe our system can be used as a complementary technique in situations where other approaches present serious limitations.", "prompt": "Context: This paper presents the concept of open programming language interpreters and the implementation of a framework-level metaobject protocol (MOP) to support them. Inquiry: We address the problem of dynamic interpreter adaptation to tailor the interpreter's behavior on the task to be solved and to introduce new features to fulfill unforeseen requirements", "orig": ". Many languages provide a MOP that to some degree supports reflection. However, MOPs are typically language-specific, their reflective functionality is often restricted, and the adaptation and application logic are often mixed which hardens the understanding and maintenance of the source code. Our system overcomes these limitations. Approach: We designed and", "generated": "This paper presents the concept of open programming language interpreters and the implementation of a framework-level metaobject protocol (MOP) to support them. Inquiry: We address the problem of dynamic interpreter adaptation to tailor the interpreter's behavior on the task to be solved and to introduce new features to fulfill unforeseen requirements.\n\nOpen"}
{"full_text": "In his 1944 paper Ver\\\"anderliche Riemannsche Fl\\\"achen , Teichm\\\"uller defined a structure of complex manifold on the set of isomorphism classes of marked closed Riemann surfaces of genus g. The complex manifold he obtained is the space called today Teichm\\\"uller space. In the same paper, Teichm\\\"uller introduced the so-called universal Teichm\\\"uller curve -- a space over Teichm\\\"uller space where the fiber above each point is a Riemann surface representing that point. In fact, Teichm\\\"uller proved the existence of the Teichm\\\"uller curve as a space of Riemann surfaces parametrized by an analytic space, with an existence and uniqueness theorem establishing this analytic structure. This result was later reformulated and proved by Grothendieck in a series of ten lectures he gave at Cartan's seminar in 1960-1961. In his approach , Grothendieck replaced Teichm\\\"uller's explicit parameters by a general construction of fiber bundles whose base is an arbitrary analytic space. This work on Teichm\\\"uller space led him to recast the bases of analytic geometry using the language of categories and functors. In Grothendieck's words, the Teichm\\\"uller curve becomes a space representing a functor from the category of analytic spaces into the category of sets. In this survey, we comment on Grothendieck's series of lectures. The survey is primarily addressed to low-dimensional topologists and geometers. In presenting Grothendieck's results, we tried to explain or rephrase in more simple terms some notions that are usually expressed in the language of algebraic geometry. However, it is not possible to short-circuit the language of categories and functors. The survey is also addressed to those algebraic geometers who wish to know how the notion of moduli space evolved in connection with Teichm\\\"uller theory. Explaining the origins of mathematical ideas contributes in dispensing justice to their authors and it usually renders the theory that is surveyed more attractive. The final version of this paper will appear as a chapter in Volume VI of the Handbook of Teichm\\\"uller theory. This volume is dedicated to the memory of Alexander Grothendieck.", "prompt": "In his 1944 paper Ver\\\"anderliche Riemannsche Fl\\\"achen, Teichm\\\"uller defined a structure of complex manifold on the set of isomorphism classes of marked closed Riemann surfaces of genus g. The complex manifold he obtained is the space called today Teichm", "orig": "\\\"uller space. In the same paper, Teichm\\\"uller introduced the so-called universal Teichm\\\"uller curve -- a space over Teichm\\\"uller space where the fiber above each point is a Riemann surface representing that point. In fact, Teichm\\\"uller", "generated": "In his 1944 paper Ver\\\"anderliche Riemannsche Fl\\\"achen, Teichm\\\"uller defined a structure of complex manifold on the set of isomorphism classes of marked closed Riemann surfaces of genus g. The complex manifold he obtained is the space called today Teichm"}
{"full_text": "Due to increasing numbers of product piracy worldwide, a cost-effective method for verifying the origin of a product is to be developed. For this purpose, a certificate of authenticity can be created using precisely measurable, unique properties of special physical objects that are difficult to reconstruct. In the context of the present work, this is a counterfeit-proof label composed of randomly distributed gold nanospheres or rods in a semi-transparent material. The characteristic positioning of the label's elements can be precisely measured using a smartphone's camera and additional technologies. This can create an offline usable verification method for the general public without the need for an existing network connection. The present work provides a first part of the proof of concept that such a system and especially the associated algorithmic computation method can be implemented and efficiently used in a mobile application. In addition, a method suitable in practice for transmitting and securing the required information is determined in each case. Furthermore, the results of the validation of counterfeit-proof product labels are analyzed in detail and existing weaknesses are pointed out. -- Auf Grund weltweit steigender Zahlen der Produktpiraterie soll ein kosteng\\\"unstiges Verfahren zur Verifizierung der Herkunft eines Produktes entwickelt werden. Daf\\\"ur l\\\"asst sich durch exakt messbare, einzigartige, jedoch schwer rekonstruierbare Eigenschaften spezieller physischer Objekte ein Echtheitszertifikat kreieren. Dieses ist im Kontext der vorliegenden Arbeit ein f\\\"alschungssicheres Label, das sich in einem semi-transparenten Material aus zuf\\\"allig verteilten Goldnanok\\\"ugelchen oder -st\\\"abchen zusammensetzt. Die charakteristischen Positionierungen der Elemente des Labels lassen sich mit der Kamera eines Smartphones und zus\\\"atzlichen Technologien pr\\\"azise messen. Dadurch kann f\\\"ur die breite Bev\\\"olkerung ohne die Notwendigkeit einer bestehenden Netzwerkverbindung ein offline verwendbares Verifikationsverfahren erschaffen werden. Die vorliegende Arbeit liefert einen ersten Teil des Machbarkeitsnachweises, dass ein derartiges System und insbesondere das damit einhergehende algorithmische Berechnungsverfahren in einer mobilen Applikation implementier -- und effizient einsetzbar ist. Zudem wird je eine in der Praxis geeignete Methode zur \\\"Ubermittlung und Sicherung der ben\\\"otigten Informationen eruiert. Des Weiteren werden die Resultate der Validierung von f\\\"alschungssicheren Produktlabeln ausf\\\"uhrlich analysiert und vorhandene Schw\\\"achen aufgezeigt.", "prompt": "Due to increasing numbers of product piracy worldwide, a cost-effective method for verifying the origin of a product is to be developed. For this purpose, a certificate of authenticity can be created using precisely measurable, unique properties of special physical objects that are difficult to reconstruct. In the context of the present work, this is a", "orig": "counterfeit-proof label composed of randomly distributed gold nanospheres or rods in a semi-transparent material. The characteristic positioning of the label's elements can be precisely measured using a smartphone's camera and additional technologies. This can create an offline usable verification method for the general public without the need for an existing network connection. The present work", "generated": "Due to increasing numbers of product piracy worldwide, a cost-effective method for verifying the origin of a product is to be developed. For this purpose, a certificate of authenticity can be created using precisely measurable, unique properties of special physical objects that are difficult to reconstruct. In the context of the present work, this is achieved"}
{"full_text": "We describe astrophysical processes around a black hole keeping primarily the physics of accretion in mind. In Section 1, we briefly discuss the formation, evolution and detection of black holes. We also discuss the difference of flow properties around a black hole and a Newtonian star. In Section 2, we present past and present developments in the study of spherically accreting flows. We study the properties of Bondi flow with and without radiative transfer. In the presence of significant angular momentum, which is especially true in a binary system, matter will be accreted as a thin Keplerian disk. In Section 3, we discuss a large number of models of these disks including the more popular standard disk model. We present magnetized disk models as well. Since the angular momentum is high in these systems, rotational motion is the most dominant component compared to the radial or the vertical velocity components. In Section 4, we study thick disk models which are of low angular momentum but still have no significant radial motion. The accretion rates could be very high causing the flow to become radiation dominated and the disk to be geometrically thick. For low accretion rates, ion pressure supported disks are formed. In Section 5, we extensively discuss the properties of transonic flows which has with sub-Keplerian angular momentum. In the absence of shock discontinuities, these sub-Keplerian flows are basically advecting, similar to Bondi flows, close to the black holes, though far away they match Keplerian or sub-Keplerian disks. In presence of shocks, the post-shock flow becomes rotation dominated similar to thick disks. In Section 6, we present results of important numerical simulations of accretion flows. Significant results from the studies of evolution of viscous transonic flows are reported. In Section 7, we discuss some observational evidences of the black hole accretion. We also present a detailed model of a generalized accretion disk and present its spectra and compare with observations. In Section 8, we summarize the review and make concluding remarks.", "prompt": "We describe astrophysical processes around a black hole keeping primarily the physics of accretion in mind. In Section 1, we briefly discuss the formation, evolution and detection of black holes. We also discuss the difference of flow properties around a black hole and a Newtonian star. In Section 2, we", "orig": "present past and present developments in the study of spherically accreting flows. We study the properties of Bondi flow with and without radiative transfer. In the presence of significant angular momentum, which is especially true in a binary system, matter will be accreted as a thin Keplerian disk. In Section", "generated": "We describe astrophysical processes around a black hole keeping primarily the physics of accretion in mind. In Section 1, we briefly discuss the formation, evolution and detection of black holes. We also discuss the difference of flow properties around a black hole and a Newtonian star. In Section 2,\n\n..."}
{"full_text": "The COVID-19 pandemic is causing a major outbreak in more than 150 countries around the world, having a severe impact on the health and life of many people globally. One of the crucial step in fighting COVID-19 is the ability to detect the infected patients early enough, and put them under special care. Detecting this disease from radiography and radiology images is perhaps one of the fastest ways to diagnose the patients. Some of the early studies showed specific abnormalities in the chest radiograms of patients infected with COVID-19. Inspired by earlier works, we study the application of deep learning models to detect COVID-19 patients from their chest radiography images. We first prepare a dataset of 5,000 Chest X-rays from the publicly available datasets. Images exhibiting COVID-19 disease presence were identified by board-certified radiologist. Transfer learning on a subset of 2,000 radiograms was used to train four popular convolutional neural networks, including ResNet18, ResNet50, SqueezeNet, and DenseNet-121, to identify COVID-19 disease in the analyzed chest X-ray images. We evaluated these models on the remaining 3,000 images, and most of these networks achieved a sensitivity rate of 98% ($\\pm$ 3%), while having a specificity rate of around 90%. Besides sensitivity and specificity rates, we also present the receiver operating characteristic (ROC) curve, precision-recall curve, average prediction, and confusion matrix of each model. We also used a technique to generate heatmaps of lung regions potentially infected by COVID-19 and show that the generated heatmaps contain most of the infected areas annotated by our board certified radiologist. While the achieved performance is very encouraging, further analysis is required on a larger set of COVID-19 images, to have a more reliable estimation of accuracy rates. The dataset, model implementations (in PyTorch), and evaluations, are all made publicly available for research community at https://github.com/shervinmin/DeepCovid.git", "prompt": "The COVID-19 pandemic is causing a major outbreak in more than 150 countries around the world, having a severe impact on the health and life of many people globally. One of the crucial step in fighting COVID-19 is the ability to detect the infected patients early enough, and put them under special care. Detect", "orig": "ing this disease from radiography and radiology images is perhaps one of the fastest ways to diagnose the patients. Some of the early studies showed specific abnormalities in the chest radiograms of patients infected with COVID-19. Inspired by earlier works, we study the application of deep learning models to detect COVID-19 patients from their", "generated": "The COVID-19 pandemic is causing a major outbreak in more than 150 countries around the world, having a severe impact on the health and life of many people globally. One of the crucial steps in fighting COVID-19 is the ability to detect the infected patients early enough, and put them under special care.\n\nDetect"}
{"full_text": "A new Riemann solver is built to address numerical resolution of complex flow models. The research direction is closely linked to a variant of the Baer and Nunziato (1986) model developed in Saurel et al. (2017a). This recent model provides a link between the Marble (1963) model for two-phase dilute suspensions and dense mixtures. As in the Marble model, Saurel et al. system is weakly hyperbolic with the same 4 characteristic waves, while the system involves 7 partial differential equations. It poses serious theoretical and practical issues to built simple and accurate flow solver. To overcome related difficulties the Riemann solver of Linde (2002) is revisited. The method is first examined in the simplified context of compressible Euler equations. Physical considerations are introduced in the solver improving robustness and accuracy of the Linde method. With these modifications the flow solver appears as accurate as the HLLC solver of Toro et al. (1994). Second the two-phase flow model is considered. A locally conservative formulation is built and validated removing issues related to non-conservative terms. However, two extra major issues appear from numerical experiments: The solution appears not self-similar and multiple contact waves appear in the dispersed phase. Building HLLC-type or any other solver appears consequently challenging. The modified Linde (2002) method is thus examined for the considered flow model. Some basic properties of the equations are used, such as shock relations of the dispersed phase and jump conditions across the contact wave. Thanks to these ingredients the new Riemann solver with internal reconstruction (RSIR), modification of the Linde method, handles stationary volume fraction discontinuities, presents low dissipation for transport waves and handles shocks and expansion waves accurately. It is validated on various test problems showing method's accuracy and versatility for complex flow models. Its capabilities are illustrated on a difficult two-phase flow instability problem, unresolved before.", "prompt": "A new Riemann solver is built to address numerical resolution of complex flow models. The research direction is closely linked to a variant of the Baer and Nunziato (1986) model developed in Saurel et al. (2017a). This recent model provides a link between the Marble (196", "orig": "3) model for two-phase dilute suspensions and dense mixtures. As in the Marble model, Saurel et al. system is weakly hyperbolic with the same 4 characteristic waves, while the system involves 7 partial differential equations. It poses serious theoretical and practical issues to built simple and accurate", "generated": "A new Riemann solver is built to address numerical resolution of complex flow models. The research direction is closely linked to a variant of the Baer and Nunziato (1986) model developed in Saurel et al. (2017a). This recent model provides a link between the Marble (196"}
{"full_text": "Minimization of deterministic automata on finite words results in a {\\em canonical\\/} automaton. For deterministic automata on infinite words, no canonical minimal automaton exists, and a language may have different minimal deterministic B\\\"uchi (DBW) or co-B\\\"uchi (DCW) automata. In recent years, researchers have studied {\\em good-for-games\\/} (GFG) automata -- nondeterministic automata that can resolve their nondeterministic choices in a way that only depends on the past. Several applications of automata in formal methods, most notably synthesis, that are traditionally based on deterministic automata, can instead be based on GFG automata. The {\\em minimization\\/} problem for DBW and DCW is NP-complete, and it stays NP-complete for GFG B\\\"uchi and co-B\\\"uchi automata. On the other hand, minimization of GFG co-B\\\"uchi automata with {\\em transition-based\\/} acceptance (GFG-tNCWs) can be solved in polynomial time. In these automata, acceptance is defined by a set $\\alpha$ of transitions, and a run is accepting if it traverses transitions in $\\alpha$ only finitely often. This raises the question of canonicity of minimal deterministic and GFG automata with transition-based acceptance. In this paper we study this problem. We start with GFG-tNCWs and show that the safe components (that is, these obtained by restricting the transitions to these not in $\\alpha$) of all minimal GFG-tNCWs are isomorphic, and that by saturating the automaton with transitions in $\\alpha$ we get isomorphism among all minimal GFG-tNCWs. Thus, a canonical form for minimal GFG-tNCWs can be obtained in polynomial time. We continue to DCWs with transition-based acceptance (tDCWs), and their dual tDBWs. We show that here, while no canonical form for minimal automata exists, restricting attention to the safe components is useful, and implies that the only minimal tDCWs that have no canonical form are these for which the transition to the GFG model results in strictly smaller automaton, which do have a canonical minimal form.", "prompt": "Minimization of deterministic automata on finite words results in a {\\em canonical\\/} automaton. For deterministic automata on infinite words, no canonical minimal automaton exists, and a language may have different minimal deterministic B\\\"uchi (DBW) or co-B\\\"uchi (DCW) automata. In", "orig": "recent years, researchers have studied {\\em good-for-games\\/} (GFG) automata -- nondeterministic automata that can resolve their nondeterministic choices in a way that only depends on the past. Several applications of automata in formal methods, most notably synthesis, that are traditionally based on deterministic autom", "generated": "Minimization of deterministic automata on finite words results in a {\\em canonical\\/} automaton. For deterministic automata on infinite words, no canonical minimal automaton exists, and a language may have different minimal deterministic B\\\"uchi (DBW) or co-B\\\"uchi (DCW) automata. However"}
{"full_text": "We have compared far-ultraviolet (FUV), near-ultraviolet (NUV), and Halpha measurements for star forming regions in 21 galaxies, in order to characterise the properties of their discs at radii beyond the main optical radius (R25). In our representative sample of extended and non-extended UV discs we find that half of the extended UV discs also exhibit extended Halpha emission. We find that extended UV discs fall into two categories, those with a sharp truncation in the Halpha disc close to the optical edge (R25), and those with extended emission in Halpha as well as in the ultraviolet. Although most galaxies with strong Halpha truncations near R25 show a significant corresponding falloff in UV emission (factor 10--100), the transition tends to be much smoother than in Halpha, and significant UV emission often extends well beyond this radius, confirming earlier results by Thilker et al. (2007) and others. After correcting for dust attenuation the median fraction of total FUV emission from regions outside of R25 is 1.7%, but it can be as high as 35% in the most extreme cases. The corresponding fractions of Halpha emission are approximately half as large on average. This difference reflects both a slightly lower ratio of Halpha to UV emission in the HII regions in the outer discs, as well as a lower fraction of star clusters showing HII regions. Most HII regions in the extended disc have fluxes consistent with small numbers of ionising O-type stars, and this poor sampling of the upper initial mass function in small clusters can probably account for the differences in the emission properties, consistent with earlier conclusions by Zaritsky & Christlein (2007), without needing to invoke a significant change in the stellar IMF itself. Consistent Ha/FUV ratios and brightest HII region to total Halpha fluxes in the inner and extended discs across our whole galaxy sample demonstrate no evidence for a change in the cluster luminosity function or the IMF in the low gas density outer disc.", "prompt": "We have compared far-ultraviolet (FUV), near-ultraviolet (NUV), and Halpha measurements for star forming regions in 21 galaxies, in order to characterise the properties of their discs at radii beyond the main optical radius (R25). In our representative sample of extended and non-", "orig": "extended UV discs we find that half of the extended UV discs also exhibit extended Halpha emission. We find that extended UV discs fall into two categories, those with a sharp truncation in the Halpha disc close to the optical edge (R25), and those with extended emission in Halpha as well as in the ultr", "generated": "We have compared far-ultraviolet (FUV), near-ultraviolet (NUV), and Halpha measurements for star forming regions in 21 galaxies, in order to characterise the properties of their discs at radii beyond the main optical radius (R25). In our representative sample of extended and non-inter"}
{"full_text": "VAEs, or variational autoencoders, are autoencoders that explicitly learn the distribution of the input image space rather than assuming no prior information about the distribution. This allows it to classify similar samples close to each other in the latent space's distribution. VAEs classically assume the latent space is normally distributed, though many distribution priors work, and they encode this assumption through a K-L divergence term in the loss function. While VAEs learn the distribution of the latent space and naturally make each dimension in the latent space as disjoint from the others as possible, they do not group together similar features -- the image space feature represented by one unit of the representation layer does not necessarily have high correlation with the feature represented by a neighboring unit of the representation layer. This makes it difficult to interpret VAEs since the representation layer is not structured in a way that is easy for humans to parse. We aim to make a more interpretable VAE by partitioning the representation layer into disjoint sets of units. Partitioning the representation layer into disjoint sets of interconnected units yields a prior that features of the input space to this new VAE, which we call a partition VAE or PVAE, are grouped together by correlation -- for example, if our image space were the space of all ping ping game images (a somewhat complex image space we use to test our architecture) then we would hope the partitions in the representation layer each learned some large feature of the image like the characteristics of the ping pong table or the characteristics and position of the players or the ball. We also add to the PVAE a cost-saving measure: subresolution. Because we do not have access to GPU training environments for long periods of time and Google Colab Pro costs money, we attempt to decrease the complexity of the PVAE by outputting an image with dimensions scaled down from the input image by a constant factor, thus forcing the model to output a smaller version of the image. We then increase the resolution to calculate loss and train by interpolating through neighboring pixels. We train a tuned PVAE on MNIST and Sports10 to test its effectiveness.", "prompt": "VAEs, or variational autoencoders, are autoencoders that explicitly learn the distribution of the input image space rather than assuming no prior information about the distribution. This allows it to classify similar samples close to each other in the latent space's distribution. VAEs classically assume the latent space is normally", "orig": "distributed, though many distribution priors work, and they encode this assumption through a K-L divergence term in the loss function. While VAEs learn the distribution of the latent space and naturally make each dimension in the latent space as disjoint from the others as possible, they do not group together similar features -- the image space", "generated": "VAEs, or variational autoencoders, are autoencoders that explicitly learn the distribution of the input image space rather than assuming no prior information about the distribution. This allows it to classify similar samples close to each other in the latent space's distribution. VAEs classically assume the latent space is normally"}
{"full_text": "Atmospheric composition measurements taken at many high-altitude stations around the world, aim to collect data representative of the free troposphere and of an intercontinental scale. However, the high-altitude environment favours vertical mixing and the transportation of air masses at local or regional scales, which has a potential influence on the composition of the sampled air masses. Mixing processes, source-receptor pathways, and atmospheric chemistry may strongly depend on local and regional weather regimes, and these should be characterized specifically for each station. The Pic du Midi (PDM) isa mountaintop observatory (2850 m a.s.l.) on the north side of the Pyrenees. PDM is associated with the Centre de Recherches Atmosph{\\'e}riques (CRA), a site in the foothills ar 600 m a.s.l. 28 km north-east of the PDM. The two centers make up the Pyrenean Platform for the Observation of the Atmosphere (P2OA). Data measured at PDM and CRA were combined to form a5-year hourly dataset of 23 meteorological variables notably: temperature, humidity, cloud cover, wind at several altitudes. The dataset was classified using hierarchical clustering, with the aim of grouping together the days which had similar meteorological characteristics. To complete the clustering, we computed several diagnostic tools, in order to provide additional information and study specific phenomena (foehn, precipitation, atmospheric vertical structure, and thermally driven circulations). This classification resulted in six clusters: three highly populated clusters which correspond to the most frequent meteorological conditions (fair weather, mixed weather and disturbed weather, respectively); a small cluster evidencing clear characteristics of winter northwesterly windstorms; and two small clusters characteristic of south foehn (south- to southwesterly large-scaleflow, associated with warm and dry downslope flow on the lee side of the chain). The diagnostic tools applied to the six clusters provided results in line with the conclusions tentatively drawn from 23 meteorological variables. This, to some extent,validates the approach of hierarchical clustering of local data to distinguish weather regimes. Then statistics of atmospheric composition at PDM were analysed and discussed for each cluster. Radon measurements, notably, revealed that the regional background in the lower troposphere dominates the influence of diurnal thermal flows when daily averaged concentrations are considered. Differences between clusters were demonstrated by the anomalies of CO, CO$_2$, CH$_4$, O$_3$ and aerosol number concentration, and interpretations in relation with chemical sinks and sources are proposed.", "prompt": "Atmospheric composition measurements taken at many high-altitude stations around the world, aim to collect data representative of the free troposphere and of an intercontinental scale. However, the high-altitude environment favours vertical mixing and the transportation of air masses at local or regional scales, which has a potential influence on the", "orig": "composition of the sampled air masses. Mixing processes, source-receptor pathways, and atmospheric chemistry may strongly depend on local and regional weather regimes, and these should be characterized specifically for each station. The Pic du Midi (PDM) isa mountaintop observatory (2850 m a.s.l.) on the north side", "generated": "Atmospheric composition measurements taken at many high-altitude stations around the world, aim to collect data representative of the free troposphere and of an intercontinental scale. However, the high-altitude environment favors vertical mixing and the transportation of air masses at local or regional scales, which has a potential influence on the represent"}
{"full_text": "Ulf Grenander was born in Vastervik, Sweden, on July 23, 1923. He started his undergraduate education at Uppsala University, and earned his B.A. degree in 1946, the Fil. Lic. degree in 1948 and the Fil. Dr. degree in 1950, all from the University of Stockholm. His Ph.D. thesis advisor was Harald Cram\\'{e}r. Professor Grenander is well known for pathbreaking research in a number of areas including pattern theory, computer vision, inference in stochastic processes, probabilities on algebraic structures and actuarial mathematics. He has published more than one dozen influential books, of which Statistical Analysis of Stationary Time Series (1957, coauthored with M. Rosenblatt), Probabilities on Algebraic Structures (1963; also in Russian) and Abstract Inference (1981b) are regarded as classics. His three-volume lecture notes, namely, Pattern Synthesis (vol. I, 1976), Pattern Analysis (vol. II, 1978) and Regular Structures (vol. III, 1981a; also in Russian) created and nurtured a brand new area of research. During 1951--1966, Professor Grenander's career path took him to the University of Chicago (1951--1952), the University of California--Berkeley (1952--1953), the University of Stockholm (1953--1957), Brown University (1957--1958) and the Institute for Insurance Mathematics and Mathematical Statistics (1958--1966) as its Professor and Director. From 1966 until his retirement he was L. Herbert Ballou University Professor at Brown University. Professor Grenander also held the position of Scientific Director (1971--1973) of the Swedish Institute of Applied Mathematics. He has earned many honors and awards, including Arhennius Fellow (1948), Fellow of the Institute of Mathematical Statistics (1953), Prize of the Nordic Actuaries (1961), Arnberger Prize of the Royal Swedish Academy of Science (1962), Member of the Royal Swedish Academy of Science (1965), Guggenheim Fellowship (1979) and Honorary Fellow of the Royal Statistical Society, London (1989). He has delivered numerous prestigious lectures, including the Rietz Lecture (1985), the Wald Lectures (1995) and the Mahalanobis Lecture (2004). Professor Grenander received an Honorary D.Sc. degree (1993) from the University of Chicago and is a Fellow of both the American Academy of Arts and Sciences (1995) and the National Academy of Sciences, U.S.A. (1998). Professor Grenander's career, life, passion and hobbies can all be summarized by one simple word: Mathematics.", "prompt": "Ulf Grenander was born in Vastervik, Sweden, on July 23, 1923. He started his undergraduate education at Uppsala University, and earned his B.A. degree in 1946, the Fil. Lic. degree in 1948 and the Fil. Dr. degree", "orig": "in 1950, all from the University of Stockholm. His Ph.D. thesis advisor was Harald Cram\\'{e}r. Professor Grenander is well known for pathbreaking research in a number of areas including pattern theory, computer vision, inference in stochastic processes, probabilities on algebraic structures and actuar", "generated": "Ulf Grenander was born in Vastervik, Sweden, on July 23, 1923. He started his undergraduate education at Uppsala University, and earned his B.A. degree in 1946, the Fil. Lic. degree in 1948 and the Fil. Dr. degree"}
{"full_text": "Background: Implementing a standardized 31P-MRS dynamic acquisition protocol to evaluate skeletal muscle energy metabolism and monitor muscle fatigability1,2, while being compatible with various longitudinal clinical studies on diversified patient cohorts, requires a high level of technicality and expertise. Furthermore, processing data to obtain reliable results also demands a great degree of expertise from the operator. In this two-part article, we present an advanced quality control approach for data acquired using a dynamic 31P-MRS protocol. The aim is to provide decision support to the operator in order to assist in data processing and obtain reliable results based on objective criteria. We present first in part one, an advanced data quality control (QC) approach of a dynamic 31P-MRS protocol. Part two is an impact study demonstrating the added value of the QC approach to explore clinical results derived from two patient populations with significant fatigue: COVID19 and multiple sclerosis (MS). Experimental: 31P-MRS was performed on a 3T clinical MRI in 175 subjects from clinical and healthy control populations conducted in a University Hospital. An advanced data QC Score (QCS) was developed using multiple objective criteria. The criteria were based on current recommendations from the literature enriched by new proposals based on clinical experience. The QCS was designed to indicate valid and corrupt data and guide necessary objective data editing to extract as much valid physiological data as possible. Dynamic acquisitions using an MR-compatible ergometer ran over a rest(40s), exercise(2min), and a recovery phase(6min). Results: Using QCS enabled rapid identification of subjects with data anomalies allowing the user to correct the data series or reject them partially or entirely as well as identify fully valid datasets. Overall, the use of the QCS resulted in the automatic classification of 45% of the subjects including 58 participants that had data with no criterion violation and 21 participants with violations that resulted in the rejection of all dynamic data. The remaining datasets were inspected manually with guidance allowing acceptance of full datasets from an additional 80 participants and recovery phase data from an additional 16 subjects. Overall, more anomalies occurred with patient data (35% of datasets) compared to healthy controls (15% of datasets). Conclusion: This paper describes typical difficulties encountered during the dynamic acquisition of 31P-MRS. Based on these observations, a standardized data quality control pipeline was created and implemented in both healthy and patient populations. The QC scoring ensures a standardized data rejection procedure and rigorous objective analysis of dynamic 31P-MRS data obtained from patients. The contribution of this methodology contributes to efforts made to standardize the practices of the 31P-MRS that has been underway for a decade, with the ultimate goal of making it an empowered tool for clinical research.", "prompt": "Background: Implementing a standardized 31P-MRS dynamic acquisition protocol to evaluate skeletal muscle energy metabolism and monitor muscle fatigability1,2, while being compatible with various longitudinal clinical studies on diversified patient cohorts, requires a high level of technicality and expertise. Furthermore, processing data to obtain reliable results also demands", "orig": "a great degree of expertise from the operator. In this two-part article, we present an advanced quality control approach for data acquired using a dynamic 31P-MRS protocol. The aim is to provide decision support to the operator in order to assist in data processing and obtain reliable results based on objective criteria. We present first", "generated": "Background: Implementing a standardized 31P-MRS dynamic acquisition protocol to evaluate skeletal muscle energy metabolism and monitor muscle fatigability1,2, while being compatible with various longitudinal clinical studies on diversified patient cohorts, requires a high level of technicality and expertise. Furthermore, processing data to obtain reliable results also demands"}
{"full_text": "Yuan Shih Chow was born in Hubei province in China, on September 1, 1924. The eldest child of a local militia and political leader, he grew up in war and turmoil. His hometown was on the front line during most of the Japanese invasion and occupation of China. When he was 16, Y. S. Chow journeyed, mostly on foot, to Chongqing (Chung-King), the wartime Chinese capital, to finish his high school education. When the Communist party gained power in China, Y. S. Chow had already followed his university job to Taiwan. In Taiwan, he taught mathematics as an assistant at National Taiwan University until he came to the United States in 1954. At the University of Illinois, he studied under J. L. Doob and received his Ph.D. in 1958. He served as a staff mathematician and adjunct faculty at the IBM Watson Research Laboratory and Columbia University from 1959 to 1962. He was a member of the Statistics Department at Purdue University from 1962 to 1968. From 1968 until his retirement in 1993, Y. S. Chow served as Professor of Mathematical Statistics at Columbia University. At different times, he was a visiting professor at the University of California at Berkeley, University of Heidelberg (Germany) and the National Central University, Taiwan. He served as Director of the Institute of Mathematics of Academia Sinica, Taiwan, and Director of the Center of Applied Statistics at Nankai University, Tianjin, China. He was instrumental in establishing the Institute of Statistics of Academia Sinica in Taiwan. He is currently Professor Emeritus at Columbia University. Y. S. Chow is a fellow of the Institute of Mathematical Statistics, a member of the International Statistical Institute and a member of Taiwan's Academia Sinica. He has numerous publications, including Great Expectations: The Theory of Optimal Stopping (1971), in collaboration with Herbert Robbins and David Siegmund, and Probability Theory (1978), in collaboration with Henry Teicher. Y. S. Chow has a strong interest in mathematics education. He taught high school mathematics for one year in 1947 and wrote a book on high school algebra in collaboration with J. H. Teng and M. L. Chu. In 1992, Y. S. Chow, together with I. S. Chang and W. C. Ho, established the Chinese Institute of Probability and Statistics in Taiwan. This conversation took place in the fall of 2003 in Dobbs Ferry, New York.", "prompt": "Yuan Shih Chow was born in Hubei province in China, on September 1, 1924. The eldest child of a local militia and political leader, he grew up in war and turmoil. His hometown was on the front line during most of the Japanese invasion and occupation of China. When he", "orig": "was 16, Y. S. Chow journeyed, mostly on foot, to Chongqing (Chung-King), the wartime Chinese capital, to finish his high school education. When the Communist party gained power in China, Y. S. Chow had already followed his university job to Taiwan. In Taiwan,", "generated": "Yuan Shih Chow was born in Hubei province in China, on September 1, 1924. The eldest child of a local militia and political leader, he grew up in war and turmoil. His hometown was on the front line during most of the Japanese invasion and occupation of China. When his"}
{"full_text": "This study is about the properties of the sets of objects associated in a structure resulting from multiple-processes involving chance as are materials whose texture is unordered and random. Being a paper scientist the author refers to the sheet of paper which is a stochastic fibrous set whose porous texture can be considered as an archetype for many natural or artificial human structures. The paper properties are correlated with its texture by taking account the effect of chance occurring during its manufacturing process. The theoretical developments, the formalism and the application methods presented in this study have a general significance beyond the only paper material.A specific property of sets of objects randomly unorderly distributed in space is their interfaces orientation distribution. This distribution is usually obtained by the analysis of images sampled in the object sets. The density of orientation probability of the fibers or of the texture interfaces, weighted by their length or by their area, can be interpreted as the radius of curvature of an outline or of a warped surface which characterizes, from a global and statistical point of view, the texture geometry in two or three dimensions. This figure named by the author the \"equivalent pore\", is with its elliptical shape similar to the one of the mean pore defined by the mean directional chord between the interfaces in the texture. Different methods of \"equivalent pore\" establishing are analyzed : by conformal map of the fiber network or of the texture interfaces, by images stereometric analysis of texture tomographical cuts, by scattering and diffraction of a laser light beam impacting the fibrous texture or the material surface replica, by hard X-ray absorption and phase contrast at the European Synchrotron Radiation Facilities(ESRF), in Grenoble. The \"equivalent pore\" concept allows us to study random unordered sets behavior in strength fields while simplifying this analysis. Thus a phenomenon occurring in a plane set, in two dimensions, can be analyzed on its \"equivalent pore\" linear outline, and a phenomenon which occurs in volume in a three dimensional set can be analyzed on its \"equivalent pore\" warped surface. This concept has been applied for physical, mechanical, optical and ionic conduction properties of materials like papers, boards, felts, nonwoven textiles, polymer foams, metallic alloys with grain joints, geological grounds, and for the surface mapping of natural relief and of materials with different gloss, worn or roughness levels.The ellipse and the ellipso\\\"id, as well as multi-modes compositions of it, are the most appropriate figures to represent the \"equivalent pore\" of materials with a random unordered texture. The fact that a law, which defines the curvature of an elliptic deterministic geometrical configuration, is essential to represent interface orientation allocation of elements whose spatial distribution is probabilistic is a noteworthy fact that makes us wonder. This assertion is corroborated by fluid flow analysis through porous media. The global dissipated energy for fluid flow is distributed along the motion (translation and rotation) and fluid deformation components on the \"equivalent pore\" whose surface is conformal to the texture interfaces tangential space. The porous media being homogenous and the fluid particles indistinguishable each ones from the others, due to permanent stochastic exchanges from one fluid volume element into another, we conclude that their motion quantification is invariant on each point of the \"equivalent pore\" surface. This quantification can be represented by a group of elliptical cylinders.The \"equivalent pore\" and cylinders group surface intersections define the fluid particle probabilistic paths in the porous media. One singular geometrical configuration of the elliptic cylinders group union with the ellipsoidal \"equivalent pore\" fulfills the minimal energy dissipation requirements in the stress field applied to the fluid. The resulting paths are ellipso\\\"ido-cylindrical curves carpeting the ellipso\\\"id by a beam of isoperim\\'etric lacets, in close or open loops depending on the possible pairing off the curves in their nodals and isoclinal tangency points. The flow laminarity and unrotationality are globally established in the porous media at the macroscopic scale, for Reynold's number small values, in agreement with the results found moreover. The so defined ellipso\\\"ido-cylindrical curve is a stationary curve at the average least action meaning, for a punctual mobile or a deformable volume element moving on the surface of the ellipsoid from a nodal spot to the other in an antipodal position. This curve establishes a gauge which characterizes, from a global point of view, the physical space embraced by the fluid during its flow, in dynamical balance conditions compatible with the stress field. The ellipso\\\"ido-cylindrical curve allows to carpet the ellipso\\\"id as a function of one cyclic angular variable, which is a new construction for the ellipso\\\"id. When the set of the objects is isotropic the curve is sphero-cylindrical whose plan mapping permit to decipher the tai-chi figure of the Tao\\\"ist Buddhism philosophy.This study of random unordered object sets, and more specially of fibrous porous media, allows to establish a structural link between their small scale texture stochastic disorder and the harmonious order that emerges from these sets at a larger scale. The largest chance possible, compatible with the stress field which put a strain on these sets, is the necessary and sufficient variable which allows to best globally adjust their behaviors according to the probabilistic and deterministic laws governing their evolution.The vocabulary used in this study is issued from the common language, most of the time adapted to the material class, it is transposable in other fields of interest. The text is completed by notes and a bibliography which refer to the works done following the presented concepts or in relation to this field of studies.", "prompt": "This study is about the properties of the sets of objects associated in a structure resulting from multiple-processes involving chance as are materials whose texture is unordered and random. Being a paper scientist the author refers to the sheet of paper which is a stochastic fibrous set whose porous texture can be considered as an archetype for many natural", "orig": "or artificial human structures. The paper properties are correlated with its texture by taking account the effect of chance occurring during its manufacturing process. The theoretical developments, the formalism and the application methods presented in this study have a general significance beyond the only paper material.A specific property of sets of objects randomly unorderly distributed in space", "generated": "This study is about the properties of the sets of objects associated in a structure resulting from multiple-processes involving chance as are materials whose texture is unordered and random. Being a paper scientist the author refers to the sheet of paper which is a stochastic fibrous set whose porous texture can be considered as an archetype for many natural"}
{"full_text": "This essay is a picaresque -- a first-person narrative relating the adventures of a rogue (me) sifting through the mind of Christopher Alexander as he left behind formalized design thinking in favor of a more intuitive, almost spiritual process. The work of Christopher Alexander is familiar to many computer scientists: for some it's patterns, for some it's the mystical **quality without a name** and \"Nature of Order\"; for many more it's \"Notes on the Synthesis of Form\" -- Alexander's formalized design method and foreshadowing ideas about cohesion and coupling in software. Since the publication of \"Design Patterns\" by Gamma et al. in 1994, there have been hundreds of books published about design / software patterns, thousands of published pattern languages, and tens of thousands of published patterns. \"Notes,\" published in 1964, was quickly followed by one of Alexander's most important essays, \"A City is Not a Tree,\" in which he repudiates the formal method described in \"Notes,\" and his Preface to the paperback edition of \"Notes\" in 1971 repeats the repudiation. For many close readers of Alexander, this discontinuity is startling and unexplained. When I finally read \"Notes\" in 2015, I was struck by the detailed worked example, along with a peculiar mathematical treatment of the method, and a hint that the modularization presented in the example was reckoned by a computer program he had written -- all in the late 1950s and early 1960s. Because of my fascination with metaheuristic optimization, I couldn't resist trying to replicate his experimental results. Computers and their programs relish dwelling on flaws in your thinking -- Alexander was not exempt. By engaging in hermeneutics and software archeology, I was able to uncover / discover the trajectory of his thinking as he encountered failures and setbacks with his computer programs. My attempted replication also failed, and that led me to try to unearth the five different programs he wrote, understand them, and figure out how one led to the next. They are not described in published papers, only in internal reports. My search for these reports led to their being made available on the Net. What I found in my voyage were the early parts of a chain of thought that started with cybernetics, mathematics, and a plain-spoken computer; passed through \"A City is Not a Tree\"; paused to \"make God appear in the middle of a field\"; and ended with this fundamental design goal: **I try to make the volume of the building so that it carries in it all feeling. To reach this feeling, I try to make the building so that it carries my eternal sadness. It comes, as nearly as I can in a building, to the point of tears.**", "prompt": "This essay is a picaresque -- a first-person narrative relating the adventures of a rogue (me) sifting through the mind of Christopher Alexander as he left behind formalized design thinking in favor of a more intuitive, almost spiritual process. The work of Christopher Alexander is familiar to many computer scientists: for some", "orig": "it's patterns, for some it's the mystical **quality without a name** and \"Nature of Order\"; for many more it's \"Notes on the Synthesis of Form\" -- Alexander's formalized design method and foreshadowing ideas about cohesion and coupling in software. Since the publication of \"Design Patterns\"", "generated": "This essay is a picaresque -- a first-person narrative relating the adventures of a rogue (me) sifting through the mind of Christopher Alexander as he left behind formalized design thinking in favor of a more intuitive, almost spiritual process. The work of Christopher Alexander is familiar to many computer scientists: for some"}
{"full_text": "We present DCL-PC: a logic for reasoning about how the abilities of agents and coalitions of agents are altered by transferring control from one agent to another. The logical foundation of DCL-PC is CL-PC, a logic for reasoning about cooperation in which the abilities of agents and coalitions of agents stem from a distribution of atomic Boolean variables to individual agents -- the choices available to a coalition correspond to assignments to the variables the coalition controls. The basic modal constructs of DCL-PC are of the form coalition C can cooperate to bring about phi. DCL-PC extends CL-PC with dynamic logic modalities in which atomic programs are of the form agent i gives control of variable p to agent j; as usual in dynamic logic, these atomic programs may be combined using sequence, iteration, choice, and test operators to form complex programs. By combining such dynamic transfer programs with cooperation modalities, it becomes possible to reason about how the power of agents and coalitions is affected by the transfer of control. We give two alternative semantics for the logic: a direct semantics, in which we capture the distributions of Boolean variables to agents; and a more conventional Kripke semantics. We prove that these semantics are equivalent, and then present an axiomatization for the logic. We investigate the computational complexity of model checking and satisfiability for DCL-PC, and show that both problems are PSPACE-complete (and hence no worse than the underlying logic CL-PC). Finally, we investigate the characterisation of control in DCL-PC. We distinguish between first-order control -- the ability of an agent or coalition to control some state of affairs through the assignment of values to the variables under the control of the agent or coalition -- and second-order control -- the ability of an agent to exert control over the control that other agents have by transferring variables to other agents. We give a logical characterisation of second-order control.", "prompt": "We present DCL-PC: a logic for reasoning about how the abilities of agents and coalitions of agents are altered by transferring control from one agent to another. The logical foundation of DCL-PC is CL-PC, a logic for reasoning about cooperation in which the abilities of agents and coalitions of agents", "orig": "stem from a distribution of atomic Boolean variables to individual agents -- the choices available to a coalition correspond to assignments to the variables the coalition controls. The basic modal constructs of DCL-PC are of the form coalition C can cooperate to bring about phi. DCL-PC extends CL-PC with dynamic logic modalities in", "generated": "We present DCL-PC: a logic for reasoning about how the abilities of agents and coalitions of agents are altered by transferring control from one agent to another. The logical foundation of DCL-PC is CL-PC, a logic for reasoning about cooperation in which the abilities of agents and coalitions of agents"}
{"full_text": "Background: Odd-odd nuclei, around doubly closed shells, have been extensively used to study proton-neutron interactions. However, the evolution of these interactions as a function of the binding energy, ultimately when nuclei become unbound, is poorly known. The $^{26}$F nucleus, composed of a deeply bound $\\pi0d\\_{5/2}$ proton and an unbound $\\nu0d\\_{3/2}$ neutron on top of an $^{24}$O core, is particularly adapted for this purpose. The coupling of this proton and neutron results in a $J^{\\pi} = 1^{+}\\_1 - 4^{+}\\_1$ multiplet, whose energies must be determined to study the influence of the proximity of the continuum on the corresponding proton-neutron interaction. The $J^{\\pi} = 1^{+}\\_1, 2^{+}\\_1,4^{+}\\_1$ bound states have been determined, and only a clear identification of the $J^{\\pi} =3^{+}\\_1$ is missing.Purpose: We wish to complete the study of the $J^{\\pi} = 1^{+}\\_1 - 4^{+}\\_1$ multiplet in $^{26}$F, by studying the energy and width of the $J^{\\pi} =3^{+}\\_1$ unbound state. The method was firstly validated by the study of unbound states in $^{25}$F, for which resonances were already observed in a previous experiment.Method: Radioactive beams of $^{26}$Ne and $^{27}$Ne, produced at about $440A$\\,MeV by the FRagment Separator at the GSI facility, were used to populate unbound states in $^{25}$F and $^{26}$F via one-proton knockout reactions on a CH$\\_2$ target, located at the object focal point of the R$^3$B/LAND setup. The detection of emitted $\\gamma$-rays and neutrons, added to the reconstruction of the momentum vector of the $A-1$ nuclei, allowed the determination of the energy of three unbound states in $^{25}$F and two in $^{26}$F. Results: Based on its width and decay properties, the first unbound state in $^{25}$F is proposed to be a $J^{\\pi} = 1/2^-$ arising from a $p\\_{1/2}$ proton-hole state. In $^{26}$F, the first resonance at 323(33)~keV is proposed to be the $J^{\\pi} =3^{+}\\_1$ member of the $J^{\\pi} = 1^{+}\\_1 - 4^{+}\\_1$ multiplet. Energies of observed states in $^{25,26}$F have been compared to calculations using the independent-particle shell model, a phenomenological shell-model, and the ab initio valence-space in-medium similarity renormalization group method.Conclusions: The deduced effective proton-neutron interaction is weakened by about 30-40\\% in comparison to the models, pointing to the need of implementing the role of the continuum in theoretical descriptions, or to a wrong determination of the atomic mass of $^{26}$F.", "prompt": "Background: Odd-odd nuclei, around doubly closed shells, have been extensively used to study proton-neutron interactions. However, the evolution of these interactions as a function of the binding energy, ultimately when nuclei become unbound, is poorly known. The $^{26}$F nucleus, composed of a deeply bound $\\", "orig": "pi0d\\_{5/2}$ proton and an unbound $\\nu0d\\_{3/2}$ neutron on top of an $^{24}$O core, is particularly adapted for this purpose. The coupling of this proton and neutron results in a $J^{\\pi} = 1^{+", "generated": "Background: Odd-odd nuclei, around doubly closed shells, have been extensively used to study proton-neutron interactions. However, the evolution of these interactions as a function of the binding energy, ultimately when nuclei become unbound, is poorly known. The $^{26}$F nucleus, composed of a deeply bound $"}
{"full_text": "For many decades, ultrahigh energy charged particles of unknown origin that can be observed from the ground have been a puzzle for particle physicists and astrophysicists. As an attempt to discriminate among several possible production scenarios, astrophysicists try to test the statistical isotropy of the directions of arrival of these cosmic rays. At the highest energies, they are supposed to point toward their sources with good accuracy. However, the observations are so rare that testing the distribution of such samples of directional data on the sphere is nontrivial. In this paper, we choose a nonparametric framework that makes weak hypotheses on the alternative distributions and allows in turn to detect various and possibly unexpected forms of anisotropy. We explore two particular procedures. Both are derived from fitting the empirical distribution with wavelet expansions of densities. We use the wavelet frame introduced by [SIAM J. Math. Anal. 38 (2006b) 574-594 (electronic)], the so-called needlets. The expansions are truncated at scale indices no larger than some ${J^{\\star}}$, and the $L^p$ distances between those estimates and the null density are computed. One family of tests (called Multiple) is based on the idea of testing the distance from the null for each choice of $J=1,\\ldots,{J^{\\star}}$, whereas the so-called PlugIn approach is based on the single full ${J^{\\star}}$ expansion, but with thresholded wavelet coefficients. We describe the practical implementation of these two procedures and compare them to other methods in the literature. As alternatives to isotropy, we consider both very simple toy models and more realistic nonisotropic models based on Physics-inspired simulations. The Monte Carlo study shows good performance of the Multiple test, even at moderate sample size, for a wide sample of alternative hypotheses and for different choices of the parameter ${J^{\\star}}$. On the 69 most energetic events published by the Pierre Auger Collaboration, the needlet-based procedures suggest statistical evidence for anisotropy. Using several values for the parameters of the methods, our procedures yield $p$-values below 1%, but with uncontrolled multiplicity issues. The flexibility of this method and the possibility to modify it to take into account a large variety of extensions of the problem make it an interesting option for future investigation of the origin of ultrahigh energy cosmic rays.", "prompt": "For many decades, ultrahigh energy charged particles of unknown origin that can be observed from the ground have been a puzzle for particle physicists and astrophysicists. As an attempt to discriminate among several possible production scenarios, astrophysicists try to test the statistical isotropy of the directions of arrival of", "orig": "these cosmic rays. At the highest energies, they are supposed to point toward their sources with good accuracy. However, the observations are so rare that testing the distribution of such samples of directional data on the sphere is nontrivial. In this paper, we choose a nonparametric framework that makes weak hypotheses on the alternative", "generated": "For many decades, ultrahigh energy charged particles of unknown origin that can be observed from the ground have been a puzzle for particle physicists and astrophysicists. As an attempt to discriminate among several possible production scenarios, astrophysicists try to test the statistical isotropy of the directions of arrival of"}
{"full_text": "Shock modelling predicts an electron density enhancement within the magnetic precursor of C-shocks. Previous observations of SiO, H13CO+, HN13C and H13CN toward the young L1448-mm outflow showed an over-excitation of the ion fluid that was attributed to an electron density enhancement in the precursor. We re-visit this interpretation and test if it still holds when we consider different source morphologies and kinetic temperatures for the observed molecules, and also give some insight on the spatial extent of the electron density enhancement around L1448-mm. We estimate the opacities of H13CO+ and HN13C by observing the J=3\\to2 lines of rarer isotopologues to confirm that the emission is optically thin. To model the excitation of the molecules, we use the large velocity gradient (LVG) approximation with updated collisional coefficients to i) re- analyse the observations toward the positions where the over-excitation of H13CO+ has previously been observed [i.e. toward L1448- mm at offsets (0,0) and (0,-10)], and ii) to investigate if the electron density enhancement is still required for the cases of extended and compact emission, and for kinetic temperatures of up to 400 K. We also report several lines of SiO, HN13C and H13CO+ toward new positions around this outflow, to investigate the spatial extent of the over-excitation of the ions in L1448-mm. From the isotopologue observations, we find that the emission of H13CO+ and HN13C from the precursor is optically thin if this emission is extended. Using the new collisional coefficients, an electron density enhancement is still needed to explain the excitation of H13CO+ for extended emission and for gas temperatures of\\le 400 K toward L1448-mm (0,-10), and possibly also toward L1448-mm (0,0). For compact emission the data cannot be fitted. We do not find any evidence for the over-excitation of the ion fluid toward the newly observed positions around L1448-mm. The observed line emission of SiO, H13CO+ and HN13C toward L1448-mm (0,0) and (0,-10) is consistent with an electron density enhancement in the precursor component, if this emission is spatially extended. This is also true for the case of high gas temperatures (\\le400 K) toward the (0,-10) offset. The electron density enhancement seems to be restricted to the southern, redshifted lobe of the L1448-mm outflow. Interferometric images of the line emission of these molecules are needed to confirm the spatial extent of the over-excitation of the ions and thus, of the electron density enhancement in the magnetic precursor of L1448-mm.", "prompt": "Shock modelling predicts an electron density enhancement within the magnetic precursor of C-shocks. Previous observations of SiO, H13CO+, HN13C and H13CN toward the young L1448-mm outflow showed an over-excitation of the ion fluid that was attributed to an electron density enhancement in the precursor", "orig": ". We re-visit this interpretation and test if it still holds when we consider different source morphologies and kinetic temperatures for the observed molecules, and also give some insight on the spatial extent of the electron density enhancement around L1448-mm. We estimate the opacities of H13CO+ and HN13C", "generated": "Shock modelling predicts an electron density enhancement within the magnetic precursor of C-shocks. Previous observations of SiO, H13CO+, HN13C and H13CN toward the young L1448-mm outflow showed an over-excitation of the ion fluid that was attributed to an electron density enhancement in the precursor"}
{"full_text": "The paper deals with the automatic analysis of real-life telephone conversations between an agent and a customer of a customer care service (ccs). The application domain is the public transportation system in Paris and the purpose is to collect statistics about customer problems in order to monitor the service and decide priorities on the intervention for improving user satisfaction. Of primary importance for the analysis is the detection of themes that are the object of customer problems. Themes are defined in the application requirements and are part of the application ontology that is implicit in the ccs documentation. Due to variety of customer population, the structure of conversations with an agent is unpredictable. A conversation may be about one or more themes. Theme mentions can be interleaved with mentions of facts that are irrelevant for the application purpose. Furthermore, in certain conversations theme mentions are localized in specific conversation segments while in other conversations mentions cannot be localized. As a consequence, approaches to feature extraction with and without mention localization are considered. Application domain relevant themes identified by an automatic procedure are expressed by specific sentences whose words are hypothesized by an automatic speech recognition (asr) system. The asr system is error prone. The word error rates can be very high for many reasons. Among them it is worth mentioning unpredictable background noise, speaker accent, and various types of speech disfluencies. As the application task requires the composition of proportions of theme mentions, a sequential decision strategy is introduced in this paper for performing a survey of the large amount of conversations made available in a given time period. The strategy has to sample the conversations to form a survey containing enough data analyzed with high accuracy so that proportions can be estimated with sufficient accuracy. Due to the unpredictable type of theme mentions, it is appropriate to consider methods for theme hypothesization based on global as well as local feature extraction. Two systems based on each type of feature extraction will be considered by the strategy. One of the four methods is novel. It is based on a new definition of density of theme mentions and on the localization of high density zones whose boundaries do not need to be precisely detected. The sequential decision strategy starts by grouping theme hypotheses into sets of different expected accuracy and coverage levels. For those sets for which accuracy can be improved with a consequent increase of coverage a new system with new features is introduced. Its execution is triggered only when specific preconditions are met on the hypotheses generated by the basic four systems. Experimental results are provided on a corpus collected in the call center of the Paris transportation system known as ratp. The results show that surveys with high accuracy and coverage can be composed with the proposed strategy and systems. This makes it possible to apply a previously published proportion estimation approach that takes into account hypothesization errors .", "prompt": "The paper deals with the automatic analysis of real-life telephone conversations between an agent and a customer of a customer care service (ccs). The application domain is the public transportation system in Paris and the purpose is to collect statistics about customer problems in order to monitor the service and decide priorities on the intervention for improving user satisfaction", "orig": ". Of primary importance for the analysis is the detection of themes that are the object of customer problems. Themes are defined in the application requirements and are part of the application ontology that is implicit in the ccs documentation. Due to variety of customer population, the structure of conversations with an agent is unpredictable. A conversation may be", "generated": "The paper deals with the automatic analysis of real-life telephone conversations between an agent and a customer of a customer care service (ccs). The application domain is the public transportation system in Paris and the purpose is to collect statistics about customer problems in order to monitor the service and decide priorities on the intervention for improving user satisfaction"}
{"full_text": "Technological progress in material synthesis, as well as artificial realization of condensed matter scenarios via ultra-cold atomic gases in optical lattices or epitaxial growth of thin films, is opening the gate to investigate a plethora of unprecedented strongly correlated electron systems. In a large subclass thereof, a metallic state of layered electrons undergoes an ordering transition below some temperature into unconventional states of matter driven by electronic correlations, such as magnetism, superconductivity, or other Fermi surface instabilities. While this type of phenomena has been a well-established direction of research in condensed matter for decades, the variety of today's accessible scenarios pose fundamental new challenges to describe them. A core complication is the multi-orbital nature of the low-energy electronic structure of these systems, such as the multi-d orbital nature of electrons in iron pnictides and transition-metal oxides in general, but also electronic states of matter on lattices with multiple sites per unit cell such as the honeycomb or kagome lattice. In this review, we propagate the functional renormalization group (FRG) as a suited approach to investigate multi-orbital Fermi surface instabilities. The primary goal of the review is to describe the FRG in explicit detail and render it accessible to everyone both at a technical and intuitive level. Summarizing recent progress in the field of multi-orbital Fermi surface instabilities, we illustrate how the unbiased fashion by which the FRG treats all kinds of ordering tendencies guarantees an adequate description of electronic phase diagrams and often allows to obtain parameter trends of sufficient accuracy to make qualitative predictions for experiments. This review includes detailed and illustrative illustrations of magnetism and, in particular, superconductivity for the iron pnictides from the viewpoint of FRG. Furthermore, it discusses candidate scenarios for topological bulk singlet superconductivity and exotic particle-hole condensates on hexagonal lattices such as sodium-doped cobaltates, graphene doped to van Hove Filling, and the kagome Hubbard model. In total, the FRG promises to be one of the most versatile and revealing numerical approaches to address unconventional Fermi surface instabilities in future fields of condensed matter research.", "prompt": "Technological progress in material synthesis, as well as artificial realization of condensed matter scenarios via ultra-cold atomic gases in optical lattices or epitaxial growth of thin films, is opening the gate to investigate a plethora of unprecedented strongly correlated electron systems. In a large subclass thereof, a metallic state of layered electrons", "orig": "undergoes an ordering transition below some temperature into unconventional states of matter driven by electronic correlations, such as magnetism, superconductivity, or other Fermi surface instabilities. While this type of phenomena has been a well-established direction of research in condensed matter for decades, the variety of today's accessible scenarios pose fundamental new", "generated": "Technological progress in material synthesis, as well as artificial realization of condensed matter scenarios via ultra-cold atomic gases in optical lattices or epitaxial growth of thin films, is opening the gate to investigate a plethora of unprecedented strongly correlated electron systems. In a large subclass thereof, a metallic state of layered electrons"}
{"full_text": "Recent technological advances have enabled the recording of neurons in intact circuits with a high spatial and temporal resolution, creating the need for modeling with the same precision. In particular, the development of ultra-fast two-photon microscopy combined with fluorescence-based genetically-encoded Ca2+-indicators allows capture of full-dendritic arbor and somatic responses associated with synaptic input and action potential output. The complexity of dendritic arbor structures and distributed patterns of activity over time results in the generation of incredibly rich 4D datasets that are challenging to analyze (Sakaki, 2020). Interpreting neural activity from fluorescence-based Ca2+ biosensors is challenging due to non-linear interactions between several factors influencing intracellular calcium ion concentration and its binding to sensors, including the ionic dynamics driven by diffusion, electrical gradients and voltage-gated conductance.To investigate those dynamics, we designed a model based on a Cable-like equation coupled to the Nernst-Planck equations for ionic fluxes in electrolytes. We employ this model to simulate signal propagation and ionic electrodiffusion across a dendritic arbor. Using these simulation results, we then designed an algorithm to detect synapses from Ca2+ imaging datasets. We finally apply this algorithm to experimental Ca2+-indicator datasets from neurons expressing jGCaMP7s (Dana et al., 2019), using full-dendritic arbor sampling in vivo in the Xenopus laevis optic tectum using fast random-access two-photon microscopy.Our model reproduces the dynamics of visual stimulus-evoked jGCaMP7s-mediated calcium signals observed experimentally, and the resulting algorithm allows prediction of the location of synapses across the dendritic arbor.Our study provides a way to predict synaptic activity and location on dendritic arbors, from fluorescence data in the full dendritic arbor of a neuron recorded in the intact and awake developing vertebrate brain.", "prompt": "Recent technological advances have enabled the recording of neurons in intact circuits with a high spatial and temporal resolution, creating the need for modeling with the same precision. In particular, the development of ultra-fast two-photon microscopy combined with fluorescence-based genetically-encoded Ca2+-indicators allows capture of full-dendritic arbor and", "orig": "somatic responses associated with synaptic input and action potential output. The complexity of dendritic arbor structures and distributed patterns of activity over time results in the generation of incredibly rich 4D datasets that are challenging to analyze (Sakaki, 2020). Interpreting neural activity from fluorescence-based Ca2+ bios", "generated": "Recent technological advances have enabled the recording of neurons in intact circuits with a high spatial and temporal resolution, creating the need for modeling with the same precision. In particular, the development of ultra-fast two-photon microscopy combined with fluorescence-based genetically-encoded Ca2+-indicators allows capture of full-dendritic arbor and"}
{"full_text": "Modern software development without reactive programming is hard to imagine. Reactive programming favors a wide class of contemporary software systems that respond to user input, network messages, and other events. While reactive programming is an active field of research, the implementation of reactive concepts remains challenging. In particular, change detection represents a hard but inevitable necessity when implementing reactive concepts. Typically, change detection mechanisms are not intended for reuse but are tightly coupled to the particular change resolution mechanism. As a result, developers often have to re-implement similar abstractions. A reusable primitive for change detection is still missing. To find a suitable primitive, we identify commonalities in existing reactive concepts. We discover a class of reactive concepts, state-based reactive concepts. All state-based reactive concepts share a common change detection mechanism: they detect changes in the evaluation result of an expression. On the basis of the identified common change detection mechanism, we propose active expressions as a reusable primitive. By abstracting the tedious implementation details of change detection, active expressions can ease the implementation of reactive programming concepts. We evaluate the design of active expressions by re-implementing a number of existing state-based reactive concepts using them. The resulting implementations highlight the expressiveness of active expressions. Active expressions enable the separation of essential from non-essential parts when reasoning about reactive programming concepts. By using active expressions as a primitive for change detection, developers of reactive language constructs and runtime support can now focus on the design of how application programmers should be able to react to change. Ultimately, we would like active expressions to encourage experiments with novel reactive programming concepts and with that to yield a wider variety of them to explore.", "prompt": "Modern software development without reactive programming is hard to imagine. Reactive programming favors a wide class of contemporary software systems that respond to user input, network messages, and other events. While reactive programming is an active field of research, the implementation of reactive concepts remains challenging. In particular, change detection represents a hard but inevitable necessity", "orig": "when implementing reactive concepts. Typically, change detection mechanisms are not intended for reuse but are tightly coupled to the particular change resolution mechanism. As a result, developers often have to re-implement similar abstractions. A reusable primitive for change detection is still missing. To find a suitable primitive, we identify commonalities in existing", "generated": "Modern software development without reactive programming is hard to imagine. Reactive programming favors a wide class of contemporary software systems that respond to user input, network messages, and other events. While reactive programming is an active field of research, the implementation of reactive concepts remains challenging. In particular, change detection represents a hard but inevitable necessity"}
{"full_text": "Multiparty session types (MST) are a well-established type theory that describes the interactive structure of a fixed number of components from a global point of view and type-checks the components through projection of the global type onto the participants of the session. They guarantee communicationsafety for a language of multiparty sessions (LMS), i.e., distributed, parallel components can exchange values without deadlocking and unexpected message types. Several variants of MST and LMS have been proposed to study key features of distributed and parallel programming. We observe that the population of the considered variants follows from only one ancestor, i.e., the original LMS/MST, and there are overlapping traits between features of the considered variants and the original. These hamper evolution of session types and languages and their adoption in practice. This paper addresses the following question: What are the essential features for MST and LMS, and how can these be modelled with simple constructs? To the best of our knowledge, this is the first time this question has been addressed. We performed a systematic analysis of the features and the constructs in MST, LMS, and the considered variants to identify the essential features. The variants are among the most influential (according to Google Scholar) and well-established systems that cover a wide set of areas in distributed, parallel programming. We used classical techniques of formal models such as BNF, structural congruence, small step operational semantics and typing judgments to build our language and type system. Lastly, the coherence of operational semantics and type system is proven by induction. This paper proposes a set of essential features, a language of structured interactions and a type theory of comprehensive multiparty session types, including global types and type system. The analysis removes overlapping features and captures the shared traits, thereby introducing the essential features. The constructs of the language are simple and fundamental, based on the $\\lambda$ and $\\pi$ calculi. Analogously, our global types reflect what is omitted and introduced in the language. Our system covers all the features of the original and variants, with a better ratio of the number of language and type constructs over the number of covered features. The features of the original, variants, and our system along with the number of constructs in the respective language and global types to model them are presented through a table. The syntax, operational semantics, meta-theory and type system of our system are given. We modelled all the motivating examples of the variants in our model, describing the reduction and typing steps. The work discusses how new features, in particular the non-essential ones (formerly excluded) and advanced ones can be either modelled atop the essential ones or added with minimal efforts, i.e. without modifying the existing ones. The fundamental properties of typed processes such as subject reduction, communication safety, and progress are established.", "prompt": "Multiparty session types (MST) are a well-established type theory that describes the interactive structure of a fixed number of components from a global point of view and type-checks the components through projection of the global type onto the participants of the session. They guarantee communicationsafety for a language of multiparty sessions (L", "orig": "MS), i.e., distributed, parallel components can exchange values without deadlocking and unexpected message types. Several variants of MST and LMS have been proposed to study key features of distributed and parallel programming. We observe that the population of the considered variants follows from only one ancestor, i.e., the original LMS/MST", "generated": "Multiparty session types (MST) are a well-established type theory that describes the interactive structure of a fixed number of components from a global point of view and type-checks the components through projection of the global type onto the participants of the session. They guarantee communicationsafety for a language of multiparty sessions (L"}
{"full_text": "Several multiagent reinforcement learning (MARL) algorithms have been proposed to optimize agents decisions. Due to the complexity of the problem, the majority of the previously developed MARL algorithms assumed agents either had some knowledge of the underlying game (such as Nash equilibria) and/or observed other agents actions and the rewards they received. We introduce a new MARL algorithm called the Weighted Policy Learner (WPL), which allows agents to reach a Nash Equilibrium (NE) in benchmark 2-player-2-action games with minimum knowledge. Using WPL, the only feedback an agent needs is its own local reward (the agent does not observe other agents actions or rewards). Furthermore, WPL does not assume that agents know the underlying game or the corresponding Nash Equilibrium a priori. We experimentally show that our algorithm converges in benchmark two-player-two-action games. We also show that our algorithm converges in the challenging Shapleys game where previous MARL algorithms failed to converge without knowing the underlying game or the NE. Furthermore, we show that WPL outperforms the state-of-the-art algorithms in a more realistic setting of 100 agents interacting and learning concurrently. An important aspect of understanding the behavior of a MARL algorithm is analyzing the dynamics of the algorithm: how the policies of multiple learning agents evolve over time as agents interact with one another. Such an analysis not only verifies whether agents using a given MARL algorithm will eventually converge, but also reveals the behavior of the MARL algorithm prior to convergence. We analyze our algorithm in two-player-two-action games and show that symbolically proving WPLs convergence is difficult, because of the non-linear nature of WPLs dynamics, unlike previous MARL algorithms that had either linear or piece-wise-linear dynamics. Instead, we numerically solve WPLs dynamics differential equations and compare the solution to the dynamics of previous MARL algorithms.", "prompt": "Several multiagent reinforcement learning (MARL) algorithms have been proposed to optimize agents decisions. Due to the complexity of the problem, the majority of the previously developed MARL algorithms assumed agents either had some knowledge of the underlying game (such as Nash equilibria) and/or observed other agents actions and the rewards they", "orig": "received. We introduce a new MARL algorithm called the Weighted Policy Learner (WPL), which allows agents to reach a Nash Equilibrium (NE) in benchmark 2-player-2-action games with minimum knowledge. Using WPL, the only feedback an agent needs is its own local reward (the agent does", "generated": "Several multiagent reinforcement learning (MARL) algorithms have been proposed to optimize agents decisions. Due to the complexity of the problem, the majority of the previously developed MARL algorithms assumed agents either had some knowledge of the underlying game (such as Nash equilibria) and/or observed other agents actions and the rewards they"}
{"full_text": "Let $\\mathbf{f}=(f\\_1,\\ldots,f\\_m)$ and $\\mathbf{g}=(g\\_1,\\ldots,g\\_m)$ be two sets of $m\\geq 1$ nonlinear polynomials over $\\mathbb{K}[x\\_1,\\ldots,x\\_n]$ ($\\mathbb{K}$ being a field). We consider the computational problem of finding -- if any -- an invertible transformation on the variables mapping $\\mathbf{f}$ to $\\mathbf{g}$. The corresponding equivalence problem is known as {\\tt Isomorphism of Polynomials with one Secret} ({\\tt IP1S}) and is a fundamental problem in multivariate cryptography. The main result is a randomized polynomial-time algorithm for solving {\\tt IP1S} for quadratic instances, a particular case of importance in cryptography and somewhat justifying {\\it a posteriori} the fact that {\\it Graph Isomorphism} reduces to only cubic instances of {\\tt IP1S} (Agrawal and Saxena). To this end, we show that {\\tt IP1S} for quadratic polynomials can be reduced to a variant of the classical module isomorphism problem in representation theory, which involves to test the orthogonal simultaneous conjugacy of symmetric matrices. We show that we can essentially {\\it linearize} the problem by reducing quadratic-{\\tt IP1S} to test the orthogonal simultaneous similarity of symmetric matrices; this latter problem was shown by Chistov, Ivanyos and Karpinski to be equivalent to finding an invertible matrix in the linear space $\\mathbb{K}^{n \\times n}$ of $n \\times n$ matrices over $\\mathbb{K}$ and to compute the square root in a matrix algebra. While computing square roots of matrices can be done efficiently using numerical methods, it seems difficult to control the bit complexity of such methods. However, we present exact and polynomial-time algorithms for computing the square root in $\\mathbb{K}^{n \\times n}$ for various fields (including finite fields). We then consider \\\\#{\\tt IP1S}, the counting version of {\\tt IP1S} for quadratic instances. In particular, we provide a (complete) characterization of the automorphism group of homogeneous quadratic polynomials. Finally, we also consider the more general {\\it Isomorphism of Polynomials} ({\\tt IP}) problem where we allow an invertible linear transformation on the variables \\emph{and} on the set of polynomials. A randomized polynomial-time algorithm for solving {\\tt IP} when \\(\\mathbf{f}=(x\\_1^d,\\ldots,x\\_n^d)\\) is presented. From an algorithmic point of view, the problem boils down to factoring the determinant of a linear matrix (\\emph{i.e.}\\ a matrix whose components are linear polynomials). This extends to {\\tt IP} a result of Kayal obtained for {\\tt PolyProj}.", "prompt": "Let $\\mathbf{f}=(f\\_1,\\ldots,f\\_m)$ and $\\mathbf{g}=(g\\_1,\\ldots,g\\_m)$ be two sets of $m\\geq 1$ nonlinear polynomials over $\\mathbb{K}[x\\_1,\\ld", "orig": "ots,x\\_n]$ ($\\mathbb{K}$ being a field). We consider the computational problem of finding -- if any -- an invertible transformation on the variables mapping $\\mathbf{f}$ to $\\mathbf{g}$. The corresponding equivalence problem is known as {\\tt Isomorphism of Polynomials", "generated": "$\\mathbf{f}=(f\\_1,\\ldots,f\\_m)$ and $\\mathbf{g}=(g\\_1,\\ldots,g\\_m)$ be two sets of $m\\geq 1$ nonlinear polynomials over $\\mathbb{K}[x\\_1,\\ldots"}
{"full_text": "An expression of the piano soundboard mechanical mobility (in the direction normal to the soundboard) depending on a small number of parameters and valid up to several kHz is given in this communication. Up to 1.1 kHz, our experimental and numerical investigations confirm previous results showing that the soundboard behaves like a homogeneous plate with isotropic properties and clamped boundary conditions. Therefore, according to the Skudrzyk mean-value theorem (Skudrzyk 1980), only the mass of the structure M, the modal density n(f), and the mean loss factor eta(f), are needed to express the average driving point mobility. Moreover, the expression of the envelope - resonances and antiresonances - of the mobility can be derived, according to (Langley 1994). We measured the modal loss factor and the modal density of the soundboard of an upright piano in playing condition, in an anechoic environment. The measurements could be done up to 2.5 kHz, with a novel high-resolution modal analysis technique (see the ICA companion-paper, Ege and Boutillon (2010)). Above 1.1 kHz, the change in the observed modal density together with numerical simulations confirm Berthaut's finding that the waves in the soundboard are confined between adjacent ribs (Berthaut et al. 2003). Extending the Skudrzyk and Langley approaches, we synthesize the mechanical mobility at the bridge up to 2.5 kHz. The validity of the computation for an extended spectral domain is discussed. It is also shown that the evolution of the modal density with frequency is consistent with the rise of mobility (fall of impedance) in this frequency range and that both are due to the inter-rib effect appearing when the half-wavelength becomes equal to the rib spacing. Results match previous observations by Wogram (1980), Conklin (1996), Giordano (1998), Nakamura (1983) and could be used for numerical simulations for example. This approach avoids the detailed description of the soundboard, based on a very high number of parameters. However, it can be used to predict the changes of the driving point mobility, and possibly of the sound radiation in the treble range, resulting from structural modifications.", "prompt": "An expression of the piano soundboard mechanical mobility (in the direction normal to the soundboard) depending on a small number of parameters and valid up to several kHz is given in this communication. Up to 1.1 kHz, our experimental and numerical investigations confirm previous results showing that the soundboard behaves like a homogeneous", "orig": "plate with isotropic properties and clamped boundary conditions. Therefore, according to the Skudrzyk mean-value theorem (Skudrzyk 1980), only the mass of the structure M, the modal density n(f), and the mean loss factor eta(f), are needed to express the average driving point mobility.", "generated": "An expression of the piano soundboard mechanical mobility (in the direction normal to the soundboard) depending on a small number of parameters and valid up to several kHz is given in this communication. Up to 1.1 kHz, our experimental and numerical investigations confirm previous results showing that the soundboard behaves like a homogeneous"}
{"full_text": "Compared to other self-propelled particles, pedestrians are able to anticipate, which gives them an edge in avoiding collisions and navigating in cluttered spaces. These capabilities are impaired by digital distraction through smartphones, a growing safety concern. To capture these features, we put forward a continuous agent-based model (dubbed ANDA) hinging on a transparent delineation of a decision-making process and a mechanical layer that handles contacts and collisions. In the decisional layer, each agent autonomously selects their desired velocity as the optimum of a perceived cost, notably balancing the will to move forward (described by a floor field) with the bio-mechanical cost of walking and the risk of collision, assessed by an anticipated time-to-collision. Altogether, the model includes less than a dozen parameters, most of which are fit using independent experimental data. Numerical simulations demonstrate the versatility of the approach, which succeeds in reproducing empirical observations in extremely diverse scenarios, often quantitatively, with a single set of parameters. These scenarios range from collision avoidance involving one, two, or more agents to collective flow properties in unidirectional and bidirectional settings and to the dynamics of evacuation through a bottleneck, where contact forces are directly accessible. Remarkably, a straightforward transcription of digital distraction into the model, by reducing the frequency of decisional updates, suffices to replicate the enhanced chaoticity of the flow, with more frequent sudden turns, observed experimentally when 'smartphone-walking' pedestrians are brought in. Finally, the conceptual transparency of the model makes it easy to pinpoint the origin of some deficiencies, notably its shortsighted account of anticipation (when agents have to cross a group of people) and the disk-like pedestrian shape (when very dense crowds are considered). Our work thus clarifies the singular position of pedestrian crowds in the midst of active-matter systems.", "prompt": "Compared to other self-propelled particles, pedestrians are able to anticipate, which gives them an edge in avoiding collisions and navigating in cluttered spaces. These capabilities are impaired by digital distraction through smartphones, a growing safety concern. To capture these features, we put forward a continuous agent-based model (dubbed ANDA", "orig": ") hinging on a transparent delineation of a decision-making process and a mechanical layer that handles contacts and collisions. In the decisional layer, each agent autonomously selects their desired velocity as the optimum of a perceived cost, notably balancing the will to move forward (described by a floor field) with the bio-m", "generated": "Compared to other self-propelled particles, pedestrians are able to anticipate, which gives them an edge in avoiding collisions and navigating in cluttered spaces. These capabilities are impaired by digital distraction through smartphones, a growing safety concern. To capture these features, we put forward a continuous agent-based model (dubbed ANDy"}
{"full_text": "In a series of precedent papers, we have presented a comprehensive methodology, termed Field Economics, for translating a standard economic model into a statistical field-formalism framework. This formalism requires a large number of heterogeneous agents, possibly of different types. It reveals the emergence of collective states among these agents or type of agents while preserving the interactions and microeconomic features of the system at the individual level. In two prior papers, we applied this formalism to analyze the dynamics of capital allocation and accumulation in a simple microeconomic framework of investors and firms.Building upon our prior work, the present paper refines the initial model by expanding its scope. Instead of considering financial firms investing solely in real sectors, we now suppose that financial agents may also invest in other financial firms. We also introduce banks in the system that act as investors with a credit multiplier. Two types of interaction are now considered within the financial sector: financial agents can lend capital to, or choose to buy shares of, other financial firms. Capital now flows between financial agents and is only partly invested in real sectors, depending on their relative returns. We translate this framework into our formalism and study the diffusion of capital and possible defaults in the system, both at the macro and micro level.At the macro level, we find that several collective states may emerge, each characterized by a distinct level of average capital and investors per sector. These collective states depend on external parameters such as level of connections between investors or firms' productivity.The multiplicity of possible collective states is the consequence of the nature of the system composed of interconnected heterogeneous agents. Several equivalent patterns of returns and portfolio allocation may emerge. The multiple collective states induce the unstable nature of financial markets, and some of them include defaults may emerge. At the micro level, we study the propagation of returns and defaults within a given collective state. Our findings highlight the significant role of banks, which can either stabilize the system through lending activities or propagate instability through loans to investors.", "prompt": "In a series of precedent papers, we have presented a comprehensive methodology, termed Field Economics, for translating a standard economic model into a statistical field-formalism framework. This formalism requires a large number of heterogeneous agents, possibly of different types. It reveals the emergence of collective states among these agents or type of agents", "orig": "while preserving the interactions and microeconomic features of the system at the individual level. In two prior papers, we applied this formalism to analyze the dynamics of capital allocation and accumulation in a simple microeconomic framework of investors and firms.Building upon our prior work, the present paper refines the initial model by expanding its scope", "generated": "In a series of precedent papers, we have presented a comprehensive methodology, termed Field Economics, for translating a standard economic model into a statistical field-formalism framework. This formalism requires a large number of heterogeneous agents, possibly of different types. It reveals the emergence of collective states among these agents or type of agent"}
{"full_text": "Low image contrast is a major limitation in transmission electron microscopy, since samples with low atomic number only weakly phase-modulate the illuminating electron beam, and beam-induced sample damage limits the usable electron dose. The contrast can be increased by converting the electron beam's phase modulation into amplitude modulation using a phase plate, a device that applies a $\\pi/2$ radian phase shift to part of the electron beam after it has passed through the sample. Previous phase plate designs rely on material placed in or near the electron beam to provide this phase shift. This results in image aberrations, an inconsistent time-varying phase shift, and resolution loss when the electron beam charges, damages, or is scattered from the material. In this thesis, I present the theory, design, and implementation of the laser phase plate, which instead uses a focused continuous-wave laser beam to phase shift the electron beam. A near-concentric Fabry-P\\'{e}rot optical cavity focuses and resonantly enhances the power of the laser beam in order to achieve the high intensity required to provide the phase shift. We demonstrate that the cavity can surpass this requirement and generate a record-high continuous-wave laser intensity of $590 \\, \\mathrm{GW}/\\mathrm{cm}^{-2}$. By integrating the cavity into a transmission electron microscope, we show that the ponderomotive potential of the laser beam applies a spatially selective phase shift to the electron beam. This enables us to make the first experimental observation of the relativistic reversal of the ponderomotive potential. We then theoretically analyze the properties of the contrast transfer function generated by the laser phase plate. We experimentally determine that resolution loss caused by thermal magnetic field noise emanating from electrically conductive materials in the cavity can be eliminated by designing the cavity with a sufficiently large electron beam aperture. Finally, we show that the laser phase plate provides a stable $\\pi/2$ phase shift and concomitant contrast enhancement when imaging frozen hydrated biological macromolecules. We use these images to successfully determine the structure of the molecules. This demonstrates the laser phase plate as the first stable and lossless phase plate for transmission electron microscopy", "prompt": "Low image contrast is a major limitation in transmission electron microscopy, since samples with low atomic number only weakly phase-modulate the illuminating electron beam, and beam-induced sample damage limits the usable electron dose. The contrast can be increased by converting the electron beam's phase modulation into amplitude modulation using a phase plate, a", "orig": "device that applies a $\\pi/2$ radian phase shift to part of the electron beam after it has passed through the sample. Previous phase plate designs rely on material placed in or near the electron beam to provide this phase shift. This results in image aberrations, an inconsistent time-varying phase shift, and", "generated": "Low image contrast is a major limitation in transmission electron microscopy, since samples with low atomic number only weakly phase-modulate the illuminating electron beam, and beam-induced sample damage limits the usable electron dose. The contrast can be increased by converting the electron beam's phase modulation into amplitude modulation using a phase plate, a"}
{"full_text": "Non-Gaussian outcomes are often modeled using members of the so-called exponential family. Notorious members are the Bernoulli model for binary data, leading to logistic regression, and the Poisson model for count data, leading to Poisson regression. Two of the main reasons for extending this family are (1) the occurrence of overdispersion, meaning that the variability in the data is not adequately described by the models, which often exhibit a prescribed mean--variance link, and (2) the accommodation of hierarchical structure in the data, stemming from clustering in the data which, in turn, may result from repeatedly measuring the outcome, for various members of the same family, etc. The first issue is dealt with through a variety of overdispersion models, such as, for example, the beta-binomial model for grouped binary data and the negative-binomial model for counts. Clustering is often accommodated through the inclusion of random subject-specific effects. Though not always, one conventionally assumes such random effects to be normally distributed. While both of these phenomena may occur simultaneously, models combining them are uncommon. This paper proposes a broad class of generalized linear models accommodating overdispersion and clustering through two separate sets of random effects. We place particular emphasis on so-called conjugate random effects at the level of the mean for the first aspect and normal random effects embedded within the linear predictor for the second aspect, even though our family is more general. The binary, count and time-to-event cases are given particular emphasis. Apart from model formulation, we present an overview of estimation methods, and then settle for maximum likelihood estimation with analytic--numerical integration. Implications for the derivation of marginal correlations functions are discussed. The methodology is applied to data from a study in epileptic seizures, a clinical trial in toenail infection named onychomycosis and survival data in children with asthma.", "prompt": "Non-Gaussian outcomes are often modeled using members of the so-called exponential family. Notorious members are the Bernoulli model for binary data, leading to logistic regression, and the Poisson model for count data, leading to Poisson regression. Two of the main reasons for extending this family are (1) the", "orig": "occurrence of overdispersion, meaning that the variability in the data is not adequately described by the models, which often exhibit a prescribed mean--variance link, and (2) the accommodation of hierarchical structure in the data, stemming from clustering in the data which, in turn, may result from repeatedly measuring the outcome,", "generated": "Non-Gaussian outcomes are often modeled using members of the so-called exponential family. Notorious members are the Bernoulli model for binary data, leading to logistic regression, and the Poisson model for count data, leading to Poisson regression. Two of the main reasons for extending this family are (1) the"}
{"full_text": "Biological data mainly comprises of Deoxyribonucleic acid (DNA) and protein sequences. These are the biomolecules which are present in all cells of human beings. Due to the self-replicating property of DNA, it is a key constitute of genetic material that exist in all breathingcreatures. This biomolecule (DNA) comprehends the genetic material obligatory for the operational and expansion of all personified lives. To save DNA data of single person we require 10CD-ROMs.Moreover, this size is increasing constantly, and more and more sequences are adding in the public databases. This abundant increase in the sequence data arise challenges in the precise information extraction from this data. Since many data analyzing and visualization tools do not support processing of this huge amount of data. To reduce the size of DNA and protein sequence, many scientists introduced various types of sequence compression algorithms such as compress or gzip, Context Tree Weighting (CTW), Lampel Ziv Welch (LZW), arithmetic coding, run-length encoding and substitution method etc. These techniques have sufficiently contributed to minimizing the volume of the biological datasets. On the other hand, traditional compression techniques are also not much suitable for the compression of these types of sequential data. In this paper, we have explored diverse types of techniques for compression of large amounts of DNA Sequence Data. In this paper, the analysis of techniques reveals that efficient techniques not only reduce the size of the sequence but also avoid any information loss. The review of existing studies also shows that compression of a DNA sequence is significant for understanding the critical characteristics of DNA data in addition to improving storage efficiency and data transmission. In addition, the compression of the protein sequence is a challenge for the research community. The major parameters for evaluation of these compression algorithms include compression ratio, running time complexity etc.", "prompt": "Biological data mainly comprises of Deoxyribonucleic acid (DNA) and protein sequences. These are the biomolecules which are present in all cells of human beings. Due to the self-replicating property of DNA, it is a key constitute of genetic material that exist in all breathingcreatures.", "orig": "This biomolecule (DNA) comprehends the genetic material obligatory for the operational and expansion of all personified lives. To save DNA data of single person we require 10CD-ROMs.Moreover, this size is increasing constantly, and more and more sequences are adding in the public databases. This abundant increase in the sequence", "generated": "Biological data mainly comprises of Deoxyribonucleic acid (DNA) and protein sequences. These are the biomolecules which are present in all cells of human beings. Due to the self-replicating property of DNA, it is a key constitute of genetic material that exist in all breathing creatures.\n\nThe"}
{"full_text": "The purpose of this paper is to point the effectiveness of the Jordan-Chevalley decomposition, i.e. the decomposition of a square matrix $U$ with coefficients in a field $k$ containing the eigenvalues of $U$ as a sum $U=D+N,$ where $D$ is a diagonalizable matrix and $N$ a nilpotent matrix which commutes with $D.$ The most general version of this decomposition shows that every separable element $u$ of a $k$-algebra $A$ can be written in a unique way as a sum $u=d+n,$ where $d \\in A$ is absolutely semi-simple and where $n\\in A$ is nilpotent and commutes with $d.$ In fact an algorithm, due to C. Chevalley, allows to compute this decomposition: this algorithm is an adaptation to this context of the Newton method, which gives here the exact value of the absolutely semi-simple part $d$ of $u$ after a finite number of iterations. We illustrate the effectiveness of this method by computing the decomposition of a $15 \\times 15$ matrix having eigenvalues of multiplicity 3 which are not computable exactly. We also discuss the other classical method, based on the chinese remainder theorem, which gives the Jordan-Chevalley decomposition under the form $u=q(u) +[u-q(u)],$ with $q(u)$ absolutely semi-simple, $u-q(u)$ nilpotent, where $q$ is any solution of a system of congruence equations related to the roots of a polynomial $p\\in k[x]$ such that $p(u)=0.$ It is indeed possible to compute $q$ without knowing the roots of $p$ by applying the algorithm discussed above to $\\pi(x),$ where $\\pi: k[x] \\to k[x]/pk[x]$ is the canonical surjection. We obtain this way after 2 iterations the polynomial $q$ of degree 14 associated to the $15\\times 15$ matrix mentioned above. We justify by historical considerations the use of the name \"Jordan-Chevalley decomposition\", instead of the name \"Dunford decomposition\" which also appears in the literature, and we discuss multiplicative versions of this decomposition in semi-simple Lie groups. We conclude this paper showing why this decomposition should play a central role in a linear algebra course, even at a rather elementary level. Our arguments are based on a teaching experience of more than 10 years in an engineering school located on the Basque Coast.", "prompt": "The purpose of this paper is to point the effectiveness of the Jordan-Chevalley decomposition, i.e. the decomposition of a square matrix $U$ with coefficients in a field $k$ containing the eigenvalues of $U$ as a sum $U=D+N,$ where $D$ is a diagonalizable", "orig": "matrix and $N$ a nilpotent matrix which commutes with $D.$ The most general version of this decomposition shows that every separable element $u$ of a $k$-algebra $A$ can be written in a unique way as a sum $u=d+n,$ where $d \\in", "generated": "The purpose of this paper is to point the effectiveness of the Jordan-Chevalley decomposition, i.e. the decomposition of a square matrix $U$ with coefficients in a field $k$ containing the eigenvalues of $U$ as a sum $U=D+N,$ where $D$ is a diagonalizable"}
{"full_text": "In 1893, the \"physicist-engineer\" Andr\\'e Blondel invents the oscilloscope for displaying voltage and current variables. With this powerful means of investigation, he first studies the phenomena of the arc then used for the coastal and urban lighting and then, the singing arc used as a transmitter of radio waves in wireless telegraphy. In 1905, he highlights a new type of non-sinusoidal oscillations in the singing arc. Twenty years later, Balthasar van der Pol will recognize that such oscillations were in fact \"relaxation oscillations\". To explain this phenomenon, he uses a representation in the phase plane and shows that its evolution takes the form of small cycles. During World War I the triode gradually replaces the singing arc in transmission systems. At the end of the war, using analogy, Blondel transposes to the triode most of the results he had obtained for the singing arc. In April 1919, he publishes a long memoir in which he introduces the terminology \"self-sustained oscillations\" and proposes to illustrate this concept starting from the example of the Tantalus cup which is then picked up by Van der Pol and Philippe Le Corbeiller. He then provides the definition of a self sustained system which is quite similar to that given later by Aleksandr Andronov and Van der Pol. To study the stability of oscillations sustained by the triode and by the singing arc he uses, this time, a representation in the complex plane and he expresses the amplitude in polar coordinates. He then justifies the maintaining of oscillations by the existence cycles which nearly present all the features of Poincar\\'e's limit cycles. Finally, in November 1919, Blondel performs, a year before Van der Pol, the setting in equation of the triode oscillations. In March 1926, Blondel establishes the differential equation characterizing the oscillations of the singing arc, completely similar to that obtained concomitantly by Van der Pol for the triode. Thus, throughout his career, Blondel, has made fundamental and relatively unknown contributions to the development of the theory of nonlinear oscillations. The purpose of this article is to analyze his main work in this area and to measure their importance or influence by placing them in the perspective of the development of this theory.", "prompt": "In 1893, the \"physicist-engineer\" Andr\\'e Blondel invents the oscilloscope for displaying voltage and current variables. With this powerful means of investigation, he first studies the phenomena of the arc then used for the coastal and urban lighting and then, the singing arc used as a", "orig": "transmitter of radio waves in wireless telegraphy. In 1905, he highlights a new type of non-sinusoidal oscillations in the singing arc. Twenty years later, Balthasar van der Pol will recognize that such oscillations were in fact \"relaxation oscillations\". To explain this phenomenon, he", "generated": "display device in the theater. He was fascinated by the electrical discharges of the singing arc and began to study them in more detail. He discovered that the shape of the electrical discharges was influenced by the magnetic field generated by the arc itself. This observation led him to experiment with the effects of magnetic fields on electrical"}
{"full_text": "Lean processes focus on doing only necessery things in an efficient way. Artificial intelligence and Machine Learning offer new opportunities to optimizing processes. The presented approach demonstrates an improvement of the test process by using Machine Learning as a support tool for test management. The scope is the semi-automation of the selection of regression tests. The proposed lean testing process uses Machine Learning as a supporting machine, while keeping the human test manager in charge of the adequate test case selection. 1 Introduction Many established long running projects and programs are execute regression tests during the release tests. The regression tests are the part of the release test to ensure that functionality from past releases still works fine in the new release. In many projects, a significant part of these regression tests are not automated and therefore executed manually. Manual tests are expensive and time intensive [1], which is why often only a relevant subset of all possible regression tests are executed in order to safe time and money. Depending on the software process, different approaches can be used to identify the right set of regression tests. The source code file level is a frequent entry point for this identification [2]. Advanced approaches combine different file level methods [3]. To handle black-box tests, methods like [4] or [5] can be used for test case prioritiza-tion. To decide which tests can be skipped, a relevance ranking of the tests in a regression test suite is needed. Based on the relevance a test is in or out of the regression test set for a specific release. This decision is a task of the test manager supported by experts. The task can be time-consuming in case of big (often a 4-to 5-digit number) regression test suites because the selection is specific to each release. Trends are going to continuous prioritization [6], which this work wants to support with the presented ML based approach for black box regression test case prioritization. Any regression test selection is made upon release specific changes. Changes can be new or deleted code based on refactoring or implementation of new features. But also changes on externals systems which are connected by interfaces have to be considered", "prompt": "Lean processes focus on doing only necessery things in an efficient way. Artificial intelligence and Machine Learning offer new opportunities to optimizing processes. The presented approach demonstrates an improvement of the test process by using Machine Learning as a support tool for test management. The scope is the semi-automation of the selection of regression tests. The", "orig": "proposed lean testing process uses Machine Learning as a supporting machine, while keeping the human test manager in charge of the adequate test case selection. 1 Introduction Many established long running projects and programs are execute regression tests during the release tests. The regression tests are the part of the release test to ensure that functionality from past releases still", "generated": "Lean processes focus on doing only necessary things in an efficient way. Artificial intelligence and Machine Learning offer new opportunities to optimizing processes. The presented approach demonstrates an improvement of the test process by using Machine Learning as a support tool for test management. The scope is the semi-automation of the selection of regression tests.\n\nThe proposed"}
{"full_text": "In recent years, heterogeneous computing has emerged as the vital way to increase computers? performance and energy efficiency by combining diverse hardware devices, such as Graphics Processing Units (GPUs) and Field Programmable Gate Arrays (FPGAs). The rationale behind this trend is that different parts of an application can be offloaded from the main CPU to diverse devices, which can efficiently execute these parts as co-processors. FPGAs are a subset of the most widely used co-processors, typically used for accelerating specific workloads due to their flexible hardware and energy-efficient characteristics. These characteristics have made them prevalent in a broad spectrum of computing systems ranging from low-power embedded systems to high-end data centers and cloud infrastructures. However, these hardware characteristics come at the cost of programmability. Developers who create their applications using high-level programming languages (e.g., Java, Python, etc.) are required to familiarize with a hardware description language (e.g., VHDL, Verilog) or recently heterogeneous programming models (e.g., OpenCL, HLS) in order to exploit the co-processors? capacity and tune the performance of their applications. Currently, the above-mentioned heterogeneous programming models support exclusively the compilation from compiled languages, such as C and C++. Thus, the transparent integration of heterogeneous co-processors to the software ecosystem of managed programming languages (e.g. Java, Python) is not seamless. In this paper we rethink the engineering trade-offs that we encountered, in terms of transparency and compilation overheads, while integrating FPGAs into high-level managed programming languages. We present a novel approach that enables runtime code specialization techniques for seamless and high-performance execution of Java programs on FPGAs. The proposed solution is prototyped in the context of the Java programming language and TornadoVM; an open-source programming framework for Java execution on heterogeneous hardware. Finally, we evaluate the proposed solution for FPGA execution against both sequential and multi-threaded Java implementations showcasing up to 224x and 19.8x performance speedups, respectively, and up to 13.82x compared to TornadoVM running on an Intel integrated GPU. We also provide a break-down analysis of the proposed compiler optimizations for FPGA execution, as a means to project their impact on the applications? characteristics.", "prompt": "In recent years, heterogeneous computing has emerged as the vital way to increase computers? performance and energy efficiency by combining diverse hardware devices, such as Graphics Processing Units (GPUs) and Field Programmable Gate Arrays (FPGAs). The rationale behind this trend is that different parts of an application can be offloaded from", "orig": "the main CPU to diverse devices, which can efficiently execute these parts as co-processors. FPGAs are a subset of the most widely used co-processors, typically used for accelerating specific workloads due to their flexible hardware and energy-efficient characteristics. These characteristics have made them prevalent in a broad spectrum of computing systems ranging", "generated": "In recent years, heterogeneous computing has emerged as the vital way to increase computers' performance and energy efficiency by combining diverse hardware devices, such as Graphics Processing Units (GPUs) and Field Programmable Gate Arrays (FPGAs). The rationale behind this trend is that different parts of an application can be offloaded from"}
{"full_text": "The problem of finding a reduced dimensionality representation of categorical variables while preserving their most relevant characteristics is fundamental for the analysis of complex data. Specifically, given a co-occurrence matrix of two variables, one often seeks a compact representation of one variable which preserves information about the other variable. We have recently introduced ``Sufficient Dimensionality Reduction' [GT-2003], a method that extracts continuous reduced dimensional features whose measurements (i.e., expectation values) capture maximal mutual information among the variables. However, such measurements often capture information that is irrelevant for a given task. Widely known examples are illumination conditions, which are irrelevant as features for face recognition, writing style which is irrelevant as a feature for content classification, and intonation which is irrelevant as a feature for speech recognition. Such irrelevance cannot be deduced apriori, since it depends on the details of the task, and is thus inherently ill defined in the purely unsupervised case. Separating relevant from irrelevant features can be achieved using additional side data that contains such irrelevant structures. This approach was taken in [CT-2002], extending the information bottleneck method, which uses clustering to compress the data. Here we use this side-information framework to identify features whose measurements are maximally informative for the original data set, but carry as little information as possible on a side data set. In statistical terms this can be understood as extracting statistics which are maximally sufficient for the original dataset, while simultaneously maximally ancillary for the side dataset. We formulate this tradeoff as a constrained optimization problem and characterize its solutions. We then derive a gradient descent algorithm for this problem, which is based on the Generalized Iterative Scaling method for finding maximum entropy distributions. The method is demonstrated on synthetic data, as well as on real face recognition datasets, and is shown to outperform standard methods such as oriented PCA.", "prompt": "The problem of finding a reduced dimensionality representation of categorical variables while preserving their most relevant characteristics is fundamental for the analysis of complex data. Specifically, given a co-occurrence matrix of two variables, one often seeks a compact representation of one variable which preserves information about the other variable. We have recently introduced ``S", "orig": "ufficient Dimensionality Reduction' [GT-2003], a method that extracts continuous reduced dimensional features whose measurements (i.e., expectation values) capture maximal mutual information among the variables. However, such measurements often capture information that is irrelevant for a given task. Widely known examples are illumination conditions, which are irrelevant as features", "generated": "The problem of finding a reduced dimensionality representation of categorical variables while preserving their most relevant characteristics is fundamental for the analysis of complex data. Specifically, given a co-occurrence matrix of two variables, one often seeks a compact representation of one variable which preserves information about the other variable. We have recently introduced ``Simple"}
{"full_text": "A large family of \"standard\" coboundary Hopf algebras is investigated. The existence of a universal R-matrix is demonstrated for the case when the parameters are in general position. Algebraic surfaces in parameter space are characterized by the appearance of certain ideals; in this case the universal R-matrix exists on the associated algebraic quotient. In special cases the quotient is a \"standard\" quantum group; all familiar quantum groups including twisted ones are obtained in this way. In other special cases one finds new types of coboundary bi-algebras. The \"standard\" universal R-matrix is shown to be the unique solution of a very simple, linear recursion relation. The classical limit is obtained in the case of quantized Kac-Moody algebras of finite and affine type. Returning to the general case, we study deformations of the standard R-matrix and the associated Hopf algebras. A preliminary investigation of the first order deformations uncovers a class of deformations that incompasses the quantization of all Kac-Moody algebras of finite and affine type. The corresponding exact deformations are described as generalized twists, $ R_\\epsilon = (F^t)^{-1}RF$, where $R$ is the standard R-matrix and the cocycle $F$ (a power series in the deformation parameter $\\epsilon$) is the solution of a linear recursion relation of the same type as that which determines $R$. Included here is the universal R-matrix for the elliptic quantum groups associated with $sl(n)$, a big surprise! Specializing again, to the case of quantized Kac-Moody algebras, and taking the classical limit of these esoteric quantum groups, one re-discovers all the trigonometric and elliptic r-matrices of Belavin and Drinfeld. The formulas obtained here are easier to use than the original ones, and the structure of the space of classical r-matrices is more transparent. The r-matrices obtained here are more general in that they are defined on the full Kac-Moody algebras, the central extensions of the loop groups.", "prompt": "A large family of \"standard\" coboundary Hopf algebras is investigated. The existence of a universal R-matrix is demonstrated for the case when the parameters are in general position. Algebraic surfaces in parameter space are characterized by the appearance of certain ideals; in this case the universal R-matrix exists on", "orig": "the associated algebraic quotient. In special cases the quotient is a \"standard\" quantum group; all familiar quantum groups including twisted ones are obtained in this way. In other special cases one finds new types of coboundary bi-algebras. The \"standard\" universal R-matrix is shown to be the unique solution of", "generated": "A large family of \"standard\" coboundary Hopf algebras is investigated. The existence of a universal R-matrix is demonstrated for the case when the parameters are in general position. Algebraic surfaces in parameter space are characterized by the appearance of certain ideals; in this case the universal R-matrix exists only"}
{"full_text": "By using meromorphic \"characters\" and \"logarithms\" built up from Euler's Gamma function, and by using convergent factorial series, we will give, in a first pat, a \"normal form\" to the solutions of a singular regular system. It will enable us to define a connexion matrix for a regular singular system. Following one of Birkhoff's idea, we will then study its link with the problem of rational classification of system. In a second part, we will be interested in the confluence of fuchsian difference systems to differential systems. We will show more particularly how we can get, under some natural hypotheses, the local monodromies of a limit differential system from the connection matrices of the deformation that we consider. The use of factorial series (which can diverge as power series) distinguish regular singular difference systems from their differential and q-difference analogues and make their study more difficult. En choisissant des \"caracteres\" et des \"logarithmes\", meromorphes sur le plan complexe, construits a l'aide de la fonction Gamma d'Euler, et en utilisant des series de factorielles convergentes, nous sommes en mesure, dans une premiere partie, de donner une \"forme normale\" pour les solutions d'un systeme aux differences singulier regulier. Nous pouvons alors definir une matrice de connexion d'un tel systeme. Nous etudions ensuite, suivant une idee de G.D. Birkhoff, le lien de celles-ci avec le probleme de la classification rationnelle des systemes. Dans une deuxieme partie, nous nous interessons la confluence des systemes aux differences fuchsiens vers les systemes differentiels. Nous montrons en particulier comment, sous certaines hypotheses naturelles, on peut reconstituer les monodromies locales d'un systeme differentiel limite a partir des matrices meromorphes de connexion des deformations considerees. Le point central, qui distingue en profondeur les systemes aux differences singuliers reguliers de leurs homonymes differentiels ou aux q-differences et qui rend leur etude plus complexe, est la necessaire utilisation de series de factorielles (qui peuvent diverger en tant que series de puissances).", "prompt": "By using meromorphic \"characters\" and \"logarithms\" built up from Euler's Gamma function, and by using convergent factorial series, we will give, in a first pat, a \"normal form\" to the solutions of a singular regular system. It will enable us to define a connexion matrix for a", "orig": "regular singular system. Following one of Birkhoff's idea, we will then study its link with the problem of rational classification of system. In a second part, we will be interested in the confluence of fuchsian difference systems to differential systems. We will show more particularly how we can get, under some natural", "generated": "Here's a repetition of the given text and a natural continuation:\n\nBy using meromorphic \"characters\" and \"logarithms\" built up from Euler's Gamma function, and by using convergent factorial series, we will give, in a first part, a \"normal form\" to the solutions of a singular regular"}
{"full_text": "The goal of this paper is to give a converse to the main result of my previous paper \\cite{[B.22]}, so to prove the existence of a pole with an hypothesis on the Bernstein polynomial of the $(a,b)$-module generated by the germ $\\omega \\in \\Omega^{n+1}_0$. A difficulty to prove such a result comes from the use of the formal completion in $f$ of the Brieskorn module of the holomorphic germ $f: (\\mathbb{C}^{n+1}, 0) \\to (\\mathbb{C}, 0)$ which does not give access to the cohomology of the Milnor's fiber of $f$, which by definition, is outside $\\{f=0\\}$. This leads to introduce convergent $(a,b)$-modules which allow this passage. In order to take in account Jordan blocs of the monodromy in our result we introduce the semi-simple filtration of a (convergent) geometric $(a,b)$-module and define the higher order Bernstein polynomials in this context which corresponds to a decomposition of the ''standard'' Bernstein polynomial in the case of frescos. Our main result is to show that the existence of a root in $-\\alpha - \\mathbb{N}$ for the $p$-th Bernstein polynomial of the fresco generated by a holomorphic form $\\omega \\in \\Omega^{n+1}_0$ in the (convergent) Brieskorn $(a,b)$-module $H^{n+1}_0 $ associated to $f$, under the hypothesis that $f$ has an isolated singularity at the origin relative to the eigenvalue $\\exp(2i\\pi\\alpha)$ of the monodromy, produces poles of order at least $p$ for the meromorphic extension of the (conjugate) analytic functional, for some $h \\in \\mathbb{Z}$: $$\\omega' \\in \\Omega^{n+1}_0 \\mapsto \\frac{1}{\\Gamma(\\lambda)}\\int_{\\mathbb{C}^{n+1}} \\vert f\\vert^{2\\lambda} \\bar f^{-h} \\rho\\omega\\wedge \\bar \\omega' $$ at points $-\\alpha - N$ for $N$ and $h$ well chosen integers. This result is new, even for $p = 1$. As a corollary, this implies that in this situation the existence of a root in $-\\alpha -\\mathbb{N}$ for the $p$-th Bernstein polynomial of the fresco generated by a holomorphic form $\\omega \\in \\Omega^{n+1}_0$ implies the existence of at least $p$ roots (counting multiplicities) for the usual reduced Bernstein polynomial of the germ $(f, 0)$. In the case of an isolated singularity we obtain that for each $\\alpha \\in ]0, 1] \\cap \\mathbb{Q}$ the biggest root $-\\alpha - m$ of the reduced Bernstein polynomial of $f$ in $-\\alpha - \\mathbb{N}$ produces a pole at $-\\alpha - m$ for some $h \\in \\mathbb{Z}$ for the meromorphic extension of the distribution $$\\square \\longrightarrow \\frac{1}{\\Gamma(\\lambda)}\\vert f\\vert^{2\\lambda} \\bar f^{-h}\\square.$$", "prompt": "The goal of this paper is to give a converse to the main result of my previous paper \\cite{[B.22]}, so to prove the existence of a pole with an hypothesis on the Bernstein polynomial of the $(a,b)$-module generated by the germ $\\omega \\in \\Omega^{n+1", "orig": "}_0$. A difficulty to prove such a result comes from the use of the formal completion in $f$ of the Brieskorn module of the holomorphic germ $f: (\\mathbb{C}^{n+1}, 0) \\to (\\mathbb{C}, 0)$ which does", "generated": "I'll repeat the text and continue it.\n\nThe goal of this paper is to give a converse to the main result of my previous paper \\cite{[B.22]}, so to prove the existence of a pole with an hypothesis on the Bernstein polynomial of the $(a,b)$-module generated by the germ $\\"}
{"full_text": "The ply elastic constants needed for classical lamination theory analysis of multi-directional laminates may differ from those obtained from unidirectional laminates because of three dimensional effects. In addition, the unidirectional laminates may not be available for testing. In such cases, full-field displacement measurements offer the potential of identifying several material properties simultaneously. For that, it is desirable to create complex displacement fields that are strongly influenced by all the elastic constants. In this work, we explore the potential of using a laminated plate with an open-hole under traction loading to achieve that and identify all four ply elastic constants (E 1, E 2, 12, G 12) at once. However, the accuracy of the identified properties may not be as good as properties measured from individual tests due to the complexity of the experiment, the relative insensitivity of the measured quantities to some of the properties and the various possible sources of uncertainty. It is thus important to quantify the uncertainty (or confidence) with which these properties are identified. Here, Bayesian identification is used for this purpose, because it can readily model all the uncertainties in the analysis and measurements, and because it provides the full coupled probability distribution of the identified material properties. In addition, it offers the potential to combine properties identified based on substantially different experiments. The full-field measurement is obtained by moir\\'e interferometry. For computational efficiency the Bayesian approach was applied to a proper orthogonal decomposition (POD) of the displacement fields. The analysis showed that the four orthotropic elastic constants are determined with quite different confidence levels as well as with significant correlation. Comparison with manufacturing specifications showed substantial difference in one constant, and this conclusion agreed with earlier measurement of that constant by a traditional four-point bending test. It is possible that the POD approach did not take full advantage of the copious data provided by the full field measurements, and for that reason that data is provided for others to use (as on line material attached to the article).", "prompt": "The ply elastic constants needed for classical lamination theory analysis of multi-directional laminates may differ from those obtained from unidirectional laminates because of three dimensional effects. In addition, the unidirectional laminates may not be available for testing. In such cases, full-field displacement measurements offer the potential of identifying", "orig": "several material properties simultaneously. For that, it is desirable to create complex displacement fields that are strongly influenced by all the elastic constants. In this work, we explore the potential of using a laminated plate with an open-hole under traction loading to achieve that and identify all four ply elastic constants (E 1, E", "generated": "The ply elastic constants needed for classical lamination theory analysis of multi-directional laminates may differ from those obtained from unidirectional laminates because of three-dimensional effects. In addition, the unidirectional laminates may not be available for testing. In such cases, full-field displacement measurements offer the potential of identifying"}
{"full_text": "The absorption of free linear chains in a polymer brush was studied with respect to chain size $L$ and compatibility $\\chi$ with the brush by means of Monte Carlo (MC) simulations and Density Functional Theory (DFT) / Self-Consistent Field Theory (SCFT) at both moderate, $\\sigma_g = 0.25$, and high, $\\sigma_g = 1.00$, grafting densities using a bead-spring model. Different concentrations of the free chains $0.0625 \\le \\phi_o \\le 0.375$ are examined. Contrary to the case of $\\chi = 0$ when all species are almost completely ejected by the polymer brush irrespective of their length $L$, for $\\chi < 0$ we find that the degree of absorption (absorbed amount) $\\Gamma(L)$ undergoes a sharp crossover from weak to strong ($\\approx 100%$) absorption, discriminating between oligomers, $1\\le L\\le 8$, and longer chains. For a moderately dense brush, $\\sigma_g = 0.25$, the longer species, $L > 8$, populate predominantly the deep inner part of the brush whereas in a dense brush $\\sigma_g = 1.00$ they penetrate into the \"fluffy\" tail of the dense brush only. Gyration radius $R_g$ and end-to-end distance $R_e$ of absorbed chains thereby scale with length $L$ as free polymers in the bulk. Using both MC and DFT/SCFT methods for brushes of different chain length $32 \\le N \\le 256$, we demonstrate the existence of unique {\\em critical} value of compatibility $\\chi = \\chi^{c}<0$. For $\\chi^{c}(\\phi_o)$ the energy of free chains attains the {\\em same} value, irrespective of length $L$ whereas the entropy of free chain displays a pronounced minimum. At $\\chi^{c}$ all density profiles of absorbing chains with different $L$ intersect at the same distance from the grafting plane. The penetration/expulsion kinetics of free chains into the polymer brush after an instantaneous change in their compatibility $\\chi$ displays a rather rich behavior. We find three distinct regimes of penetration kinetics of free chains regarding the length $L$: I ($1\\le L\\le 8$), II ($8 \\le L \\le N$), and III ($L > N$), in which the time of absorption $\\tau$ grows with $L$ at a different rate. During the initial stages of penetration into the brush one observes a power-law increase of $\\Gamma \\propto t^\\alpha$ with power $\\alpha \\propto -\\ln \\phi_o$ whereby penetration of the free chains into the brush gets {\\em slower} as their concentration rises.", "prompt": "The absorption of free linear chains in a polymer brush was studied with respect to chain size $L$ and compatibility $\\chi$ with the brush by means of Monte Carlo (MC) simulations and Density Functional Theory (DFT) / Self-Consistent Field Theory (SCFT) at both moderate, $\\sigma_g", "orig": "= 0.25$, and high, $\\sigma_g = 1.00$, grafting densities using a bead-spring model. Different concentrations of the free chains $0.0625 \\le \\phi_o \\le 0.375$ are examined. Contrary to the case of $\\chi =", "generated": "The absorption of free linear chains in a polymer brush was studied with respect to chain size $L$ and compatibility $\\chi$ with the brush by means of Monte Carlo (MC) simulations and Density Functional Theory (DFT) / Self-Consistent Field Theory (SCFT) at both moderate, $\\sigma_g"}
{"full_text": "$\\bullet$ Background : In Heart rate variability analysis, the rate-rate time series suffer often from aperiodic non-stationarity, presence of ectopic beats etc. It would be hard to extract helpful information from the original signals. 10 $\\bullet$ Problem : Trend removal methods are commonly practiced to reduce the influence of the low frequency and aperiodic non-stationary in RR data. This can unfortunately affect the signal and make the analysis on detrended data less appropriate. $\\bullet$ Objective : Investigate the detrending effect (linear \\& nonlinear) in temporal / nonliear analysis of heart rate variability of long-term RR data (in normal sinus rhythm, atrial fibrillation, 15 congestive heart failure and ventricular premature arrhythmia conditions). $\\bullet$ Methods : Temporal method : standard measure SDNN; Nonlinear methods : multi-scale Fractal Dimension (FD), Detrended Fluctuation Analysis (DFA) \\& Sample Entropy (Sam-pEn) analysis. $\\bullet$ Results : The linear detrending affects little the global characteristics of the RR data, either 20 in temporal analysis or in nonlinear complexity analysis. After linear detrending, the SDNNs are just slightly shifted and all distributions are well preserved. The cross-scale complexity remained almost the same as the ones for original RR data or correlated. Nonlinear detrending changed not only the SDNNs distribution, but also the order among different types of RR data. After this processing, the SDNN became indistinguishable be-25 tween SDNN for normal sinus rhythm and ventricular premature beats. Different RR data has different complexity signature. Nonlinear detrending made the all RR data to be similar , in terms of complexity. It is thus impossible to distinguish them. The FD showed that nonlinearly detrended RR data has a dimension close to 2, the exponent from DFA is close to zero and SampEn is larger than 1.5 -- these complexity values are very close to those for 30 random signal. $\\bullet$ Conclusions : Pre-processing by linear detrending can be performed on RR data, which has little influence on the corresponding analysis. Nonlinear detrending could be harmful and it is not advisable to use this type of pre-processing. Exceptions do exist, but only combined with other appropriate techniques to avoid complete change of the signal's intrinsic dynamics. 35 Keywords $\\bullet$ heart rate variability $\\bullet$ linear / nonlinear detrending $\\bullet$ complexity analysis $\\bullet$ mul-tiscale analysis $\\bullet$ detrended fluctuation analysis $\\bullet$ fractal dimension $\\bullet$ sample entropy;", "prompt": "$\\bullet$ Background : In Heart rate variability analysis, the rate-rate time series suffer often from aperiodic non-stationarity, presence of ectopic beats etc. It would be hard to extract helpful information from the original signals. 10 $\\bullet$ Problem : Trend removal methods are commonly practiced to reduce the", "orig": "influence of the low frequency and aperiodic non-stationary in RR data. This can unfortunately affect the signal and make the analysis on detrended data less appropriate. $\\bullet$ Objective : Investigate the detrending effect (linear \\& nonlinear) in temporal / nonliear analysis of heart rate variability of", "generated": "$\\bullet$ Background : In Heart rate variability analysis, the rate-rate time series suffer often from aperiodic non-stationarity, presence of ectopic beats etc. It would be hard to extract helpful information from the original signals. 10 \n\n$\\bullet$ Problem : Trend removal methods are commonly practiced to reduce"}
{"full_text": "(l) I have enough evidence to render the sentence S probable. (la) So, relative to what I know, it is rational of me to believe S. (2) Now that I have more evidence, S may no longer be probable. (2a) So now, relative to what I know, it is not rational of me to believe S. These seem a perfectly ordinary, common sense, pair of situations. Generally and vaguely, I take them to embody what I shall call probabilistic inference. This form of inference is clearly non-monotonic. Relatively few people have taken this form of inference, based on high probability, to serve as a foundation for non-monotonic logic or for a logical or defeasible inference. There are exceptions: Jane Nutter [16] thinks that sometimes probability has something to do with non-monotonic reasoning. Judea Pearl [ 17] has recently been exploring the possibility. There are any number of people whom one might call probability enthusiasts who feel that probability provides all the answers by itself, with no need of help from logic. Cheeseman [1], Henrion [5] and others think it useful to look at a distribution of probabilities over a whole algebra of statements, to update that distribution in the light of new evidence, and to use the latest updated distribution of probability over the algebra as a basis for planning and decision making. A slightly weaker form of this approach is captured by Nilsson [15], where one assumes certain probabilities for certain statements, and infers the probabilities, or constraints on the probabilities of other statement. None of this corresponds to what I call probabilistic inference. All of the inference that is taking place, either in Bayesian updating, or in probabilistic logic, is strictly deductive. Deductive inference, particularly that concerned with the distribution of classical probabilities or chances, is of great importance. But this is not to say that there is no important role for what earlier logicians have called \"ampliative\" or \"inductive\" or \"scientific\" inference, in which the conclusion goes beyond the premises, asserts more than do the premises. This depends on what David Israel [6] has called \"real rules of inference\". It is characteristic of any such logic or inference procedure that it can go wrong: that statements accepted at one point may be rejected at a later point. Research underlying the results reported here has been partially supported by the Signals Warfare Center of the United States Army.", "prompt": "(l) I have enough evidence to render the sentence S probable. (la) So, relative to what I know, it is rational of me to believe S. (2) Now that I have more evidence, S may no longer be probable. (2a) So now, relative to what I know,", "orig": "it is not rational of me to believe S. These seem a perfectly ordinary, common sense, pair of situations. Generally and vaguely, I take them to embody what I shall call probabilistic inference. This form of inference is clearly non-monotonic. Relatively few people have taken this form of inference, based on", "generated": "I have enough evidence to render the sentence S probable. (la) So, relative to what I know, it is rational of me to believe S. (2) Now that I have more evidence, S may no longer be probable. (2a) So now, relative to what I know, I must"}
{"full_text": "The Fisher information matrix (FIM) is a key quantity in statistics as it is required for example for evaluating asymptotic precisions of parameter estimates, for computing test statistics or asymptotic distributions in statistical testing, for evaluating post model selection inference results or optimality criteria in experimental designs. However its exact computation is often not trivial. In particular in many latent variable models, it is intricated due to the presence of unobserved variables. Therefore the observed FIM is usually considered in this context to estimate the FIM. Several methods have been proposed to approximate the observed FIM when it can not be evaluated analytically. Among the most frequently used approaches are Monte-Carlo methods or iterative algorithms derived from the missing information principle. All these methods require to compute second derivatives of the complete data log-likelihood which leads to some disadvantages from a computational point of view. In this paper, we present a new approach to estimate the FIM in latent variable model. The advantage of our method is that only the first derivatives of the log-likelihood is needed, contrary to other approaches based on the observed FIM. Indeed we consider the empirical estimate of the covariance matrix of the score. We prove that this estimate of the Fisher information matrix is unbiased, consistent and asymptotically Gaussian. Moreover we highlight that none of both estimates is better than the other in terms of asymptotic covariance matrix. When the proposed estimate can not be directly analytically evaluated, we present a stochastic approximation estimation algorithm to compute it. This algorithm provides this estimate of the FIM as a by-product of the parameter estimates. We emphasize that the proposed algorithm only requires to compute the first derivatives of the complete data log-likelihood with respect to the parameters. We prove that the estimation algorithm is consistent and asymptotically Gaussian when the number of iterations goes to infinity. We evaluate the finite sample size properties of the proposed estimate and of the observed FIM through simulation studies in linear mixed effects models and mixture models. We also investigate the convergence properties of the estimation algorithm in non linear mixed effects models. We compare the performances of the proposed algorithm to those of other existing methods.", "prompt": "The Fisher information matrix (FIM) is a key quantity in statistics as it is required for example for evaluating asymptotic precisions of parameter estimates, for computing test statistics or asymptotic distributions in statistical testing, for evaluating post model selection inference results or optimality criteria in experimental designs. However its exact computation is often", "orig": "not trivial. In particular in many latent variable models, it is intricated due to the presence of unobserved variables. Therefore the observed FIM is usually considered in this context to estimate the FIM. Several methods have been proposed to approximate the observed FIM when it can not be evaluated analytically. Among", "generated": "The Fisher information matrix (FIM) is a key quantity in statistics as it is required for example for evaluating asymptotic precisions of parameter estimates, for computing test statistics or asymptotic distributions in statistical testing, for evaluating post model selection inference results or optimality criteria in experimental designs. However, its exact computation is"}
{"full_text": "The purpose of this paper is to present for the first time an elementary summary of a few recent results obtained through the application of the formal theory of partial differential equations and Lie pseudogroups in order to revisit the mathematical foundations of general relativity. Other engineering examples (control theory, elasticity theory, electromagnetism) will also be considered in order to illustrate the three fundamental results that we shall provide. The paper is therefore divided into three parts corresponding to the different formal methods used. 1) CARTAN VERSUS VESSIOT: The quadratic terms appearing in the \" Riemann tensor \" according to the \" Vessiot structure equations \" must not be identified with the quadratic terms appearing in the well known \" Cartan structure equations \" for Lie groups and a similar comment can be done for the \" Weyl tensor \". In particular, \" curvature+torsion\" (Cartan) must not be considered as a generalization of \"curvature alone\" (Vessiot). Roughly, Cartan and followers have not been able to \" quotient down to the base manifold \", a result only obtained by Spencer in 1970 through the \"nonlinear Spencer sequence\" but in a way quite different from the one followed by Vessiot in 1903 for the same purpose and still ignored. 2) JANET VERSUS SPENCER: The \" Ricci tensor \" only depends on the nonlinear transformations (called \" elations \" by Cartan in 1922) that describe the \"difference \" existing between the Weyl group (10 parameters of the Poincar\\'e subgroup + 1 dilatation) and the conformal group of space-time (15 parameters). It can be defined by a canonical splitting, that is to say without using the indices leading to the standard contraction or trace of the Riemann tensor. Meanwhile, we shall obtain the number of components of the Riemann and Weyl tensors without any combinatoric argument on the exchange of indices. Accordingly, the Spencer sequence for the conformal Killing system and its formal adjoint fully describe the Cosserat/Maxwell/Weyl theory but General Relativity is not coherent at all with this result. 3) ALGEBRAIC ANALYSIS: Contrary to other equations of physics (Cauchy equations, Cosserat equations, Maxwell equations), the Einstein equations cannot be \" parametrized \", that is the generic solution cannot be expressed by means of the derivatives of a certain number of arbitrary potential-like functions, solving therefore negatively a 1000 $ challenge proposed by J. Wheeler in 1970. Accordingly, the mathematical foundations of mathematical physics must be revisited within this formal framework, though striking it may look like for certain apparently well established theories such as electromagnetism and general relativity. We insist on the fact that the arguments presented are of a purely mathematical nature and are thus unavoidable.", "prompt": "The purpose of this paper is to present for the first time an elementary summary of a few recent results obtained through the application of the formal theory of partial differential equations and Lie pseudogroups in order to revisit the mathematical foundations of general relativity. Other engineering examples (control theory, elasticity theory, electromagnetism)", "orig": "will also be considered in order to illustrate the three fundamental results that we shall provide. The paper is therefore divided into three parts corresponding to the different formal methods used. 1) CARTAN VERSUS VESSIOT: The quadratic terms appearing in the \" Riemann tensor \" according to the \" Vess", "generated": "The purpose of this paper is to present for the first time an elementary summary of a few recent results obtained through the application of the formal theory of partial differential equations and Lie pseudogroups in order to revisit the mathematical foundations of general relativity. Other engineering examples (control theory, elasticity theory, electromagnetism)"}
{"full_text": "The equioscillation theorem interleaves the Haar condition, the existence and uniqueness and strong uniqueness of the optimal Chebyshev approximation and its characterization by the equioscillation condition in a way that cannot extend to multivariate approximation: Rice~[\\emph{Transaction of the AMS}, 1963] says ''A form of alternation is still present for functions of several variables. However, there is apparently no simple method of distinguishing between the alternation of a best approximation and the alternation of other approximating functions. This is due to the fact that there is no natural ordering of the critical points.'' In addition, in the context of multivariate approximation the Haar condition is typically not satisfied and strong uniqueness may hold or not. The present paper proposes an multivariate equioscillation theorem, which includes such a simple alternation condition on error extrema, existence and a sufficient condition for strong uniqueness. To this end, the relationship between the properties interleaved in the univariate equioscillation theorem is clarified: first, a weak Haar condition is proposed, which simply implies existence. Second, the equioscillation condition is shown to be equivalent to the optimality condition of convex optimization, hence characterizing optimality independently from uniqueness. It is reformulated as the synchronized oscillations between the error extrema and the components of a related Haar matrix kernel vector, in a way that applies to multivariate approximation. Third, an additional requirement on the involved Haar matrix and its kernel vector, called strong equioscillation, is proved to be sufficient for the strong uniqueness of the solution. These three disconnected conditions give rise to a multivariate equioscillation theorem, where existence, characterization by equioscillation and strong uniqueness are separated, without involving the too restrictive Haar condition. Remarkably, relying on optimality condition of convex optimization gives rise to a quite simple proof. Instances of multivariate problems with strongly unique, non-strong but unique and non-unique solutions are presented to illustrate the scope of the theorem.", "prompt": "The equioscillation theorem interleaves the Haar condition, the existence and uniqueness and strong uniqueness of the optimal Chebyshev approximation and its characterization by the equioscillation condition in a way that cannot extend to multivariate approximation: Rice~[\\emph{Transaction of the AMS}, 1963", "orig": "] says ''A form of alternation is still present for functions of several variables. However, there is apparently no simple method of distinguishing between the alternation of a best approximation and the alternation of other approximating functions. This is due to the fact that there is no natural ordering of the critical points.'' In addition", "generated": "The equioscillation theorem interleaves the Haar condition, the existence and uniqueness and strong uniqueness of the optimal Chebyshev approximation and its characterization by the equioscillation condition in a way that cannot extend to multivariate approximation: Rice~[\\emph{Transaction of the AMS}, 1963"}
{"full_text": "Schnyder woods are decompositions of simple triangulations into three edge-disjoint spanning trees crossing each other in a specific way. In this article, we define a generalization of Schnyder woods to $d$-angulations (plane graphs with faces of degree $d$) for all $d\\geq 3$. A \\emph{Schnyder decomposition} is a set of $d$ spanning forests crossing each other in a specific way, and such that each internal edge is part of exactly $d-2$ of the spanning forests. We show that a Schnyder decomposition exists if and only if the girth of the $d$-angulation is $d$. As in the case of Schnyder woods ($d=3$), there are alternative formulations in terms of orientations (\"fractional\" orientations when $d\\geq 5$) and in terms of corner-labellings. Moreover, the set of Schnyder decompositions on a fixed $d$-angulation of girth $d$ is a distributive lattice. We also show that the structures dual to Schnyder decompositions (on $d$-regular plane graphs of mincut $d$ rooted at a vertex $v^*$) are decompositions into $d$ spanning trees rooted at $v^*$ such that each edge not incident to $v^*$ is used in opposite directions by two trees. Additionally, for even values of $d$, we show that a subclass of Schnyder decompositions, which are called even, enjoy additional properties that yield a reduced formulation; in the case d=4, these correspond to well-studied structures on simple quadrangulations (2-orientations and partitions into 2 spanning trees). In the case d=4, the dual of even Schnyder decompositions yields (planar) orthogonal and straight-line drawing algorithms. For a 4-regular plane graph $G$ of mincut 4 with $n$ vertices plus a marked vertex $v$, the vertices of $G\\backslash v$ are placed on a $(n-1) \\times (n-1)$ grid according to a permutation pattern, and in the orthogonal drawing each of the $2n-2$ edges of $G\\backslash v$ has exactly one bend. Embedding also the marked vertex $v$ is doable at the cost of two additional rows and columns and 8 additional bends for the 4 edges incident to $v$. We propose a further compaction step for the drawing algorithm and show that the obtained grid-size is strongly concentrated around $25n/32\\times 25n/32$ for a uniformly random instance with $n$ vertices.", "prompt": "Schnyder woods are decompositions of simple triangulations into three edge-disjoint spanning trees crossing each other in a specific way. In this article, we define a generalization of Schnyder woods to $d$-angulations (plane graphs with faces of degree $d$) for all $d", "orig": "\\geq 3$. A \\emph{Schnyder decomposition} is a set of $d$ spanning forests crossing each other in a specific way, and such that each internal edge is part of exactly $d-2$ of the spanning forests. We show that a Schnyder decomposition exists if", "generated": "Schnyder woods are decompositions of simple triangulations into three edge-disjoint spanning trees crossing each other in a specific way. In this article, we define a generalization of Schnyder woods to $d$-angulations (plane graphs with faces of degree $d$) for all $d"}
{"full_text": "Situations where individuals have to contribute to joint efforts or share scarce resources are ubiquitous. Yet, without proper mechanisms to ensure cooperation, the evolutionary pressure to maximize individual success tends to create a tragedy of the commons (such as over-fishing or the destruction of our environment). This contribution addresses a number of related puzzles of human behavior with an evolutionary game theoretical approach as it has been successfully used to explain the behavior of other biological species many times, from bacteria to vertebrates. Our agent-based model distinguishes individuals applying four different behavioral strategies: non-cooperative individuals (\"defectors\"), cooperative individuals abstaining from punishment efforts (called \"cooperators\" or \"second-order free-riders\"), cooperators who punish non-cooperative behavior (\"moralists\"), and defectors, who punish other defectors despite being non-cooperative themselves (\"immoralists\"). By considering spatial interactions with neighboring individuals, our model reveals several interesting effects: First, moralists can fully eliminate cooperators. This spreading of punishing behavior requires a segregation of behavioral strategies and solves the \"second-order free-rider problem\". Second, the system behavior changes its character significantly even after very long times (\"who laughs last laughs best effect\"). Third, the presence of a number of defectors can largely accelerate the victory of moralists over non-punishing cooperators. Forth, in order to succeed, moralists may profit from immoralists in a way that appears like an \"unholy collaboration\". Our findings suggest that the consideration of punishment strategies allows to understand the establishment and spreading of \"moral behavior\" by means of game-theoretical concepts. This demonstrates that quantitative biological modeling approaches are powerful even in domains that have been addressed with non-mathematical concepts so far. The complex dynamics of certain social behaviors becomes understandable as result of an evolutionary competition between different behavioral strategies.", "prompt": "Situations where individuals have to contribute to joint efforts or share scarce resources are ubiquitous. Yet, without proper mechanisms to ensure cooperation, the evolutionary pressure to maximize individual success tends to create a tragedy of the commons (such as over-fishing or the destruction of our environment). This contribution addresses a number of related puzzles", "orig": "of human behavior with an evolutionary game theoretical approach as it has been successfully used to explain the behavior of other biological species many times, from bacteria to vertebrates. Our agent-based model distinguishes individuals applying four different behavioral strategies: non-cooperative individuals (\"defectors\"), cooperative individuals abstaining from punishment efforts (called \"co", "generated": "Situations where individuals have to contribute to joint efforts or share scarce resources are ubiquitous. Yet, without proper mechanisms to ensure cooperation, the evolutionary pressure to maximize individual success tends to create a tragedy of the commons (such as over-fishing or the destruction of our environment). This contribution addresses a number of related puzzles"}
{"full_text": "Reactive programming is a popular paradigm to program event-driven applications, and it is often proposed as a paradigm to write distributed applications. One such type of application is *prosumer* applications, which are distributed applications that both produce and consume many events. We analyse the problems that occur when using a reactive programming language or framework to implement prosumer applications. We find that the assumption of an open network, which means prosumers of various types spontaneously join and leave the network, can cause a lot of code complexity or run-time inefficiency. At the basis of these issues lies *acquaintance management*: the ability to discover prosumers as they join and leave the network, and correctly maintaining this state throughout the reactive program. Most existing reactive programming languages and frameworks have limited support for managing acquaintances, resulting in accidental complexity of the code or inefficient computations. In this paper we present acquaintance management for reactive programs. First, we design an *acquaintance discovery* mechanism to create a *flock* that automatically discovers prosumers on the network. An important aspect of flocks is their integration with reactive programs, such that a reactive program can correctly and efficiently maintain its state. To this end we design an *acquaintance maintenance* mechanism: a new type of operator for functional reactive programming languages that we call `deploy-*`. The `deploy-*` operator enables correct and efficient reactions to time-varying collections of discovered prosumers. The proposed mechanisms are implemented in a reactive programming language called Stella, which serves as a linguistic vehicle to demonstrate the ideas of our approach. Our implementation of acquaintance management results in computationally efficient and idiomatic reactive code. We evaluate our approach quantitatively via benchmarks that show that our implementation is efficient: computations will efficiently update whenever a new prosumer is discovered, or a connected prosumer is dropped. To evaluate the distributed capabilities of our prototype implementation, we implement a use-case that simulates the bike-sharing infrastructure of Brussels, and we run it on a Raspberry Pi cluster computer. We consider our work to be an important step to use functional reactive programming to build distributed systems for open networks, in other words, distributed reactive programs that involve many prosumer devices and sensors that spontaneously join and leave the network.", "prompt": "Reactive programming is a popular paradigm to program event-driven applications, and it is often proposed as a paradigm to write distributed applications. One such type of application is *prosumer* applications, which are distributed applications that both produce and consume many events. We analyse the problems that occur when using a reactive programming language or", "orig": "framework to implement prosumer applications. We find that the assumption of an open network, which means prosumers of various types spontaneously join and leave the network, can cause a lot of code complexity or run-time inefficiency. At the basis of these issues lies *acquaintance management*: the ability to discover pro", "generated": "Reactive programming is a popular paradigm to program event-driven applications, and it is often proposed as a paradigm to write distributed applications. One such type of application is *prosumer* applications, which are distributed applications that both produce and consume many events. We analyse the problems that occur when using a reactive programming language or"}
{"full_text": "Disaggregated memory is promising for improving memory utilization in computer clusters in which memory demands significantly vary across computer nodes under utilization. It allows applications with high memory demands to use memory in other computer nodes. However, disaggregated memory is not easy to use for implementing data structures in C++ because the C++ standard does not provide an adequate abstraction to use it efficiently in a high-level, modular manner. Because accessing remote memory involves high latency, disaggregated memory is often used as a far-memory system, which forms a kind of swap memory where part of local memory is used as a cache area, while the remaining memory is not subject to swapping. To pursue performance, programmers have to be aware of this nonuniform memory view and place data appropriately to minimize swapping. In this work, we model the address space of memory-disaggregated systems as the far-memory model, present the collective allocator abstraction, which enables us to specify object placement aware of memory address subspaces, and apply it to programming aware of the far-memory model. The far-memory model provides a view of the nonuniform memory space while hiding the details. In the model, the virtual address space is divided into two subspaces; one is subject to swapping and the other is not. The swapping subspace is further divided into even-sized pages, which are units of swapping. The collective allocator abstraction forms an allocator as a collection of sub-allocators, each of which owns a distinct subspace, where every allocation is done via sub-allocators. It enables us to control object placement at allocation time by selecting an appropriate sub-allocator according to different criteria, such as subspace characteristics and object collocation. It greatly facilitates implementing container data structures aware of the far-memory model. We develop an allocator based on the collective allocator abstraction by extending the C++ standard allocator for container data structures on the far-memory model and experimentally demonstrate that it facilitates implementing containers equipped with object placement strategies aware of spatial locality under the far-memory model in a high-level, modular manner. More specifically, we have successfully implemented B-trees and skip lists with the combined use of two placement strategies. The modifications therein for the original implementations are fairly modest: addition is mostly due to specifying object placement; deletion and modification are at most 1.2 % and 3.2 % of lines of the original code, respectively. We have experimentally confirmed that the modified implementations successfully have data layouts suppressing swapping. We forecast that the collective allocator abstraction would be a key to high-level integration with different memory hardware technologies because it straightforwardly accommodates new interfaces for subspaces.", "prompt": "Disaggregated memory is promising for improving memory utilization in computer clusters in which memory demands significantly vary across computer nodes under utilization. It allows applications with high memory demands to use memory in other computer nodes. However, disaggregated memory is not easy to use for implementing data structures in C++ because the C++ standard does", "orig": "not provide an adequate abstraction to use it efficiently in a high-level, modular manner. Because accessing remote memory involves high latency, disaggregated memory is often used as a far-memory system, which forms a kind of swap memory where part of local memory is used as a cache area, while the remaining memory is not subject to", "generated": "Disaggregated memory is promising for improving memory utilization in computer clusters in which memory demands significantly vary across computer nodes under utilization. It allows applications with high memory demands to use memory in other computer nodes. However, disaggregated memory is not easy to use for implementing data structures in C++ because the C++ standard does"}
{"full_text": "Device-to-Device (D2D) communication is expected to be a key feature supported by 5G networks, especially due to the proliferation of Mobile Edge Computing (MEC), which has a prominent role in reducing network stress by shifting computational tasks from the Internet to the mobile edge. Apart from being part of MEC, D2D can extend cellular coverage allowing users to communicate directly when telecommunication infrastructure is highly congested or absent. This significant departure from the typical cellular paradigm imposes the need for decentralised network routing protocols. Moreover, enhanced capabilities of mobile devices and D2D networking will likely result in proliferation of new malware types and epidemics. Although the literature is rich in terms of D2D routing protocols that enhance quality-of-service and energy consumption, they provide only basic security support, e.g., in the form of encryption. Routing decisions can, however, contribute to collaborative detection of mobile malware by leveraging different kinds of anti-malware software installed on mobile devices. Benefiting from the cooperative nature of D2D communications, devices can rely on each other's contributions to detect malware. The impact of our work is geared towards having more malware-free D2D networks. To achieve this, we designed and implemented a novel routing protocol for D2D communications that optimises routing decisions for explicitly improving malware detection. The protocol identifies optimal network paths, in terms of malware mitigation and energy spent for malware detection, based on a game theoretic model. Diverse capabilities of network devices running different types of anti-malware software and their potential for inspecting messages relayed towards an intended destination device are leveraged using game theoretic tools. An optimality analysis of both Nash and Stackelberg security games is undertaken, including both zero and non-zero sum variants, and the Defender's equilibrium strategies. By undertaking network simulations, theoretical results obtained are illustrated through randomly generated network scenarios showing how our protocol outperforms conventional routing protocols, in terms of expected payoff, which consists of: security damage in inflicted by malware and malware detection cost.", "prompt": "Device-to-Device (D2D) communication is expected to be a key feature supported by 5G networks, especially due to the proliferation of Mobile Edge Computing (MEC), which has a prominent role in reducing network stress by shifting computational tasks from the Internet to the mobile edge. Apart from being part of", "orig": "MEC, D2D can extend cellular coverage allowing users to communicate directly when telecommunication infrastructure is highly congested or absent. This significant departure from the typical cellular paradigm imposes the need for decentralised network routing protocols. Moreover, enhanced capabilities of mobile devices and D2D networking will likely result in proliferation of new malware", "generated": "Device-to-Device (D2D) communication is expected to be a key feature supported by 5G networks, especially due to the proliferation of Mobile Edge Computing (MEC), which has a prominent role in reducing network stress by shifting computational tasks from the Internet to the mobile edge. Apart from being part of"}
{"full_text": "Understanding how effective high-level NICUs (neonatal intensive care units that have the capacity for sustained mechanical assisted ventilation and high volume) are compared to low-level NICUs is important and valuable for both individual mothers and for public policy decisions. The goal of this paper is to estimate the effect on mortality of premature babies being delivered in a high-level NICU vs. a low-level NICU through an observational study where there are unmeasured confounders as well as nonignorable missing covariates. We consider the use of excess travel time as an instrumental variable (IV) to control for unmeasured confounders. In order for an IV to be valid, we must condition on confounders of the IV---outcome relationship, for example, month prenatal care started must be conditioned on for excess travel time to be a valid IV. However, sometimes month prenatal care started is missing, and the missingness may be nonignorable because it is related to the not fully measured mother's/infant's risk of complications. We develop a method to estimate the causal effect of a treatment using an IV when there are nonignorable missing covariates as in our data, where we allow the missingness to depend on the fully observed outcome as well as the partially observed compliance class, which is a proxy for the unmeasured risk of complications. A simulation study shows that under our nonignorable missingness assumption, the commonly used estimation methods, complete-case analysis and multiple imputation by chained equations assuming missingness at random, provide biased estimates, while our method provides approximately unbiased estimates. We apply our method to the NICU study and find evidence that high-level NICUs significantly reduce deaths for babies of small gestational age, whereas for almost mature babies like 37 weeks, the level of NICUs makes little difference. A sensitivity analysis is conducted to assess the sensitivity of our conclusions to key assumptions about the missing covariates. The method we develop in this paper may be useful for many observational studies facing similar issues of unmeasured confounders and nonignorable missing data as ours.", "prompt": "Understanding how effective high-level NICUs (neonatal intensive care units that have the capacity for sustained mechanical assisted ventilation and high volume) are compared to low-level NICUs is important and valuable for both individual mothers and for public policy decisions. The goal of this paper is to estimate the effect on mortality of premature babies", "orig": "being delivered in a high-level NICU vs. a low-level NICU through an observational study where there are unmeasured confounders as well as nonignorable missing covariates. We consider the use of excess travel time as an instrumental variable (IV) to control for unmeasured confounders.", "generated": "Understanding how effective high-level NICUs (neonatal intensive care units that have the capacity for sustained mechanical assisted ventilation and high volume) are compared to low-level NICUs is important and valuable for both individual mothers and for public policy decisions. The goal of this paper is to estimate the effect on mortality of premature babies"}
{"full_text": "It is generally taken for granted that our Universe is free of antimatter objects and domains. This certitude has recently been challenged by the possible detection of anti-helium nuclei by AMS-02. Should the observation be confirmed, the existence of nearby antistars would make a plausible hypothesis to explain the origin of the antinuclei. In this paper we use the 10-years Fermi Large Area Telescope (LAT) gamma-ray source catalog to set constraints on the abundance of antistars around the Sun. We identify in the catalog 14 antistar candidates not associated with any objects belonging to established gamma-ray source classes and with a spectrum compatible with baryon-antibaryon annihilation. We use them along with an estimate of the LAT sensitivity to antistars to set upper limits on the local antistar fraction $f_{\\bar{\\ast}}$ with respect to normal stars. We provide parametric limits as a function of the closest antistar mass, velocity, and surrounding matter density. We also employ a novel Monte~Carlo method to set limits for a few hypotheses about the antistar population. For a population with properties equivalent to those of regular stars concentrated in the Galactic disk we obtain $f_{\\bar{\\ast}} < 2.5 \\times 10^{-6}$ at 95\\% confidence level, which is 20 times more constraining than limits previously available. For a primordial population of antistars distributed in the Galactic halo we obtain new local upper limits which decrease as a function of antistar mass $M$ from $f_{\\bar{\\ast}} < 0.2$ at 95\\% confidence level for $M = 1 \\; M_\\odot$ to $f_{\\bar{\\ast}} < 1.6 \\times 10^{-4}$ at 95\\% confidence level for $M = 10 \\; M_\\odot$. By combining these limits with existing microlensing constraints for lighter objects in the Magellanic clouds, we infer that a primordial population of halo antistars must have a density lower than $\\mathcal{O}(10^{-5}\\;\\text{pc}^{-3})$ to $\\mathcal{O}(10^{-2}\\;\\text{pc}^{-3})$ depending on their masses. Our limits can constrain models for the origin and propagation of antinuclei in cosmic rays.", "prompt": "It is generally taken for granted that our Universe is free of antimatter objects and domains. This certitude has recently been challenged by the possible detection of anti-helium nuclei by AMS-02. Should the observation be confirmed, the existence of nearby antistars would make a plausible hypothesis to explain the origin of", "orig": "the antinuclei. In this paper we use the 10-years Fermi Large Area Telescope (LAT) gamma-ray source catalog to set constraints on the abundance of antistars around the Sun. We identify in the catalog 14 antistar candidates not associated with any objects belonging to established gamma-ray source classes and", "generated": "It is generally taken for granted that our Universe is free of antimatter objects and domains. This certitude has recently been challenged by the possible detection of anti-helium nuclei by AMS-02. Should the observation be confirmed, the existence of nearby antistars would make a plausible hypothesis to explain the origin of"}
{"full_text": "Homozygous mutations in the progranulin gene (GRN) are associated with neuronal ceroid lipofuscinosis 11 (CLN11), a rare lysosomal-storage disorder characterized by cerebellar ataxia, seizures, retinitis pigmentosa, and cognitive disorders, usually beginning between 13 and 25 years of age. This is a rare condition, previously reported in only four families. In contrast, heterozygous GRN mutations are a major cause of frontotemporal dementia associated with neuronal cytoplasmic TDP-43 inclusions. We identified homozygous GRN mutations in six new patients. The phenotypic spectrum is much broader than previously reported, with two remarkably distinct presentations, depending on the age of onset. A childhood/juvenile form is characterized by classical CLN11 symptoms at an early age at onset. Unexpectedly, other homozygous patients presented a distinct delayed phenotype of frontotemporal dementia and parkinsonism after 50 years; none had epilepsy or cerebellar ataxia. Another major finding of this study is that all GRN mutations may not have the same impact on progranulin protein synthesis. A hypomorphic effect of some mutations is supported by the presence of residual levels of plasma progranulin and low levels of normal transcript detected in one case with a homozygous splice-site mutation and late onset frontotemporal dementia. This is a new critical finding that must be considered in therapeutic trials based on replacement strategies. The first neuropathological study in a homozygous carrier provides new insights into the pathological mechanisms of the disease. Hallmarks of neuronal ceroid lipofuscinosis were present. The absence of TDP-43 cytoplasmic inclusions markedly differs from observations of heterozygous mutations, suggesting a pathological shift between lysosomal and TDP-43 pathologies depending on the mono or bi-allelic status. An intriguing observation was the loss of normal TDP-43 staining in the nucleus of some neurons, which could be the first stage of the TDP-43 pathological process preceding the formation of typical cytoplasmic inclusions. Finally, this study has important implications for genetic counselling and molecular diagnosis. Semi-dominant inheritance of GRN mutations implies that specific genetic counseling should be delivered to children and parents of CLN11 patients, as they are heterozygous carriers with a high risk of developing dementia. More broadly, this study illustrates the fact that genetic variants can lead to different phenotypes according to their mono- or bi-allelic state, which is a challenge for genetic diagnosis.", "prompt": "Homozygous mutations in the progranulin gene (GRN) are associated with neuronal ceroid lipofuscinosis 11 (CLN11), a rare lysosomal-storage disorder characterized by cerebellar ataxia, seizures, retinitis pigmentosa, and cognitive disorders, usually beginning", "orig": "between 13 and 25 years of age. This is a rare condition, previously reported in only four families. In contrast, heterozygous GRN mutations are a major cause of frontotemporal dementia associated with neuronal cytoplasmic TDP-43 inclusions. We identified homozygous GR", "generated": "Homozygous mutations in the progranulin gene (GRN) are associated with neuronal ceroid lipofuscinosis 11 (CLN11), a rare lysosomal-storage disorder characterized by cerebellar ataxia, seizures, retinitis pigmentosa, and cognitive disorders, usually beginning"}
{"full_text": "Domain-specific languages (DSLs) are touted as both easy to embed in programs and easy to optimize. Yet these goals are often in tension. Embedded or internal DSLs fit naturally with a host language, while inheriting the host's performance characteristics. External DSLs can use external optimizers and languages but sit apart from the host. We present Sham, a toolkit designed to enable internal DSLs with high performance. Sham provides a domain-specific language (embedded in Racket) for implementing other high-performance DSLs, with transparent compilation to assembly code at runtime. Sham is well suited as both a compilation target for other embedded DSLs and for transparently replacing DSL support code with faster versions. Sham provides seamless inter-operation with its host language without requiring any additional effort from its users. Sham also provides a framework for defining language syntax which implements Sham's own language interface as well. We validate Sham's design on a series of case studies, ranging from Krishnamurthi's classic automata DSL to a sound synthesis DSL and a probabilistic programming language. All of these are existing DSLs where we replaced the backend using Sham, resulting in major performance gains. We present an example-driven description of how Sham can smoothly enhance an existing DSL into a high-performance one. When compared to existing approaches for implementing high-performance DSLs, Sham's design aims for both simplicity and programmer control. This makes it easier to port our techniques to other languages and frameworks, or borrow Sham's innovations \"\\`a la carte\" without adopting the whole approach. Sham builds a sophisticated and powerful DSL construction toolkit atop fundamental language features including higher-order functions, data structures, and a foreign-function interface (FFI), all readily available in other languages. Furthermore, Sham's approach allows DSL developers to simply write functions, either using Sham or generating Sham, without needing to work through complex staging or partial evaluation systems.", "prompt": "Domain-specific languages (DSLs) are touted as both easy to embed in programs and easy to optimize. Yet these goals are often in tension. Embedded or internal DSLs fit naturally with a host language, while inheriting the host's performance characteristics. External DSLs can use external optimizers and languages but sit apart", "orig": "from the host. We present Sham, a toolkit designed to enable internal DSLs with high performance. Sham provides a domain-specific language (embedded in Racket) for implementing other high-performance DSLs, with transparent compilation to assembly code at runtime. Sham is well suited as both a compilation target for other embedded DSLs and", "generated": "Domain-specific languages (DSLs) are touted as both easy to embed in programs and easy to optimize. Yet these goals are often in tension. Embedded or internal DSLs fit naturally with a host language, while inheriting the host's performance characteristics. External DSLs can use external optimizers and languages but sit apart"}
{"full_text": "In this series of articles we describe a systematic approach to studying QCD vacuum structure using the methods of lattice gauge theory. Our framework incorporates four major components. (i) The recently established existence of space-time order at all scales (fundamental structure) observed directly in typical configurations of regularized path-integral ensembles. (ii) The notion of scale-dependent vacuum structure (effective structure) providing the means for representing and quantifying the influence of fluctuations at various scales on physical observables (phenomena). (iii) The unified description of gauge and fermionic aspects of the theory which facilitates a high level of space-time order in the path-integral ensembles. (iv) The strict ``Bottom-Up'' approach wherein the process of identifying the vacuum structure proceeds inductively, using the information from valid lattice QCD ensembles as the only input. In this work we first elaborate on the meaning of the notion of space-time order in a given configuration which is conceptually at the heart of the path-integral approach to vacuum structure. It is argued that the algorithmic complexity of binary strings associated with coarse-grained descriptions of the configuration provides a relevant quantitative measure. The corresponding ensemble averages define the ranking of different lattice theories at given cutoff by the degree of space-time order generated via their dynamics. We then introduce the set of local transformations of a configuration, chiral orderings, in which the transformed gauge connection represents an effective matrix phase acquired by chiral fermion when hopping over a given link. It is proposed that chiral orderings facilitate the evolution in the set of actions which increases the degree of space-time order while preserving the physical content of the theory, and should thus be used in the search for the fundamental QCD vacuum structure. The relation to renormalization group ideas is discussed, and the first step in general formulation of effective lattice QCD realizing the notion of scale-dependent vacuum structure is given.", "prompt": "In this series of articles we describe a systematic approach to studying QCD vacuum structure using the methods of lattice gauge theory. Our framework incorporates four major components. (i) The recently established existence of space-time order at all scales (fundamental structure) observed directly in typical configurations of regularized path-integral ensembles", "orig": ". (ii) The notion of scale-dependent vacuum structure (effective structure) providing the means for representing and quantifying the influence of fluctuations at various scales on physical observables (phenomena). (iii) The unified description of gauge and fermionic aspects of the theory which facilitates a high level of space-time order in", "generated": "In this series of articles we describe a systematic approach to studying QCD vacuum structure using the methods of lattice gauge theory. Our framework incorporates four major components. (i) The recently established existence of space-time order at all scales (fundamental structure) observed directly in typical configurations of regularized path-integral ensemble simulations"}
{"full_text": "Emerging computer architectures will feature drastically decreased flops/byte (ratio of peak processing rate to memory bandwidth) as highlighted by recent studies on Exascale architectural trends. Further, flops are getting cheaper while the energy cost of data movement is increasingly dominant. The understanding and characterization of data locality properties of computations is critical in order to guide efforts to enhance data locality. Reuse distance analysis of memory address traces is a valuable tool to perform data locality characterization of programs. A single reuse distance analysis can be used to estimate the number of cache misses in a fully associative LRU cache of any size, thereby providing estimates on the minimum bandwidth requirements at different levels of the memory hierarchy to avoid being bandwidth bound. However, such an analysis only holds for the particular execution order that produced the trace. It cannot estimate potential improvement in data locality through dependence preserving transformations that change the execution schedule of the operations in the computation. In this article, we develop a novel dynamic analysis approach to characterize the inherent locality properties of a computation and thereby assess the potential for data locality enhancement via dependence preserving transformations. The execution trace of a code is analyzed to extract a computational directed acyclic graph (CDAG) of the data dependences. The CDAG is then partitioned into convex subsets, and the convex partitioning is used to reorder the operations in the execution trace to enhance data locality. The approach enables us to go beyond reuse distance analysis of a single specific order of execution of the operations of a computation in characterization of its data locality properties. It can serve a valuable role in identifying promising code regions for manual transformation, as well as assessing the effectiveness of compiler transformations for data locality enhancement. We demonstrate the effectiveness of the approach using a number of benchmarks, including case studies where the potential shown by the analysis is exploited to achieve lower data movement costs and better performance.", "prompt": "Emerging computer architectures will feature drastically decreased flops/byte (ratio of peak processing rate to memory bandwidth) as highlighted by recent studies on Exascale architectural trends. Further, flops are getting cheaper while the energy cost of data movement is increasingly dominant. The understanding and characterization of data locality properties of computations is", "orig": "critical in order to guide efforts to enhance data locality. Reuse distance analysis of memory address traces is a valuable tool to perform data locality characterization of programs. A single reuse distance analysis can be used to estimate the number of cache misses in a fully associative LRU cache of any size, thereby providing estimates on the minimum bandwidth", "generated": "Emerging computer architectures will feature drastically decreased flops/byte (ratio of peak processing rate to memory bandwidth) as highlighted by recent studies on Exascale architectural trends. Further, flops are getting cheaper while the energy cost of data movement is increasingly dominant. The understanding and characterization of data locality properties of computations is"}
{"full_text": "This paper presents an application of photogrammetry on ceramic fragments from two excavation sites located north-west of France. The restitution by photogrammetry of these different fragments allowed reconstructions of the potteries in their original state or at least to get to as close as possible. We used the 3D reconstructions to compute some metrics and to generate a presentation support by using a 3D printer. This work is based on affordable tools and illustrates how 3D technologies can be quite easily integrated in archaeology process with limited financial resources. 1. INTRODUCTION Today, photogrammetry and 3D modelling are an integral part of the methods used in archeology and heritage management. They provide answers to scientific needs in the fields of conservation, preservation, restoration and mediation of architectural, archaeological and cultural heritage [2] [6] [7] [9]. Photogrammetry on ceramic fragments was one of the first applications contemporary of the development of this technique applied in the archaeological community [3]. More recently and due to its democratization, it was applied more generally to artifacts [5]. Finally joined today by the rise of 3D printing [8] [10], it can restore fragmented artifacts [1] [12]. These examples target one or several particular objects and use different types of equipment that can be expensive. These aspects can put off uninitiated archaeologists. So it would be appropriate to see if these techniques could be generalized to a whole class of geometrically simple and common artifacts, such as ceramics. From these observations, associated to ceramics specialists with fragments of broken ceramics, we aimed at arranging different tools and methods, including photogrammetry, to explore opportunities for a cheap and attainable reconstruction methodology and its possible applications. Our first objective was to establish a protocol for scanning fragments with photogrammetry, and for reconstruction of original ceramics. We used the digital reconstitutions of the ceramics we got following our process to calculate some metrics and to design and 3D print a display for the remaining fragments of one pottery.", "prompt": "This paper presents an application of photogrammetry on ceramic fragments from two excavation sites located north-west of France. The restitution by photogrammetry of these different fragments allowed reconstructions of the potteries in their original state or at least to get to as close as possible. We used the 3D reconstructions", "orig": "to compute some metrics and to generate a presentation support by using a 3D printer. This work is based on affordable tools and illustrates how 3D technologies can be quite easily integrated in archaeology process with limited financial resources. 1. INTRODUCTION Today, photogrammetry and 3D modelling are an integral", "generated": "This paper presents an application of photogrammetry on ceramic fragments from two excavation sites located north-west of France. The restitution by photogrammetry of these different fragments allowed reconstructions of the potteries in their original state or at least to get to as close as possible. We used the 3D reconstruction techniques"}
{"full_text": "In this paper we propose a minimal model for free reeds taking into account the significant phenomena. This free reed model may be used to build models of free reed instruments which permit numerical simulations. Several definitions for the section by which the airflow passes through the reed are reviewed and a new one is proposed which takes into account the entire escape area under the reed and the reed thickness. To derive this section, it is necessary to distinguish the neutral section (the only section of the reed which always keeps its length constant while moving) from the upstream or downstream sections. A minimal configuration is chosen to permit the instabilities of both (-,+) and (+,-) reeds on the basis of a linear analysis of instabilities conditions. This configuration is used to illustrate, with temporal simulations, the minimal model for both kinds of reeds and to discuss the model assumptions. Some clues are given about the influence, on the playing frequency and on the dynamic of the sound, of two main parameters of the geometrical model: the size of the volume and the level of the excitation. It is shown that the playing frequency of a (+,-) reed can vary in a large range according to the size of the volume upstream of the reed; that the playing frequency is nearly independent of the excitation but that the dynamic of the sound increases with the excitation level. Some clues are also proposed to determine the nature of the bifurcation for free reeds: it seems that free reeds may present inverse bifurcations. The influence of the reed thickness is also studied for configurations where the reed length or the reed width vary to keep the mass constant. This study shows that the reed thickness can have a great influence on the sound magnitude, the playing frequency and the magnitude of the reed displacement which justifies its introduction in the reed model.This article has been published in Acta Acustica united with Acustica, Vol. 93 (2007), p. 122-144.", "prompt": "In this paper we propose a minimal model for free reeds taking into account the significant phenomena. This free reed model may be used to build models of free reed instruments which permit numerical simulations. Several definitions for the section by which the airflow passes through the reed are reviewed and a new one is proposed which", "orig": "takes into account the entire escape area under the reed and the reed thickness. To derive this section, it is necessary to distinguish the neutral section (the only section of the reed which always keeps its length constant while moving) from the upstream or downstream sections. A minimal configuration is chosen to permit the instabilities", "generated": "In this paper we propose a minimal model for free reeds taking into account the significant phenomena. This free reed model may be used to build models of free reed instruments which permit numerical simulations. Several definitions for the section by which the airflow passes through the reed are reviewed and a new one is proposed which"}
{"full_text": "Virtualization allows the simulation of automotive ECUs on a Windows PC executing in closed-loop with a vehicle simulation model. This approach enables to move certain development tasks from road or test rigs and HiL (Hardware in the loop) to PCs, where they can often be performed faster and cheaper. Renault has recently established such a virtualization process for powertrain control software based on Simulink models. If the number of runnables exceeds a threshold (about 1500) the execution of the virtual ECU is no longer straight forward and specific techniques are required. This paper describes the motivation behind a Simulink model based process, the virtualization process and applications of the resulting virtual ECUs. Domain: Critical Transportation Systems Topic: Processes, methods and tools, in particular: virtual engineering and simulation 1. Motivation Since 2010, Renault has established a framework to develop engine control software for Diesel and Gasoline engines [6]. The framework is heavily based on MATLAB/Simulink and the idea of model-based development, which facilitates the carry-over and carry-across of application software between software projects. In the Renault EMS architecture software is composed in to about 20 functions, such as Air System, Combustion etc. A function consists of modules. A module is the smallest testable software unit and contains runnables to be scheduled and executed by the Operating System (Os) of the ECU. The Renault EMS development process includes basically the following steps [5]. 1. Specification of about 200 generic configurable modules per ECU using MATLAB/Simulink. 2. Generation of C code (EMS application software) from all module specifications using MATLAB/Simulink Embedded Coder. 3. MiL (Model in the Loop) test and validation of the resulting executable specifications at module level in a simulated system environment, considering only essential interactions with other modules and system environment. This is essentially a back-to-back test to make sure that the Simulink model of a module and the corresponding production C code show equivalent and intended behaviour. To insure software quality, this step is repeatedly performed with steps 1 and 2, based on the simulation capabilities of MATLAB/Simulink. 4. Configuration of modules to fit to the specific needs of a software project, such as absence or presence of certain components. 5. Integration of generated configured C code and hand-coded platform software (basic software) on supplied target hardware, a real ECU that communicates with other controllers via CAN and other busses. 6. Validation and test of all modules on system level using the real ECU. In contrast to step 3, the interactions of all modules and interactions with the system environment are visible then and subject to testing. For example, the Os runs all scheduled runnables then, not just those of the modules considered to be 'essential' for a module under test. Critical assessment of the above process shows that there is a considerable delay between delivery of a set of specifications to the software project team (at the end of step 3) and system-level tests based on an ECU that runs entire software (step 6). Typical delays are weeks or months.", "prompt": "Virtualization allows the simulation of automotive ECUs on a Windows PC executing in closed-loop with a vehicle simulation model. This approach enables to move certain development tasks from road or test rigs and HiL (Hardware in the loop) to PCs, where they can often be performed faster and cheaper. Renault has recently established such", "orig": "a virtualization process for powertrain control software based on Simulink models. If the number of runnables exceeds a threshold (about 1500) the execution of the virtual ECU is no longer straight forward and specific techniques are required. This paper describes the motivation behind a Simulink model based process,", "generated": "Virtualization allows the simulation of automotive ECUs on a Windows PC executing in closed-loop with a vehicle simulation model. This approach enables to move certain development tasks from road or test rigs and HiL (Hardware in the loop) to PCs, where they can often be performed faster and cheaper. Renault has recently established such"}
{"full_text": "Many uncertainty sets encountered in control systems analysis and design can be expressed in terms of semialgebraic sets, that is as the intersection of sets described by means of polynomial inequalities. Important examples are for instance the solution set of linear matrix inequalities or the Schur/Hurwitz stability domains. These sets often have very complicated shapes (non-convex, and even non-connected), which renders very difficult their manipulation. It is therefore of considerable importance to find simple-enough approximations of these sets, able to capture their main characteristics while maintaining a low level of complexity. For these reasons, in the past years several convex approximations, based for instance on hyperrect-angles, polytopes, or ellipsoids have been proposed. In this work, we move a step further, and propose possibly non-convex approximations , based on a small volume polynomial superlevel set of a single positive polynomial of given degree. We show how these sets can be easily approximated by minimizing the L1 norm of the polynomial over the semialgebraic set, subject to positivity constraints. Intuitively, this corresponds to the trace minimization heuristic commonly encounter in minimum volume ellipsoid problems. From a computational viewpoint, we design a hierarchy of linear matrix inequality problems to generate these approximations, and we provide theoretically rigorous convergence results, in the sense that the hierarchy of outer approximations converges in volume (or, equivalently, almost everywhere and almost uniformly) to the original set. Two main applications of the proposed approach are considered. The first one aims at reconstruction/approximation of sets from a finite number of samples. In the second one, we show how the concept of polynomial superlevel set can be used to generate samples uniformly distributed on a given semialgebraic set. The efficiency of the proposed approach is demonstrated by different numerical examples.", "prompt": "Many uncertainty sets encountered in control systems analysis and design can be expressed in terms of semialgebraic sets, that is as the intersection of sets described by means of polynomial inequalities. Important examples are for instance the solution set of linear matrix inequalities or the Schur/Hurwitz stability domains. These sets often have", "orig": "very complicated shapes (non-convex, and even non-connected), which renders very difficult their manipulation. It is therefore of considerable importance to find simple-enough approximations of these sets, able to capture their main characteristics while maintaining a low level of complexity. For these reasons, in the past years several convex approximations", "generated": "Many uncertainty sets encountered in control systems analysis and design can be expressed in terms of semialgebraic sets, that is as the intersection of sets described by means of polynomial inequalities. Important examples are for instance the solution set of linear matrix inequalities or the Schur/Hurwitz stability domains. These sets often have"}
{"full_text": "Reaction-diffusion analytical modeling of predator-prey systems has shown that specialist natural enemies can slow, stop and even reverse pest invasions, assuming that the prey population displays a strong Allee effect in its growth. Few additional analytical results have been obtained for other spatially distributed predator-prey systems, as traveling waves of non-monotonous systems are notoriously difficult to obtain. Traveling waves have indeed recently been shown to exist in predator-prey systems, but the direction of the wave, an essential item of information in the context of the control of biological invasions, is generally unknown. Preliminary numerical explorations have hinted that control by generalist predators might be possible for prey populations displaying logistic growth. We aimed to formalize the conditions in which spatial biological control can be achieved by generalists, through an analytical approach based on reaction-diffusion equations. The population of the focal prey - the invader - is assumed to grow according to a logistic function. The predator has a type II functional response and is present everywhere in the domain, at its carrying capacity, on alternative hosts. Control, defined as the invader becoming extinct in the domain, may result from spatially independent demographic dynamics or from a spatial extinction wave. Using comparison principles, we obtain sufficient conditions for control and for invasion, based on scalar bistable partial differential equations (PDEs). The searching efficiency and functional response plateau of the predator are identified as the main parameters defining the parameter space for prey extinction and invasion. Numerical explorations are carried out in the region of those control parameters space between the super-and subso-lutions, in which no conclusion about controllability can be drawn on the basis of analytical solutions. The ability of generalist predators to control prey populations with logistic growth lies in the bis-table dynamics of the coupled system, rather than in the bistability of prey-only dynamics as observed for specialist predators attacking prey populations displaying Allee effects. The consideration of space in predator-prey systems involving generalist predators with a parabolic functional response is crucial. Analysis of the ordinary differential equations (ODEs) system identifies parameter regions with monostable (extinction) and bistable (extinction or invasion) dynamics. By contrast, analysis of the associated PDE system distinguishes different and additional regions of invasion and extinction. Depending on the relative positions of these different zones, four patterns of spatial dynamics can be identified : traveling waves of extinction and invasion, pulse waves of extinction and heterogeneous stationary positive solutions of the Turing type. As a consequence, prey control is predicted to be possible when space is considered in additional situations other than those identified without considering space. The reverse situation is also possible. None of these considerations apply to spatial predator-prey systems with specialist natural enemies.", "prompt": "Reaction-diffusion analytical modeling of predator-prey systems has shown that specialist natural enemies can slow, stop and even reverse pest invasions, assuming that the prey population displays a strong Allee effect in its growth. Few additional analytical results have been obtained for other spatially distributed predator-prey systems, as traveling waves", "orig": "of non-monotonous systems are notoriously difficult to obtain. Traveling waves have indeed recently been shown to exist in predator-prey systems, but the direction of the wave, an essential item of information in the context of the control of biological invasions, is generally unknown. Preliminary numerical explorations have hinted that control", "generated": "Reaction-diffusion analytical modeling of predator-prey systems has shown that specialist natural enemies can slow, stop and even reverse pest invasions, assuming that the prey population displays a strong Allee effect in its growth. Few additional analytical results have been obtained for other spatially distributed predator-prey systems, as traveling wave"}
{"full_text": "This volume contains the formal proceedings of the 5th International Workshop on Rewriting Techniques for Program Transformations and Evaluation (WPTE 2018), held on 8th of Juli 2018 in Oxford, United Kingdom, and affiliated with FLoC 2018 and FSCD 2018. Scope of WPTE: Rewriting techniques are of great help for studying correctness of program transformations, translations and evaluation, and the aim of WPTE is to bring together the researchers working on program transformations, evaluation, and operationally-based programming language semantics, using rewriting methods, in order to share the techniques and recent developments and to exchange ideas to encourage further activation of research in this area. Topics in the scope of WPTE include the correctness of program transformations, optimisations and translations; program transformations for proving termination, confluence and other properties; correctness of evaluation strategies; operational semantics of programs, operationally-based program equivalences such as contextual equivalences and bisimulations; cost-models for reasoning about the optimizing power of transformations and the costs of evaluation; program transformations for verification and theorem proving purposes; translation, simulation, equivalence of programs with different formalisms, and evaluation strategies; program transformations for applying rewriting techniques to programs in specific programming languages; program transformations for program inversions and program synthesis; program transformation and evaluation for Haskell and rewriting. Research Paper Selection: At the workshop six research papers were presented of which five were accepted for the postproceedings. Each submission was reviewed by three or four members of the Program Committee in two to three rounds, one round for workshop presentation and at most two rounds for publication to the postproceedings. The program also included one invited talk by Jean-Pierre Jouannaud (Polytec, Palaiseau, Grand Paris, France) on a framework for graph rewriting; the abstract of this talk is included in this volume.", "prompt": "This volume contains the formal proceedings of the 5th International Workshop on Rewriting Techniques for Program Transformations and Evaluation (WPTE 2018), held on 8th of Juli 2018 in Oxford, United Kingdom, and affiliated with FLoC 2018 and FSCD 2018.", "orig": "Scope of WPTE: Rewriting techniques are of great help for studying correctness of program transformations, translations and evaluation, and the aim of WPTE is to bring together the researchers working on program transformations, evaluation, and operationally-based programming language semantics, using rewriting methods, in order to share the techniques and recent developments and", "generated": "This volume contains the formal proceedings of the 5th International Workshop on Rewriting Techniques for Program Transformations and Evaluation (WPTE 2018), held on 8th of July 2018 in Oxford, United Kingdom, and affiliated with FLoC 2018 and FSCD 2018."}
{"full_text": "We study the molecular correlations in a lattice model of a solution of a low-solubility solute, with emphasis on how the thermodynamics is reflected in the correlation functions. The model is treated in Bethe-Guggenheim approximation, which is exact on a Bethe lattice (Cayley tree). The solution properties are obtained in the limit of infinite dilution of the solute. With $h_{11}(r)$, $h_{12}(r)$, and $h_{22}(r)$ the three pair correlation functions as functions of the separation $r$ (subscripts 1 and 2 referring to solvent and solute, respectively), we find for $r \\geq 2$ lattice steps that $h_{22}(r)/h_{12}(r) \\equiv h_{12}(r)/h_{11}(r)$. This illustrates a general theorem that holds in the asymptotic limit of infinite $r$. The three correlation functions share a common exponential decay length (correlation length), but when the solubility of the solute is low the amplitude of the decay of $h_{22}(r)$ is much greater than that of $h_{12}(r)$, which in turn is much greater than that of $h_{11}(r)$. As a consequence the amplitude of the decay of $h_{22}(r)$ is enormously greater than that of $h_{11}(r)$. The effective solute-solute attraction then remains discernible at distances at which the solvent molecules are essentially no longer correlated, as found in similar circumstances in an earlier model. The second osmotic virial coefficient is large and negative, as expected. We find that the solvent-mediated part $W(r)$ of the potential of mean force between solutes, evaluated at contact, $r=1$, is related in this model to the Gibbs free energy of solvation at fixed pressure, $\\Delta G_p^*$, by $(Z/2) W(1) + \\Delta G_p^* \\equiv p v_0$, where $Z$ is the coordination number of the lattice, $p$ the pressure, and $v_0$ the volume of the cell associated with each lattice site. A large, positive $\\Delta G_p^*$ associated with the low solubility is thus reflected in a strong attraction (large negative $W$ at contact), which is the major contributor to the second osmotic virial coefficient. In this model, the low solubility (large positive $\\Delta G_p^*$) is due partly to an unfavorable enthalpy of solvation and partly to an unfavorable solvation entropy, unlike in the hydrophobic effect, where the enthalpy of solvation itself favors high solubility, but is overweighed by the unfavorable solvation entropy.", "prompt": "We study the molecular correlations in a lattice model of a solution of a low-solubility solute, with emphasis on how the thermodynamics is reflected in the correlation functions. The model is treated in Bethe-Guggenheim approximation, which is exact on a Bethe lattice (Cayley tree). The", "orig": "solution properties are obtained in the limit of infinite dilution of the solute. With $h_{11}(r)$, $h_{12}(r)$, and $h_{22}(r)$ the three pair correlation functions as functions of the separation $r$ (subscripts 1 and 2 referring to", "generated": "We study the molecular correlations in a lattice model of a solution of a low-solubility solute, with emphasis on how the thermodynamics is reflected in the correlation functions. The model is treated in Bethe-Guggenheim approximation, which is exact on a Bethe lattice (Cayley tree). This"}
{"full_text": "The ability to compose parts to form a more complex whole, and to analyze a whole as a combination of elements, is desirable across disciplines. This workshop bring together researchers applying compositional approaches to physics, NLP, cognitive science, and game theory. Within NLP, a long-standing aim is to represent how words can combine to form phrases and sentences. Within the framework of distributional semantics, words are represented as vectors in vector spaces. The categorical model of Coecke et al. [2010], inspired by quantum protocols, has provided a convincing account of compositionality in vector space models of NLP. There is furthermore a history of vector space models in cognitive science. Theories of categorization such as those developed by Nosofsky [1986] and Smith et al. [1988] utilise notions of distance between feature vectors. More recently G\\\"ardenfors [2004, 2014] has developed a model of concepts in which conceptual spaces provide geometric structures, and information is represented by points, vectors and regions in vector spaces. The same compositional approach has been applied to this formalism, giving conceptual spaces theory a richer model of compositionality than previously [Bolt et al., 2018]. Compositional approaches have also been applied in the study of strategic games and Nash equilibria. In contrast to classical game theory, where games are studied monolithically as one global object, compositional game theory works bottom-up by building large and complex games from smaller components. Such an approach is inherently difficult since the interaction between games has to be considered. Research into categorical compositional methods for this field have recently begun [Ghani et al., 2018]. Moreover, the interaction between the three disciplines of cognitive science, linguistics and game theory is a fertile ground for research. Game theory in cognitive science is a well-established area [Camerer, 2011]. Similarly game theoretic approaches have been applied in linguistics [J\\\"ager, 2008]. Lastly, the study of linguistics and cognitive science is intimately intertwined [Smolensky and Legendre, 2006, Jackendoff, 2007]. Physics supplies compositional approaches via vector spaces and categorical quantum theory, allowing the interplay between the three disciplines to be examined.", "prompt": "The ability to compose parts to form a more complex whole, and to analyze a whole as a combination of elements, is desirable across disciplines. This workshop bring together researchers applying compositional approaches to physics, NLP, cognitive science, and game theory. Within NLP, a long-standing aim is to represent how words", "orig": "can combine to form phrases and sentences. Within the framework of distributional semantics, words are represented as vectors in vector spaces. The categorical model of Coecke et al. [2010], inspired by quantum protocols, has provided a convincing account of compositionality in vector space models of NLP. There is furthermore a", "generated": "The ability to compose parts to form a more complex whole, and to analyze a whole as a combination of elements, is desirable across disciplines. This workshop bring together researchers applying compositional approaches to physics, NLP, cognitive science, and game theory. Within NLP, a long-standing aim is to represent how words"}
{"full_text": "In 1953, the physicists E. Inon\\\"u and E.P. Wigner introduced the concept of deformation of a Lie algebra by claiming that the limit $1/c \\rightarrow 0$, when c is the speed of light, of the composition law $(u,v) \\rightarrow (u+v)/(1+(uv/c^2))$ of speeds in special relativity (Poincar\\'e group) should produce the composition law $(u,v) \\rightarrow u + v $ used in classical mechanics (Galil\\'ee group). However, the dimensionless composition law $(u'=u/c,v'=v/c) \\rightarrow (u'+v')/(1+u'v')$ does not contain any longer a perturbation parameter. Nevertheless, this idea brought the birth of the \" deformation theory of algebraic structures\", culminating in the use of the Chevalley-Eilenberg cohomology of Lie algebras and one of the first applications of computer algebra in the seventies. One may also notice that the main idea of general relativity is to deform the Minkowski metric of space-time by means of the small dimensionless parameter $\\phi/c^2$ where $\\phi=GM/r$ is the gravitational potential at a distance r of a central attractive mass M with gravitational constant G. A few years later, a \" deformation theory of geometric structures \" on manifolds of dimension n was introduced and one may quote riemannian, symplectic or complex analytic structures. Though often conjectured, the link between the two approaches has never been exhibited and the aim of this paper is to provide the solution of this problem by new methods. The key tool is made by the \" Vessiot structure equations \" (1903) for Lie groups or Lie pseudogroups of transformations, which, contrary to the \" Cartan structure equations \", are still unknown today and contain \" structure constants \" which, like in the case of constant riemannian curvature, have in general nothing to do with any Lie algebra. The main idea is then to introduce the purely differential Janet sequence $0 \\rightarrow \\Theta \\rightarrow T \\rightarrow F_0 \\rightarrow F_1 \\rightarrow ... \\rightarrow F_n \\rightarrow 0$ as a resolution of the sheaf $\\Theta \\subset T$ of infinitesimal transformations and to induce a purely algebraic \" deformation sequence \" with finite dimensional vector spaces and linear maps, even if $\\Theta$ is infinite dimensional. The infinitesimal equivalence problem for geometric structures has to do with the local exactness at $ F_0 $ of the Janet sequence while the deformation problem for algebraic structures has to do with the exactness of the deformation sequence at the invariant sections of $F_1 $, that is ONE STEP FURTHER ON in the sequence and this unexpected result explains why the many tentatives previously quoted have not been successful. Finally, we emphasize through examples the part that could be played by computer algebra in any explicit computation.", "prompt": "In 1953, the physicists E. Inon\\\"u and E.P. Wigner introduced the concept of deformation of a Lie algebra by claiming that the limit $1/c \\rightarrow 0$, when c is the speed of light, of the composition law $(u,v) \\rightarrow (u+v)/(", "orig": "1+(uv/c^2))$ of speeds in special relativity (Poincar\\'e group) should produce the composition law $(u,v) \\rightarrow u + v $ used in classical mechanics (Galil\\'ee group). However, the dimensionless composition law $(u'=u/c,v'=v/c", "generated": "In 1953, the physicists E. Inonu and E.P. Wigner introduced the concept of deformation of a Lie algebra by claiming that the limit $1/c \\rightarrow 0$, when c is the speed of light, of the composition law $(u,v) \\rightarrow (u+v)/(1-\\"}
{"full_text": "On the basis of general relativity and quantum statistics, it was shown (Neslu\\v{s}an L.: 2009, Phys. Rev. D 80, 024015, arxiv:0808.3484) that the equation of state (ES) of extremely hot Fermi-Dirac gas in the surface layer of an ultra-relativistic compact object converges to the same form as the relativistic equation of thermodynamical equilibrium (RETE), which is the condition of stability of the object. The description of energy state of a gas particle was completed with the term corresponding with the potential-type energy. The necessity of such the term is set by the demand of convergence of the relativistic particle-impulse distribution law to its Maxwell-Boltzmann form in the classical limit. The identity of the ES and RETE, both applied to the gas in the object's surface layer, becomes perfect, yielding the stable object, when the object's physical radius is identical to its gravitational radius. In this state, the internal energy of gas particles in a volume of the object's surface layer increases over all limits in the frame of the volume and this opens the question if the horizon of events actually is an insuperable barrier. It seems to be possible that some matter can be temporarily lifted above the surface or, so far, be ejected from the object and can emit a radiation detectable by a distant observer. In our contribution, we demonstrate a general validity of the functional form of the potential-type energy found in our previous work. The consistency of the RETE with its non-relativistic approximation can occur only for this functional form. We also point out some observational consequences of the approximate identity of ES and RETE before the object collapses, in the proper time, to its gravitational radius as well as the possible observational consequences of the infinitely high internal energy in the surface layer of already collapsed object. In general, we propagate the idea that a lot of phenomena observed at the stellar-sized or supermassive black holes (or not-yet black holes) can be not necessarily related to the structures in a vicinity of the black hole, e.g. to an accretion disk, but they can be linked directly to the behaviour of the central, ultra-compact object.", "prompt": "On the basis of general relativity and quantum statistics, it was shown (Neslu\\v{s}an L.: 2009, Phys. Rev. D 80, 024015, arxiv:0808.3484) that the equation of state (ES) of extremely hot Fermi-", "orig": "Dirac gas in the surface layer of an ultra-relativistic compact object converges to the same form as the relativistic equation of thermodynamical equilibrium (RETE), which is the condition of stability of the object. The description of energy state of a gas particle was completed with the term corresponding with the potential-type", "generated": "On the basis of general relativity and quantum statistics, it was shown (Nesl\\v{s}an L.: 2009, Phys. Rev. D 80, 024015, arxiv:0808.3484) that the equation of state (ES) of extremely hot Fermi-g"}
{"full_text": "Data is a dominant force during the decision-making process. It can help determine which roads to expand and the optimal location for a grocery store. Data can also be used to influence which schools to open or to shutter and \"appropriate\" city services to continue or discontinue. Considered fact-based, objective, and impartial, data can trump emotional appeals during the final evaluation of a project; thus creating a power imbalance between those with the resources to access data and those without. Most often left behind are communities already struggling to stay afloat due to years of disinvestment by market forces and external decision-makers. For long ignored residents in Atlanta's Westside neighborhoods, the burden of inaccessible data continuously thwarts their opportunity for mobility. However, with the advent of the internet and the global push for open data, access to information is no longer solely in the hands of those with power, influence and money. Online tools, like the Westside Communities Alliance (WCA) Data Dashboard, quickly disseminate data to those most impacted by \"data driven decision-making,\" thus creating the potential of a genuinely equitable society. Based out of the Georgia Institute of Technology, the WCA works to build and sustain relationships among constituencies located in West Atlanta with the goal to strengthen partnerships around issues of common concern. The creation of the Data Dashboard stemmed from a recognized community desire for more localized control and the need for improvements to the communities' overall prosperity. Development of the site progressed through significant engagement between the WCA, community groups, and local agencies. The Dashboard takes the vast abundance of data and synthesizes it into a format that is both visually and geographically user-friendly. Through different portals, users can access neighborhood-level data around demographics, housing, education, and history that is formatting in a way that is easily accessible and understandable. The site includes qualitative research that goes beyond data and stats to give a totality of the community. By allowing West Atlanta advocacy groups to easily retrieve data, the WCA Data Dashboard empowers residents, nonprofits, and neighborhood associations to be full participants in the decision-making process.", "prompt": "Data is a dominant force during the decision-making process. It can help determine which roads to expand and the optimal location for a grocery store. Data can also be used to influence which schools to open or to shutter and \"appropriate\" city services to continue or discontinue. Considered fact-based, objective, and impartial", "orig": ", data can trump emotional appeals during the final evaluation of a project; thus creating a power imbalance between those with the resources to access data and those without. Most often left behind are communities already struggling to stay afloat due to years of disinvestment by market forces and external decision-makers. For long ignored residents in Atlanta's", "generated": "Data is a dominant force during the decision-making process. It can help determine which roads to expand and the optimal location for a grocery store. Data can also be used to influence which schools to open or to shutter and \"appropriate\" city services to continue or discontinue. Considered fact-based, objective, and impartial"}
{"full_text": "Modern modeling languages for general physical systems, such as Modelica, Amesim, or Simscape, rely on Differential Algebraic Equations (DAE), i.e., constraints of the form f(dot{x},x,u)=0. This drastically facilitates modeling from first principles of the physics and the reuse of models. In this paper we develop the mathematical theory needed to establish the development of compilers and tools for DAE based physical modeling languages on solid mathematical bases. Unlike Ordinary Differential Equations, DAE exhibit subtle issues because of the notion of differentiation index and related latent equations -- ODE are DAE of index zero for which no latent equation needs to be considered. Prior to generating execution code and calling solvers, the compilation of such languages requires a nontrivial \\emph{structural analysis} step that reduces the differentiation index to a level acceptable by DAE solvers. The models supported by tools of the Modelica class involve multiple modes with mode-dependent DAE based dynamics and state-dependent mode switching. Multimode DAE are much more difficult than DAE. The main difficulty is the handling of the events of mode change. Unfortunately, the large literature devoted to the mathematical analysis of DAEs does not cover the multimode case, typically saying nothing about mode changes. This lack of foundations causes numerous difficulties to the existing modeling tools. Some models are well handled, others are not, with no clear boundary between the two classes. In this paper, we develop a comprehensive mathematical approach supporting compilation and code generation for this class of languages. Its core is the structural analysis of multimode DAE systems. As a byproduct of this structural analysis, we propose well sound criteria for accepting or rejecting models. For our mathematical development, we rely on nonstandard analysis, which allows us to cast hybrid systems dynamics to discrete time dynamics with infinitesimal step size, thus providing a uniform framework for handling both continuous dynamics and mode change events.", "prompt": "Modern modeling languages for general physical systems, such as Modelica, Amesim, or Simscape, rely on Differential Algebraic Equations (DAE), i.e., constraints of the form f(dot{x},x,u)=0. This drastically facilitates modeling from first principles of the physics and the reuse of models.", "orig": "In this paper we develop the mathematical theory needed to establish the development of compilers and tools for DAE based physical modeling languages on solid mathematical bases. Unlike Ordinary Differential Equations, DAE exhibit subtle issues because of the notion of differentiation index and related latent equations -- ODE are DAE of index zero for which no latent", "generated": "Modern modeling languages for general physical systems, such as Modelica, Amesim, or Simscape, rely on Differential Algebraic Equations (DAE), i.e., constraints of the form f(dot{x},x,u)=0. This drastically facilitates modeling from first principles of the physics and the reuse of models."}
{"full_text": "This article is devoted to the study of the following semilinear equation with measure data which originates in the gravitational Maxwell gauged $O(3)$ sigma model, $$-\\Delta u + A_0(\\prod^k_{j=1}|x-p_j|^{2n_j} )^{-a} \\frac{e^u}{(1+e^u)^{1+a}} = 4\\pi\\sum_{j=1}^k n_j\\delta_{p_j} - 4\\pi\\sum^l_{j=1}m_j\\delta_{q_j} \\quad{\\rm in}\\;\\; \\mathbb{R}^2.\\qquad(E)$$ In this equation the $\\{\\delta_{p_j}\\}_{j=1}^k$ (resp. $\\{\\delta_{q_j}\\}_{j=1}^l$ ) are Dirac masses concentrated at the points $\\{p_j\\}_{j=1}^k$, (resp. $\\{q_j\\}_{j=1}^l$), $n_j$ and $m_j$ are positive integers, and $a$ is a nonnegative real number. We set $ N=\\sum^k_{j=1}n_j $ and $M= \\sum^l_{j=1}m_j$. In previous works \\cite{C,Y2}, some qualitative properties of solutions of $(E)$ with $a=0$ have been established. Our aim in this article is to study the more general case where $a>0$. The additional difficulties of this case come from the fact that the nonlinearity is no longer monotone and the data are signed measures. As a consequence we cannot anymore construct directly the solutions by the monotonicity method combined with the supersolutions and subsolutions technique. Instead we develop a new and self-contained approach which enables us to emphasize the role played by the gravitation in the gauged $O(3)$ sigma model. Without the gravitational term, i.e. if $a=0$, problem $(E)$ has a layer's structure of solutions $\\{u_\\beta\\}_{\\beta\\in(-2(N-M),\\, -2]}$, where $u_\\beta$ is the unique non-topological solution such that $u_{\\beta}=\\beta\\ln |x|+O(1)$ for $-2(N-M)<\\beta<-2$ and $u_{-2}=-2\\ln |x|-2\\ln\\ln |x|+O(1)$ at infinity respectively. On the contrary, when $a>0$, the set of solutions to problem $(E)$ has a much richer structure: besides the topological solutions, there exists a sequence of non-topological solutions in type I, i.e. such that $u $ tends to $-\\infty$ at infinity, and of non-topological solutions of type II, which tend to $\\infty$ at infinity. The existence of these types of solutions depends on the values of the parameters $N,\\, M,\\, \\beta$ and on the gravitational interaction associated to $a$.", "prompt": "This article is devoted to the study of the following semilinear equation with measure data which originates in the gravitational Maxwell gauged $O(3)$ sigma model, $$-\\Delta u + A_0(\\prod^k_{j=1}|x-p_j|^{2n_j} )^{-a}", "orig": "\\frac{e^u}{(1+e^u)^{1+a}} = 4\\pi\\sum_{j=1}^k n_j\\delta_{p_j} - 4\\pi\\sum^l_{j=1}m_j\\delta_{q_j} \\quad", "generated": "I'll repeat the given text and continue it.\n\nThis article is devoted to the study of the following semilinear equation with measure data which originates in the gravitational Maxwell gauged $O(3)$ sigma model, \n\n$$-\\Delta u + \\frac{1}{(\\prod^k_{j=1}|"}
{"full_text": "It is a classical result of Ginibre that the normalized bulk $k$-point correlation functions of a complex $n\\times n$ Gaussian matrix with independent entries of mean zero and unit variance are asymptotically given by the determinantal point process on $\\mathbb{C}$ with kernel $K_{\\infty}(z,w):=\\frac{1}{\\pi}e^{-|z|^2/2-|w|^2/2+z\\bar{w}}$ in the limit $n\\to\\infty$. In this paper, we show that this asymptotic law is universal among all random $n\\times n$ matrices $M_n$ whose entries are jointly independent, exponentially decaying, have independent real and imaginary parts and whose moments match that of the complex Gaussian ensemble to fourth order. Analogous results at the edge of the spectrum are also obtained. As an application, we extend a central limit theorem for the number of eigenvalues of complex Gaussian matrices in a small disk to these more general ensembles. These results are non-Hermitian analogues of some recent universality results for Hermitian Wigner matrices. However, a key new difficulty arises in the non-Hermitian case, due to the instability of the spectrum for such matrices. To resolve this issue, we the need to work with the log-determinants $\\log|\\det(M_n-z_0)|$ rather than with the Stieltjes transform $\\frac{1}{n}\\operatorname {tr}(M_n-z_0)^{-1}$, in order to exploit Girko's Hermitization method. Our main tools are a four moment theorem for these log-determinants, together with a strong concentration result for the log-determinants in the Gaussian case. The latter is established by studying the solutions of a certain nonlinear stochastic difference equation. With some extra consideration, we can extend our arguments to the real case, proving universality for correlation functions of real matrices which match the real Gaussian ensemble to the fourth order. As an application, we show that a real $n\\times n$ matrix whose entries are jointly independent, exponentially decaying and whose moments match the real Gaussian ensemble to fourth order has $\\sqrt{\\frac{2n}{\\pi}}+o(\\sqrt{n})$ real eigenvalues asymptotically almost surely.", "prompt": "It is a classical result of Ginibre that the normalized bulk $k$-point correlation functions of a complex $n\\times n$ Gaussian matrix with independent entries of mean zero and unit variance are asymptotically given by the determinantal point process on $\\mathbb{C}$ with kernel $K_{\\", "orig": "infty}(z,w):=\\frac{1}{\\pi}e^{-|z|^2/2-|w|^2/2+z\\bar{w}}$ in the limit $n\\to\\infty$. In this paper, we show that this asymptotic law is universal among all random $n", "generated": "It is a classical result of Ginibre that the normalized bulk $k$-point correlation functions of a complex $n\\times n$ Gaussian matrix with independent entries of mean zero and unit variance are asymptotically given by the determinantal point process on $\\mathbb{C}$ with kernel $K_{ij"}
{"full_text": "Context: Computational notebooks are a contemporary style of literate programming, in which users can communicate and transfer knowledge by interleaving executable code, output, and prose in a single rich document. A Domain-Specific Language (DSL) is an artificial software language tailored for a particular application domain. Usually, DSL users are domain experts that may not have a software engineering background. As a consequence, they might not be familiar with Integrated Development Environments (IDEs). Thus, the development of tools that offer different interfaces for interacting with a DSL is relevant. Inquiry: However, resources available to DSL designers are limited. We would like to leverage tools used to interact with general purpose languages in the context of DSLs. Computational notebooks are an example of such tools. Then, our main question is: What is an efficient and effective method of designing and implementing notebook interfaces for DSLs? By addressing this question we might be able to speed up the development of DSL tools, and ease the interaction between end-users and DSLs. Approach: In this paper, we present Bacat\\'a, a mechanism for generating notebook interfaces for DSLs in a language parametric fashion. We designed this mechanism in a way in which language engineers can reuse as many language components (e.g., language processors, type checkers, code generators) as possible. Knowledge: Our results show that notebook interfaces generated by Bacat\\'a can be automatically generated with little manual configuration. There are few considerations and caveats that should be addressed by language engineers that rely on language design aspects. The creation of a notebook for a DSL with Bacat\\'a becomes a matter of writing the code that wires existing language components in the Rascal language workbench with the Jupyter platform. Grounding: We evaluate Bacat\\'a by generating functional computational notebook interfaces for three different non-trivial DSLs, namely: a small subset of Halide (a DSL for digital image processing), SweeterJS (an extended version of JavaScript), and QL (a DSL for questionnaires). Additionally, it is relevant to generate notebook implementations rather than implementing them manually. We measured and compared the number of Source Lines of Code (SLOCs) that we reused from existing implementations of those languages. Importance: The adoption of notebooks by novice-programmers and end-users has made them very popular in several domains such as exploratory programming, data science, data journalism, and machine learning. Why are they popular? In (data) science, it is essential to make results reproducible as well as understandable. However, notebooks are only available for GPLs. This paper opens up the notebook metaphor for DSLs to improve the end-user experience when interacting with code and to increase DSLs adoption.", "prompt": "Context: Computational notebooks are a contemporary style of literate programming, in which users can communicate and transfer knowledge by interleaving executable code, output, and prose in a single rich document. A Domain-Specific Language (DSL) is an artificial software language tailored for a particular application domain. Usually, DSL users are domain", "orig": "experts that may not have a software engineering background. As a consequence, they might not be familiar with Integrated Development Environments (IDEs). Thus, the development of tools that offer different interfaces for interacting with a DSL is relevant. Inquiry: However, resources available to DSL designers are limited. We would like to leverage tools", "generated": "Computational notebooks are a contemporary style of literate programming, in which users can communicate and transfer knowledge by interleaving executable code, output, and prose in a single rich document. A Domain-Specific Language (DSL) is an artificial software language tailored for a particular application domain. Usually, DSL users are domain experts"}
{"full_text": "Ranking schemes drive many real-world decisions, like, where to study, whom to hire, what to buy, etc. Many of these decisions often come with high consequences. For example, a university can be deemed less prestigious if not featured in a top-k list, and consumers might not even explore products that do not get recommended to buyers. At the heart of most of these decisions are opaque ranking schemes, which dictate the ordering of data entities, but their internal logic is inaccessible or proprietary. Drawing inferences about the ranking differences is like a guessing game to the stakeholders, like, the rankees (i.e., the entities who are ranked, like product companies) and the decision-makers (i.e., who use the rankings, like buyers). In this paper, we aim to enable transparency in ranking interpretation by using algorithmic rankers that learn from available data and by enabling human reasoning about the learned ranking differences using explainable AI (XAI) methods. To realize this aim, we leverage the exploration-explanation paradigm of human-data interaction to let human stakeholders explore subsets and groupings of complex multi-attribute ranking data using visual explanations of model fit and attribute influence on rankings. We realize this explanation paradigm for transparent ranking interpretation in TRIVEA, a visual analytic system that is fueled by: i) visualizations of model fit derived from algorithmic rankers that learn the associations between attributes and rankings from available data and ii) visual explanations derived from XAI methods that help abstract important patterns, like, the relative influence of attributes in different ranking ranges. Using TRIVEA, end users not trained in data science have the agency to transparently reason about the global and local behavior of the rankings without the need to open black-box ranking models and develop confidence in the resulting attribute-based inferences. We demonstrate the efficacy of TRIVEA using multiple usage scenarios and subjective feedback from researchers with diverse domain expertise. Keywords: Visual Analytics, Learning-to-Rank, Explainable ML, Ranking", "prompt": "Ranking schemes drive many real-world decisions, like, where to study, whom to hire, what to buy, etc. Many of these decisions often come with high consequences. For example, a university can be deemed less prestigious if not featured in a top-k list, and consumers might not even explore products that do", "orig": "not get recommended to buyers. At the heart of most of these decisions are opaque ranking schemes, which dictate the ordering of data entities, but their internal logic is inaccessible or proprietary. Drawing inferences about the ranking differences is like a guessing game to the stakeholders, like, the rankees (i.e., the entities who", "generated": "Ranking schemes drive many real-world decisions, like, where to study, whom to hire, what to buy, etc. Many of these decisions often come with high consequences. For example, a university can be deemed less prestigious if not featured in a top-k list, and consumers might not even explore products that don"}
{"full_text": "A closed equilateral random walk in 3-space is a selection of unit length vectors giving the steps of the walk conditioned on the assumption that the sum of the vectors is zero. The sample space of such walks with $n$ edges is the $(2n-3)$-dimensional Riemannian manifold of equilateral closed polygons in $\\mathbb{R}^3$. We study closed random walks using the symplectic geometry of the $(2n-6)$-dimensional quotient of the manifold of polygons by the action of the rotation group $\\operatorname {SO}(3)$. The basic objects of study are the moment maps on equilateral random polygon space given by the lengths of any $(n-3)$-tuple of nonintersecting diagonals. The Atiyah-Guillemin-Sternberg theorem shows that the image of such a moment map is a convex polytope in $(n-3)$-dimensional space, while the Duistermaat-Heckman theorem shows that the pushforward measure on this polytope is Lebesgue measure on $\\mathbb{R}^{n-3}$. Together, these theorems allow us to define a measure-preserving set of \"action-angle\" coordinates on the space of closed equilateral polygons. The new coordinate system allows us to make explicit computations of exact expectations for total curvature and for some chord lengths of closed (and confined) equilateral random walks, to give statistical criteria for sampling algorithms on the space of polygons and to prove that the probability that a randomly chosen equilateral hexagon is unknotted is at least $\\frac{1}{2}$. We then use our methods to construct a new Markov chain sampling algorithm for equilateral closed polygons, with a simple modification to sample (rooted) confined equilateral closed polygons. We prove rigorously that our algorithm converges geometrically to the standard measure on the space of closed random walks, give a theory of error estimators for Markov chain Monte Carlo integration using our method and analyze the performance of our method. Our methods also apply to open random walks in certain types of confinement, and in general to walks with arbitrary (fixed) edgelengths as well as equilateral walks.", "prompt": "A closed equilateral random walk in 3-space is a selection of unit length vectors giving the steps of the walk conditioned on the assumption that the sum of the vectors is zero. The sample space of such walks with $n$ edges is the $(2n-3)$-dimensional Riemannian manifold", "orig": "of equilateral closed polygons in $\\mathbb{R}^3$. We study closed random walks using the symplectic geometry of the $(2n-6)$-dimensional quotient of the manifold of polygons by the action of the rotation group $\\operatorname {SO}(3)$. The basic objects of study", "generated": "Here is the repeated text with a continuation:\n\nA closed equilateral random walk in 3-space is a selection of unit length vectors giving the steps of the walk conditioned on the assumption that the sum of the vectors is zero. The sample space of such walks with $n$ edges is the $(2n-3"}
{"full_text": "Scaling temporal dynamics in functional MRI (fMRI) signals have been evidenced for a decade as intrinsic characteristics of ongoing brain activity (Zarahn et al., 1997). Recently, scaling properties were shown to fluctuate across brain networks and to be modulated between rest and task (He, 2011): notably, Hurst exponent, quantifying long memory, decreases under task in activating and deactivating brain regions. In most cases, such results were obtained: First, from univariate (voxelwise or regionwise) analysis, hence focusing on specific cognitive systems such as Resting-State Networks (RSNs) and raising the issue of the specificity of this scale-free dynamics modulation in RSNs. Second, using analysis tools designed to measure a single scaling exponent related to the second order statistics of the data, thus relying on models that either implicitly or explicitly assume Gaussianity and (asymptotic) self-similarity, while fMRI signals may significantly depart from those either of those two assumptions (Ciuciu et al., 2008; Wink et al., 2008). To address these issues, the present contribution elaborates on the analysis of the scaling properties of fMRI temporal dynamics by proposing two significant variations. First, scaling properties are technically investigated using the recently introduced Wavelet Leader-based Multifractal formalism (WLMF; Wendt et al., 2007). This measures a collection of scaling exponents, thus enables a richer and more versatile description of scale invariance (beyond correlation and Gaussianity), referred to as multifractality. Also, it benefits from improved estimation performance compared to tools previously used in the literature. Second, scaling properties are investigated in both RSN and non-RSN structures (e.g., artifacts), at a broader spatial scale than the voxel one, using a multivariate approach, namely the Multi-Subject Dictionary Learning (MSDL) algorithm (Varoquaux et al., 2011) that produces a set of spatial components that appear more sparse than their Independent Component Analysis (ICA) counterpart. These tools are combined and applied to a fMRI dataset comprising 12 subjects with resting-state and activation runs (Sadaghiani et al., 2009). Results stemming from those analysis confirm the already reported task-related decrease of long memory in functional networks, but also show that it occurs in artifacts, thus making this feature not specific to functional networks. Further, results indicate that most fMRI signals appear multifractal at rest except in non-cortical regions. Task-related modulation of multifractality appears only significant in functional networks and thus can be considered as the key property disentangling functional networks from artifacts. These finding are discussed in the light of the recent literature reporting scaling dynamics of EEG microstate sequences at rest and addressing non-stationarity issues in temporally independent fMRI modes.", "prompt": "Scaling temporal dynamics in functional MRI (fMRI) signals have been evidenced for a decade as intrinsic characteristics of ongoing brain activity (Zarahn et al., 1997). Recently, scaling properties were shown to fluctuate across brain networks and to be modulated between rest and task (He, 2011):", "orig": "notably, Hurst exponent, quantifying long memory, decreases under task in activating and deactivating brain regions. In most cases, such results were obtained: First, from univariate (voxelwise or regionwise) analysis, hence focusing on specific cognitive systems such as Resting-State Networks (RSNs) and", "generated": "Scaling temporal dynamics in functional MRI (fMRI) signals have been evidenced for a decade as intrinsic characteristics of ongoing brain activity (Zarahn et al., 1997). Recently, scaling properties were shown to fluctuate across brain networks and to be modulated between rest and task (He, 2011)."}
{"full_text": "Context: When software is released publicly, it is common to include with it either the full text of the license or licenses under which it is published, or a detailed reference to them. Therefore public licenses, including FOSS (free, open source software) licenses, are usually publicly available in source code repositories.Objective: To compile a dataset containing as many documents as possible that contain the text of software licenses, or references to the license terms. Once compiled, characterize the dataset so that it can be used for further research, or practical purposes related to license analysis.Method: Retrieve from Software Heritage-the largest publicly available archive of FOSS source code-all versions of all files whose names are commonly used to convey licensing terms. All retrieved documents will be characterized in various ways, using automated and manual analyses.Results: The dataset consists of 6.9 million unique license files. Additional metadata about shipped license files is also provided, making the dataset ready to use in various contexts, including: file length measures, MIME type, SPDX license (detected using ScanCode), and oldest appearance. The results of a manual analysis of 8102 documents is also included, providing a ground truth for further analysis. The dataset is released as open data as an archive file containing all deduplicated license files, plus several portable CSV files with metadata, referencing files via cryptographic checksums.Conclusions: Thanks to the extensive coverage of Software Heritage, the dataset presented in this paper covers a very large fraction of all software licenses for public code. We have assembled a large body of software licenses, characterized it quantitatively and qualitatively, and validated that it is mostly composed of licensing information and includes almost all known license texts. The dataset can be used to conduct empirical studies on open source licensing, training of automated license classifiers, natural language processing (NLP) analyses of legal texts, as well as historical and phylogenetic studies on FOSS licensing. It can also be used in practice to improve tools detecting licenses in source code.", "prompt": "Context: When software is released publicly, it is common to include with it either the full text of the license or licenses under which it is published, or a detailed reference to them. Therefore public licenses, including FOSS (free, open source software) licenses, are usually publicly available in source code repositories.Objective", "orig": ": To compile a dataset containing as many documents as possible that contain the text of software licenses, or references to the license terms. Once compiled, characterize the dataset so that it can be used for further research, or practical purposes related to license analysis.Method: Retrieve from Software Heritage-the largest publicly available archive of FOSS source", "generated": "Objectiv Objectiv\n\nWhen software is released publicly, it is common to include with it either the full text of the license or licenses under which it is published, or a detailed reference to them. Therefore public licenses, including FOSS (free, open source software) licenses, are usually publicly available in source code"}
{"full_text": "-Complex manufacturing systems are subject to high levels of variability that decrease productivity, increase cycle times and severely impact the systems tractability. As accurate modelling of the sources of variability is a cornerstone to intelligent decision making, we investigate the consequences of the assumption of independent and identically distributed variables that is often made when modelling sources of variability such as down-times, arrivals, or process-times. We first explain the experiment setting that allows, through simulations and statistical tests, to measure the variability potential stored in a specific sequence of data. We show from industrial data that dependent behaviors might actually be the rule with potentially considerable consequences in terms of cycle time. As complex industries require strong levers to allow their tractability, this work underlines the need for a richer and more accurate modelling of real systems. Keywords-variability; cycle time; dependent events; simulation; complex manufacturing; industry 4.0 I. Accurate modelling of variability and the independence assumption Industry 4.0 is said to be the next industrial revolution. The proper use of real-time information in complex manufacturing systems is expected to allow more customization of products in highly flexible production factories. Semiconductor High Mix Low Volume (HMLV) manufacturing facilities (called fabs) are one example of candidates for this transition towards \"smart industries\". However, because of the high levels of variability, the environment of a HMLV fab is highly stochastic and difficult to manage. The uncontrolled variability limits the predictability of the system and thus the ability to meet delivery requirements in terms of volumes, cycle times and due dates. Typically, the HMLV STMicroelectronics Crolles 300 fab regularly experiences significant mix changes that result in unanticipated bottlenecks, leading to firefighting to meet commitment to customers. The overarching goal of our strategy is to improve the forecasting of future occurrences of bottlenecks and cycle time issues in order to anticipate them through allocation of the correct attention and resources. Our current finite capacity projection engine can effectively forecast bottlenecks, but it does not include reliable cycle time estimates. In order to enhance our projections, better forecast cycle time losses (queuing times), improve the tractability of our system and reduce our cycle times, we now need accurate dynamic cycle time predictions. As increased cycle-time is the main reason workflow variability is studied (both by the scientific community and practitioners, see e.g. [1] and [2]), what follows concentrates on cycle times. Moreover, the \"variability\" we account for should be understood as the potential to create higher cycle times, even though \"variability\" may be understood in a broader meaning. This choice is made for the sake of clarity, but the methodology we propose and the discussion we lead can be applied to any other measurable indicator. Sources of variability have been intensely investigated in both the literature and the industry, and tool down-times, arrivals variability as well as process-time variability are recognized as the major sources of variability in that sense that they create higher cycle times (see [3] for a review and discussion). As a consequence, these factors are widely integrated into queuing formulas and simulation models with the objective to better model the complex reality of manufacturing facilities. One commonly accepted assumption in the development of these models is that the variables (MTBF, MTTR, processing times, time between arrivals, etc.) are independent and identically distributed (i.i.d.) random variables. However, these assumptions might be the reason for models inaccuracies as [4] points out in a literature review on queuing theory. Several authors have studied the potential effects of dependencies, such as [5] who studied the potential effects of dependencies between arrivals and process-times or [6] who investigated dependent process times, [4] also gives further references for studies on dependencies effects. In a previous work [3], we pinpointed a few elements from industrial data that questioned the viability of this assumption in complex manufacturing systems. Figure 1: Number of arrivals per week from real data (A) and generated by removing dependencies (B)", "prompt": "-Complex manufacturing systems are subject to high levels of variability that decrease productivity, increase cycle times and severely impact the systems tractability. As accurate modelling of the sources of variability is a cornerstone to intelligent decision making, we investigate the consequences of the assumption of independent and identically distributed variables that is often made when modelling sources", "orig": "of variability such as down-times, arrivals, or process-times. We first explain the experiment setting that allows, through simulations and statistical tests, to measure the variability potential stored in a specific sequence of data. We show from industrial data that dependent behaviors might actually be the rule with potentially considerable consequences in terms of cycle time.", "generated": "Complex manufacturing systems are subject to high levels of variability that decrease productivity, increase cycle times and severely impact the systems tractability. As accurate modelling of the sources of variability is a cornerstone to intelligent decision making, we investigate the consequences of the assumption of independent and identically distributed variables that is often made when modelling sources of"}
{"full_text": "In this article we address the first part of the programme presented in \\cite{Teleman_arXiv_III}, \\S 2; we construct the local $K$- theory level of the index formula. Our construction is sufficiently general to encompass the algebra of pseudo-differential operators of order zero on smooth manifolds, elliptic pseudo-differential operators of order zero, their abstract symbol (see Introduction \\S 2.) and their local $K$- theory analytical and topological index classes, see \\cite{Teleman_arXiv_III}, \\S 5, Definition 5 and 6. Our definitions are sufficiently general to apply to exact sequences of singular integral operators, which are of interest in the case of the index theorem on Lipschitz and quasi-conformal manifolds, see \\cite{Teleman_IHES}, \\cite{Teleman_Acta}, \\cite{Donaldson_Sullivan}, \\cite{Connes_Sullivan_Teleman}. In this article we introduce localised algebras (Definition 3) $\\mathit{A}$ and in \\S 6 we define their local algebraic $K$-theory. A localised algebra $\\mathit{A}$ is an algebra in which a decreasing filtration by vector sub-spaces $\\mathit{A}_{\\mu}$ is introduced. The filtration $\\mathit{A}_{\\mu}$ induces a filtration on the space of matrices $\\mathbb{M}(\\mathit{A}_{\\mu})$. Although we define solely $K^{loc}_{\\ast}(\\mathit{A})$ for $\\ast= 0, \\; 1$, we expect our construction could be extended in higher degrees. We stress that our construction of $K^{loc}_{0}(\\mathit{A})$ uses exclusively idempotent matrices and that the use of finite projective modules is totally avoided. (Idempotent matrices, rather than projective modules, contain less arbitrariness in the description of the $K_{0}$ classes and allow a better filtration control). The group $K^{loc}_{0}(\\mathit{A})$ is by definition the quotient space of the space of the Grothendieck completion of the space of idempotent matrices through three equivalence relations: -i) stabilisation $\\sim_{s}$, -2) local conjugation $\\sim_{l}$, {\\em and} -3) projective limit with respect to the filtration. By definition, the $K_{1}^{loc} (\\mathit{A})$ is the projective limit of the local $K_{1}(\\mathit{A}_{\\mu})$ groups. The group $K_{1}(\\mathit{A}_{\\mu})$ is by definition the quotient of $\\mathbb{GL}(\\mathit{A}_{\\mu})$ modulo the equivalence relation generated by: -1) stabilisation $\\sim_{s}$, --2) local conjugation $\\sim_{l}$ and -3) $\\sim_{\\mathbb{O}(\\mathit{A}_{\\mu})}$, where $\\mathbb{O}(\\mathit{A}_{\\mu})$ is the sub-module generated by elements of the form $ u \\oplus u^{-1} $, for any $u \\in \\mathbb{GL}(\\mathit{A}_{\\mu})$. The class of any invertible element $u$ modulo conjugation (inner auto-morphisms) we call the Jordan canonical form of $u$. The local conjugation preserves the local Jordan canonical form of invertible elements. The equivalence relation $\\sim_{\\mathbb{O}(\\mathit{A}_{\\mu})}$ insures existence of opposite elements in $K_{1}(\\mathit{A}_{\\mu})$ and $K_{1}^{loc}(\\mathit{A})$. Our definition of $K^{loc}_{1}(\\mathit{A})$ does not use the commutator sub-group $[\\mathbb{GL}(\\mathit{A}), \\mathbb{GL}(\\mathit{A})]$ nor elementary matrices in its construction. We define short exact sequences of localised algebras. To get the corresponding (open) six terms exact sequence (Theorem 51) one has to take the tensor product of the expected six terms exact sequence by $\\mathbb{Z}[\\frac{1}{2}]$. We expect the factor $\\otimes_{\\mathbb{Z}[\\frac{1}{2}}]$ to have important consequences. Our work shows that the basic structure of $K_{1}$ resides in the {\\em additive} sub-group generated by elements of the form $u \\oplus u^{-1}$, $u \\in \\mathbb{GL}(\\mathit{A})$, rather than in the {\\em multiplicativ} commutator sub-group $[\\mathbb{GL}(\\mathit{A}), \\mathbb{GL}(\\mathit{A})]$. Even into the case of trivially filtered algebras, $\\mathit{A}_{\\mu} = \\mathit{A}$, for all $\\mu \\in \\mathbb{N}$, the introduced group $K^{loc}_{1}(\\mathit{A})$ should provide more information than the classical group $K_{1}(\\mathit{A})$.", "prompt": "In this article we address the first part of the programme presented in \\cite{Teleman_arXiv_III}, \\S 2; we construct the local $K$- theory level of the index formula. Our construction is sufficiently general to encompass the algebra of pseudo-differential operators of order zero on smooth", "orig": "manifolds, elliptic pseudo-differential operators of order zero, their abstract symbol (see Introduction \\S 2.) and their local $K$- theory analytical and topological index classes, see \\cite{Teleman_arXiv_III}, \\S 5, Definition 5 and 6.", "generated": "In this article we address the first part of the programme presented in \\cite{Teleman_arXiv_III}, \\S 2; we construct the local $K$-theory level of the index formula. Our construction is sufficiently general to encompass the algebra of pseudo-differential operators of order zero on smooth"}
{"full_text": "Object-oriented languages often use virtual machines (VMs) that provide mechanisms such as just-in-time (JIT) compilation and garbage collection (GC). These VM components are typically implemented in a separate layer, isolating them from the application. While this approach brings the software engineering benefits of clear separation and decoupling, it introduces barriers for both understanding VM behavior and evolving the VM implementation. For example, the GC and JIT compiler are typically fixed at VM build time, limiting arbitrary adaptation at run time. Furthermore, because of this separation, the implementation of the VM cannot typically be inspected and debugged in the same way as application code, enshrining a distinction in easy-to-work-with application and hard-to-work-with VM code. These characteristics pose a barrier for application developers to understand the engine on top of which their own code runs, and fosters a knowledge gap that prevents application developers to change the VM. We propose Live Metacircular Runtimes (LMRs) to overcome this problem. LMRs are language runtime systems that seamlessly integrate the VM into the application in live programming environments. Unlike classic metacircular approaches, we propose to completely remove the separation between application and VM. By systematically applying object-oriented design to VM components, we can build live runtime systems that are small and flexible enough, where VM engineers can benefit of live programming features such as short feedback loops, and application developers with fewer VM expertise can benefit of the stronger causal connections between their programs and the VM implementation. To evaluate our proposal, we implemented Bee/LMR, a live VM for a Smalltalk-derivative environment in 22057 lines of code. We analyze case studies on tuning the garbage collector, avoiding recompilations by the just-in-time compiler, and adding support to optimize code with vector instructions to demonstrate the trade-offs of extending exploratory programming to VM development in the context of an industrial application used in production. Based on the case studies, we illustrate how our approach facilitates the daily development work of a small team of application developers. Our approach enables VM developers to gain access to live programming tools traditionally reserved for application developers, while application developers can interact with the VM and modify it using the high-level tools they use every day. Both application and VM developers can seamlessly inspect, debug, understand, and modify the different parts of the VM with shorter feedback loops and higher-level tools.", "prompt": "Object-oriented languages often use virtual machines (VMs) that provide mechanisms such as just-in-time (JIT) compilation and garbage collection (GC). These VM components are typically implemented in a separate layer, isolating them from the application. While this approach brings the software engineering benefits of clear separation and decoupling", "orig": ", it introduces barriers for both understanding VM behavior and evolving the VM implementation. For example, the GC and JIT compiler are typically fixed at VM build time, limiting arbitrary adaptation at run time. Furthermore, because of this separation, the implementation of the VM cannot typically be inspected and debugged in the same way as application code", "generated": "Object-oriented languages often use virtual machines (VMs) that provide mechanisms such as just-in-time (JIT) compilation and garbage collection (GC). These VM components are typically implemented in a separate layer, isolating them from the application. While this approach brings the software engineering benefits of clear separation and decoupling"}
{"full_text": "Plasma electrolytic oxidation (PEO) processing of EV 31 magnesium alloy has been carried out in fluoride containing electrolyte under bipolar pulse current regime. Unusual PEO cathodic micro-discharges have been observed and investigated. It is shown that the cathodic micro-discharges exhibit a collective intermittent behavior which is discussed in terms of charge accumulations at the layer/electrolyte and layer/metal interfaces. Optical emission spectroscopy is used to determine the electron density (typ. 10 15 cm-3) and the electron temperature (typ. 7500 K) while the role of F-anions on the appearance of cathodic micro-discharges is pointed out. Plasma Electrolytic Oxidation (PEO) is a promising plasma-assisted surface treatment of light metallic alloys (e.g. Al, Mg, Ti). Although the PEO process makes it possible to grow oxide coatings with interesting corrosion and wear resistant properties, the physical mechanisms of coating growth are not yet completely understood. Typically, the process consists in applying a high voltage difference between a metallic piece and a counter-electrode which are both immersed in an electrolyte bath. Compare to anodizing, the main differences concern the electrolyte composition and the current and voltage ranges which are at least one order of magnitude higher in PEO 1. These significant differences in current and voltage imply the dielectric breakdown and consequently the appearance of micro-discharges on the surface of the sample under processing. Those micro-discharges are recognized as being the main contributors to the formation of a dielectric porous crystalline oxide coating. 2 Nevertheless, the breakdown mechanism that governs the appearance of those micro-discharges is still under investigation. Hussein et al. 3 proposed a mechanism with three different plasma formation processes based on differences in plasma chemical composition. The results of Jovovi{\\'c} et al. 4,5 concerning physical properties of the plasma seem to corroborate this mechanism, and also point out the importance of the substrate material in the plasma composition. 6 Compared with DC conducted PEO process, using a bipolar pulsed DC or AC current supply gives supplementary control latitude through the current waveform parameters. The effect of these parameter on the micro-discharges behavior has been investigated in several previous works. 2,3,7,8 One of the main results of these studies is the absence of micro-discharge during the cathodic current half-period. 9-11 Even if the cathodic half-period has an obvious effect on the efficiency of PEO as well as on the coating growth and composition, the micro-plasmas appear only in anodic half-period. Sah et al. 8 have observed the cathodic breakdown of an oxide layer but at very high current density (10 kA.dm-${}^2$), and after several steps of sample preparation. Several models of micro-discharges appearance in AC current have already been proposed. 1,2,8,12,13 Though cathodic micro-discharges have never been observed within usual process conditions, the present study aims at defining suitable conditions to promote cathodic micro-discharges and at studying the main characteristics of these micro-plasmas.", "prompt": "Plasma electrolytic oxidation (PEO) processing of EV 31 magnesium alloy has been carried out in fluoride containing electrolyte under bipolar pulse current regime. Unusual PEO cathodic micro-discharges have been observed and investigated. It is shown that the cathodic micro-discharges exhibit a collective intermittent behavior which is", "orig": "discussed in terms of charge accumulations at the layer/electrolyte and layer/metal interfaces. Optical emission spectroscopy is used to determine the electron density (typ. 10 15 cm-3) and the electron temperature (typ. 7500 K) while the role of F-anions on the", "generated": "Plasma electrolytic oxidation (PEO) processing of EV 31 magnesium alloy has been carried out in fluoride containing electrolyte under bipolar pulse current regime. Unusual PEO cathodic micro-discharges have been observed and investigated. It is shown that the cathodic micro-discharges exhibit a collective intermittent behavior which is"}
{"full_text": "Accurate traffic state information plays a pivotal role in the Intelligent Transportation Systems (ITS), and it is an essential input to various smart mobility applications such as signal coordination and traffic flow prediction. The current practice to obtain the traffic state information is through specialized sensors such as loop detectors and speed cameras. In most metropolitan areas, traffic monitoring cameras have been installed to monitor the traffic conditions on arterial roads and expressways, and the collected videos or images are mainly used for visual inspection by traffic engineers. Unfortunately, the data collected from traffic monitoring cameras are affected by the 4L characteristics: Low frame rate, Low resolution, Lack of annotated data, and Located in complex road environments. Therefore, despite the great potentials of the traffic monitoring cameras, the 4L characteristics hinder them from providing useful traffic state information (e.g., speed, flow, density). This paper focuses on the traffic density estimation problem as it is widely applicable to various traffic surveillance systems. To the best of our knowledge, there is a lack of the holistic framework for addressing the 4L characteristics and extracting the traffic density information from traffic monitoring camera data. In view of this, this paper proposes a framework for estimating traffic density using uncalibrated traffic monitoring cameras with 4L characteristics. The proposed framework consists of two major components: camera calibration and vehicle detection. The camera calibration method estimates the actual length between pixels in the images and videos, and the vehicle counts are extracted from the deep-learning-based vehicle detection method. Combining the two components, high-granular traffic density can be estimated. To validate the proposed framework, two case studies were conducted in Hong Kong and Sacramento. The results show that the Mean Absolute Error (MAE) in camera calibration is less than 0.2 meters out of 6 meters, and the accuracy of vehicle detection under various conditions is approximately 90%. Overall, the MAE for the estimated density is 9.04 veh/km/lane in Hong Kong and 1.30 veh/km/lane in Sacramento. The research outcomes can be used to calibrate the speed-density fundamental diagrams, and the proposed framework can provide accurate and real-time traffic information without installing additional sensors.", "prompt": "Accurate traffic state information plays a pivotal role in the Intelligent Transportation Systems (ITS), and it is an essential input to various smart mobility applications such as signal coordination and traffic flow prediction. The current practice to obtain the traffic state information is through specialized sensors such as loop detectors and speed cameras. In most metropolitan areas,", "orig": "traffic monitoring cameras have been installed to monitor the traffic conditions on arterial roads and expressways, and the collected videos or images are mainly used for visual inspection by traffic engineers. Unfortunately, the data collected from traffic monitoring cameras are affected by the 4L characteristics: Low frame rate, Low resolution, Lack of annotated data,", "generated": "Accurate traffic state information plays a pivotal role in the Intelligent Transportation Systems (ITS), and it is an essential input to various smart mobility applications such as signal coordination and traffic flow prediction. The current practice to obtain the traffic state information is through specialized sensors such as loop detectors and speed cameras. In most metropolitan areas,"}
{"full_text": "When we consider a proper holomorphic map \\ $\\tilde{f}: X \\to C$ \\ of a complex manifold \\ $X$ \\ on a smooth complex curve \\ $C$ \\ with a critical value at a point \\ $0$ \\ in \\ $C$, the choice of a local coordinate near this point allows to dispose of an holomorphic function \\ $f$. Then we may construct, using this function, an (a,b)-modules structure on the cohomology sheaves of the formal completion (in \\ $f$) \\ of the complex of sheaves \\ $(Ker\\, df^{\\bullet},d^{\\bullet})$. These (a,b)-modules represent a filtered version of the Gauss-Manin connection of \\ $f$. The most simple example of this construction is the Brieskorn module (see [Br.70]) of a function with an isolated singular point. See [B.08] for the case of a 1-dimensional critical locus. But it is clear that this construction depends seriously on the choice of the function \\ $f$ \\ that is to say on the choice of the local coordinate near the critical point \\ $0$ \\ in the complex curve \\ $C$. The aim of the present paper is to study the behaviour of such constructions when we make a change of local coordinate near the origin. We consider the case of \\ $[\\lambda]-$primitive frescos, which are monogenic geometric (a,b)-modules corresponding to a minimal filtered differential equation associated to a relative de Rham cohomology class on \\ $X$ \\ (see [B.09-a] and [B.09-b]). An holomorphic parameter is a function on the set of isomorphism classes of frescos which behave holomorphically in an holomorphic family of frescos. In general, an holomorphic parameter is not invariant by a change of variable, but we prove a theorem of stability of holomorphic families of frescos by a change of variable and it implies that an holomorphic parameter gives again an holomorphic parameter by a change of variable. We construct here two different kinds of holomorphic parameters which are (quasi-)invariant by change of variable. The first kind is associated to Jordan blocks of the monodromy with size at least two. The second kind is associated to the semi-simple part of the monodromy and look like some \"cross ratio\" of eigenvectors. They allow, in the situation describe above, to associate to a given (vanishing) relative de Rham cohomology class some numbers, which will depend holomorphically of our data, and are independant of the choice of the local coordinate near \\ $0$ \\ to study the Gauss-Manin connection of this degeneration of compact complex manifolds.", "prompt": "When we consider a proper holomorphic map \\ $\\tilde{f}: X \\to C$ \\ of a complex manifold \\ $X$ \\ on a smooth complex curve \\ $C$ \\ with a critical value at a point \\ $0$ \\ in \\ $C$, the choice of a local coordinate", "orig": "near this point allows to dispose of an holomorphic function \\ $f$. Then we may construct, using this function, an (a,b)-modules structure on the cohomology sheaves of the formal completion (in \\ $f$) \\ of the complex of sheaves \\ $(Ker\\, df^", "generated": "When we consider a proper holomorphic map \\ $\\tilde{f}: X \\to C$ \\ of a complex manifold \\ $X$ \\ on a smooth complex curve \\ $C$ \\ with a critical value at a point \\ $0$ \\ in \\ $C$, the choice of a local coordinate"}
{"full_text": "Consider a class of decomposable combinatorial structures, using different types of atoms $\\Atoms = \\{\\At_1,\\ldots ,\\At_{|{\\Atoms}|}\\}$. We address the random generation of such structures with respect to a size $n$ and a targeted distribution in $k$ of its \\emph{distinguished} atoms. We consider two variations on this problem. In the first alternative, the targeted distribution is given by $k$ real numbers $\\TargFreq_1, \\ldots, \\TargFreq_k$ such that $0 < \\TargFreq_i < 1$ for all $i$ and $\\TargFreq_1+\\cdots+\\TargFreq_k \\leq 1$. We aim to generate random structures among the whole set of structures of a given size $n$, in such a way that the {\\em expected} frequency of any distinguished atom $\\At_i$ equals $\\TargFreq_i$. We address this problem by weighting the atoms with a $k$-tuple $\\Weights$ of real-valued weights, inducing a weighted distribution over the set of structures of size $n$. We first adapt the classical recursive random generation scheme into an algorithm taking $\\bigO{n^{1+o(1)}+mn\\log{n}}$ arithmetic operations to draw $m$ structures from the $\\Weights$-weighted distribution. Secondly, we address the analytical computation of weights such that the targeted frequencies are achieved asymptotically, i. e. for large values of $n$. We derive systems of functional equations whose resolution gives an explicit relationship between $\\Weights$ and $\\TargFreq_1, \\ldots, \\TargFreq_k$. Lastly, we give an algorithm in $\\bigO{k n^4}$ for the inverse problem, {\\it i.e.} computing the frequencies associated with a given $k$-tuple $\\Weights$ of weights, and an optimized version in $\\bigO{k n^2}$ in the case of context-free languages. This allows for a heuristic resolution of the weights/frequencies relationship suitable for complex specifications. In the second alternative, the targeted distribution is given by a $k$ natural numbers $n_1, \\ldots, n_k$ such that $n_1+\\cdots+n_k+r=n$ where $r \\geq 0$ is the number of undistinguished atoms. The structures must be generated uniformly among the set of structures of size $n$ that contain {\\em exactly} $n_i$ atoms $\\At_i$ ($1 \\leq i \\leq k$). We give a $\\bigO{r^2\\prod_{i=1}^k n_i^2 +m n k \\log n}$ algorithm for generating $m$ structures, which simplifies into a $\\bigO{r\\prod_{i=1}^k n_i +m n}$ for regular specifications.", "prompt": "Consider a class of decomposable combinatorial structures, using different types of atoms $\\Atoms = \\{\\At_1,\\ldots,\\At_{|{\\Atoms}|}\\}$. We address the random generation of such structures with respect to a size $n$ and a targeted distribution in $k$ of", "orig": "its \\emph{distinguished} atoms. We consider two variations on this problem. In the first alternative, the targeted distribution is given by $k$ real numbers $\\TargFreq_1, \\ldots, \\TargFreq_k$ such that $0 < \\TargFreq_i < 1", "generated": "Consider a class of decomposable combinatorial structures, using different types of atoms $\\Atoms = \\{\\At_1,\\ldots,\\At_{|{\\Atoms}|}\\}$. We address the random generation of such structures with respect to a size $n$ and a targeted distribution in $k$ of $\\"}
{"full_text": "Aims. The comparative study of several molecular species at the origin of the gas phase chemistry in the diffuse interstellar medium (ISM) is a key input in unraveling the coupled chemical and dynamical evolution of the ISM. Methods. The lowest rotational lines of HCO+, HCN, HNC, and CN were observed at the IRAM-30m telescope in absorption against the \\lambda 3 mm and \\lambda 1.3 mm continuum emission of massive star-forming regions in the Galactic plane. The absorption lines probe the gas over kiloparsecs along these lines of sight. The excitation temperatures of HCO+ are inferred from the comparison of the absorptions in the two lowest transitions. The spectra of all molecular species on the same line of sight are decomposed into Gaussian velocity components. Most appear in all the spectra of a given line of sight. For each component, we derived the central opacity, the velocity dispersion, and computed the molecular column density. We compared our results to the predictions of UV-dominated chemical models of photodissociation regions (PDR models) and to those of non-equilibrium models in which the chemistry is driven by the dissipation of turbulent energy (TDR models). Results. The molecular column densities of all the velocity components span up to two orders of magnitude. Those of CN, HCN, and HNC are linearly correlated with each other with mean ratios N(HCN)/N(HNC) = 4.8 $\\pm$ 1.3 and N(CN)/N(HNC) = 34 $\\pm$ 12, and more loosely correlated with those of HCO+, N(HNC)/N(HCO+) = 0.5 $\\pm$ 0.3, N(HCN)/N(HCO+) = 1.9 $\\pm$ 0.9, and N(CN)/N(HCO+) = 18 $\\pm$ 9. These ratios are similar to those inferred from observations of high Galactic latitude lines of sight, suggesting that the gas sampled by absorption lines in the Galactic plane has the same chemical properties as that in the Solar neighbourhood. The FWHM of the Gaussian velocity components span the range 0.3 to 3 km s-1 and those of the HCO+ lines are found to be 30% broader than those of CN-bearing molecules. The PDR models fail to reproduce simultaneously the observed abundances of the CN-bearing species and HCO+, even for high-density material (100 cm-3 < nH < 104 cm-3). The TDR models, in turn, are able to reproduce the observed abundances and abundance ratios of all the analysed molecules for the moderate gas densities (30 cm-3 < nH < 200 cm-3) and the turbulent energy observed in the diffuse interstellar medium. Conclusions. Intermittent turbulent dissipation appears to be a promising driver of the gas phase chemistry of the diffuse and translucent gas throughout the Galaxy. The details of the dissipation mechanisms still need to be investigated.", "prompt": "Aims. The comparative study of several molecular species at the origin of the gas phase chemistry in the diffuse interstellar medium (ISM) is a key input in unraveling the coupled chemical and dynamical evolution of the ISM. Methods. The lowest rotational lines of HCO+, HCN, HNC, and", "orig": "CN were observed at the IRAM-30m telescope in absorption against the \\lambda 3 mm and \\lambda 1.3 mm continuum emission of massive star-forming regions in the Galactic plane. The absorption lines probe the gas over kiloparsecs along these lines of sight. The excitation temperatures of H", "generated": "Aims. The comparative study of several molecular species at the origin of the gas phase chemistry in the diffuse interstellar medium (ISM) is a key input in unraveling the coupled chemical and dynamical evolution of the ISM. Methods. The lowest rotational lines of HCO+, HCN, HNC, and"}
{"full_text": "Parametrized families of PDEs arise in various contexts such as inverse problems, control and optimization, risk assessment, and uncertainty quantification. In most of these applications, the number of parameters is large or perhaps even infinite. Thus, the development of numerical methods for these parametric problems is faced with the possible curse of dimensionality. This article is directed at (i) identifying and understanding which properties of parametric equations allow one to avoid this curse and (ii) developing and analyzing effective numerical methodd which fully exploit these properties and, in turn, are immune to the growth in dimensionality. The first part of this article studies the smoothness and approximability of the solution map, that is, the map $a\\mapsto u(a)$ where $a$ is the parameter value and $u(a)$ is the corresponding solution to the PDE. It is shown that for many relevant parametric PDEs, the parametric smoothness of this map is typically holomorphic and also highly anisotropic in that the relevant parameters are of widely varying importance in describing the solution. These two properties are then exploited to establish convergence rates of $n$-term approximations to the solution map for which each term is separable in the parametric and physical variables. These results reveal that, at least on a theoretical level, the solution map can be well approximated by discretizations of moderate complexity, thereby showing how the curse of dimensionality is broken. This theoretical analysis is carried out through concepts of approximation theory such as best $n$-term approximation, sparsity, and $n$-widths. These notions determine a priori the best possible performance of numerical methods and thus serve as a benchmark for concrete algorithms. The second part of this article turns to the development of numerical algorithms based on the theoretically established sparse separable approximations. The numerical methods studied fall into two general categories. The first uses polynomial expansions in terms of the parameters to approximate the solution map. The second one searches for suitable low dimensional spaces for simultaneously approximating all members of the parametric family. The numerical implementation of these approaches is carried out through adaptive and greedy algorithms. An a priori analysis of the performance of these algorithms establishes how well they meet the theoretical benchmarks.", "prompt": "Parametrized families of PDEs arise in various contexts such as inverse problems, control and optimization, risk assessment, and uncertainty quantification. In most of these applications, the number of parameters is large or perhaps even infinite. Thus, the development of numerical methods for these parametric problems is faced with the possible", "orig": "curse of dimensionality. This article is directed at (i) identifying and understanding which properties of parametric equations allow one to avoid this curse and (ii) developing and analyzing effective numerical methodd which fully exploit these properties and, in turn, are immune to the growth in dimensionality. The first part of this article", "generated": "Parametrized families of PDEs arise in various contexts such as inverse problems, control and optimization, risk assessment, and uncertainty quantification. In most of these applications, the number of parameters is large or perhaps even infinite. Thus, the development of numerical methods for these parametric problems is faced with the possibility"}
{"full_text": "Context: Container data types are ubiquitous in computer programming, enabling developers to efficiently store and process collections of data with an easy-to-use programming interface. Many programming languages offer a variety of container implementations in their standard libraries based on data structures offering different capabilities and performance characteristics. Inquiry: Choosing the *best* container for an application is not always straightforward, as performance characteristics can change drastically in different scenarios, and as real-world performance is not always correlated to theoretical complexity. Approach: We present Primrose, a language-agnostic tool for selecting the best performing valid container implementation from a set of container data types that satisfy *properties* given by application developers. Primrose automatically selects the set of valid container implementations for which the *library specifications*, written by the developers of container libraries, satisfies the specified properties. Finally, Primrose ranks the valid library implementations based on their runtime performance. Knowledge: With Primrose, application developers can specify the expected behaviour of a container as a type refinement with *semantic properties*, e.g., if the container should only contain unique values (such as a `set`) or should satisfy the LIFO property of a `stack`. Semantic properties nicely complement *syntactic properties* (i.e., traits, interfaces, or type classes), together allowing developers to specify a container's programming interface *and* behaviour without committing to a concrete implementation. Grounding: We present our prototype implementation of Primrose that preprocesses annotated Rust code, selects valid container implementations and ranks them on their performance. The design of Primrose is, however, language-agnostic, and is easy to integrate into other programming languages that support container data types and traits, interfaces, or type classes. Our implementation encodes properties and library specifications into verification conditions in Rosette, an interface for SMT solvers, which determines the set of valid container implementations. We evaluate Primrose by specifying several container implementations, and measuring the time taken to select valid implementations for various combinations of properties with the solver. We automatically validate that container implementations conform to their library specifications via property-based testing. Importance: This work provides a novel approach to bring abstract modelling and specification of container types directly into the programmer's workflow. Instead of selecting concrete container implementations, application programmers can now work on the level of specification, merely stating the behaviours they require from their container types, and the best implementation can be selected automatically.", "prompt": "Context: Container data types are ubiquitous in computer programming, enabling developers to efficiently store and process collections of data with an easy-to-use programming interface. Many programming languages offer a variety of container implementations in their standard libraries based on data structures offering different capabilities and performance characteristics. Inquiry: Choosing the *best* container for an", "orig": "application is not always straightforward, as performance characteristics can change drastically in different scenarios, and as real-world performance is not always correlated to theoretical complexity. Approach: We present Primrose, a language-agnostic tool for selecting the best performing valid container implementation from a set of container data types that satisfy *properties* given by application", "generated": "Container data types are ubiquitous in computer programming, enabling developers to efficiently store and process collections of data with an easy-to-use programming interface. Many programming languages offer a variety of container implementations in their standard libraries based on data structures offering different capabilities and performance characteristics.\n\nChoosing the *best* container for a specific task can be"}
{"full_text": "Linear latent variable models such as principal component analysis (PCA), independent component analysis (ICA), canonical correlation analysis (CCA), and factor analysis (FA) identify latent directions (or loadings) either ordered or unordered. The data is then projected onto the latent directions to obtain their projected representations (or scores). For example, PCA solvers usually rank the principal directions by explaining the most to least variance, while ICA solvers usually return independent directions unordered and often with single sources spread across multiple directions as multiple sub-sources, which is of severe detriment to their usability and interpretability. This paper proposes a general framework to enhance latent space representations for improving the interpretability of linear latent spaces. Although the concepts in this paper are language agnostic, the framework is written in Python. This framework automates the clustering and ranking of latent vectors to enhance the latent information per latent vector, as well as, the interpretation of latent vectors. Several innovative enhancements are incorporated including latent ranking (LR), latent scaling (LS), latent clustering (LC), and latent condensing (LCON). For a specified linear latent variable model, LR ranks latent directions according to a specified metric, LS scales latent directions according to a specified metric, LC automatically clusters latent directions into a specified number of clusters, while, LCON automatically determines an appropriate number of clusters into which to condense the latent directions for a given metric. Additional functionality of the framework includes single-channel and multi-channel data sources, data preprocessing strategies such as Hankelisation to seamlessly expand the applicability of linear latent variable models (LLVMs) to a wider variety of data. The effectiveness of LR, LS, and LCON are showcased on two crafted foundational problems with two applied latent variable models, namely, PCA and ICA.", "prompt": "Linear latent variable models such as principal component analysis (PCA), independent component analysis (ICA), canonical correlation analysis (CCA), and factor analysis (FA) identify latent directions (or loadings) either ordered or unordered. The data is then projected onto the latent directions to obtain their projected representations (or scores). For example", "orig": ", PCA solvers usually rank the principal directions by explaining the most to least variance, while ICA solvers usually return independent directions unordered and often with single sources spread across multiple directions as multiple sub-sources, which is of severe detriment to their usability and interpretability. This paper proposes a general framework to enhance latent space", "generated": "Linear latent variable models such as principal component analysis (PCA), independent component analysis (ICA), canonical correlation analysis (CCA), and factor analysis (FA) identify latent directions (or loadings) either ordered or unordered. The data is then projected onto the latent directions to obtain their projected representations (or scores). For example"}
{"full_text": "Most seismic risk assessment methods focus on estimating the damages to the built environment and the consequent socioeconomic losses without fully taking into account the social aspect of risk. Yet, human behaviour is a key element in predicting the human impact of an earthquake, therefore, it is important to include it in quantitative risk assessment studies. In this study, an interdisciplinary approach simulating pedestrians' evacuation during earthquakes at the city scale is developed using an agent-based model. The model integrates the seismic hazard, the physical vulnerability as well as individuals' behaviours and mobility. The simulator is applied to the case of Beirut, Lebanon. Lebanon is at the heart of the Levant fault system that has generated several Mw>7 earthquakes, the latest being in 1759. It is one of the countries with the highest seismic risk in the Mediterranean region. This is due to the high seismic vulnerability of the buildings due to the absence of mandatory seismic regulation until 2012, the high level of urbanization, and the lack of adequate spatial planning and risk prevention policies. Beirut as the main residential, economic and institutional hub of Lebanon is densely populated. To accommodate the growing need for urban development, constructions have almost taken over all of the green areas of the city; squares and gardens are disappearing to give place to skyscrapers. However, open spaces are safe places to shelter, away from debris, and therefore play an essential role in earthquake evacuation. Despite the massive urbanization, there are a few open spaces but locked gates and other types of anthropogenic barriers often limit their access. To simulate this complex context, pedestrians' evacuation simulations are run in a highly realistic spatial environment implemented in GAMA [1]. Previous data concerning soil and buildings in Beirut [2, 3] are complemented by new geographic data extracted from high-resolution Pleiades satellite images. The seismic loading is defined as a peak ground acceleration of 0.3g, as stated in Lebanese seismic regulations. Building damages are estimated using an artificial neural network trained to predict the mean damage [4] based on the seismic loading as well as the soil and building vibrational properties [5]. Moreover, the quantity and the footprint of the generated debris around each building are also estimated and included in the model. We simulate how topography, buildings, debris, and access to open spaces, affect individuals' mobility. Two city configurations are implemented: 1. Open spaces are accessible without any barriers; 2. Access to some open spaces is blocked. The first simulation results show that while 52% of the population is able to arrive to an open space within 5 minutes after an earthquake, this number is reduced to 39% when one of the open spaces is locked. These results show that the presence of accessible open spaces in a city and their proximity to the residential buildings is a crucial factor for ensuring people's safety when an earthquake occurs.", "prompt": "Most seismic risk assessment methods focus on estimating the damages to the built environment and the consequent socioeconomic losses without fully taking into account the social aspect of risk. Yet, human behaviour is a key element in predicting the human impact of an earthquake, therefore, it is important to include it in quantitative risk assessment studies. In", "orig": "this study, an interdisciplinary approach simulating pedestrians' evacuation during earthquakes at the city scale is developed using an agent-based model. The model integrates the seismic hazard, the physical vulnerability as well as individuals' behaviours and mobility. The simulator is applied to the case of Beirut, Lebanon. Lebanon is at the heart of the Lev", "generated": "Most seismic risk assessment methods focus on estimating the damages to the built environment and the consequent socioeconomic losses without fully taking into account the social aspect of risk. Yet, human behavior is a key element in predicting the human impact of an earthquake, therefore, it is important to include it in quantitative risk assessment studies. Incorpor"}
{"full_text": "The type problem is the problem of deciding, for a simply connected Riemann surface, whether it is conformally equivalent to the complex plane or to the unit dic in the complex plane. We report on Teichm{\\\"u}ller's results on the type problem from his two papers Eine Anwendung quasikonformer Abbildungen auf das Typenproblem (An application of quasiconformal map-pings to the type problem) (1937) and Untersuchungen{\\\"u}ber kon-forme und quasikonforme Abbildungen (Investigations on con-formal and quasiconformal mappings) (1938). They concern simply connected Riemann surfaces defined as branched covers of the sphere. At the same time, we review the theory of line complexes, a combinatorial device used by Teichm{\\\"u}ller and others to encode branched coverings of the sphere. In the first paper, Teichm{\\\"u}ller proves that any two simply connected Riemann surfaces which are branched coverings of the Riemann sphere with finitely many branch values and which have the same line complex are quasiconformally equivalent. For this purpose, he introduces a technique for piecing together quasi-conformal mappings. He also obtains a result on the extension of smooth orientation-preserving diffeomorphisms of the circle to quasiconformal mappings of the disc which are conformal at the boundary. In the second paper, using line complexes, Teichm{\\\"u}ller gives a type criterion for a simply-connected surface which is a branched covering of the sphere, in terms of an adequately defined measure of ramification, defined by a limiting process. The result says that if the surface is ''sufficiently ramified'' (in a sense to be made precise), then it is hyperbolic. In the same paper, Te-ichm{\\\"u}ller answers by the negative a conjecture made by Nevan-linna which states a criterion for parabolicity in terms of the value of a (different) measure of ramification, defined by a limiting process. Teichm{\\\"u}ller's results in his first paper are used in the proof of the results of the second one.The final version of this paper will appear in Vol. VII of the Handbook of Teichm{\\\"u}ller theory (European Mathematical Society Publishing House, 2020).", "prompt": "The type problem is the problem of deciding, for a simply connected Riemann surface, whether it is conformally equivalent to the complex plane or to the unit dic in the complex plane. We report on Teichm{\\\"u}ller's results on the type problem from his two papers Eine Anwendung quas", "orig": "ikonformer Abbildungen auf das Typenproblem (An application of quasiconformal map-pings to the type problem) (1937) and Untersuchungen{\\\"u}ber kon-forme und quasikonforme Abbildungen (Investigations on con-formal and quasiconformal", "generated": "The type problem is the problem of deciding, for a simply connected Riemann surface, whether it is conformally equivalent to the complex plane or to the unit disc in the complex plane. We report on Teichm{\\\"u}ller's results on the type problem from his two papers \"Eine Anwendung der"}
{"full_text": "We propose a novel theoretical biomolecular design to implement any Boolean circuit using the mechanism of DNA strand displacement. The design is scalable: all species of DNA strands can in principle be mixed and prepared in a single test tube, rather than requiring separate purification of each species, which is a barrier to large-scale synthesis. The design is time-responsive: the concentration of output species changes in response to the concentration of input species, so that time-varying inputs may be continuously processed. The design is digital: Boolean values of wires in the circuit are represented as high or low concentrations of certain species, and we show how to construct a single-input, single-output signal restoration gate that amplifies the difference between high and low, which can be distributed to each wire in the circuit to overcome signal degradation. This means we can achieve a digital abstraction of the analog values of concentrations. Finally, the design is energy-efficient: if input species are specified ideally (meaning absolutely 0 concentration of unwanted species), then output species converge to their ideal concentrations at steady-state, and the system at steady-state is in (dynamic) equilibrium, meaning that no energy is consumed by irreversible reactions until the input again changes. Drawbacks of our design include the following. If input is provided non-ideally (small positive concentration of unwanted species), then energy must be continually expended to maintain correct output concentrations even at steady-state. In addition, our fuel species - those species that are permanently consumed in irreversible reactions - are not \"generic\"; each gate in the circuit is powered by its own specific type of fuel species. Hence different circuits must be powered by different types of fuel. Finally, we require input to be given according to the dual-rail convention, so that an input of 0 is specified not only by the absence of a certain species, but by the presence of another. That is, we do not construct a \"true NOT gate\" that sets its output to high concentration if and only if its input's concentration is low. It remains an open problem to design scalable, time-responsive, digital, energy-efficient molecular circuits that additionally solve one of these problems, or to prove that some subset of their resolutions are mutually incompatible.", "prompt": "We propose a novel theoretical biomolecular design to implement any Boolean circuit using the mechanism of DNA strand displacement. The design is scalable: all species of DNA strands can in principle be mixed and prepared in a single test tube, rather than requiring separate purification of each species, which is a barrier to large-scale synthesis. The", "orig": "design is time-responsive: the concentration of output species changes in response to the concentration of input species, so that time-varying inputs may be continuously processed. The design is digital: Boolean values of wires in the circuit are represented as high or low concentrations of certain species, and we show how to construct a single-input", "generated": "We propose a novel theoretical biomolecular design to implement any Boolean circuit using the mechanism of DNA strand displacement. The design is scalable: all species of DNA strands can in principle be mixed and prepared in a single test tube, rather than requiring separate purification of each species, which is a barrier to large-scale synthesis.\n\nThis"}
{"full_text": "Diagrammatic reasoning (DR) is pervasive in human problem solving as a powerful adjunct to symbolic reasoning based on language-like representations. The research reported in this paper is a contribution to building a general purpose DR system as an extension to a SOAR-like problem solving architecture. The work is in a framework in which DR is modeled as a process where subtasks are solved, as appropriate, either by inference from symbolic representations or by interaction with a diagram, i.e., perceiving specified information from a diagram or modifying/creating objects in a diagram in specified ways according to problem solving needs. The perceptions and actions in most DR systems built so far are hand-coded for the specific application, even when the rest of the system is built using the general architecture. The absence of a general framework for executing perceptions/actions poses as a major hindrance to using them opportunistically -- the essence of open-ended search in problem solving. Our goal is to develop a framework for executing a wide variety of specified perceptions and actions across tasks/domains without human intervention. We observe that the domain/task-specific visual perceptions/actions can be transformed into domain/task-independent spatial problems. We specify a spatial problem as a quantified constraint satisfaction problem in the real domain using an open-ended vocabulary of properties, relations and actions involving three kinds of diagrammatic objects -- points, curves, regions. Solving a spatial problem from this specification requires computing the equivalent simplified quantifier-free expression, the complexity of which is inherently doubly exponential. We represent objects as configuration of simple elements to facilitate decomposition of complex problems into simpler and similar subproblems. We show that, if the symbolic solution to a subproblem can be expressed concisely, quantifiers can be eliminated from spatial problems in low-order polynomial time using similar previously solved subproblems. This requires determining the similarity of two problems, the existence of a mapping between them computable in polynomial time, and designing a memory for storing previously solved problems so as to facilitate search. The efficacy of the idea is shown by time complexity analysis. We demonstrate the proposed approach by executing perceptions and actions involved in DR tasks in two army applications.", "prompt": "Diagrammatic reasoning (DR) is pervasive in human problem solving as a powerful adjunct to symbolic reasoning based on language-like representations. The research reported in this paper is a contribution to building a general purpose DR system as an extension to a SOAR-like problem solving architecture. The work is in a framework in which DR is", "orig": "modeled as a process where subtasks are solved, as appropriate, either by inference from symbolic representations or by interaction with a diagram, i.e., perceiving specified information from a diagram or modifying/creating objects in a diagram in specified ways according to problem solving needs. The perceptions and actions in most DR systems built so far", "generated": "Diagrammatic reasoning (DR) is pervasive in human problem solving as a powerful adjunct to symbolic reasoning based on language-like representations. The research reported in this paper is a contribution to building a general purpose DR system as an extension to a SOAR-like problem solving architecture. The work is in a framework in which DR is"}
{"full_text": "Social attachment theory states that individuals seek the proximity of attachment figures (e.g. family members, friends, colleagues, familiar places or objects) when faced with threat. During disasters, this means that family members may seek each other before evacuating, gather personal property before heading to familiar exits and places, or follow groups/crowds, etc. This hard-wired human tendency should be considered in the assessment of risk and the creation of disaster management plans. Doing so may result in more realistic evacuation procedures and may minimise the number of casualties and injuries. In this context, a dynamic spatio-temporal analysis of seismic risk is presented using SOLACE, a multi-agent model of pedestrian behaviour based on social attachment theory implemented using the Belief-Desire-Intention approach. The model focuses on the influence of human, social, physical and temporal factors on successful evacuation. Human factors considered include perception and mobility defined by age. Social factors are defined by attachment bonds, social groups, population distribution, and cultural norms. Physical factors refer to the location of the epicentre of the earthquake, spatial distribution/layout and attributes of environmental objects such as buildings, roads, barriers (cars), placement of safe areas, evacuation routes, and the resulting debris/damage from the earthquake. Experiments tested the influence of time of the day, presence of disabled persons and earthquake intensity. Initial results show that factors that influence arrivals in safe areas include (a) human factors (age, disability, speed), (b) pre-evacuation behaviours, (c) perception distance (social attachment, time of day), (d) social interaction during evacuation, and (e) physical and spatial aspects, such as limitations imposed by debris (damage), and the distance to safe areas. To validate the results, scenarios will be designed with stakeholders, who will also take part in the definition of a serious game. The recommendation of this research is that both social and physical aspects should be considered when defining vulnerability in the analysis of risk.", "prompt": "Social attachment theory states that individuals seek the proximity of attachment figures (e.g. family members, friends, colleagues, familiar places or objects) when faced with threat. During disasters, this means that family members may seek each other before evacuating, gather personal property before heading to familiar exits and places, or follow groups", "orig": "/crowds, etc. This hard-wired human tendency should be considered in the assessment of risk and the creation of disaster management plans. Doing so may result in more realistic evacuation procedures and may minimise the number of casualties and injuries. In this context, a dynamic spatio-temporal analysis of seismic risk is presented using", "generated": "Social attachment theory states that individuals seek the proximity of attachment figures (e.g. family members, friends, colleagues, familiar places or objects) when faced with threat. During disasters, this means that family members may seek each other before evacuating, gather personal property before heading to familiar exits and places, or follow group"}
{"full_text": "Context: The identification of long-gamma-ray-bursts (LGRBs) is still uncertain, although the collapsar engine of fast-rotating massive stars is gaining a strong consensus. Aims: We propose that low-metallicity Be and Oe stars, which are massive fast rotators, as potential LGRBs progenitors. Methods: We checked this hypothesis by 1) testing the global specific angular momentum of Oe/Be stars in the ZAMS with the SMC metallicity, 2) comparing the ZAMS ($\\Omega/\\Omega_{\\rm c},M/M_{\\odot}$) parameters of these stars with the area predicted theoretically for progenitors with metallicity $Z=0.002$, and 3) calculating the expected rate of LGRBs/year/galaxy and comparing them with the observed ones. To this end, we determined the ZAMS linear and angular rotational velocities for SMC Be and Oe stars using the observed vsini parameters, corrected from the underestimation induced by the gravitational darkening effect. Results: The angular velocities of SMC Oe/Be stars are on average $<\\Omega/\\Omega_{\\rm c}>=0.95$ in the ZAMS. These velocities are in the area theoretically predicted for the LGRBs progenitors. We estimated the yearly rate per galaxy of LGRBs and the number of LGRBs produced in the local Universe up to z=0.2. We have considered that the mass range of LGRB progenitors corresponds to stars hotter than spectral types B0-B1 and used individual beaming angles from 5 to 15\\degr. We thus obtain $R^{\\rm pred}_{\\rm LGRB}\\sim10^{-7}$ to $\\sim10^{-6}$ LGRBs/year/galaxy, which represents on average 2 to 14 LGRB predicted events in the local Universe during the past 11 years. The predicted rates could widely surpass the observed ones [(0.2-3)$\\times10^{-7}$ LGRBs/year/galaxy; 8 LGRBs observed in the local Universe during the last 11 years] if the stellar counts were made from the spectral type B1-B2, in accordance with the expected apparent spectral types of the appropriate massive fast rotators. Conclusion: We conclude that the massive Be/Oe stars with SMC metallicity could be LGRBs progenitors. Nevertheless, other SMC O/B stars without emission lines, which have high enough specific angular momentum, can enhance the predicted $R_{\\rm LGRB}$ rate.", "prompt": "Context: The identification of long-gamma-ray-bursts (LGRBs) is still uncertain, although the collapsar engine of fast-rotating massive stars is gaining a strong consensus. Aims: We propose that low-metallicity Be and Oe stars, which are massive fast rotators, as potential", "orig": "LGRBs progenitors. Methods: We checked this hypothesis by 1) testing the global specific angular momentum of Oe/Be stars in the ZAMS with the SMC metallicity, 2) comparing the ZAMS ($\\Omega/\\Omega_{\\rm c},M/M_{\\odot}$)", "generated": "The identification of long-gamma-ray-bursts (LGRBs) is still uncertain, although the collapsar engine of fast-rotating massive stars is gaining a strong consensus. Aims: We propose that low-metallicity Be and Oe stars, which are massive fast rotators, as potential  candidates"}
{"full_text": "The act and experience of programming is, at its heart, a fundamentally human activity that results in the production of artifacts. When considering programming, therefore, it would be a glaring omission to not involve people who specialize in studying artifacts and the human activity that yields them: archaeologists. Here we consider this with respect to computer games, the focus of archaeology's nascent subarea of archaeogaming. One type of archaeogaming research is digital excavation, a technical examination of the code and techniques used in old games' implementation. We apply that in a case study of Entombed, an Atari 2600 game released in 1982 by US Games. The player in this game is, appropriately, an archaeologist who must make their way through a zombie-infested maze. Maze generation is a fruitful area for comparative retrogame archaeology, because a number of early games on different platforms featured mazes, and their variety of approaches can be compared. The maze in Entombed is particularly interesting: it is shaped in part by the extensive real-time constraints of the Atari 2600 platform, and also had to be generated efficiently and use next to no memory. We reverse engineered key areas of the game's code to uncover its unusual maze-generation algorithm, which we have also built a reconstruction of, and analyzed the mysterious table that drives it. In addition, we discovered what appears to be a 35-year-old bug in the code, as well as direct evidence of code-reuse practices amongst game developers. What further makes this game's development interesting is that, in an era where video games were typically solo projects, a total of five people were involved in various ways with Entombed. We piece together some of the backstory of the game's development and intoxicant-fueled design using interviews to complement our technical work. Finally, we contextualize this example in archaeology and lay the groundwork for a broader interdisciplinary discussion about programming, one that includes both computer scientists and archaeologists.", "prompt": "The act and experience of programming is, at its heart, a fundamentally human activity that results in the production of artifacts. When considering programming, therefore, it would be a glaring omission to not involve people who specialize in studying artifacts and the human activity that yields them: archaeologists. Here we consider this with respect to", "orig": "computer games, the focus of archaeology's nascent subarea of archaeogaming. One type of archaeogaming research is digital excavation, a technical examination of the code and techniques used in old games' implementation. We apply that in a case study of Entombed, an Atari 2600 game released in", "generated": "The act and experience of programming is, at its heart, a fundamentally human activity that results in the production of artifacts. When considering programming, therefore, it would be a glaring omission to not involve people who specialize in studying artifacts and the human activity that yields them: archaeologists. Here we consider this with respect to"}
{"full_text": "This paper presents Skini, a programming methodology and an execution environment for interactive structured music. With this system, the composer programs his scores in the HipHop.js synchronous reactive language. They are then executed, or played, in live concerts, in interaction with the audience. The system aims at helping composers to find a good balance between the determinism of the compositions and the nondeterminism of the interactions with the public. Each execution of a Skini score yields to a different but aesthetically consistent interpretation. This work raises many questions in the musical fields. How to combine composition and interaction? How to control the musical style when the audience influences what is to play next? What are the possible connections with generative music? These are important questions for the Skini system but they are out of the scope of this paper that focuses exclusively on the computer science aspects of the system. From that perspective, the main questions are how to program the scores and in which language? General purpose languages are inappropriate because their elementary constructs (i.e., variables, functions, loops, etc.) do not match the constructions needed to express music and musical constraints. We show that synchronous programming languages are a much better fit because they rely on temporal constructs that can be directly used to represent musical scores and because their malleability enables composers to experiment easily with artistic variations of their initial scores. The paper mostly focuses on scores programming. It exposes the process a composer should follow from his very first musical intuitions up to the generation of a musical artifact. The paper presents some excerpts of the programming of a classical music composition that it then precisely relates to an actual recording. Examples of techno music and jazz are also presented, with audio artifact, to demonstrate the versatility of the system. Finally, brief presentations of past live concerts are presented as an evidence of viability of the system.", "prompt": "This paper presents Skini, a programming methodology and an execution environment for interactive structured music. With this system, the composer programs his scores in the HipHop.js synchronous reactive language. They are then executed, or played, in live concerts, in interaction with the audience. The system aims at helping composers to find a", "orig": "good balance between the determinism of the compositions and the nondeterminism of the interactions with the public. Each execution of a Skini score yields to a different but aesthetically consistent interpretation. This work raises many questions in the musical fields. How to combine composition and interaction? How to control the musical style when the", "generated": "This paper presents Skini, a programming methodology and an execution environment for interactive structured music. With this system, the composer programs his scores in the HipHop.js synchronous reactive language. They are then executed, or played, in live concerts, in interaction with the audience. The system aims at helping composers to find a"}
{"full_text": "Context. The availability of large bandwidth receivers for millimeter radio telescopes allows the acquisition of position-position-frequency data cubes over a wide field of view and a broad frequency coverage. These cubes contain much information on the physical, chemical, and kinematical properties of the emitting gas. However, their large size coupled with inhomogenous signal-to-noise ratio (SNR) are major challenges for consistent analysis and interpretation.Aims. We search for a denoising method of the low SNR regions of the studied data cubes that would allow to recover the low SNR emission without distorting the signals with high SNR.Methods. We perform an in-depth data analysis of the 13 CO and C 17 O (1 -- 0) data cubes obtained as part of the ORION-B large program performed at the IRAM 30m telescope. We analyse the statistical properties of the noise and the evolution of the correlation of the signal in a given frequency channel with that of the adjacent channels. This allows us to propose significant improvements of typical autoassociative neural networks, often used to denoise hyperspectral Earth remote sensing data. Applying this method to the 13 CO (1 -- 0) cube, we compare the denoised data with those derived with the multiple Gaussian fitting algorithm ROHSA, considered as the state of the art procedure for data line cubes.Results. The nature of astronomical spectral data cubes is distinct from that of the hyperspectral data usually studied in the Earth remote sensing literature because the observed intensities become statistically independent beyond a short channel separation. This lack of redundancy in data has led us to adapt the method, notably by taking into account the sparsity of the signal along the spectral axis. The application of the proposed algorithm leads to an increase of the SNR in voxels with weak signal, while preserving the spectral shape of the data in high SNR voxels.Conclusions. The proposed algorithm that combines a detailed analysis of the noise statistics with an innovative autoencoder architecture is a promising path to denoise radio-astronomy line data cubes. In the future, exploring whether a better use of the spatial correlations of the noise may further improve the denoising performances seems a promising avenue. In addition,", "prompt": "Context. The availability of large bandwidth receivers for millimeter radio telescopes allows the acquisition of position-position-frequency data cubes over a wide field of view and a broad frequency coverage. These cubes contain much information on the physical, chemical, and kinematical properties of the emitting gas. However, their large size coupled with", "orig": "inhomogenous signal-to-noise ratio (SNR) are major challenges for consistent analysis and interpretation.Aims. We search for a denoising method of the low SNR regions of the studied data cubes that would allow to recover the low SNR emission without distorting the signals with high SNR.Methods.", "generated": "Context. The availability of large bandwidth receivers for millimeter radio telescopes allows the acquisition of position-position-frequency data cubes over a wide field of view and a broad frequency coverage. These cubes contain much information on the physical, chemical, and kinematical properties of the emitting gas. However, their large size coupled with"}
{"full_text": "As the nearest known AGB star (d=64pc) and one of the brightest (mK-2), L2 Pup is a particularly interesting benchmark object to monitor the final stages of stellar evolution. We report new lucky imaging observations of this star with the VLT/NACO adaptive optics system in twelve narrow band filters covering the 1.0-4.0 microns wavelength range. These diffraction limited images reveal an extended circumstellar dust lane in front of the star, that exhibits a high opacity in the J band and becomes translucent in the H and K bands. In the L band, extended thermal emission from the dust is detected. We reproduce these observations using Monte-Carlo radiative transfer modeling of a dust disk with the RADMC-3D code. We also present new interferometric observations with the VLTI/VINCI and MIDI instruments. We measure in the K band an upper limit to the limb-darkened angular diameter of theta_LD = 17.9 +/- 1.6 mas, converting to a maximum linear radius of R = 123 +/- 14 Rsun. Considering the geometry of the extended K band emission in the NACO images, this upper limit is probably close to the actual angular diameter of the star. The position of L2 Pup in the Herzsprung-Russell diagram indicates that this star has a mass around 2 Msun and is probably experiencing an early stage of the asymptotic giant branch. We do not detect any stellar companion of L2 Pup in our adaptive optics and interferometric observations, and we attribute its apparent astrometric wobble in the Hipparcos data to variable lighting effects on its circumstellar material. We however do not exclude the presence of a binary companion, as the large loop structure extending to more than 10 AU to the North-East of the disk in our L band images may be the result of interaction between the stellar wind of L2 Pup and a hidden secondary object. The geometric configuration that we propose, with a large dust disk seen almost edge-on, appears particularly favorable to test and develop our understanding of the formation of bipolar nebulae.", "prompt": "As the nearest known AGB star (d=64pc) and one of the brightest (mK-2), L2 Pup is a particularly interesting benchmark object to monitor the final stages of stellar evolution. We report new lucky imaging observations of this star with the VLT/NACO adaptive optics system in", "orig": "twelve narrow band filters covering the 1.0-4.0 microns wavelength range. These diffraction limited images reveal an extended circumstellar dust lane in front of the star, that exhibits a high opacity in the J band and becomes translucent in the H and K bands. In the L band, extended thermal emission", "generated": "As the nearest known AGB star (d=64pc) and one of the brightest (mK-2), L2 Pup is a particularly interesting benchmark object to monitor the final stages of stellar evolution. We report new lucky imaging observations of this star with the VLT/NACO adaptive optics system in"}
{"full_text": "The connected system of Boltzman equations for the interacting system of electrons, positrons and photons in high external electric E and arbitrary magnetic H fields is solved. The consideration is made under the conditions of arbitrary heating and the mutual drag of carriers and photons. The non-stationary and non-uniform distribution function (DF) of photons for the all considered cases is obtained. It is shown that the DF of photons have the stationary limit for the drift velocities $(\\overset{\\to }\\to{u}\\overset{\\to}\\to{q}/\\hbar \\omega_q) <1$ and becomes exponentially grown by the time for the drift velocities $(\\overset{\\to}\\to{u}\\overset{\\to}\\to{q}/\\hbar \\omega_q) \\geq 1$. It is shown that the mutual drag of carriers and photons leads to the formation of ''quasi-particles''\\_ the ''electron dressed by photons'' and ''positron dressed by photons'', i.e. the mutual drag plays the role of the dressing mechanism of carriers and leads to the renormalization of the mass and frequency of photons. As a result of analyses of the phenomena of connected by the mutual drag system of carriers and photons we receive some fundamental results: a) the finiteness of the mass of photon (i.e. the rest mass of photons do not equal to zero); b) reality of takhyons as a quasi-particles with the real mass in amplifying system (or regime); c) identity of the mechanism of relativity and the Doppler effect which coincides with the renormalization of frequency or mass as a result of the mutual drag of carriers and photons at external field (force). These conclusions were received as a result of the fact that the relativistic factor enters to the expressions of the DF of photons and other physical expressions in the first order in the form $[ 1-(u^2/c^2) ] ^{-1}$, instead the $[ 1-(u^2/c^2) ] ^{-1/2}$ in Einstein theory. It is shown that the relativistic effect of deceleration of time really taking place for the relaxation time of carriers or for the life time of carriers, or for the period of electromagnetic oscillations. The latest is a direct result of the Doppler effect. Also it is shown that the velocity of light is an averaged velocity of photons at the ground state, as well as a velocity of sound for the phonons.", "prompt": "The connected system of Boltzman equations for the interacting system of electrons, positrons and photons in high external electric E and arbitrary magnetic H fields is solved. The consideration is made under the conditions of arbitrary heating and the mutual drag of carriers and photons. The non-stationary and non-uniform distribution function (DF", "orig": ") of photons for the all considered cases is obtained. It is shown that the DF of photons have the stationary limit for the drift velocities $(\\overset{\\to }\\to{u}\\overset{\\to}\\to{q}/\\hbar \\omega_q) <1$ and becomes exponentially grown by the time", "generated": "The connected system of Boltzman equations for the interacting system of electrons, positrons and photons in high external electric E and arbitrary magnetic H fields is solved. The consideration is made under the conditions of arbitrary heating and the mutual drag of carriers and photons. The non-stationary and non-uniform distribution function (DF"}
{"full_text": "Some implants have approximately a lifetime of 15 years. The femoral stem, for example, should be made of 316L/316LN stainless steel. Fretting corrosion, friction under small displacements, should occur during human gait, due to repeated loadings and un-loadings, between stainless steel and bone for instance. Some experimental investigations of fretting corrosion have been practiced. As well known, metallic alloys and especially stainless steels are covered with a passive film that prevents from the corrosion and degradation. This passive layer of few nanometers, at ambient temperature, is the key of our civilization according to some authors. This work is dedicated to predict the passive layer thicknesses of stainless steel under fretting corrosion with a specific emphasis on the role of proteins. The model is based on the Point Defect Model (micro scale) and an update of the model on the friction process (micro-macro scale). Genetic algorithm was used for finding solution of the problem. The major results are, as expected from experimental results, albumin prevents from degradation at the lowest concentration of chlorides; an incubation time is necessary for degrading the passive film; under fretting corrosion and high concentration of chlorides the passive behavior is annihilated. Les implants orthop\\'ediques de hanche ont une dur\\'ee de vie d'environ 15 ans. Par exemple, la tige f\\'emorale d'un tel implant peut \\^etre r\\'ealis\\'ee en acier inoxydable 316L ou 316LN. Le fretting corrosion, frottement sous petits d\\'eplacements, peut se produire pendant la marche humaine en raison des chargements r\\'ep\\'et\\'es entre le m\\'etal de la proth\\`ese et l'os. Plusieurs investigations exp\\'erimentales du fretting corrosion ont \\'et\\'e entreprises. Cette couche passive de quelques nanom\\`etres, \\`a temp\\'erature ambiante, est le point clef sur lequel repose le d\\'eveloppement de notre civilisation, selon certains auteurs. Ce travail vise \\`a pr\\'edire les \\'epaisseurs de cette couche passive de l'acier inoxydable soumis au fretting corrosion, avec une attention sp\\'ecifique sur le r\\^ole des prot\\'eines. Le mod\\`ele utilis\\'e est bas\\'e sur le Point Defect Model, PDM (\\`a une \\'echelle microscopique) et une am\\'elioration de ce mod\\`ele en prenant en compte le processus de frottement sous petits d\\'ebattements. L'algorithme g\\'en\\'etique a \\'et\\'e utilis\\'e pour optimiser la convergence du probl\\`eme. Les r\\'esultats les plus importants sont, comme d\\'emontr\\'e avec les essais exp\\'erimentaux, que l'albumine, la prot\\'eine \\'etudi\\'ee, emp\\^eche les d\\'egradations de l'acier inoxydable aux plus faibles concentrations d'ions chlorure ; ensuite, aux plus fortes concentrations de chlorures, un temps d'incubation est n\\'ecessaire pour d\\'etruire le film passif.", "prompt": "Some implants have approximately a lifetime of 15 years. The femoral stem, for example, should be made of 316L/316LN stainless steel. Fretting corrosion, friction under small displacements, should occur during human gait, due to repeated loadings and un-loadings, between stainless steel", "orig": "and bone for instance. Some experimental investigations of fretting corrosion have been practiced. As well known, metallic alloys and especially stainless steels are covered with a passive film that prevents from the corrosion and degradation. This passive layer of few nanometers, at ambient temperature, is the key of our civilization according to some authors.", "generated": "Some implants have approximately a lifetime of 15 years. The femoral stem, for example, should be made of 316L/316LN stainless steel. Fretting corrosion, friction under small displacements, should occur during human gait, due to repeated loadings and un-loadings, between stainless steel"}
{"full_text": "We consider the identification of heterogeneous linear elastic moduli in the context of time-harmonic elastodynamics. This inverse problem is formulated as the minimization of the modified error in constitutive equation (MECE), an energy-based cost functional defined as an weighted additive combination $\\mathcal{E}+\\kappa\\mathcal{D}$ of the error in constitutive equation (ECE) $\\mathcal{E}$, expressed using an energy seminorm, and a quadratic error term $\\mathcal{D}$ incorporating the kinematical measurements. MECE-based identification are known from existing computational evidence to enjoy attractive properties such as improved convexity, robustness to resonant frequencies, and tolerance to incompletely specified boundary conditions (BCs). The main goal of this work is to develop theoretical foundations, in a continuous setting, allowing to explain and justify some of the aforementioned beneficial properties, in particular addressing the general case where BCs may be underspecified. A specific feature of MECE formulations is that forward and adjoint solutions are governed by a fully coupled system, whose mathematical properties play a fundamental role in the qualitative and computational aspects of MECE minimization. We prove that this system has a unique and stable solution at any frequency, provided data is abundant enough (in a sense made precise therein) to at least compensate for any missing information on BCs. As a result, our formulation leads in such situations to a well-defined solution even though the relevant forward problem is not \\emph{a priori} clearly defined. This result has practical implications such as applicability of MECE to partial interior data (with important practical applications including ultrasound elastography), convergence of finite element discretizations and differentiability of the reduced MECE functional. In addition, we establish that usual least squares and pure ECE formulations are limiting cases of MECE formulations for small and large values of $\\kappa$, respectively. For the latter case, which corresponds to exact enforcement of kinematic data, we furthermore show that the reduced MECE Hessian is asymptotically positive for any parameter perturbation supported on the measurement region, thereby corroborating existing computational evidence on convexity improvement brought by MECE functionals. Finally, numerical studies that support and illustrate our theoretical findings, including a parameter reconstruction example using interior data, are presented.", "prompt": "We consider the identification of heterogeneous linear elastic moduli in the context of time-harmonic elastodynamics. This inverse problem is formulated as the minimization of the modified error in constitutive equation (MECE), an energy-based cost functional defined as an weighted additive combination $\\mathcal{E}+\\kappa\\", "orig": "mathcal{D}$ of the error in constitutive equation (ECE) $\\mathcal{E}$, expressed using an energy seminorm, and a quadratic error term $\\mathcal{D}$ incorporating the kinematical measurements. MECE-based identification are known from existing computational evidence to enjoy attractive properties such", "generated": "We consider the identification of heterogeneous linear elastic moduli in the context of time-harmonic elastodynamics. This inverse problem is formulated as the minimization of the modified error in constitutive equation (MECE), an energy-based cost functional defined as an weighted additive combination $\\mathcal{E}+\\kappa\\"}
{"full_text": "This article examines the importance of graphic representations in the social sciences, and particularly in (medieval) history, taking as its starting point a reflection by {\\'E}tienne-Jules Marey, a physiologist and pioneer of 19th-century photography and cinema. Marey believed that the visual should replace language in many fields. Indeed, the twentieth and early twenty-first centuries saw an exponential multiplication of visual media, particularly with the advent of digital technology. However, this ''graphics revolution'' has not affected all disciplines equally. Significant differences remain between scientific fields such as astrophysics, anthropology, chemistry and medieval history, despite their shared commitment to describing dynamic processes and changes of state. Yet, while historians have already digitized a large part of the cultural heritage from Antiquity to the 10th-13th centuries, exploration of this corpus using visualizations remains limited. There is therefore untapped potential in this field.This article begins by outlining a typology and quantification of the past and potential roles of visual representations in medieval history. It examines two distinct intellectual approaches: 1. the use of visuals to support a scientific discourse (majority) and 2. the construction of a historical discourse based on observations made from visual figures with the aim of modeling phenomena invisible to the naked eye. The author thus examines the use of ''images'' in medievalism, focusing on the annual volumes of the Soci{\\'e}t{\\'e} des historiens m{\\'e}di{\\'e}vistes de l'enseignement sup{\\'e}rieur (SHMESP), up to 2006. Two other parts of the text look at the still-rare forms of visual representation in medieval history, particularly those with a ''heuristic vocation'', using iconographic objects, parchments, buildings and digitized texts. The article suggests various visualization techniques, such as network analysis, the creation of ''stemmas 2.0'' and interactive chronologies, which could benefit the discipline. These methods could potentially profoundly change our understanding of ancient societies, by showing the dynamic relationships between different aspects of these societies. One of the most important advances expected from these visual methods is a better understanding of the patterns of development in medieval Europe, which varied from region to region. The hypothesis is that the scarcity of heuristic graphics in medieval history stems from the relationship with ancient documents and the historical method based on narration and exemplarity. The article thus questions the value of ''visual modelling'' in medieval history, and highlights the challenges associated with the widespread adoption of this approach in the humanities and social sciences. Finally, the text invites us to reflect on the nature and functioning of heuristic visual devices, by comparing medieval ''images'' and contemporary scientific visuals. In both cases, the point is to materialize the invisible in order to show something that exists beyond the visual. The author suggests that this way of approaching visuals could play a growing role in the decades to come, particularly in the field of data science.", "prompt": "This article examines the importance of graphic representations in the social sciences, and particularly in (medieval) history, taking as its starting point a reflection by {\\'E}tienne-Jules Marey, a physiologist and pioneer of 19th-century photography and cinema. Marey believed that the visual should", "orig": "replace language in many fields. Indeed, the twentieth and early twenty-first centuries saw an exponential multiplication of visual media, particularly with the advent of digital technology. However, this ''graphics revolution'' has not affected all disciplines equally. Significant differences remain between scientific fields such as astrophysics, anthropology, chemistry and medieval history,", "generated": "This article examines the importance of graphic representations in the social sciences, and particularly in (medieval) history, taking as its starting point a reflection by {\\'E}tienne-Jules Marey, a physiologist and pioneer of 19th-century photography and cinema. Marey believed that the visual should"}
{"full_text": "Surface magnetic fields have been detected in 5 to 10% of isolated massive stars, hosting outer radiative envelopes. They are often thought to have a fossil origin, resulting from the stellar formation phase. Yet, magnetic massive stars are scarcer in (close) short-period binaries, as reported by the BinaMIcS (Binarity and Magnetic Interaction in various classes of Stars) collaboration. Different physical conditions in the molecular clouds giving birth to isolated stars and binaries are commonly invoked. In addition, we propose that the observed lower magnetic incidence in close binaries may be due to nonlinear tides. Indeed, close binaries are probably prone to tidal instability, a fluid instability growing upon the equilibrium tidal flow via nonlinear effects. Yet, stratified effects have hitherto been largely overlooked. We theoretically and numerically investigate tidal instability in rapidly rotating, stably stratified fluids permeated by magnetic fields. We use the short-wavelength stability method to propose a comprehensive (local) theory of tidal instability at the linear onset, discussing damping effects. Then, we propose a mixing-length theory for the mixing generated by tidal instability in the nonlinear regime. We successfully assess our theoretical predictions against proof-of-concept, direct numerical simulations. Finally, we compare our predictions with the observations of short-period, double-lined spectroscopic binary systems. Using new analytical results, cross-validated by a direct integration of the stability equations, we show that tidal instability can be generated by nonlinear couplings of inertia-gravity waves with the equilibrium tidal flow in short-period massive binaries, even against the Joule diffusion. In the nonlinear regime, a fossil magnetic field can be dissipated by the turbulent magnetic diffusion induced by the saturated tidal flows. We predict that the turbulent Joule diffusion of fossil fields would occur in a few million years for several short-period massive binaries. Therefore, turbulent tidal flows could explain the observed dearth of some short-period magnetic binaries.", "prompt": "Surface magnetic fields have been detected in 5 to 10% of isolated massive stars, hosting outer radiative envelopes. They are often thought to have a fossil origin, resulting from the stellar formation phase. Yet, magnetic massive stars are scarcer in (close) short-period binaries, as reported by the Bina", "orig": "MIcS (Binarity and Magnetic Interaction in various classes of Stars) collaboration. Different physical conditions in the molecular clouds giving birth to isolated stars and binaries are commonly invoked. In addition, we propose that the observed lower magnetic incidence in close binaries may be due to nonlinear tides. Indeed, close binaries are probably prone", "generated": "Surface magnetic fields have been detected in 5 to 10% of isolated massive stars, hosting outer radiative envelopes. They are often thought to have a fossil origin, resulting from the stellar formation phase. Yet, magnetic massive stars are scarcer in (close) short-period binaries, as reported by the Bina"}
{"full_text": "Context: Generic programming, as defined by Stepanov, is a methodology for writing efficient and reusable algorithms by considering only the required properties of their underlying data types and operations. Generic programming has proven to be an effective means of constructing libraries of reusable software components in languages that support it. Generics-related language design choices play a major role in how conducive generic programming is in practice. Inquiry: Several mainstream programming languages (e.g. Java and C++) were first created without generics; features to support generic programming were added later, gradually. Much of the existing literature on supporting generic programming focuses thus on retrofitting generic programming into existing languages and identifying related implementation challenges. Is the programming experience significantly better, or different when programming with a language designed for generic programming without limitations from prior language design choices? Approach: We examine Magnolia, a language designed to embody generic programming. Magnolia is representative of an approach to language design rooted in algebraic specifications. We repeat a well-known experiment, where we put Magnolia's generic programming facilities under scrutiny by implementing a subset of the Boost Graph Library, and reflect on our development experience. Knowledge: We discover that the idioms identified as key features for supporting Stepanov-style generic programming in the previous studies and work on the topic do not tell a full story. We clarify which of them are more of a means to an end, rather than fundamental features for supporting generic programming. Based on the development experience with Magnolia, we identify variadics as an additional key feature for generic programming and point out limitations and challenges of genericity by property. Grounding: Our work uses a well-known framework for evaluating the generic programming facilities of a language from the literature to evaluate the algebraic approach through Magnolia, and we draw comparisons with well-known programming languages. Importance: This work gives a fresh perspective on generic programming, and clarifies what are fundamental language properties and their trade-offs when considering supporting Stepanov-style generic programming. The understanding of how to set the ground for generic programming will inform future language design.", "prompt": "Context: Generic programming, as defined by Stepanov, is a methodology for writing efficient and reusable algorithms by considering only the required properties of their underlying data types and operations. Generic programming has proven to be an effective means of constructing libraries of reusable software components in languages that support it. Generics-related language design choices", "orig": "play a major role in how conducive generic programming is in practice. Inquiry: Several mainstream programming languages (e.g. Java and C++) were first created without generics; features to support generic programming were added later, gradually. Much of the existing literature on supporting generic programming focuses thus on retrofitting generic programming into existing languages and", "generated": "Generic programming, as defined by Stepanov, is a methodology for writing efficient and reusable algorithms by considering only the required properties of their underlying data types and operations. Generic programming has proven to be an effective means of constructing libraries of reusable software components in languages that support it.\n\nGenerics-related language design choices typically involve"}
{"full_text": "Scientific progress increasingly depends on data management, particularly to clean and curate data so that it can be systematically analyzed and reused. A wealth of techniques for managing and curating data (and its provenance) have been proposed, largely in the database community. In particular, a number of influential papers have proposed collecting provenance information explaining where a piece of data was copied from, or what other records were used to derive it. Most of these techniques, however, exist only as research prototypes and are not available in mainstream database systems. This means scientists must either implement such techniques themselves or (all too often) go without. This is essentially a code reuse problem: provenance techniques currently cannot be implemented reusably, only as ad hoc, usually unmaintained extensions to standard databases. An alternative, relatively unexplored approach is to support such techniques at a higher abstraction level, using metaprogramming or reflection techniques. Can advanced programming techniques make it easier to transfer provenance research results into practice? We build on a recent approach called language-integrated provenance, which extends language-integrated query techniques with source-to-source query translations that record provenance. In previous work, a proof of concept was developed in a research programming language called Links, which supports sophisticated Web and database programming. In this paper, we show how to adapt this approach to work in Haskell building on top of the Database-Supported Haskell (DSH) library. Even though it seemed clear in principle that Haskell's rich programming features ought to be sufficient, implementing language-integrated provenance in Haskell required overcoming a number of technical challenges due to interactions between these capabilities. Our implementation serves as a proof of concept showing how this combination of metaprogramming features can, for the first time, make data provenance facilities available to programmers as a library in a widely-used, general-purpose language. In our work we were successful in implementing forms of provenance known as where-provenance and lineage. We have tested our implementation using a simple database and query set and established that the resulting queries are executed correctly on the database. Our implementation is publicly available on GitHub. Our work makes provenance tracking available to users of DSH at little cost. Although Haskell is not widely used for scientific database development, our work suggests which languages features are necessary to support provenance as library. We also highlight how combining Haskell's advanced type programming features can lead to unexpected complications, which may motivate further research into type system expressiveness.", "prompt": "Scientific progress increasingly depends on data management, particularly to clean and curate data so that it can be systematically analyzed and reused. A wealth of techniques for managing and curating data (and its provenance) have been proposed, largely in the database community. In particular, a number of influential papers have proposed collecting", "orig": "provenance information explaining where a piece of data was copied from, or what other records were used to derive it. Most of these techniques, however, exist only as research prototypes and are not available in mainstream database systems. This means scientists must either implement such techniques themselves or (all too often) go without. This is", "generated": "Scientific progress increasingly depends on data management, particularly to clean and curate data so that it can be systematically analyzed and reused. A wealth of techniques for managing and curating data (and its provenance) have been proposed, largely in the database community. In particular, a number of influential papers have proposed collecting"}
{"full_text": "Robust radiative transfer techniques are requisite for efficiently extracting the physical and chemical information from molecular rotational lines.We study several hypotheses that enable robust estimations of the column densities and physical conditions when fitting one or two transitions per molecular species. We study the extent to which simplifying assumptions aimed at reducing the complexity of the problem introduce estimation biases and how to detect them.We focus on the CO and HCO+ isotopologues and analyze maps of a 50 square arcminutes field. We used the RADEX escape probability model to solve the statistical equilibrium equations and compute the emerging line profiles, assuming that all species coexist. Depending on the considered set of species, we also fixed the abundance ratio between some species and explored different values. We proposed a maximum likelihood estimator to infer the physical conditions and considered the effect of both the thermal noise and calibration uncertainty. We analyzed any potential biases induced by model misspecifications by comparing the results on the actual data for several sets of species and confirmed with Monte Carlo simulations. The variance of the estimations and the efficiency of the estimator were studied based on the Cram{\\'e}r-Rao lower bound.Column densities can be estimated with 30% accuracy, while the best estimations of the volume density are found to be within a factor of two. Under the chosen model framework, the peak 12CO(1--0) is useful for constraining the kinetic temperature. The thermal pressure is better and more robustly estimated than the volume density and kinetic temperature separately. Analyzing CO and HCO+ isotopologues and fitting the full line profile are recommended practices with respect to detecting possible biases.Combining a non-local thermodynamic equilibrium model with a rigorous analysis of the accuracy allows us to obtain an efficient estimator and identify where the model is misspecified. We note that other combinations of molecular lines could be studied in the future.", "prompt": "Robust radiative transfer techniques are requisite for efficiently extracting the physical and chemical information from molecular rotational lines.We study several hypotheses that enable robust estimations of the column densities and physical conditions when fitting one or two transitions per molecular species. We study the extent to which simplifying assumptions aimed at reducing the complexity of the problem", "orig": "introduce estimation biases and how to detect them.We focus on the CO and HCO+ isotopologues and analyze maps of a 50 square arcminutes field. We used the RADEX escape probability model to solve the statistical equilibrium equations and compute the emerging line profiles, assuming that all species coexist. Depending on the", "generated": "Robust radiative transfer techniques are requisite for efficiently extracting the physical and chemical information from molecular rotational lines. We study several hypotheses that enable robust estimations of the column densities and physical conditions when fitting one or two transitions per molecular species. We study the extent to which simplifying assumptions aimed at reducing the complexity of the"}
{"full_text": "Decay modes of excited nuclei are investigated in $^{78,82}$Kr + $^{40}$Ca reactions at 5.5 MeV/nucleon. Charged products were measured by means of the $4\\pi$ INDRA array. Kinetic-energy spectra and angular distributions of fragments with atomic number 3 $\\le Z \\le$ 28 indicate a high degree of relaxation and are compatible with a fission-like phenomenon. Persistence of structure effects is evidenced from elemental cross-sections ($\\sigma_{Z}$) as well as a strong odd-even-staggering (o-e-s) of the light-fragment yields. The magnitude of the staggering does not significantly depend on the neutron content of the emitting system. Fragment-particle coincidences suggest that the light partners in very asymmetric fission are emitted either cold or at excitation energies below the particle emission thresholds. The evaporation residue cross-section of the $^{78}$Kr + $^{40}$Ca reaction is slightly higher than the one measured in $^{82}$Kr + $^{40}$Ca reaction. The fission-like component is larger by $\\sim$ 25% for the reaction having the lowest neutron-to-proton ratio. These experimental features are confronted to the predictions of theoretical models. The Hauser-Feshbach approach including the emission of fragments up to $Z$ = 14 in their ground states as well as excited states does not account for the main features of $\\sigma_{Z}$. For both reactions, the transition-state formalism reasonably reproduces the $Z$-distribution of the fragments with charge 12 $\\le Z \\le$ 28. However, this model strongly overestimates the light-fragment cross-sections and does not explain the o-e-s of the yields for 6 $\\le Z \\le$ 10. The shape of the whole $Z$-distribution and the o-e-s of the light-fragment yields are satisfactorily reproduced within the dinuclear system framework which treats the competition between evaporation, fusion-fission and quasifission processes. The model suggests that heavy fragments come mainly from quasifission while light fragments are predominantly populated by fusion. An underestimation of the cross sections for 16 $\\le Z \\le$ 22 could signal a mechanism in addition to the capture process.", "prompt": "Decay modes of excited nuclei are investigated in $^{78,82}$Kr + $^{40}$Ca reactions at 5.5 MeV/nucleon. Charged products were measured by means of the $4\\pi$ INDRA array. Kinetic-energy spectra and angular distributions of fragments with atomic", "orig": "number 3 $\\le Z \\le$ 28 indicate a high degree of relaxation and are compatible with a fission-like phenomenon. Persistence of structure effects is evidenced from elemental cross-sections ($\\sigma_{Z}$) as well as a strong odd-even-staggering (o-e-s) of the light-", "generated": "Decay modes of excited nuclei are investigated in $^{78,82}$Kr + $^{40}$Ca reactions at 5.5 MeV/nucleon. Charged products were measured by means of the $4\\pi$ INDRA array. Kinetic-energy spectra and angular distributions of fragments with atomic"}
{"full_text": "We consider the high frequency Helmholtz equation with a variable refraction index $n^2(x)$ ($x \\in \\R^d$), supplemented with a given high frequency source term supported near the origin $x=0$. A small absorption parameter $\\alpha_{\\varepsilon}>0$ is added, which somehow prescribes a radiation condition at infinity for the considered Helmholtz equation. The semi-classical parameter is $\\varepsilon>0$. We let $\\eps$ and $\\a_\\eps$ go to zero {\\em simultaneaously}. We study the question whether the indirectly prescribed radiation condition at infinity is satisfied {\\em uniformly} along the asymptotic process $\\eps \\to 0$, or, in other words, whether the conveniently rescaled solution to the considered equation goes to the {\\em outgoing} solution to the natural limiting Helmholtz equation. This question has been previously studied by the first autor. It is proved that the radiation condition is indeed satisfied uniformly in $\\eps$, provided the refraction index satisfies a specific {\\em non-refocusing condition}, a condition that is first pointed out in this reference. The non-refocusing condition requires, in essence, that the rays of geometric optics naturally associated with the high-frequency Helmholtz operator, and that are sent from the origin $x=0$ at time $t=0$, should not refocus at some later time $t>0$ near the origin again. In the present text we show the {\\em optimality} of the above mentionned non-refocusing condition, in the following sense. We exhibit a refraction index which {\\em does} refocus the rays of geometric optics sent from the origin near the origin again, and, on the other hand, we completely compute the asymptotic behaviour of the solution to the associated Helmholtz equation: we show that the limiting solution {\\em does not} satisfy the natural radiation condition at infinity. More precisely, we show that the limiting solution is a {\\em perturbation} of the outgoing solution to the natural limiting Helmholtz equation, and that the perturbing term explicitly involves the contribution of the rays radiated from the origin which go back to the origin. This term is also conveniently modulated by a phase factor, which turns out to be the action along the above rays of the hamiltonian associated with the semiclassical Helmholtz equation.", "prompt": "We consider the high frequency Helmholtz equation with a variable refraction index $n^2(x)$ ($x \\in \\R^d$), supplemented with a given high frequency source term supported near the origin $x=0$. A small absorption parameter $\\alpha_{\\varepsilon}>0$", "orig": "is added, which somehow prescribes a radiation condition at infinity for the considered Helmholtz equation. The semi-classical parameter is $\\varepsilon>0$. We let $\\eps$ and $\\a_\\eps$ go to zero {\\em simultaneaously}. We study the question whether the indirectly prescribed radiation", "generated": "We consider the high frequency Helmholtz equation with a variable refraction index $n^2(x)$ ($x \\in \\R^d$), supplemented with a given high frequency source term supported near the origin $x=0$. A small absorption parameter $\\alpha_{\\varepsilon}>0$"}
{"full_text": "During the summer of 1989, the Forecast Systems Laboratory of the National Oceanic and Atmospheric Administration sponsored an evaluation of artificial intelligence-based systems that forecast severe convective storms. The evaluation experiment, called Shootout-89, took place in Boulder, and focussed on storms over the northeastern Colorado foothills and plains (Moninger, et al., 1990). Six systems participated in Shootout-89. These included traditional expert systems, an analogy-based system, and a system developed using methods from the cognitive science/judgment analysis tradition. Each day of the exercise, the systems generated 2 to 9 hour forecasts of the probabilities of occurrence of: non significant weather, significant weather, and severe weather, in each of four regions in northeastern Colorado. A verification coordinator working at the Denver Weather Service Forecast Office gathered ground-truth data from a network of observers. Systems were evaluated on the basis of several measures of forecast skill, and on other metrics such as timeliness, ease of learning, and ease of use. Systems were generally easy to operate, however the various systems required substantially different levels of meteorological expertise on the part of their users--reflecting the various operational environments for which the systems had been designed. Systems varied in their statistical behavior, but on this difficult forecast problem, the systems generally showed a skill approximately equal to that of persistence forecasts and climatological (historical frequency) forecasts. The two systems that appeared best able to discriminate significant from non significant weather events were traditional expert systems. Both of these systems required the operator to make relatively sophisticated meteorological judgments. We are unable, based on only one summer's worth of data, to determine the extent to which the greater skill of the two systems was due to the content of their knowledge bases, or to the subjective judgments of the operator. A follow-on experiment, Shootout-91, is currently being planned. Interested potential participants are encouraged to contact the author at the address above.", "prompt": "During the summer of 1989, the Forecast Systems Laboratory of the National Oceanic and Atmospheric Administration sponsored an evaluation of artificial intelligence-based systems that forecast severe convective storms. The evaluation experiment, called Shootout-89, took place in Boulder, and focussed on storms over the northeastern Colorado foothills and plains", "orig": "(Moninger, et al., 1990). Six systems participated in Shootout-89. These included traditional expert systems, an analogy-based system, and a system developed using methods from the cognitive science/judgment analysis tradition. Each day of the exercise, the systems generated 2 to 9 hour forecasts of", "generated": "During the summer of 1989, the Forecast Systems Laboratory of the National Oceanic and Atmospheric Administration sponsored an evaluation of artificial intelligence-based systems that forecast severe convective storms. The evaluation experiment, called Shootout-89, took place in Boulder, and focused on storms over the northeastern Colorado foothills and plains."}
{"full_text": "Background: Recent studies reported postural balance disorders in patients and soccer players with groin pain (GP) compared to controls. Since postural balance asymmetry identified after an initial injury contributes for subsequent injuries, identification of this asymmetry in soccer players with GP may highlight the risk of sustaining subsequent noncontact lower extremity musculoskeletal injuries in these players. Therefore, the aims of this study were to (i) examine static and dynamic unipedal postural balance asymmetry in soccer players with GP compared to healthy ones, and (ii) quantify the risk of subsequent noncontact lower extremity injuries in these players. Research question: Do soccer players with GP exhibit higher static and dynamic unipedal postural balance asymmetry, and higher risk of sustaining subsequent injuries compared to controls. Methods: In this prospective case control study, 27 soccer players with non-time loss GP (GP group: GPG), and 27 healthy ones (control group: CG) were enrolled. Static and dynamic unipedal postural balance asymmetry were evaluated with a force platform using symmetry index (SI), and Y-balance test (Y-BT), respectively. Additionally, subsequent noncontact lower extremity musculoskeletal injuries were tracked for 10 months. Results: The GPG revealed higher (p < 0.01) SI in eyes closed condition, higher (p < 0.001) side-to-side asymmetry in anterior, posteromedial and posterolateral reach distances and in composite Y-BT score compared to CG. They showed lower (p < 0.001) composite score for injured limb and higher (p < 0.001) side-to-side asymmetry in posteromedial reach distance compared to the cutoff values of 89.6 % and 4 cm, respectively. Moreover, GPG exhibited higher odds (OR= 7.48; 95 % CI = 2.15, 26.00; p < 0.01) of sustaining subsequent injuries compared to CG. Significance: The Y-BT should be instituted into existing pre-participation physical examinations to screen for soccer players with non-time loss GP at an elevated risk of sustaining subsequent injuries. This could help coaches and clinicians make valid return to play decisions.", "prompt": "Background: Recent studies reported postural balance disorders in patients and soccer players with groin pain (GP) compared to controls. Since postural balance asymmetry identified after an initial injury contributes for subsequent injuries, identification of this asymmetry in soccer players with GP may highlight the risk of sustaining subsequent noncontact lower extremity mus", "orig": "culoskeletal injuries in these players. Therefore, the aims of this study were to (i) examine static and dynamic unipedal postural balance asymmetry in soccer players with GP compared to healthy ones, and (ii) quantify the risk of subsequent noncontact lower extremity injuries in these players. Research question: Do", "generated": "Background: Recent studies reported postural balance disorders in patients and soccer players with groin pain (GP) compared to controls. Since postural balance asymmetry identified after an initial injury contributes for subsequent injuries, identification of this asymmetry in soccer players with GP may highlight the risk of sustaining subsequent noncontact lower extremity muscle"}
{"full_text": "Fermi can measure the spectral properties of gamma-ray bursts over a very large energy range and is opening a new window on the prompt emission of these energetic events. Localizations by the instruments on Fermi in combination with follow-up by Swift provide accurate positions for observations at longer wavelengths leading to the determination of redshifts, the true energy budget, host galaxy properties and facilitate comparison with pre-Fermi bursts. Multi-wavelength follow-up observations were performed on the afterglows of four bursts with high energy emission detected by Fermi/LAT : GRB090323, GRB090328, GRB090510 and GRB090902B. They were obtained in the optical/near-infrared bands with GROND mounted at the MPG/ESO 2.2m telescope and additionally of GRB090323 in the optical with the 2 m telescope in Tautenburg, Germany. Three of the events are classified as long bursts while GRB090510 is a well localized short GRB with GeV emission. In addition, host galaxies were detected for three of the four bursts. Spectroscopic follow-up was initiated with the VLT for GRB090328 and GRB090510. The afterglow observations in 7 bands are presented for all bursts and their host galaxies are investigated. Knowledge of the distance and the local dust extinction enables comparison of the afterglows of LAT-detected GRBs with the general sample. The spectroscopic redshifts of GRB090328 and GRB090510 were determined to be z=0.7354+/-0.0003 and z=0.903 +/- 0.001 and dust corrected star-formation rates of 4.8 Mdot yr^-1 and 0.60 M_dot yr^-1 were derived for their host galaxies, respectively. The afterglows of long bursts exhibit power-law decay indices alpha from less than 1 to ~2.3 and spectral indices (beta) values from 0.65 to ~1.2 which are fairly standard for GRB afterglows. Constraints are placed on the jet half opening angles of less than 2.1 deg to greater than 6.4 deg which allows limits to be placed on the beaming corrected energies. These range from less than 5x10^50 erg to the one of the highest values ever recorded, greater than 2.2x10^52 erg for GRB090902B, and are not consistent with a standard candle. The extremely energetic long Fermi bursts have optical afterglows which lie in the top half of the brightness distribution of all optical afterglows detected in the Swift era or even in the top 5% if incompleteness is considered. The properties of the host galaxies of these LAT detected bursts in terms of extinction, star formation rates and masses do not appear to differ from previous samples.", "prompt": "Fermi can measure the spectral properties of gamma-ray bursts over a very large energy range and is opening a new window on the prompt emission of these energetic events. Localizations by the instruments on Fermi in combination with follow-up by Swift provide accurate positions for observations at longer wavelengths leading to the determination of redshift", "orig": "s, the true energy budget, host galaxy properties and facilitate comparison with pre-Fermi bursts. Multi-wavelength follow-up observations were performed on the afterglows of four bursts with high energy emission detected by Fermi/LAT : GRB090323, GRB090328, GRB090510 and GRB", "generated": "Fermi can measure the spectral properties of gamma-ray bursts over a very large energy range and is opening a new window on the prompt emission of these energetic events. Localizations by the instruments on Fermi in combination with follow-up by Swift provide accurate positions for observations at longer wavelengths leading to the determination of redshift"}
{"full_text": "This paper demonstrates the usefulness of distributed local verification of proofs, as a tool for the design of self-stabilizing algorithms.In particular, it introduces a somewhat generalized notion of distributed local proofs, and utilizes it for improving the time complexity significantly, while maintaining space optimality. As a result, we show that optimizing the memory size carries at most a small cost in terms of time, in the context of Minimum Spanning Tree (MST). That is, we present algorithms that are both time and space efficient for both constructing an MST and for verifying it.This involves several parts that may be considered contributions in themselves.First, we generalize the notion of local proofs, trading off the time complexity for memory efficiency. This adds a dimension to the study of distributed local proofs, which has been gaining attention recently. Specifically, we design a (self-stabilizing) proof labeling scheme which is memory optimal (i.e., $O(\\log n)$ bits per node), and whose time complexity is $O(\\log ^2 n)$ in synchronous networks, or $O(\\Delta \\log ^3 n)$ time in asynchronous ones, where $\\Delta$ is the maximum degree of nodes. This answers an open problem posed by Awerbuch and Varghese (FOCS 1991). We also show that $\\Omega(\\log n)$ time is necessary, even in synchronous networks. Another property is that if $f$ faults occurred, then, within the requireddetection time above, they are detected by some node in the $O(f\\log n)$ locality of each of the faults.Second, we show how to enhance a known transformer that makes input/output algorithms self-stabilizing. It now takes as input an efficient construction algorithm and an efficient self-stabilizing proof labeling scheme, and produces an efficient self-stabilizing algorithm. When used for MST, the transformer produces a memory optimal self-stabilizing algorithm, whose time complexity, namely, $O(n)$, is significantly better even than that of previous algorithms. (The time complexity of previous MST algorithms that used $\\Omega(\\log^2 n)$ memory bits per node was $O(n^2)$, and the time for optimal space algorithms was $O(n|E|)$.) Inherited from our proof labelling scheme, our self-stabilising MST construction algorithm also has the following two properties: (1) if faults occur after the construction ended, then they are detected by some nodes within $O(\\log ^2 n)$ time in synchronous networks, or within $O(\\Delta \\log ^3 n)$ time in asynchronous ones, and (2) if $f$ faults occurred, then, within the required detection time above, they are detected within the $O(f\\log n)$ locality of each of the faults. We also show how to improve the above two properties, at the expense of some increase in the memory.", "prompt": "This paper demonstrates the usefulness of distributed local verification of proofs, as a tool for the design of self-stabilizing algorithms.In particular, it introduces a somewhat generalized notion of distributed local proofs, and utilizes it for improving the time complexity significantly, while maintaining space optimality. As a result, we show that optimizing the", "orig": "memory size carries at most a small cost in terms of time, in the context of Minimum Spanning Tree (MST). That is, we present algorithms that are both time and space efficient for both constructing an MST and for verifying it.This involves several parts that may be considered contributions in themselves.First, we generalize the notion", "generated": "This paper demonstrates the usefulness of distributed local verification of proofs, as a tool for the design of self-stabilizing algorithms. In particular, it introduces a somewhat generalized notion of distributed local proofs, and utilizes it for improving the time complexity significantly, while maintaining space optimality. As a result, we show that optimizing"}
{"full_text": "Context. An extension method is a method declared in a package other than the package of its host class. Thanks to extension methods, developers can adapt to their needs classes they do not own: adding methods to core classes is a typical use case. This is particularly useful for adapting software and therefore to increase reusability. Inquiry. In most dynamically-typed languages, extension methods are globally visible. Because any developer can define extension methods for any class, naming conflicts occur: if two developers define an extension method with the same signature in the same class, only one extension method is visible and overwrites the other. Similarly, if two developers each define an extension method with the same name in a class hierarchy, one overrides the other. To avoid such \"accidental overrides\", some dynamically-typed languages limit the visibility of an extension method to a particular scope. However, their semantics have not been fully described and compared. In addition, these solutions typically rely on a dedicated and slow method lookup algorithm to resolve conflicts at runtime. Approach. In this article, we present a formalization of the underlying models of Ruby refinements, Groovy categories, Classboxes, and Method Shelters that are scoping extension method solutions in dynamically-typed languages. Knowledge. Our formal framework allows us to objectively compare and analyze the shortcomings of the studied solutions and other different approaches such as MultiJava. In addition, language designers can use our formal framework to determine which mechanism has less risk of \"accidental overrides\". Grounding. Our comparison and analysis of existing solutions is grounded because it is based on denotational semantics formalizations. Importance. Extension methods are widely used in programming languages that support them, especially dynamically-typed languages such as Pharo, Ruby or Python. However, without a carefully designed mechanism, this feature can cause insidious hidden bugs or can be voluntarily used to gain access to protected operations, violate encapsulation or break fundamental invariants.", "prompt": "Context. An extension method is a method declared in a package other than the package of its host class. Thanks to extension methods, developers can adapt to their needs classes they do not own: adding methods to core classes is a typical use case. This is particularly useful for adapting software and therefore to increase reusability", "orig": ". Inquiry. In most dynamically-typed languages, extension methods are globally visible. Because any developer can define extension methods for any class, naming conflicts occur: if two developers define an extension method with the same signature in the same class, only one extension method is visible and overwrites the other. Similarly, if two developers", "generated": "Context. An extension method is a method declared in a package other than the package of its host class. Thanks to extension methods, developers can adapt to their needs classes they do not own: adding methods to core classes is a typical use case. This is particularly useful for adapting software and therefore to increase reusability"}
{"full_text": "We establish weak well-posedness for critical symmetric stable driven SDEs in R d with additive noise Z, d $\\ge$ 1. Namely, we study the case where the stable index of the driving process Z is $\\alpha$ = 1 which exactly corresponds to the order of the drift term having the coefficient b which is continuous and bounded. In particular, we cover the cylindrical case when Zt = (Z 1 t ,. .. , Z d t) and Z 1 ,. .. , Z d are independent one dimensional Cauchy processes. Our approach relies on L p-estimates for stable operators and uses perturbative arguments. 1. Statement of the problem and main results We are interested in proving well-posedness for the martingale problem associated with the following SDE: (1.1) X t = x + t 0 b(X s)ds + Z t , where (Z s) s$\\ge$0 stands for a symmetric d-dimensional stable process of order $\\alpha$ = 1 defined on some filtered probability space ($\\Omega$, F, (F t) t$\\ge$0 , P) (cf. [2] and the references therein) under the sole assumptions of continuity and boundedness on the vector valued coefficient b: (C) The drift b : R d $\\rightarrow$ R d is continuous and bounded. 1 Above, the generator L of Z writes: L$\\Phi$(x) = p.v. R d \\{0} [$\\Phi$(x + z) -- $\\Phi$(x)]$\\nu$(dz), x $\\in$ R d , $\\Phi$ $\\in$ C 2 b (R d), $\\nu$(dz) = d$\\rho$ $\\rho$ 2$\\mu$ (d$\\theta$), z = $\\rho$$\\theta$, ($\\rho$, $\\theta$) $\\in$ R * + x S d--1. (1.2) (here $\\times$, $\\times$ (or $\\times$) and | $\\times$ | denote respectively the inner product and the norm in R d). In the above equation, $\\nu$ is the L{\\'e}vy intensity measure of Z, S d--1 is the unit sphere of R d and$\\mu$ is a spherical measure on S d--1. It is well know, see e.g. [20] that the L{\\'e}vy exponent $\\Phi$ of Z writes as: (1.3) $\\Phi$($\\lambda$) = E[exp(i $\\lambda$, Z 1)] = exp -- S d--1 | $\\lambda$, $\\theta$ |$\\mu$(d$\\theta$) , $\\lambda$ $\\in$ R d , where $\\mu$ = c 1$\\mu$ , for a positive constant c 1 , is the so-called spectral measure of Z. We will assume some non-degeneracy conditions on $\\mu$. Namely we introduce assumption (ND) There exists $\\kappa$ $\\ge$ 1 s.t. (1.4) $\\forall$$\\lambda$ $\\in$ R d , $\\kappa$ --1 |$\\lambda$| $\\le$ S d--1 | $\\lambda$, $\\theta$ |$\\mu$(d$\\theta$) $\\le$ $\\kappa$|$\\lambda$|. 1 The boundedness of b is here assumed for technical simplicity. Our methodology could apply, up to suitable localization arguments, to a drift b having linear growth.", "prompt": "We establish weak well-posedness for critical symmetric stable driven SDEs in R d with additive noise Z, d $\\ge$ 1. Namely, we study the case where the stable index of the driving process Z is $\\alpha$ = 1 which exactly corresponds to the order of the drift term", "orig": "having the coefficient b which is continuous and bounded. In particular, we cover the cylindrical case when Zt = (Z 1 t,..., Z d t) and Z 1,..., Z d are independent one dimensional Cauchy processes. Our approach relies on L p-estimates for stable operators", "generated": "We establish weak well-posedness for critical symmetric stable driven SDEs in R^d with additive noise Z, d $\\ge$ 1. Namely, we study the case where the stable index of the driving process Z is $\\alpha$ = 1 which exactly corresponds to the order of the drift"}
{"full_text": "Aims. The present study aims at providing a deeper insight into the power and limitation of an unsupervised classification algorithm (called Fisher-EM) on spectra of galaxies. This algorithm uses a Gaussian mixture in a discriminative latent subspace. To this end, we investigate the capacity of this algorithm to segregate the physical parameters used to generate mock spectra and the influence of the noise on the classification. Methods. With the code CIGALE and different values for nine input parameters characterising the stellar population, we have simulated a sample of 11 475 optical spectra of galaxies containing 496 monochromatic fluxes. The statistical model and the optimum number of clusters is given in Fisher-EM by the integrated completed likelihood (ICL) criterion. We repeated the analyses several times to assess the robustness of the results. Results. Two distinct classifications can be distinguished in the case of the noiseless spectra. The one above 13 clusters disappears when noise is added, while the classification with 12 clusters is very robust against noise down to a signal to noise ratio (SNR) of 3. At SNR=1, the optimum is 5 clusters, but the classification is still compatible with the previous one. The distribution of the parameters used for the simulation shows an excellent discrimination between classes. A higher dispersion both in the spectra within each class and in the parameter distribution, leads us to conclude that despite a much higher ICL, the classification with more than 13 clusters in the noiseless case is not physically relevant. Conclusions. This study yields two conclusions valid at least for the Fisher-EM algorithm. Firstly, the unsupervised classification of spectra of galaxies is both reliable and robust to noise. Secondly, such analyses are able to extract the useful physical information contained in the spectra and to build highly meaningful classifications. In an epoch of data-driven astrophysics, it is important to trust unsupervised machine learning approaches that do not require training samples which are unavoidably biased.", "prompt": "Aims. The present study aims at providing a deeper insight into the power and limitation of an unsupervised classification algorithm (called Fisher-EM) on spectra of galaxies. This algorithm uses a Gaussian mixture in a discriminative latent subspace. To this end, we investigate the capacity of this algorithm to segregate", "orig": "the physical parameters used to generate mock spectra and the influence of the noise on the classification. Methods. With the code CIGALE and different values for nine input parameters characterising the stellar population, we have simulated a sample of 11 475 optical spectra of galaxies containing 496 monochromatic fluxes. The statistical", "generated": "Aims. The present study aims at providing a deeper insight into the power and limitation of an unsupervised classification algorithm (called Fisher-EM) on spectra of galaxies. This algorithm uses a Gaussian mixture in a discriminative latent subspace. To this end, we investigate the capacity of this algorithm to segregate"}
{"full_text": "Microarray analysis to monitor expression activities in thousands of genes simultaneously has become routine in biomedical research during the past decade. A tremendous amount of expression profiles are generated and stored in the public domain and information integration by meta-analysis to detect differentially expressed (DE) genes has become popular to obtain increased statistical power and validated findings. Methods that aggregate transformed $p$-value evidence have been widely used in genomic settings, among which Fisher's and Stouffer's methods are the most popular ones. In practice, raw data and $p$-values of DE evidence are often not available in genomic studies that are to be combined. Instead, only the detected DE gene lists under a certain $p$-value threshold (e.g., DE genes with $p$-value${}<0.001$) are reported in journal publications. The truncated $p$-value information makes the aforementioned meta-analysis methods inapplicable and researchers are forced to apply a less efficient vote counting method or na\\\"{i}vely drop the studies with incomplete information. The purpose of this paper is to develop effective meta-analysis methods for such situations with partially censored $p$-values. We developed and compared three imputation methods - mean imputation, single random imputation and multiple imputation - for a general class of evidence aggregation methods of which Fisher's and Stouffer's methods are special examples. The null distribution of each method was analytically derived and subsequent inference and genomic analysis frameworks were established. Simulations were performed to investigate the type I error, power and the control of false discovery rate (FDR) for (correlated) gene expression data. The proposed methods were applied to several genomic applications in colorectal cancer, pain and liquid association analysis of major depressive disorder (MDD). The results showed that imputation methods outperformed existing na\\\"{i}ve approaches. Mean imputation and multiple imputation methods performed the best and are recommended for future applications.", "prompt": "Microarray analysis to monitor expression activities in thousands of genes simultaneously has become routine in biomedical research during the past decade. A tremendous amount of expression profiles are generated and stored in the public domain and information integration by meta-analysis to detect differentially expressed (DE) genes has become popular to obtain increased statistical power and validated findings", "orig": ". Methods that aggregate transformed $p$-value evidence have been widely used in genomic settings, among which Fisher's and Stouffer's methods are the most popular ones. In practice, raw data and $p$-values of DE evidence are often not available in genomic studies that are to be combined. Instead,", "generated": "Microarray analysis to monitor expression activities in thousands of genes simultaneously has become routine in biomedical research during the past decade. A tremendous amount of expression profiles are generated and stored in the public domain and information integration by meta-analysis to detect differentially expressed (DE) genes has become popular to obtain increased statistical power and validated finding"}
{"full_text": "An extensive {\\it ab initio} study of the physical properties of both linear and zigzag atomic chains of all 4$d$ and 5$d$ transition metals (TM) within the GGA by using the accurate PAW method, has been carried out. All the TM linear chains are found to be unstable against the corresponding zigzag structures. All the TM chains, except Nb, Ag and La, have a stable (or metastable) magnetic state in either the linear or zigzag or both structures. Magnetic states appear also in the sufficiently stretched Nb and La linear chains and in the largely compressed Y and La chains. The spin magnetic moments in the Mo, Tc, Ru, Rh, W, Re chains could be large ($\\geq$1.0 $\\mu_B$/atom). Structural transformation from the linear to zigzag chains could suppress the magnetism already in the linear chain, induce the magnetism in the zigzag structure, and also cause a change of the magnetic state (ferromagnetic to antiferroamgetic or vice verse). The calculations including the spin-orbit coupling reveal that the orbital moments in the Zr, Tc, Ru, Rh, Pd, Hf, Ta, W, Re, Os, Ir and Pt chains could be rather large ($\\geq$0.1 $\\mu_B$/atom). Importantly, large magnetic anisotropy energy ($\\geq$1.0 meV/atom) is found in most of the magnetic TM chains, suggesting that these nanowires could have fascinating applications in ultrahigh density magnetic memories and hard disks. In particular, giant magnetic anisotropy energy ($\\geq$10.0 meV/atom) could appear in the Ru, Re, Rh, and Ir chains. Furthermore, the magnetic anisotropy energy in several elongated linear chains could be as large as 40.0 meV/atom. A spin-reorientation transition occurs in the Ru, Ir, Ta, Zr, La and Zr, Ru, La, Ta and Ir linear chains when they are elongated. Remarkably, all the 5$d$ as well as Tc and Pd chains show the colossal magnetic anisotropy (i.e., it is impossible to rotate magnetization into certain directions). Finally, the electronic band structure and density of states of the nanowires have also been calculated in order to understand the electronic origin of the large magnetic anisotropy and orbital magnetic moment as well as to estimate the conduction electron spin polarization.", "prompt": "An extensive {\\it ab initio} study of the physical properties of both linear and zigzag atomic chains of all 4$d$ and 5$d$ transition metals (TM) within the GGA by using the accurate PAW method, has been carried out. All the TM linear chains are found to be", "orig": "unstable against the corresponding zigzag structures. All the TM chains, except Nb, Ag and La, have a stable (or metastable) magnetic state in either the linear or zigzag or both structures. Magnetic states appear also in the sufficiently stretched Nb and La linear chains and in the largely compressed Y and La chains.", "generated": "An extensive {\\it ab initio} study of the physical properties of both linear and zigzag atomic chains of all 4$d$ and 5$d$ transition metals (TM) within the GGA by using the accurate PAW method, has been carried out. All the TM linear chains are found to be"}
{"full_text": "There are two major routes to address the ubiquitous family of inverse problems appearing in signal and image processing, such as denoising or deblurring. A first route relies on Bayesian modeling, where prior probabilities are used to embody models of both the distribution of the unknown variables and their statistical dependence with the observed data. The estimation process typically relies on the minimization of an expected loss (e.g. minimum mean squared error, or MMSE). The second route has received much attention in the context of sparse regularization and compressive sensing: it consists in designing (often convex) optimization problems involving the sum of a data fidelity term and a penalty term promoting certain types of unknowns (e.g., sparsity, promoted through an 1 norm). Well known relations between these two approaches have lead to some widely spread misconceptions. In particular, while the so-called Maximum A Posterori (MAP) estimate with a Gaussian noise model does lead to an optimization problem with a quadratic data-fidelity term, we disprove through explicit examples the common belief that the converse would be true. It has already been shown [7, 9] that for denoising in the presence of additive Gaussian noise, for any prior probability on the unknowns, MMSE estimation can be expressed as a penalized least squares problem, with the apparent characteristics of a MAP estimation problem with Gaussian noise and a (generally) different prior on the unknowns. In other words, the variational approach is rich enough to build all possible MMSE estimators associated to additive Gaussian noise via a well chosen penalty. We generalize these results beyond Gaussian denoising and characterize noise models for which the same phenomenon occurs. In particular, we prove that with (a variant of) Poisson noise and any prior probability on the unknowns, MMSE estimation can again be expressed as the solution of a penalized least squares optimization problem. For additive scalar denois-ing the phenomenon holds if and only if the noise distribution is log-concave. In particular, Laplacian denoising can (perhaps surprisingly) be expressed as the solution of a penalized least squares problem. In the multivariate case, the same phenomenon occurs when the noise model belongs to a particular subset of the exponential family. For multivariate additive denoising, the phenomenon holds if and only if the noise is white and Gaussian.", "prompt": "There are two major routes to address the ubiquitous family of inverse problems appearing in signal and image processing, such as denoising or deblurring. A first route relies on Bayesian modeling, where prior probabilities are used to embody models of both the distribution of the unknown variables and their statistical dependence with the observed data.", "orig": "The estimation process typically relies on the minimization of an expected loss (e.g. minimum mean squared error, or MMSE). The second route has received much attention in the context of sparse regularization and compressive sensing: it consists in designing (often convex) optimization problems involving the sum of a data fidelity term and a", "generated": "There are two major routes to address the ubiquitous family of inverse problems appearing in signal and image processing, such as denoising or deblurring. A first route relies on Bayesian modeling, where prior probabilities are used to embody models of both the distribution of the unknown variables and their statistical dependence with the observed data."}
{"full_text": "SUMMARY Geophysical imaging using the inversion procedure is a powerful tool for the exploration of the Earth's subsurface. However, the interpretation of inverted images can sometimes be difficult, due to the inherent limitations of existing inversion algorithms, which produce smoothed sections. In order to improve and automate the processing and interpretation of inverted geophysical models, we propose an approach inspired from data mining. We selected an algorithm known as DBSCAN (Density-Based Spatial Clustering of Applications with Noise) to perform clustering of inverted geophysical sections. The methodology relies on the automatic sorting and clustering of data. DBSCAN detects clusters in the inverted electrical resistivity values, with no prior knowledge of the number of clusters. This algorithm has the advantage of being defined by only two parameters: the neighbourhood of a point in the data space, and the minimum number of data points in this neighbourhood. We propose an objective procedure for the determination of these two parameters. The proof of concept described here is applied to simulated ERT (electrical resistivity tomography) sections, for the following three cases: two layers with a step, two layers with a rebound, and two layers with an anomaly embedded in the upper layer. To validate this approach, sensitivity studies were carried out on both of the above parameters, as well as to assess the influence of noise on the algorithm's performance. Finally, this methodology was tested on real field data. DBSCAN detects clusters in the inverted electrical resistivity models, and the former are then associated with various types of earth materials, thus allowing the structure of the prospected area to be determined. The proposed data-mining algorithm is shown to be effective, and to improve the interpretation of the inverted ERT sections. This new approach has considerable potential, as it can be applied to any geophysical data represented in the form of sections or maps.", "prompt": "SUMMARY Geophysical imaging using the inversion procedure is a powerful tool for the exploration of the Earth's subsurface. However, the interpretation of inverted images can sometimes be difficult, due to the inherent limitations of existing inversion algorithms, which produce smoothed sections. In order to improve and automate the processing and interpretation of inverted ge", "orig": "ophysical models, we propose an approach inspired from data mining. We selected an algorithm known as DBSCAN (Density-Based Spatial Clustering of Applications with Noise) to perform clustering of inverted geophysical sections. The methodology relies on the automatic sorting and clustering of data. DBSCAN detects clusters in the inverted electrical resistivity values,", "generated": "SUMMARY Geophysical imaging using the inversion procedure is a powerful tool for the exploration of the Earth's subsurface. However, the interpretation of inverted images can sometimes be difficult, due to the inherent limitations of existing inversion algorithms, which produce smoothed sections. In order to improve and automate the processing and interpretation of inverted ge"}
{"full_text": "Of the four giant planets in the Solar system, only Jupiter and Neptune are currently known to possess swarms of Trojan asteroids - small objects that experience a 1:1 mean motion resonance with their host planet. In Lykawka et al. (2009), we performed extensive dynamical simulations, including planetary migration, to investigate the origin of the Neptunian Trojan population. Utilising the vast amount of simulation data obtained for that work, together with fresh results from new simulations, we here investigate the dynamical capture of Trojans by all four giant planets from a primordial trans-Neptunian disk. We find the likelihood of a given planetesimal from this region being captured onto an orbit within Jupiter's Trojan cloud lies between several times 10^-6 and 10^-5. For Saturn, the probability is found to be in the range <10^-6 to 10^-5, whilst for Uranus the probabilities range between 10^-5 and 10^-4. Finally, Neptune displays the greatest probability of Trojan capture, with values ranging between 10^-4 and 10^-3. Our results suggest that all four giant planets are able to capture and retain a significant population of Trojan objects from the disk by the end of planetary migration. As a result of encounters with the giant planets prior to Trojan capture, these objects tend to be captured on orbits that are spread over a wide range of orbital eccentricities and inclinations. The bulk of captured objects are to some extent dynamically unstable, and therefore the populations of these objects tend to decay over the age of the Solar System, providing an important ongoing source of new objects moving on dynamically unstable orbits among the giant planets. Given that a huge population of objects would be displaced by Neptune's outward migration (with a potential cumulative mass a number of times that of the Earth), we conclude that the surviving remnant of the Trojans captured during the migration of the outer planets might be sufficient to explain the currently known Trojan populations in the outer Solar system.", "prompt": "Of the four giant planets in the Solar system, only Jupiter and Neptune are currently known to possess swarms of Trojan asteroids - small objects that experience a 1:1 mean motion resonance with their host planet. In Lykawka et al. (2009), we performed extensive dynamical simulations, including planetary", "orig": "migration, to investigate the origin of the Neptunian Trojan population. Utilising the vast amount of simulation data obtained for that work, together with fresh results from new simulations, we here investigate the dynamical capture of Trojans by all four giant planets from a primordial trans-Neptunian disk.", "generated": "Of the four giant planets in the Solar system, only Jupiter and Neptune are currently known to possess swarms of Trojan asteroids - small objects that experience a 1:1 mean motion resonance with their host planet. In Lykawka et al. (2009), we performed extensive dynamical simulations, including planetary"}
{"full_text": "We present an experimental and theoretical description of the kinetics of coalescence of two water drops on a plane solid surface. The case of partial wetting is considered. The drops are in an atmosphere of nitrogen saturated with water where they grow by condensation and eventually touch each other and coalesce. A new convex composite drop is rapidly formed that then exponentially and slowly relaxes to an equilibrium hemispherical cap. The characteristic relaxation time is proportional to the drop radius R * at final equilibrium. This relaxation time appears to be nearly 10 7 times larger than the bulk capillary relaxation time t b = R * $\\eta$/$\\sigma$, where $\\sigma$ is the gas--liquid surface tension and $\\eta$ is the liquid shear viscosity. In order to explain this extremely large relaxation time, we consider a model that involves an Arrhenius kinetic factor resulting from a liquid--vapour phase change in the vicinity of the contact line. The model results in a large relaxation time of order t b exp(L/RT) where L is the molar latent heat of vaporization, R is the gas constant and T is the temperature. We model the late time relaxation for a near spherical cap and find an exponential relaxation whose typical time scale agrees reasonably well with the experiment. 1. Introduction Fusion or coalescence between drops is a key process in a wide range of phenomena: phase transition in fluids and liquid mixtures or polymers, stability of foams and emulsions, and sintering in metallurgy (Eggers 1998), which is why the problem of coalescence has already received considerable attention. Most of the studies of this process so far have been devoted to the coalescence of two spherical drops floating in a medium. The kinetics of the process before and after the drops have touched each other is governed by the hydrodynamics inside and outside the drops and by the van der Waals forces when the drops are within mesoscopic distance from each other (Yiantsios \\& Davis 1991). The composite drop that results from the coalescence of two drops relaxes to a spherical shape within a time which is dominated by the relaxation of the flow inside and outside (Nikolayev, Beysens \\& Guenoun 1996; Nikolayev \\& Beysens 1997). There are no studies, to our knowledge, of the coalescence of two sessile drops after they touch each other. In this paper, we report a preliminary study of the dynamics and morphology of this process, in the case of hemispherical water droplets which grow slowly on a plane surface at the expense of the surrounding atmosphere, forming what is called 'dew' or 'breath figures' (Beysens et al. 1991; Beysens 1995). The drops eventually touch each other and coalesce to form an elongated composite", "prompt": "We present an experimental and theoretical description of the kinetics of coalescence of two water drops on a plane solid surface. The case of partial wetting is considered. The drops are in an atmosphere of nitrogen saturated with water where they grow by condensation and eventually touch each other and coalesce. A new convex composite", "orig": "drop is rapidly formed that then exponentially and slowly relaxes to an equilibrium hemispherical cap. The characteristic relaxation time is proportional to the drop radius R * at final equilibrium. This relaxation time appears to be nearly 10 7 times larger than the bulk capillary relaxation time t b = R * $\\eta$/$", "generated": "We present an experimental and theoretical description of the kinetics of coalescence of two water drops on a plane solid surface. The case of partial wetting is considered. The drops are in an atmosphere of nitrogen saturated with water where they grow by condensation and eventually touch each other and coalesce. A new convex composite"}
{"full_text": "Watson-Crick (WK) finite automata are working on a Watson-Crick tape, that is, on a DNA molecule. A double stranded DNA molecule contains two strands, each having a 5' and a 3' end, and these two strands together form the molecule with the following properties. The strands have the same length, their 5' to 3' directions are opposite, and in each position, the two strands have nucleotides that are complement of each other (by the Watson-Crick complementary relation). Consequently, WK automata have two reading heads, one for each strand. In traditional WK automata both heads read the whole input in the same physical direction, but in 5'->3' WK automata the heads start from the two extremes and read the input in opposite direction. In sensing 5'->3' WK automata, the process on the input is finished when the heads meet, and the model is capable to accept the class of linear context-free languages. Deterministic variants are weaker, the class named 2detLIN, a proper subclass of linear languages is accepted by them. Recently, another specific variants, the state-deterministic sensing 5'->3' WK automata are investigated in which the graph of the automaton has the special property that for each node of the graph, all out edges (if any) go to a sole node, i.e., for each state there is (at most) one state that can be reached by a direct transition. It was shown that this concept is somewhat orthogonal to the usual concept of determinism in case of sensing 5'->3' WK automata. In this paper a new concept, the quasi-determinism is investigated, that is in each configuration of a computation (if it is not finished yet), the next state is uniquely determined although the next configuration may not be, in case various transitions are enabled at the same time. We show that this new concept is a common generalisation of the usual determinism and the state-determinism, i.e., the class of quasi-deterministic sensing 5'->3' WK automata is a superclass of both of the mentioned other classes. There are various usual restrictions on WK automata, e.g., stateless or 1-limited variants. We also prove some hierarchy results among language classes accepted by various subclasses of quasi-deterministic sensing 5'->3' WK automata and also some other already known language classes.", "prompt": "Watson-Crick (WK) finite automata are working on a Watson-Crick tape, that is, on a DNA molecule. A double stranded DNA molecule contains two strands, each having a 5' and a 3' end, and these two strands together form the molecule with the following properties. The", "orig": "strands have the same length, their 5' to 3' directions are opposite, and in each position, the two strands have nucleotides that are complement of each other (by the Watson-Crick complementary relation). Consequently, WK automata have two reading heads, one for each strand. In traditional WK autom", "generated": "Watson-Crick (WK) finite automata are working on a Watson-Crick tape, that is, on a DNA molecule. A double stranded DNA molecule contains two strands, each having a 5' and a 3' end, and these two strands together form the molecule with the following properties. The"}
{"full_text": "Non-target screening consists in searching a sample for all present substances, suspected or unknown, with very little prior knowledge about the sample. This approach has been introduced more than a decade ago in the field of water analysis, together with dedicated compound identification tools, but is still very scarce for indoor and atmospheric trace gas measurements, despite the clear need for a better understanding of the atmospheric trace gas composition.For a systematic detection of emerging trace gases in the atmosphere, a new and powerful analytical method is gas chromatography (GC) of preconcentrated samples, followed by electron ionisation, high resolution mass spectrometry (EI-HRMS). In this work, we present data analysis tools to enable automated fragment formula annotation for unknown compounds measured by GC-EI-HRMS. Based on co-eluting mass/charge fragments, we developed an innovative data analysis method to reliably reconstruct the chemical formulae of the fragments, using efficient combinatorics and graph theory. The method does not require the presence of the molecular ion, which is absent in ~40% of EI spectra. Our method has been trained and validated on \\textgreater50 halocarbons and hydrocarbons, with 3 to 20 atoms and molar masses of 30 to 330 g mol-1, measured with a mass resolution of approx.~3500. For 90% of the compounds, more than 90% of the annotated fragment formulae are correct. Cases of wrong identification can be attributed to the scarcity of detected fragments per compound or the lack of isotopic constraint (no minor isotopocule detected).Our method enables to reconstruct most probable chemical formulae independently from spectral databases. Therefore, it demonstrates the suitability of EI-HRMS data for non-target analysis and paves the way for the identification of substances for which no EI mass spectrum is registered in databases. We illustrate the performances of our method for atmospheric trace gases and suggest that it may be well suited for many other types of samples. The L-GPL licenced Python code is released under the name ALPINAC for ALgorithmic Process for Identification of Non-targeted Atmospheric Compounds.", "prompt": "Non-target screening consists in searching a sample for all present substances, suspected or unknown, with very little prior knowledge about the sample. This approach has been introduced more than a decade ago in the field of water analysis, together with dedicated compound identification tools, but is still very scarce for indoor and atmospheric trace gas measurements,", "orig": "despite the clear need for a better understanding of the atmospheric trace gas composition.For a systematic detection of emerging trace gases in the atmosphere, a new and powerful analytical method is gas chromatography (GC) of preconcentrated samples, followed by electron ionisation, high resolution mass spectrometry (EI-HRMS).", "generated": "Non-target screening consists in searching a sample for all present substances, suspected or unknown, with very little prior knowledge about the sample. This approach has been introduced more than a decade ago in the field of water analysis, together with dedicated compound identification tools, but is still very scarce for indoor and atmospheric trace gas measurements."}
{"full_text": "The observable quantities in optical interferometry, which are the modulus and the phase of the complex visibility, may be corrupted by parasitic fringes superimposed on the genuine fringe pattern. These fringes are due to an interference phenomenon occurring from straylight effects inside an interferometric instrument. We developed an analytical approach to better understand this phenomenon when straylight causes crosstalk between beams. We deduced that the parasitic interference significantly affects the interferometric phase and thus the associated observables including the differential phase and the closure phase. The amount of parasitic flux coupled to the piston between beams appears to be very influential in this degradation. For instance, considering a point-like source and a piston ranging from $\\lambda/500$ to $\\lambda/5$ in L band ($\\lambda=3.5\\:\\mu$m), a parasitic flux of about 1\\% of the total flux produces a parasitic phase reaching at most one third of the intrinsic phase. The piston, which can have different origins (instrumental stability, atmospheric perturbations, ...), thus amplifies the effect of parasitic interference. According to specifications of piston correction in space or at ground level (respectively $\\lambda/500\\approx 2$nm and $\\lambda/30\\approx 100$nm), the detection of hot Jupiter-like planets, one of the most challenging aims for current ground-based interferometers, limits parasitic radiation to about 5\\% of the incident intensity. This was evaluated by considering different types of hot Jupiter synthetic spectra. Otherwise, if no fringe tracking is used, the detection of a typical hot Jupiter-like system with a solar-like star would admit a maximum level of parasitic intensity of 0.01\\% for piston errors equal to $\\lambda$/15. If the fringe tracking specifications are not precisely observed, it thus appears that the allowed level of parasitic intensity dramatically decreases and may prevent the detection. In parallel, the calibration of the parasitic phase by a reference star, at this accuracy level, seems very difficult. Moreover, since parasitic phase is an object-dependent quantity, the use of a hypothetical phase abacus, directly giving the parasitic phase from a given parasitic flux level, is also impossible. Some instrumental solutions, implemented at the instrument design stage for limiting or preventing this parasitic interference, appears to be crucial and are presented in this paper.", "prompt": "The observable quantities in optical interferometry, which are the modulus and the phase of the complex visibility, may be corrupted by parasitic fringes superimposed on the genuine fringe pattern. These fringes are due to an interference phenomenon occurring from straylight effects inside an interferometric instrument. We developed an analytical approach to", "orig": "better understand this phenomenon when straylight causes crosstalk between beams. We deduced that the parasitic interference significantly affects the interferometric phase and thus the associated observables including the differential phase and the closure phase. The amount of parasitic flux coupled to the piston between beams appears to be very influential in this degradation.", "generated": "The observable quantities in optical interferometry, which are the modulus and the phase of the complex visibility, may be corrupted by parasitic fringes superimposed on the genuine fringe pattern. These fringes are due to an interference phenomenon occurring from straylight effects inside an interferometric instrument. We developed an analytical approach to"}
{"full_text": "We present the results of a new series of non-gray calculations of the atmospheres, spectra, colors, and evolution of extrasolar giant planets (EGPs) and brown dwarfs for effective temperatures below 1300 K. This theory encompasses most of the mass/age parameter space occupied by substellar objects and is the first spectral study down to 100 K. These calculations are in aid of the multitude of searches being conducted or planned around the world for giant planets and brown dwarfs and reveal the exotic nature of the class. Generically, absorption by H_2 at longer wavelengths and H_2O opacity windows at shorter wavelengths conspire to redistribute flux blueward. Below 1200 K, methane is the dominant carbon bearing molecule and is a universal diagnostic feature of EGP and brown dwarf spectra. We find that the primary bands in which to search are $Z$ (\\sim 1.05 \\mic), $J$ (\\sim 1.2 \\mic), $H$ (\\sim 1.6 \\mic), $K$ (\\sim 2.2 \\mic), $M$ (\\sim 5 \\mic), and $N$ (\\sim 10 \\mic), that enhancements of the emergent flux over blackbody values, in particular in the near infrared, can be by many orders of magnitude, and that the infrared colors of EGPs and brown dwarfs are much bluer than previously believed. In particular, relative to $J$ and $H$, the $K$ band flux is reduced by CH_4 and H_2 absorption. Furthermore, we derive that for T_{eff}s below 1200 K most or all true metals are sequestered below the photosphere, that an interior radiative zone is a generic feature of substellar objects, and that clouds of H_2O and NH_3 are formed for T_{eff}s below \\sim 400 K and \\sim 200 K, respectively. This study is done for solar-metallicity objects in isolation and does not include the effects of stellar insolation. Nevertheless, it is an attempt to bridge the gap between the planetary and stellar realms and to develop a non-gray theory of objects from 0.3 \\mj (``saturn'') to 70 \\mj ($\\sim$0.07 \\mo). We find that the detection ranges for brown dwarf/EGP discovery of both ground-- and space-based telescopes are larger than previously estimated.", "prompt": "We present the results of a new series of non-gray calculations of the atmospheres, spectra, colors, and evolution of extrasolar giant planets (EGPs) and brown dwarfs for effective temperatures below 1300 K. This theory encompasses most of the mass/age parameter space occupied by substellar objects and is", "orig": "the first spectral study down to 100 K. These calculations are in aid of the multitude of searches being conducted or planned around the world for giant planets and brown dwarfs and reveal the exotic nature of the class. Generically, absorption by H_2 at longer wavelengths and H_2O opacity windows at shorter wavelengths", "generated": "We present the results of a new series of non-gray calculations of the atmospheres, spectra, colors, and evolution of extrasolar giant planets (EGPs) and brown dwarfs for effective temperatures below 1300 K. This theory encompasses most of the mass/age parameter space occupied by substellar objects and incorporates"}
{"full_text": "In stochastic systems with weak noise, the logarithm of the stationary distribution becomes proportional to a large deviation rate function called the quasi-potential. The quasi-potential, and its characterization through a variational problem, lies at the core of the Freidlin-Wentzell large deviations theory%.~\\cite{freidlin1984}.In many interacting particle systems, the particle density is described by fluctuating hydrodynamics governed by Macroscopic Fluctuation Theory%, ~\\cite{bertini2014},which formally fits within Freidlin-Wentzell's framework with a weak noise proportional to $1/\\sqrt{N}$, where $N$ is the number of particles. The quasi-potential then appears as a natural generalization of the equilibrium free energy to non-equilibrium particle systems. A key physical and practical issue is to actually compute quasi-potentials from their variational characterization for non-equilibrium systems for which detailed balance does not hold. We discuss how to perform such a computation perturbatively in an external parameter $\\lambda$, starting from a known quasi-potential for $\\lambda=0$. In a general setup, explicit iterative formulae for all terms of the power-series expansion of the quasi-potential are given for the first time. The key point is a proof of solvability conditions that assure the existence of the perturbation expansion to all orders. We apply the perturbative approach to diffusive particles interacting through a mean-field potential. For such systems, the variational characterization of the quasi-potential was proven by Dawson and Gartner%. ~\\cite{dawson1987,dawson1987b}. Our perturbative analysis provides new explicit results about the quasi-potential and about fluctuations of one-particle observables in a simple example of mean field diffusions: the Shinomoto-Kuramoto model of coupled rotators%. ~\\cite{shinomoto1986}. This is one of few systems for which non-equilibrium free energies can be computed and analyzed in an effective way, at least perturbatively.", "prompt": "In stochastic systems with weak noise, the logarithm of the stationary distribution becomes proportional to a large deviation rate function called the quasi-potential. The quasi-potential, and its characterization through a variational problem, lies at the core of the Freidlin-Wentzell large deviations theory%.~\\cite", "orig": "{freidlin1984}.In many interacting particle systems, the particle density is described by fluctuating hydrodynamics governed by Macroscopic Fluctuation Theory%, ~\\cite{bertini2014},which formally fits within Freidlin-Wentzell's framework with a weak noise proportional to $1/\\sqrt", "generated": "In stochastic systems with weak noise, the logarithm of the stationary distribution becomes proportional to a large deviation rate function called the quasi-potential. The quasi-potential, and its characterization through a variational problem, lies at the core of the Freidlin-Wentzell large deviations theory.\n\nThe quasi-p"}
{"full_text": "Context: Embedded Domain-Specific Languages (EDSLs) are a common and widely used approach to DSLs in various languages, including Haskell and Scala. There are two main implementation techniques for EDSLs: shallow embeddings and deep embeddings. Inquiry: Shallow embeddings are quite simple, but they have been criticized in the past for being quite limited in terms of modularity and reuse. In particular, it is often argued that supporting multiple DSL interpretations in shallow embeddings is difficult. Approach: This paper argues that shallow EDSLs and Object-Oriented Programming (OOP) are closely related. Gibbons and Wu already discussed the relationship between shallow EDSLs and procedural abstraction, while Cook discussed the connection between procedural abstraction and OOP. We make the transitive step in this paper by connecting shallow EDSLs directly to OOP via procedural abstraction. The knowledge about this relationship enables us to improve on implementation techniques for EDSLs. Knowledge: This paper argues that common OOP mechanisms (including inheritance, subtyping, and type-refinement) increase the modularity and reuse of shallow EDSLs when compared to classical procedural abstraction by enabling a simple way to express multiple, possibly dependent, interpretations. Grounding: We make our arguments by using Gibbons and Wu's examples, where procedural abstraction is used in Haskell to model a simple shallow EDSL. We recode that EDSL in Scala and with an improved OO-inspired Haskell encoding. We further illustrate our approach with a case study on refactoring a deep external SQL query processor to make it more modular, shallow, and embedded. Importance: This work is important for two reasons. Firstly, from an intellectual point of view, this work establishes the connection between shallow embeddings and OOP, which enables a better understanding of both concepts. Secondly, this work illustrates programming techniques that can be used to improve the modularity and reuse of shallow EDSLs.", "prompt": "Context: Embedded Domain-Specific Languages (EDSLs) are a common and widely used approach to DSLs in various languages, including Haskell and Scala. There are two main implementation techniques for EDSLs: shallow embeddings and deep embeddings. Inquiry: Shallow embeddings are quite simple, but they have been criticized in", "orig": "the past for being quite limited in terms of modularity and reuse. In particular, it is often argued that supporting multiple DSL interpretations in shallow embeddings is difficult. Approach: This paper argues that shallow EDSLs and Object-Oriented Programming (OOP) are closely related. Gibbons and Wu already discussed the relationship", "generated": "EDSLs are a common and widely used approach to DSLs in various languages, including Haskell and Scala. There are two main implementation techniques for EDSLs: shallow embeddings and deep embeddings. Shallow embeddings are quite simple, but they have been criticized in that they can lead to code that is harder to understand"}
{"full_text": "Aims. The HIFI instrument onboard Herschel has allowed high spectral resolution and sensitive observations of ground-state transi- tions of three molecular ions: the methylidyne cation CH+, its isotopologue 13CH+, and sulfanylium SH+. Because of their unique chemical properties, a comparative analysis of these cations provides essential clues to the link between the chemistry and dynamics of the diffuse interstellar medium. Methods. The CH+, 13CH+, and SH+ lines are observed in absorption towards the distant high-mass star-forming regions (SFRs) DR21(OH), G34.3+0.1, W31C, W33A, W49N, and W51, and towards two sources close to the Galactic centre, SgrB2(N) and SgrA*+50. All sight lines sample the diffuse interstellar matter along pathlengths of several kiloparsecs across the Galactic Plane. In order to compare the velocity structure of each species, the observed line profiles were deconvolved from the hyperfine structure of the SH+ transition and the CH+, 13CH+, and SH+ spectra were independently decomposed into Gaussian velocity components. To analyse the chemical composition of the foreground gas, all spectra were divided, in a second step, into velocity intervals over which the CH+, 13CH+, and SH+ column densities and abundances were derived. Results. SH+ is detected along all observed lines of sight, with a velocity structure close to that of CH+ and 13CH+. The linewidth distributions of the CH+, SH+, and 13CH+ Gaussian components are found to be similar. These distributions have the same mean (<\\delta\\u{psion}> ~ 4.2 km s-1) and standard deviation (\\sigma(\\delta\\u{psion}) ~ 1.5 km s-1). This mean value is also close to that of the linewidth distribution of the CH+ visible transitions detected in the solar neighbourhood. We show that the lack of absorption components narrower than 2 km s-1 is not an artefact caused by noise: the CH+, 13CH+, and SH+ line profiles are therefore statistically broader than those of most species detected in absorption in diffuse interstellar gas (e. g. HCO+, CH, or CN). The SH+/CH+ column density ratio observed in the components located away from the Galactic centre spans two orders of magnitude and correlates with the CH+ abundance. Conversely, the ratio observed in the components close to the Galactic centre varies over less than one order of magnitude with no apparent correlation with the CH+ abundance. The observed dynamical and chemical properties of SH+ and CH+ are proposed to trace the ubiquitous process of turbulent dissipation, in shocks or shears, in the diffuse ISM and the specific environment of the Galactic centre regions.", "prompt": "Aims. The HIFI instrument onboard Herschel has allowed high spectral resolution and sensitive observations of ground-state transi- tions of three molecular ions: the methylidyne cation CH+, its isotopologue 13CH+, and sulfanylium SH+. Because of their unique chemical properties, a", "orig": "comparative analysis of these cations provides essential clues to the link between the chemistry and dynamics of the diffuse interstellar medium. Methods. The CH+, 13CH+, and SH+ lines are observed in absorption towards the distant high-mass star-forming regions (SFRs) DR21(OH), G34.", "generated": "Aims. The HIFI instrument onboard Herschel has allowed high spectral resolution and sensitive observations of ground-state transitions of three molecular ions: the methylidyne cation CH+, its isotopologue 13CH+, and sulfanylium SH+. Because of their unique chemical properties, and the fact that they"}
{"full_text": "Numerical schemes for the solution of the Euler equations have recently been developed, which involve the discretisation of the internal energy equation, with corrective terms to ensure the correct capture of shocks, and, more generally, the consistency in the Lax-Wendroff sense. These schemes may be staggered or colocated, using either struc-tured meshes or general simplicial or tetrahedral/hexahedral meshes. The time discretization is performed by fractional-step algorithms; these may be either based on semi-implicit pressure correction techniques or segregated in such a way that only explicit steps are involved (referred to hereafter as \"explicit\" variants). In order to ensure the positivity of the density, the internal energy and the pressure, the discrete convection operators for the mass and internal energy balance equations are carefully designed; they use an upwind technique with respect to the material velocity only. The construction of the fluxes thus does not need any Rie-mann or approximate Riemann solver, and yields easily implementable algorithms. The stability is obtained without restriction on the time step for the pressure correction scheme and under a CFL-like condition for explicit variants: preservation of the integral of the total energy over the computational domain, and positivity of the density and the internal energy. The semi-implicit first-order upwind scheme satisfies a local discrete entropy inequality. If a MUSCL-like scheme is used in order to limit the scheme diffusion, then a weaker property holds: the entropy inequality is satisfied up to a remainder term which is shown to tend to zero with the space and time steps, if the discrete solution is controlled in L $\\infty$ and BV norms. The explicit upwind variant also satisfies such a weaker property, at the price of an estimate for the velocity which could be derived from the introduction of a new stabilization term in the momentum balance. Still for the explicit scheme, with the above-mentioned MUSCL-like scheme, the same result only holds if the ratio of the time to the space step tends to zero.", "prompt": "Numerical schemes for the solution of the Euler equations have recently been developed, which involve the discretisation of the internal energy equation, with corrective terms to ensure the correct capture of shocks, and, more generally, the consistency in the Lax-Wendroff sense. These schemes may be staggered or colocated", "orig": ", using either struc-tured meshes or general simplicial or tetrahedral/hexahedral meshes. The time discretization is performed by fractional-step algorithms; these may be either based on semi-implicit pressure correction techniques or segregated in such a way that only explicit steps are involved (referred to hereafter", "generated": "Numerical schemes for the solution of the Euler equations have recently been developed, which involve the discretisation of the internal energy equation, with corrective terms to ensure the correct capture of shocks, and, more generally, the consistency in the Lax-Wendroff sense. These schemes may be staggered or colocated"}
{"full_text": "In the late seventies, Clark [In Communication Systems and Random Process Theory (Proc. 2nd NATO Advanced Study Inst., Darlington, 1977) (1978) 721-734, Sijthoff & Noordhoff] pointed out that it would be natural for $\\pi_t$, the solution of the stochastic filtering problem, to depend continuously on the observed data $Y=\\{Y_s,s\\in[0,t]\\}$. Indeed, if the signal and the observation noise are independent one can show that, for any suitably chosen test function $f$, there exists a continuous map $\\theta^f_t$, defined on the space of continuous paths $C([0,t],\\mathbb{R}^d)$ endowed with the uniform convergence topology such that $\\pi_t(f)=\\theta^f_t(Y)$, almost surely; see, for example, Clark [In Communication Systems and Random Process Theory (Proc. 2nd NATO Advanced Study Inst., Darlington, 1977) (1978) 721-734, Sijthoff & Noordhoff], Clark and Crisan [Probab. Theory Related Fields 133 (2005) 43-56], Davis [Z. Wahrsch. Verw. Gebiete 54 (1980) 125-139], Davis [Teor. Veroyatn. Primen. 27 (1982) 160-167], Kushner [Stochastics 3 (1979) 75-83]. As shown by Davis and Spathopoulos [SIAM J. Control Optim. 25 (1987) 260-278], Davis [In Stochastic Systems: The Mathematics of Filtering and Identification and Applications, Proc. NATO Adv. Study Inst. Les Arcs, Savoie, France 1980 505-528], [In The Oxford Handbook of Nonlinear Filtering (2011) 403-424 Oxford Univ. Press], this type of robust representation is also possible when the signal and the observation noise are correlated, provided the observation process is scalar. For a general correlated noise and multidimensional observations such a representation does not exist. By using the theory of rough paths we provide a solution to this deficiency: the observation process $Y$ is \"lifted\" to the process $\\mathbf{Y}$ that consists of $Y$ and its corresponding L\\'{e}vy area process, and we show that there exists a continuous map $\\theta_t^f$, defined on a suitably chosen space of H\\\"{o}lder continuous paths such that $\\pi_t(f)=\\theta_t^f(\\mathbf{Y})$, almost surely.", "prompt": "In the late seventies, Clark [In Communication Systems and Random Process Theory (Proc. 2nd NATO Advanced Study Inst., Darlington, 1977) (1978) 721-734, Sijthoff & Noordhoff] pointed out that it would be natural for $\\pi_t$,", "orig": "the solution of the stochastic filtering problem, to depend continuously on the observed data $Y=\\{Y_s,s\\in[0,t]\\}$. Indeed, if the signal and the observation noise are independent one can show that, for any suitably chosen test function $f$, there exists a continuous map $\\theta^", "generated": "In the late seventies, Clark [In Communication Systems and Random Process Theory (Proc. 2nd NATO Advanced Study Inst., Darlington, 1977) (1978) 721-734, Sijthoff & Noordhoff] pointed out that it would be natural for $\\pi_t$,"}
{"full_text": "Context: Internet of Things (IoT) has become an important kind of distributed systems thanks to the wide-spread of cheap embedded devices equipped with different networking technologies. Although ubiquitous, developing IoT systems remains challenging. Inquiry: A recent field study with 194 IoT developers identifies debugging as one of the main challenges faced when developing IoT systems. This comes from the lack of debugging tools taking into account the unique properties of IoT systems such as non-deterministic data, and hardware restricted devices. On the one hand, offline debuggers allow developers to analyse post-failure recorded program information, but impose too much overhead on the devices while generating such information. Furthermore, the analysis process is also time-consuming and might miss contextual information relevant to find the root cause of bugs. On the other hand, online debuggers do allow debugging a program upon a failure while providing contextual information (e.g., stack trace). In particular, remote online debuggers enable debugging of devices without physical access to them. However, they experience debugging interference due to network delays which complicates bug reproducibility, and have limited support for dynamic software updates on remote devices. Approach: This paper proposes out-of-things debugging, an online debugging approach especially designed for IoT systems. The debugger is always-on as it ensures constant availability to for instance debug post-deployment situations. Upon a failure or breakpoint, out-of-things debugging moves the state of a deployed application to the developer's machine. Developers can then debug the application locally by applying operations (e.g., step commands) to the retrieved state. Once debugging is finished, developers can commit bug fixes to the device through live update capabilities. Finally, by means of a fine-grained flexible interface for accessing remote resources, developers have full control over the debugging overhead imposed on the device, and the access to device hardware resources (e.g., sensors) needed during local debugging. Knowledge: Out-of-things debugging maintains good properties of remote debugging as it does not require physical access to the device to debug it, while reducing debugging interference since there are no network delays on operations (e.g., stepping) issued on the debugger since those happen locally. Furthermore, device resources are only accessed when requested by the user which further mitigates overhead and opens avenues for mocking or simulation of non-accessed resources. Grounding: We implemented an out-of-things debugger as an extension to a WebAssembly Virtual Machine and benchmarked its suitability for IoT. In particular, we compared our solution to remote debugging alternatives based on metrics such as network overhead, memory usage, scalability, and usability in production settings. From the benchmarks, we conclude that our debugger exhibits competitive performance in addition to confining overhead without sacrificing debugging convenience and flexibility. Importance: Out-of-things debugging enables debugging of IoT systems by means of classical online operations (e.g., stepwise execution) while addressing IoT-specific concerns (e.g., hardware limitations). We show that having the debugger always-on does not have to come at cost of performance loss or increased overhead but instead can enforce a smooth-going and flexible debugging experience of IoT systems.", "prompt": "Context: Internet of Things (IoT) has become an important kind of distributed systems thanks to the wide-spread of cheap embedded devices equipped with different networking technologies. Although ubiquitous, developing IoT systems remains challenging. Inquiry: A recent field study with 194 IoT developers identifies debugging as one of the main challenges faced when", "orig": "developing IoT systems. This comes from the lack of debugging tools taking into account the unique properties of IoT systems such as non-deterministic data, and hardware restricted devices. On the one hand, offline debuggers allow developers to analyse post-failure recorded program information, but impose too much overhead on the devices while generating such", "generated": "Here's the repeated text with a natural continuation:\n\nThe repeated text is: \n\"A recent field study with 194 IoT developers identifies debugging as one of the main challenges faced when developing IoT systems.\"\n\nHere's a natural continuation:\n\nThe study, which was conducted by a leading research institution, aimed to understand the experiences and"}
{"full_text": "As a traditional tool of external assistance, crutches play an important role in society. They have a wide range of applications to help either the elderly and disabled to walk or to treat certain illnesses or for post-operative rehabilitation. But there are many different types of crutches, including shoulder crutches and elbow crutches. How to choose has become an issue that deserves to be debated. Because while crutches help people walk, they also have an impact on the body. Inappropriate choice of crutches or long-term misuse can lead to problems such as scoliosis. Previous studies were mainly experimental measurements or the construction of dynamic models to calculate the load on joints with crutches. These studies focus only on the level of the joints, ignoring the role that muscles play in this process. Although some also take into account the degree of muscle activation, there is still a lack of quantitative analysis. The traditional dynamic model can be used to calculate the load on each joint. However, due to the activation of the muscle, this situation only causes part of the load transmitted to the joint, and the work of the chair will compensate the other part of the load. Analysis at the muscle level allows a better understanding of the impact of crutches on the body. By comparing the levels of activation of the trunk muscles, it was found that the use of crutches for walking, especially a single crutch, can cause a large difference in the activation of the back muscles on the left and right sides, and this difference will cause muscle degeneration for a long time, leading to scoliosis. In this article taking scoliosis as an example, by analyzing the muscles around the spine, we can better understand the pathology and can better prevent diseases. The objective of this article is to analyze normal walking compared to walking with one or two crutches using OpenSim software to obtain the degree of activation of different muscles in order to analyze the impact of crutches on the body.", "prompt": "As a traditional tool of external assistance, crutches play an important role in society. They have a wide range of applications to help either the elderly and disabled to walk or to treat certain illnesses or for post-operative rehabilitation. But there are many different types of crutches, including shoulder crutches and elbow crutches.", "orig": "How to choose has become an issue that deserves to be debated. Because while crutches help people walk, they also have an impact on the body. Inappropriate choice of crutches or long-term misuse can lead to problems such as scoliosis. Previous studies were mainly experimental measurements or the construction of dynamic models to calculate", "generated": "As a traditional tool of external assistance, crutches play an important role in society. They have a wide range of applications to help either the elderly and disabled to walk or to treat certain illnesses or for post-operative rehabilitation. But there are many different types of crutches, including shoulder crutches and elbow crutches."}
{"full_text": "One of the biggest challenges in medicine is to dampen the pathophysiological stress induced by an episode of ischemia. Such stress, due to various pathological or clinical situations, follows a restriction in blood and oxygen supply to tissue, causing a shortage of oxygen and nutrients that are required for cellular metabolism. Ischemia can cause irreversible damage to target tissue leading to a poor physiological recovery outcome for the patient. Contrariwise, preconditioning by brief periods of ischemia has been shown in multiple organs to confer tolerance against subsequent normally lethal ischemia. By definition, preconditioning of organs must be applied preemptively. This limits the applicability of preconditioning in clinical situations, which arise unpredictably, such as myocardial infarction and stroke. There are, however, clinical situations that arise as a result of ischemia-reperfusion injury, which can be anticipated, and are therefore adequate candidates for preconditioning. Organ and more particularly kidney transplantation, the optimal treatment for suitable patients with end stage renal disease (ESRD), is a predictable surgery that permits the use of preconditioning protocols to prepare the organ for subsequent ischemic/reperfusion stress. It therefore seems crucial to develop appropriate preconditioning protocols against ischemia that will occur under transplantation conditions, which up to now mainly referred to mechanical ischemic preconditioning that triggers innate responses. It is not known if preconditioning has to be applied to the donor, the recipient, or both. No drug/target pair has been envisioned and validated in the clinic. Options for identifying new target/drug pairs involve the use of model animals, such as drosophila, in which some physiological pathways, such as the management of oxygen, are highly conserved across evolution. Oxygen is the universal element of life existence on earth. In this review we focus on a very specific pathway of pharmacological preconditioning identified in drosophila that was successfully transferred to mammalian models that has potential application in human health. Very few mechanisms identified in these model animals have been translated to an upper evolutionary level. This review highlights the commonality between oxygen regulation between diverse animals.", "prompt": "One of the biggest challenges in medicine is to dampen the pathophysiological stress induced by an episode of ischemia. Such stress, due to various pathological or clinical situations, follows a restriction in blood and oxygen supply to tissue, causing a shortage of oxygen and nutrients that are required for cellular metabolism. Ischemia", "orig": "can cause irreversible damage to target tissue leading to a poor physiological recovery outcome for the patient. Contrariwise, preconditioning by brief periods of ischemia has been shown in multiple organs to confer tolerance against subsequent normally lethal ischemia. By definition, preconditioning of organs must be applied preemptively. This limits the applicability", "generated": "One of the biggest challenges in medicine is to dampen the pathophysiological stress induced by an episode of ischemia. Such stress, due to various pathological or clinical situations, follows a restriction in blood and oxygen supply to tissue, causing a shortage of oxygen and nutrients that are required for cellular metabolism. Ischemia"}
{"full_text": "Context. The TOPoS project has the goal to find and analyse Turn-Off (TO) stars of extremely low metallicity. To select the targets for spectroscopic follow-up at high spectral resolution, we have relied on low-resolution spectra from the Sloan Digital Sky Survey. Aims. In this paper we use the metallicity estimates we have obtained from our analysis of the SDSS spectra to construct the metallicity distribution function (MDF) of the Milky Way, with special emphasis on its metal-weak tail. The goal is to provide the underlying distribution out of which the TOPoS sample was extracted. Methods. We make use of SDSS photometry, Gaia photometry and distance estimates derived from the Gaia parallaxes to derive a metallicity estimate for a large sample of over 24 million TO stars. This sample is used to derive the metallicity bias of the sample for which SDSS spectra are available. Results. We determined that the spectroscopic sample is strongly biased in favour of metal-poor stars, as intended. A comparison with the unbiased photometric sample allows to correct for the selection bias. We select a sub-sample of stars with reliable parallaxes for which we combine the SDSS radial velocities with Gaia proper motions and parallaxes to compute actions and orbital parameters in the Galactic potential. This allows us to characterize the stars dynamically, and in particular to select a sub-sample that belongs to the Gaia-Sausage-Enceladus (GSE) accretion event. We are thus able to provide also the MDF of GSE. Conclusions. The metal-weak tail derived in our study is very similar to that derived in the H3 survey and in the Hamburg/ESO Survey. This allows us to average the three MDFs and provide an error bar for each metallicity bin. Inasmuch the GSE structure is representative of the progenitor galaxy that collided with the Milky Way, that galaxy appears to be strongly deficient in metal-poor stars compared to the Milky Way, suggesting that the metal-weak tail of the latter has been largely formed by accretion of low mass galaxies rather than massive galaxies, such as the GSE progenitor.", "prompt": "Context. The TOPoS project has the goal to find and analyse Turn-Off (TO) stars of extremely low metallicity. To select the targets for spectroscopic follow-up at high spectral resolution, we have relied on low-resolution spectra from the Sloan Digital Sky Survey. Aims. In this paper we use the", "orig": "metallicity estimates we have obtained from our analysis of the SDSS spectra to construct the metallicity distribution function (MDF) of the Milky Way, with special emphasis on its metal-weak tail. The goal is to provide the underlying distribution out of which the TOPoS sample was extracted. Methods. We make use of", "generated": "Context. The TOPoS project has the goal to find and analyse Turn-Off (TO) stars of extremely low metallicity. To select the targets for spectroscopic follow-up at high spectral resolution, we have relied on low-resolution spectra from the Sloan Digital Sky Survey. Aims. In this paper we use the"}
{"full_text": "Context: Tables are ubiquitous formats for data. Therefore, techniques for writing correct programs over tables, and debugging incorrect ones, are vital. Our specific focus in this paper is on rich types that articulate the properties of tabular operations. We wish to study both their expressive power and _diagnostic quality_. Inquiry: There is no \"standard library\" of table operations. As a result, every paper (and project) is free to use its own (sub)set of operations. This makes artifacts very difficult to compare, and it can be hard to tell whether omitted operations were left out by oversight or because they cannot actually be expressed. Furthermore, virtually no papers discuss the quality of type error feedback. Approach: We combed through several existing languages and libraries to create a \"standard library\" of table operations. Each entry is accompanied by a detailed specification of its \"type,\" expressed independent of (and hence not constrained by) any type language. We also studied and categorized a corpus of (student) program edits that resulted in table-related errors. We used this to generate a suite of erroneous programs. Finally, we adapted the concept of a datasheet to facilitate comparisons of different implementations. Knowledge: Our benchmark creates a common ground to frame work in this area. Language designers who claim to support typed programming over tables have a clear suite against which to demonstrate their system's expressive power. Our family of errors also gives them a chance to demonstrate the quality of feedback. Researchers who improve one aspect -- especially error reporting -- without changing the other can demonstrate their improvement, as can those who engage in trade-offs between the two. The net result should be much better science in both expressiveness and diagnostics. We also introduce a datasheet format for presenting this knowledge in a methodical way. Grounding: We have generated our benchmark from real languages, libraries, and programs, as well as personal experience conducting and teaching data science. We have drawn on experience in engineering and, more recently, in data science to generate the datasheet. Importance: Claims about type support for tabular programming are hard to evaluate. However, tabular programming is ubiquitous, and the expressive power of type systems keeps growing. Our benchmark and datasheet can help lead to more orderly science. It also benefits programmers trying to choose a language.", "prompt": "Context: Tables are ubiquitous formats for data. Therefore, techniques for writing correct programs over tables, and debugging incorrect ones, are vital. Our specific focus in this paper is on rich types that articulate the properties of tabular operations. We wish to study both their expressive power and _diagnostic quality_. Inquiry: There", "orig": "is no \"standard library\" of table operations. As a result, every paper (and project) is free to use its own (sub)set of operations. This makes artifacts very difficult to compare, and it can be hard to tell whether omitted operations were left out by oversight or because they cannot actually be expressed. Furthermore", "generated": "Tables are ubiquitous formats for data. Therefore, techniques for writing correct programs over tables, and debugging incorrect ones, are vital. Our specific focus in this paper is on rich types that articulate the properties of tabular operations. We wish to study both their expressive power and diagnostic quality. Inquiry: These are particularly challenging to"}
{"full_text": "In this paper, we describe a scheme for propagating belief functions in certain kinds of trees using only local computations. This scheme generalizes the computational scheme proposed by Shafer and Logan1 for diagnostic trees of the type studied by Gordon and Shortliffe, and the slightly more general scheme given by Shafer for hierarchical evidence. It also generalizes the scheme proposed by Pearl for Bayesian causal trees (see Shenoy and Shafer). Pearl's causal trees and Gordon and Shortliffe's diagnostic trees are both ways of breaking the evidence that bears on a large problem down into smaller items of evidence that bear on smaller parts of the problem so that these smaller problems can be dealt with one at a time. This localization of effort is often essential in order to make the process of probability judgment feasible, both for the person who is making probability judgments and for the machine that is combining them. The basic structure for our scheme is a type of tree that generalizes both Pearl's and Gordon and Shortliffe's trees. Trees of this general type permit localized computation in Pearl's sense. They are based on qualitative judgments of conditional independence. We believe that the scheme we describe here will prove useful in expert systems. It is now clear that the successful propagation of probabilities or certainty factors in expert systems requires much more structure than can be provided in a pure production-system framework. Bayesian schemes, on the other hand, often make unrealistic demands for structure. The propagation of belief functions in trees and more general networks stands on a middle ground where some sensible and useful things can be done. We would like to emphasize that the basic idea of local computation for propagating probabilities is due to Judea Pearl. It is a very innovative idea; we do not believe that it can be found in the Bayesian literature prior to Pearl's work. We see our contribution as extending the usefulness of Pearl's idea by generalizing it from Bayesian probabilities to belief functions. In the next section, we give a brief introduction to belief functions. The notions of qualitative independence for partitions and a qualitative Markov tree are introduced in Section III. Finally, in Section IV, we describe a scheme for propagating belief functions in qualitative Markov trees.", "prompt": "In this paper, we describe a scheme for propagating belief functions in certain kinds of trees using only local computations. This scheme generalizes the computational scheme proposed by Shafer and Logan1 for diagnostic trees of the type studied by Gordon and Shortliffe, and the slightly more general scheme given by Shafer for hierarchical", "orig": "evidence. It also generalizes the scheme proposed by Pearl for Bayesian causal trees (see Shenoy and Shafer). Pearl's causal trees and Gordon and Shortliffe's diagnostic trees are both ways of breaking the evidence that bears on a large problem down into smaller items of evidence that bear on smaller parts of the problem so", "generated": "In this paper, we describe a scheme for propagating belief functions in certain kinds of trees using only local computations. This scheme generalizes the computational scheme proposed by Shafer and Logan1 for diagnostic trees of the type studied by Gordon and Shortliffe, and the slightly more general scheme given by Shafer for hierarchical"}
{"full_text": "The Gaussian graphical model, a popular paradigm for studying relationship among variables in a wide range of applications, has attracted great attention in recent years. This paper considers a fundamental question: When is it possible to estimate low-dimensional parameters at parametric square-root rate in a large Gaussian graphical model? A novel regression approach is proposed to obtain asymptotically efficient estimation of each entry of a precision matrix under a sparseness condition relative to the sample size. When the precision matrix is not sufficiently sparse, or equivalently the sample size is not sufficiently large, a lower bound is established to show that it is no longer possible to achieve the parametric rate in the estimation of each entry. This lower bound result, which provides an answer to the delicate sample size question, is established with a novel construction of a subset of sparse precision matrices in an application of Le Cam's lemma. Moreover, the proposed estimator is proven to have optimal convergence rate when the parametric rate cannot be achieved, under a minimal sample requirement. The proposed estimator is applied to test the presence of an edge in the Gaussian graphical model or to recover the support of the entire model, to obtain adaptive rate-optimal estimation of the entire precision matrix as measured by the matrix $\\ell_q$ operator norm and to make inference in latent variables in the graphical model. All of this is achieved under a sparsity condition on the precision matrix and a side condition on the range of its spectrum. This significantly relaxes the commonly imposed uniform signal strength condition on the precision matrix, irrepresentability condition on the Hessian tensor operator of the covariance matrix or the $\\ell_1$ constraint on the precision matrix. Numerical results confirm our theoretical findings. The ROC curve of the proposed algorithm, Asymptotic Normal Thresholding (ANT), for support recovery significantly outperforms that of the popular GLasso algorithm.", "prompt": "The Gaussian graphical model, a popular paradigm for studying relationship among variables in a wide range of applications, has attracted great attention in recent years. This paper considers a fundamental question: When is it possible to estimate low-dimensional parameters at parametric square-root rate in a large Gaussian graphical model? A novel regression approach is proposed", "orig": "to obtain asymptotically efficient estimation of each entry of a precision matrix under a sparseness condition relative to the sample size. When the precision matrix is not sufficiently sparse, or equivalently the sample size is not sufficiently large, a lower bound is established to show that it is no longer possible to achieve the parametric", "generated": "The Gaussian graphical model, a popular paradigm for studying relationship among variables in a wide range of applications, has attracted great attention in recent years. This paper considers a fundamental question: When is it possible to estimate low-dimensional parameters at parametric square-root rate in a large Gaussian graphical model? A novel regression approach is proposed"}
{"full_text": "We consider the problem of deterministic load balancing of tokens in the discrete model. A set of $n$ processors is connected into a $d$-regular undirected network. In every time step, each processor exchanges some of its tokens with each of its neighbors in the network. The goal is to minimize the discrepancy between the number of tokens on the most-loaded and the least-loaded processor as quickly as possible. Rabani et al. (1998) present a general technique for the analysis of a wide class of discrete load balancing algorithms. Their approach is to characterize the deviation between the actual loads of a discrete balancing algorithm with the distribution generated by a related Markov chain. The Markov chain can also be regarded as the underlying model of a continuous diffusion algorithm. Rabani et al. showed that after time $T = O(\\log (Kn)/\\mu)$, any algorithm of their class achieves a discrepancy of $O(d\\log n/\\mu)$, where $\\mu$ is the spectral gap of the transition matrix of the graph, and $K$ is the initial load discrepancy in the system. In this work we identify some natural additional conditions on deterministic balancing algorithms, resulting in a class of algorithms reaching a smaller discrepancy. This class contains well-known algorithms, eg., the Rotor-Router. Specifically, we introduce the notion of cumulatively fair load-balancing algorithms where in any interval of consecutive time steps, the total number of tokens sent out over an edge by a node is the same (up to constants) for all adjacent edges. We prove that algorithms which are cumulatively fair and where every node retains a sufficient part of its load in each step, achieve a discrepancy of $O(\\min\\{d\\sqrt{\\log n/\\mu},d\\sqrt{n}\\})$ in time $O(T)$. We also show that in general neither of these assumptions may be omitted without increasing discrepancy. We then show by a combinatorial potential reduction argument that any cumulatively fair scheme satisfying some additional assumptions achieves a discrepancy of $O(d)$ almost as quickly as the continuous diffusion process. This positive result applies to some of the simplest and most natural discrete load balancing schemes.", "prompt": "We consider the problem of deterministic load balancing of tokens in the discrete model. A set of $n$ processors is connected into a $d$-regular undirected network. In every time step, each processor exchanges some of its tokens with each of its neighbors in the network. The goal is to minimize the discrepancy", "orig": "between the number of tokens on the most-loaded and the least-loaded processor as quickly as possible. Rabani et al. (1998) present a general technique for the analysis of a wide class of discrete load balancing algorithms. Their approach is to characterize the deviation between the actual loads of a discrete balancing algorithm with the distribution", "generated": "We consider the problem of deterministic load balancing of tokens in the discrete model. A set of $n$ processors is connected into a $d$-regular undirected network. In every time step, each processor exchanges some of its tokens with each of its neighbors in the network. The goal is to minimize the discrepancy"}
{"full_text": "A fluid model is developed for multicomponent two-temperature magnetized plasmas in chemical non-equilibrium from the partially- to fully-ionized collisional regimes. We focus on transport phenomena aiming at representing the chromosphere of the Sun. Graille et al. [M3AS 19(04):527-599, 2009] have derived an asymptotic fluid model for multicomponent plamas from kinetic theory, yielding a rigorous description of the dissipative effects. The governing equations and consistent transport properties are obtained using a multiscale Chapman-Enskog perturbative solution to the Boltzmann equation based on a non-dimensional analysis. The mass disparity between the electrons and heavy particles is accounted for, as well as the influence of the electromagnetic field. We couple this model to the Maxwell equations for the electromagnetic field and derive the generalized Ohm's law for multicomponent plasmas. The model inherits a well-identified mathematical structure leading to an extended range of validity for the Sun chromosphere conditions. We compute consistent transport properties by means of a spectral Galerkin method using the Laguerre-Sonine polynomial approximation. Two non-vanishing polynomial terms are used when deriving the transport systems for electrons, whereas only one term is retained for heavy particles. In a simplified framework where the plasma is fully ionized, we compare the transport properties for the Sun chromosphere to conventional expressions for magnetized plasmas due to Braginskii, showing a good agreement between both results. For more general partially ionized conditions, representative of the Sun chromosphere, we compute the muticomponent transport properties corresponding to the species diffusion velocities, heavy-particle and electron heat fluxes, and viscous stress tensor of the model, for a Helium-Hydrogen mixture in local thermodynamic equilibrium. The model is assessed for the 3D radiative magnetohydrodynamic simulation of a pore, in the highly turbulent upper layer of the solar convective zone. The resistive term is found to dominate mainly the dynamics of the electric field at the pore location. The battery term for heavy particles appears to be higher at the pore location and at some intergranulation boundaries.", "prompt": "A fluid model is developed for multicomponent two-temperature magnetized plasmas in chemical non-equilibrium from the partially- to fully-ionized collisional regimes. We focus on transport phenomena aiming at representing the chromosphere of the Sun. Graille et al. [M3AS 19(04):", "orig": "527-599, 2009] have derived an asymptotic fluid model for multicomponent plamas from kinetic theory, yielding a rigorous description of the dissipative effects. The governing equations and consistent transport properties are obtained using a multiscale Chapman-Enskog perturbative solution to the Boltzmann equation based", "generated": "A fluid model is developed for multicomponent two-temperature magnetized plasmas in chemical non-equilibrium from the partially- to fully-ionized collisional regimes. We focus on transport phenomena aiming at representing the chromosphere of the Sun. Graille et al. [M3AS 19(04)"}
{"full_text": "De Finetti's classical result of [18] identifying the law of an exchangeable family of random variables as a mixture of i.i.d. laws was extended to structure theorems for more complex notions of exchangeability by Aldous [1,2,3], Hoover [41,42], Kallenberg [44] and Kingman [47]. On the other hand, such exchangeable laws were first related to questions from combinatorics in an independent analysis by Fremlin and Talagrand [29], and again more recently in Tao [62], where they appear as a natural proxy for the `leading order statistics' of colourings of large graphs or hypergraphs. Moreover, this relation appears implicitly in the study of various more bespoke formalisms for handling `limit objects' of sequences of dense graphs or hypergraphs in a number of recent works, including Lov\\'{a}sz and Szegedy [52], Borgs, Chayes, Lov\\'{a}sz, S\\'{o}s, Szegedy and Vesztergombi [17], Elek and Szegedy [24] and Razborov [54,55]. However, the connection between these works and the earlier probabilistic structural results seems to have gone largely unappreciated. In this survey we recall the basic results of the theory of exchangeable laws, and then explain the probabilistic versions of various interesting questions from graph and hypergraph theory that their connection motivates (particularly extremal questions on the testability of properties for graphs and hypergraphs). We also locate the notions of exchangeability of interest to us in the context of other classes of probability measures subject to various symmetries, in particular contrasting the methods employed to analyze exchangeable laws with related structural results in ergodic theory, particular the Furstenberg-Zimmer structure theorem for probability-preserving $\\mathbb {Z}$-systems, which underpins Furstenberg's ergodic-theoretic proof of Szemer\\'{e}di's Theorem. The forthcoming paper [10]--hereditarytest will make a much more elaborate appeal to the link between exchangeable laws and dense (directed) hypergraphs to establish various results in property testing.", "prompt": "De Finetti's classical result of [18] identifying the law of an exchangeable family of random variables as a mixture of i.i.d. laws was extended to structure theorems for more complex notions of exchangeability by Aldous [1,2,3], Hoover [41,42], Kallenberg", "orig": "[44] and Kingman [47]. On the other hand, such exchangeable laws were first related to questions from combinatorics in an independent analysis by Fremlin and Talagrand [29], and again more recently in Tao [62], where they appear as a natural proxy for the `leading order statistics'", "generated": "De Finetti's classical result of [18] identifying the law of an exchangeable family of random variables as a mixture of i.i.d. laws was extended to structure theorems for more complex notions of exchangeability by Aldous [1,2,3], Hoover [41,42], Kallenberg"}
{"full_text": "Context. Using observations to deduce dust properties, grain size distribution, and physical conditions in molecular clouds is a highly degenerate problem. Aims. The coreshine phenomenon, a scattering process at 3.6 and 4.5 $\\mu$m that dominates absorption, has revealed its ability to explore the densest parts of clouds. We want to use this effect to constrain the dust parameters. The goal is to investigate to what extent grain growth (at constant dust mass) inside molecular clouds is able to explain the coreshine observations. We aim to find dust models that can explain a sample of Spitzer coreshine data. We also look at the consistency with near-infrared data we obtained for a few clouds. Methods. We selected four regions with a very high occurrence of coreshine cases: Taurus-Perseus, Cepheus, Chameleon and L183/L134. We built a grid of dust models and investigated the key parameters to reproduce the general trend of surface bright- nesses and intensity ratios of both coreshine and near-infrared observations with the help of a 3D Monte-Carlo radiative transfer code. The grid parameters allow to investigate the effect of coagulation upon spherical grains up to 5 $\\mu$m in size derived from the DustEm diffuse interstellar medium grains. Fluffiness (porosity or fractal degree), ices, and a handful of classical grain size distributions were also tested. We used the near- and mostly mid-infrared intensity ratios as strong discriminants between dust models. Results. The determination of the background field intensity at each wavelength is a key issue. In particular, an especially strong background field explains why we do not see coreshine in the Galactic plane at 3.6 and 4.5 $\\mu$m. For starless cores, where detected, the observed 4.5 $\\mu$m / 3.6 $\\mu$m coreshine intensity ratio is always lower than $\\sim$0.5 which is also what we find in the models for the Taurus-Perseus and L183 directions. Embedded sources can lead to higher fluxes (up to four times greater than the strongest starless core fluxes) and higher coreshine ratios (from 0.5 to 1.1 in our selected sample). Normal interstellar radiation field conditions are sufficient to find suitable grain models at all wavelengths for starless cores. The standard interstellar grains are not able to reproduce observations and, due to the multi-wavelength approach, only a few grain types meet the criteria set by the data. Porosity does not affect the flux ratios while the fractal dimension helps to explain coreshine ratios but does not seem able to reproduce near-infrared observations without a mix of other grain types. Conclusions. Combined near- and mid-infrared wavelengths confirm the potential to reveal the nature and size distribution of dust grains. Careful assessment of the environmental parameters (interstellar and background fields, embedded or nearby reddened sources) is required to validate this new diagnostic.", "prompt": "Context. Using observations to deduce dust properties, grain size distribution, and physical conditions in molecular clouds is a highly degenerate problem. Aims. The coreshine phenomenon, a scattering process at 3.6 and 4.5 $\\mu$m that dominates absorption, has revealed its ability to explore the", "orig": "densest parts of clouds. We want to use this effect to constrain the dust parameters. The goal is to investigate to what extent grain growth (at constant dust mass) inside molecular clouds is able to explain the coreshine observations. We aim to find dust models that can explain a sample of Spitzer coreshine", "generated": "Context. Using observations to deduce dust properties, grain size distribution, and physical conditions in molecular clouds is a highly degenerate problem. Aims. The coreshine phenomenon, a scattering process at 3.6 and 4.5 $\\mu$m that dominates absorption, has revealed its ability to explore the"}
{"full_text": "Context: Database-backed applications often run queries with more authority than necessary. Since programs can access more data than they legitimately need, flaws in security checks at the application level can enable malicious or buggy code to view or modify data in violation of intended access control policies. Inquiry: Although database management systems provide tools to control access to data, these tools are not well-suited for modern applications which often have many users and consist of many different software components. First, databases are unaware of application users, and creating a new database user for each application user is impractical for applications with many users. Second, different components of the same application may require different authority, which would require creating different database users for different software components. Thus, it is difficult to use existing tools to properly limit the authority an application has when executing queries. For this reason, we consider a new, language-based approach to application-specific database security. Approach: Prior work has addressed the difficulty of running applications with least privilege using capability-based security and software contracts, which we adapt to the setting of database-backed applications. Knowledge: This paper's main contribution is the design and implementation of ShillDB, a language for writing secure database-backed applications. ShillDB enables reasoning about database access at the language level through capabilities, which limit which database tables a program can access, and contracts, which limit what operations a program can perform on those tables. ShillDB contracts are expressed as part of function interfaces, making it easy to specify different access control policies for different components. Contracts act as executable security documentation for ShillDB programs and are enforced by the language runtime. Further, ShillDB provides database access control guarantees independent of (and in addition to) the security mechanisms of the underlying database management system. Grounding: We have implemented a prototype of ShillDB and have used it to implement the backend for a lending library reservation system with contracts for each endpoint to evaluate the performance and usability of ShillDB. Further, we benchmark individual database operations in ShillDB to better understand the language's performance overhead. Importance: Our experience indicates that ShillDB is a practical language for enforcing database access control policies in realistic, multi-user applications and has reasonable performance overhead. ShillDB allows developers to reason about security at the component level, safely compose components, and reuse third-party components with their own application-specific database security policies.", "prompt": "Context: Database-backed applications often run queries with more authority than necessary. Since programs can access more data than they legitimately need, flaws in security checks at the application level can enable malicious or buggy code to view or modify data in violation of intended access control policies. Inquiry: Although database management systems provide tools to control access", "orig": "to data, these tools are not well-suited for modern applications which often have many users and consist of many different software components. First, databases are unaware of application users, and creating a new database user for each application user is impractical for applications with many users. Second, different components of the same application may require", "generated": "Here is the repeated text and a natural continuation:\n\nAlthough database management systems provide tools to control access and enforce data isolation, flaws in security checks at the application level can enable malicious or buggy code to view or modify data in violation of intended access control policies.\n\nDatabase-backed applications often run queries with more authority than necessary."}
{"full_text": "We present an attempt to reach realistic turbulent regime in direct numerical simulations of the geodynamo. We rely on a sequence of three convection-driven simulations in a rapidly rotating spherical shell. The most extreme case reaches towards the Earth's core regime by lowering viscosity (magnetic Prandtl number Pm=0.1) while maintaining vigorous convection (magnetic Reynolds number Rm>500) and rapid rotation (Ekman number E=1e-7), at the limit of what is feasible on today's supercomputers. A detailed and comprehensive analysis highlights several key features matching geomagnetic observations or dynamo theory predictions -- all present together in the same simulation -- but it also unveils interesting insights relevant for Earth's core dynamics.In this strong-field, dipole-dominated dynamo simulation, the magnetic energy is one order of magnitude larger than the kinetic energy. The spatial distribution of magnetic intensity is highly heterogeneous, and a stark dynamical contrast exists between the interior and the exterior of the tangent cylinder (the cylinder parallel to the axis of rotation that circumscribes the inner core).In the interior, the magnetic field is strongest, and is associated with a vigorous twisted polar vortex, whose dynamics may occasionally lead to the formation of a reverse polar flux patch at the surface of the shell. Furthermore, the strong magnetic field also allows accumulation of light material within the tangent cylinder, leading to stable stratification there. Torsional Alfv{\\'e}n waves are frequently triggered in the vicinity of the tangent cylinder and propagate towards the equator.Outside the tangent cylinder, the magnetic field inhibits the growth of zonal winds and the kinetic energy is mostly non-zonal. Spatio-temporal analysis indicates that the low-frequency, non-zonal flow is quite geostrophic (columnar) and predominantly large-scale: an m=1 eddy spontaneously emerges in our most extreme simulations, without any heterogeneous boundary forcing.Our spatio-temporal analysis further reveals that (i) the low-frequency, large-scale flow is governed by a balance between Coriolis and buoyancy forces -- magnetic field and flow tend to align, minimizing the Lorentz force; (ii) the high-frequency flow obeys a balance between magnetic and Coriolis forces; (iii) the convective plumes mostly live at an intermediate scale, whose dynamics is driven by a 3-term 1 MAC balance -- involving Coriolis, Lorentz and buoyancy forces. However, small-scale (E^{1/3}) quasi-geostrophic convection is still observed in the regions of low magnetic intensity.", "prompt": "We present an attempt to reach realistic turbulent regime in direct numerical simulations of the geodynamo. We rely on a sequence of three convection-driven simulations in a rapidly rotating spherical shell. The most extreme case reaches towards the Earth's core regime by lowering viscosity (magnetic Prandtl number Pm=0.", "orig": "1) while maintaining vigorous convection (magnetic Reynolds number Rm>500) and rapid rotation (Ekman number E=1e-7), at the limit of what is feasible on today's supercomputers. A detailed and comprehensive analysis highlights several key features matching geomagnetic observations or dynamo theory predictions --", "generated": "We present an attempt to reach realistic turbulent regime in direct numerical simulations of the geodynamo. We rely on a sequence of three convection-driven simulations in a rapidly rotating spherical shell. The most extreme case reaches towards the Earth's core regime by lowering viscosity (magnetic Prandtl number Pm=0."}
{"full_text": "Heavy Ion Collisions (HIC) represent a unique tool to probe the in-medium nuclear interaction in regions away from saturation. In this report we present a selection of new reaction observables in dissipative collisions particularly sensitive to the symmetry term of the nuclear Equation of State ($Iso-EoS$). We will first discuss the Isospin Equilibration Dynamics. At low energies this manifests via the recently observed Dynamical Dipole Radiation, due to a collective neutron-proton oscillation with the symmetry term acting as a restoring force. At higher beam energies Iso-EoS effects will be seen in an Isospin Diffusion mechanism, via Imbalance Ratio Measurements, in particular from correlations to the total kinetic energy loss. For fragmentation reactions in central events we suggest to look at the coupling between isospin distillation and radial flow. In Neck Fragmentation reactions important Iso-EoS information can be obtained from fragment isospin content, velocity and alignement correlations. The high density symmetry term can be probed from isospin effects on heavy ion reactions at relativistic energies (few AGeV range), in particular for high transverse momentum selections of the reaction products. Rather isospin sensitive observables are proposed from nucleon/cluster emissions, collective flows and meson production. The possibility to shed light on the controversial neutron/proton effective mass splitting in asymmetric matter is also suggested. A large symmetry repulsion at high baryon density will also lead to an \"earlier\" hadron-deconfinement transition in n-rich matter. The binodal transition line of the (T,\\rho_B) diagram is lowered to a region accessible through heavy ion collisions in the energy range of the new planned facilities, e.g. the FAIR/NICA projects. Some observable effects of the formation of a Mixed Phase are suggested, in particular a Neutron Trapping mechanism. The dependence of the results on a suitable treatment of the isovector part of the interaction in effective QCD Lagrangian approaches is critically discussed. We stress the interest of this study in nuclear astrophysics, in particular for supernovae explosions and neutron star structure, where the knowledge of the Iso-EoS is important at low as well as at high baryon density.", "prompt": "Heavy Ion Collisions (HIC) represent a unique tool to probe the in-medium nuclear interaction in regions away from saturation. In this report we present a selection of new reaction observables in dissipative collisions particularly sensitive to the symmetry term of the nuclear Equation of State ($Iso-EoS$). We will first discuss", "orig": "the Isospin Equilibration Dynamics. At low energies this manifests via the recently observed Dynamical Dipole Radiation, due to a collective neutron-proton oscillation with the symmetry term acting as a restoring force. At higher beam energies Iso-EoS effects will be seen in an Isospin Diffusion mechanism, via", "generated": "Heavy Ion Collisions (HIC) represent a unique tool to probe the in-medium nuclear interaction in regions away from saturation. In this report we present a selection of new reaction observables in dissipative collisions particularly sensitive to the symmetry term of the nuclear Equation of State ($Iso-EoS$). We will first discuss"}
{"full_text": "Parameterized complexity theory has enabled a refined classification of the difficulty of NP-hard optimization problems on graphs with respect to key structural properties, and so to a better understanding of their true difficulties. More recently, hardness results for problems in P were achieved using reasonable complexity theoretic assumptions such as: Strong Exponential Time Hypothesis (SETH), 3SUM and All-Pairs Shortest-Paths (APSP). According to these assumptions, many graph theoretic problems do not admit truly subquadratic algorithms, nor even truly subcubic algorithms (Williams and Williams, FOCS 2010 and Abboud, Grandoni, Williams, SODA 2015). A central technique used to tackle the difficulty of the above mentioned problems is fixed-parameter algorithms for polynomial-time problems with polynomial dependency in the fixed parameter (P-FPT). This technique was introduced by Abboud, Williams and Wang in SODA 2016 and continued by Husfeldt (IPEC 2016) and Fomin et al. (SODA 2017), using the treewidth as a parameter. Applying this technique to clique-width, another important graph parameter, remained to be done. In this paper we study several graph theoretic problems for which hardness results exist such as cycle problems (triangle detection, triangle counting, girth, diameter), distance problems (diameter, eccentricities, Gromov hyperbolicity, betweenness centrality) and maximum matching. We provide hardness results and fully polynomial FPT algorithms, using clique-width and some of its upper-bounds as parameters (split-width, modular-width and $P\\_4$-sparseness). We believe that our most important result is an ${\\cal O}(k^4 \\cdot n + m)$-time algorithm for computing a maximum matching where $k$ is either the modular-width or the $P\\_4$-sparseness. The latter generalizes many algorithms that have been introduced so far for specific subclasses such as cographs, $P\\_4$-lite graphs, $P\\_4$-extendible graphs and $P\\_4$-tidy graphs. Our algorithms are based on preprocessing methods using modular decomposition, split decomposition and primeval decomposition. Thus they can also be generalized to some graph classes with unbounded clique-width.", "prompt": "Parameterized complexity theory has enabled a refined classification of the difficulty of NP-hard optimization problems on graphs with respect to key structural properties, and so to a better understanding of their true difficulties. More recently, hardness results for problems in P were achieved using reasonable complexity theoretic assumptions such as: Strong Exponential Time Hypo", "orig": "thesis (SETH), 3SUM and All-Pairs Shortest-Paths (APSP). According to these assumptions, many graph theoretic problems do not admit truly subquadratic algorithms, nor even truly subcubic algorithms (Williams and Williams, FOCS 2010 and Abboud, Grandoni, Williams", "generated": "Parameterized complexity theory has enabled a refined classification of the difficulty of NP-hard optimization problems on graphs with respect to key structural properties, and so to a better understanding of their true difficulties. More recently, hardness results for problems in P were achieved using reasonable complexity theoretic assumptions such as: Strong Exponential Time Hypo"}
{"full_text": "In the sparse linear regression setting, we consider testing the significance of the predictor variable that enters the current lasso model, in the sequence of models visited along the lasso solution path. We propose a simple test statistic based on lasso fitted values, called the covariance test statistic, and show that when the true model is linear, this statistic has an $\\operatorname {Exp}(1)$ asymptotic distribution under the null hypothesis (the null being that all truly active variables are contained in the current lasso model). Our proof of this result for the special case of the first predictor to enter the model (i.e., testing for a single significant predictor variable against the global null) requires only weak assumptions on the predictor matrix $X$. On the other hand, our proof for a general step in the lasso path places further technical assumptions on $X$ and the generative model, but still allows for the important high-dimensional case $p>n$, and does not necessarily require that the current lasso model achieves perfect recovery of the truly active variables. Of course, for testing the significance of an additional variable between two nested linear models, one typically uses the chi-squared test, comparing the drop in residual sum of squares (RSS) to a $\\chi^2_1$ distribution. But when this additional variable is not fixed, and has been chosen adaptively or greedily, this test is no longer appropriate: adaptivity makes the drop in RSS stochastically much larger than $\\chi^2_1$ under the null hypothesis. Our analysis explicitly accounts for adaptivity, as it must, since the lasso builds an adaptive sequence of linear models as the tuning parameter $\\lambda$ decreases. In this analysis, shrinkage plays a key role: though additional variables are chosen adaptively, the coefficients of lasso active variables are shrunken due to the $\\ell_1$ penalty. Therefore, the test statistic (which is based on lasso fitted values) is in a sense balanced by these two opposing properties - adaptivity and shrinkage - and its null distribution is tractable and asymptotically $\\operatorname {Exp}(1)$.", "prompt": "In the sparse linear regression setting, we consider testing the significance of the predictor variable that enters the current lasso model, in the sequence of models visited along the lasso solution path. We propose a simple test statistic based on lasso fitted values, called the covariance test statistic, and show that when the true model", "orig": "is linear, this statistic has an $\\operatorname {Exp}(1)$ asymptotic distribution under the null hypothesis (the null being that all truly active variables are contained in the current lasso model). Our proof of this result for the special case of the first predictor to enter the model (i.e., testing for a", "generated": "In the sparse linear regression setting, we consider testing the significance of the predictor variable that enters the current lasso model, in the sequence of models visited along the lasso solution path. We propose a simple test statistic based on lasso fitted values, called the covariance test statistic, and show that when the true model"}
{"full_text": "The context of this work is cooperative scheduling, a concurrency paradigm, where task execution is not arbitrarily preempted. Instead, language constructs exist that let a task voluntarily yield the right to execute to another task. The inquiry is the design of provably fair schedulers and suitable notions of fairness for cooperative scheduling languages. To the best of our knowledge, this problem has not been addressed so far. Our approach is to study fairness independently from syntactic constructs or environments, purely from the point of view of the semantics of programming languages, i.e., we consider fairness criteria using the formal definition of a program execution. We develop our concepts for classic structural operational semantics (SOS) as well as for the recent locally abstract, globally concrete (LAGC) semantics. The latter is a highly modular approach to semantics ensuring the separation of concerns between local statement evaluation and scheduling decisions. The new knowledge contributed by our work is threefold: first, we show that a new fairness notion, called quiescent fairness, is needed to characterize fairness adequately in the context of cooperative scheduling; second, we define a provably fair scheduler for cooperative scheduling languages; third, a qualitative comparison between the SOS and LAGC versions yields that the latter, while taking higher initial effort, is more amenable to proving fairness and scales better under language extensions than SOS. The grounding of our work is a detailed formal proof of quiescent fairness for the scheduler defined in LAGC semantics. The importance of our work is that it provides a formal foundation for the implementation of fair schedulers for cooperative scheduling languages, an increasingly popular paradigm (for example: akka/Scala, JavaScript, async Rust). Being based solely on semantics, our ideas are widely applicable. Further, our work makes clear that the standard notion of fairness in concurrent languages needs to be adapted for cooperative scheduling and, more generally, for any language that combines atomic execution sequences with some form of preemption.", "prompt": "The context of this work is cooperative scheduling, a concurrency paradigm, where task execution is not arbitrarily preempted. Instead, language constructs exist that let a task voluntarily yield the right to execute to another task. The inquiry is the design of provably fair schedulers and suitable notions of fairness for cooperative scheduling languages. To", "orig": "the best of our knowledge, this problem has not been addressed so far. Our approach is to study fairness independently from syntactic constructs or environments, purely from the point of view of the semantics of programming languages, i.e., we consider fairness criteria using the formal definition of a program execution. We develop our concepts for classic", "generated": "The context of this work is cooperative scheduling, a concurrency paradigm, where task execution is not arbitrarily preempted. Instead, language constructs exist that let a task voluntarily yield the right to execute to another task. The inquiry is the design of provably fair schedulers and suitable notions of fairness for cooperative scheduling languages. To"}
{"full_text": "The context of this work is specification, detection and ultimately removal of detectable harmful patterns in source code that are associated with defects in design and implementation of software. In particular, we investigate five code smells and four antipatterns previously defined in papers and books. Our inquiry is about detecting those in source code written in Python programming language, which is substantially different from all prior research, most of which concerns Java or C-like languages. Our approach was that of software engineers: we have processed existing research literature on the topic, extracted both the abstract definitions of nine design defects and their concrete implementation specifications, implemented them all in a tool we have programmed and let it loose on a huge test set obtained from open source code from thousands of GitHub projects. When it comes to knowledge, we have found that more than twice as many methods in Python can be considered too long (statistically extremely longer than their neighbours within the same project) than in Java, but long parameter lists are seven times less likely to be found in Python code than in Java code. We have also found that Functional Decomposition, the way it was defined for Java, is not found in the Python code at all, and Spaghetti Code and God Classes are extremely rare there as well. The grounding and the confidence in these results comes from the fact that we have performed our experiments on 32'058'823 lines of Python code, which is by far the largest test set for a freely available Python parser. We have also designed the experiment in such a way that it aligned with prior research on design defect detection in Java in order to ease the comparison if we treat our own actions as a replication. Thus, the importance of the work is both in the unique open Python grammar of highest quality, tested on millions of lines of code, and in the design defect detection tool which works on something else than Java.", "prompt": "The context of this work is specification, detection and ultimately removal of detectable harmful patterns in source code that are associated with defects in design and implementation of software. In particular, we investigate five code smells and four antipatterns previously defined in papers and books. Our inquiry is about detecting those in source code written", "orig": "in Python programming language, which is substantially different from all prior research, most of which concerns Java or C-like languages. Our approach was that of software engineers: we have processed existing research literature on the topic, extracted both the abstract definitions of nine design defects and their concrete implementation specifications, implemented them all in a tool we", "generated": "The context of this work is specification, detection and ultimately removal of detectable harmful patterns in source code that are associated with defects in design and implementation of software. In particular, we investigate five code smells and four antipatterns previously defined in papers and books. Our inquiry is about detecting those in source code written"}
{"full_text": "In the scope of gestural action recognition, the size of the feature vector representing movements is in general quite large especially when full body movements are considered. Furthermore, this feature vector evolves during the movement performance so that a complete movement is fully represented by a matrix M of size DxT , whose element M i, j represents the value of feature i at timestamps j. Many studies have addressed dimensionality reduction considering only the size of the feature vector lying in R D to reduce both the variability of gestural sequences expressed in the reduced space, and the computational complexity of their processing. In return, very few of these methods have explicitly addressed the dimensionality reduction along the time axis. Yet this is a major issue when considering the use of elastic distances which are characterized by a quadratic complexity along the time axis. We present in this paper an evaluation of straightforward approaches aiming at reducing the dimensionality of the matrix M for each movement, leading to consider both the dimensionality reduction of the feature vector as well as its reduction along the time axis. The dimensionality reduction of the feature vector is achieved by selecting remarkable joints in the skeleton performing the movement, basically the extremities of the articulatory chains composing the skeleton. The temporal dimen-sionality reduction is achieved using either a regular or adaptive down-sampling that seeks to minimize the reconstruction error of the movements. Elastic and Euclidean kernels are then compared through support vector machine learning. Two data sets 1 that are widely referenced in the domain of human gesture recognition, and quite distinctive in terms of quality of motion capture, are used for the experimental assessment of the proposed approaches. On these data sets we experimentally show that it is feasible, and possibly desirable, to significantly reduce simultaneously the size of the feature vector and the number of skeleton frames to represent body movements while maintaining a very good recognition rate. The method proves to give satisfactory results at a level currently reached by state-of-the-art methods on these data sets. We experimentally show that the computational complexity reduction that is obtained makes this approach eligible for real-time applications.", "prompt": "In the scope of gestural action recognition, the size of the feature vector representing movements is in general quite large especially when full body movements are considered. Furthermore, this feature vector evolves during the movement performance so that a complete movement is fully represented by a matrix M of size DxT, whose element M i, j", "orig": "represents the value of feature i at timestamps j. Many studies have addressed dimensionality reduction considering only the size of the feature vector lying in R D to reduce both the variability of gestural sequences expressed in the reduced space, and the computational complexity of their processing. In return, very few of these methods have explicitly addressed the", "generated": "In the scope of gestural action recognition, the size of the feature vector representing movements is in general quite large especially when full body movements are considered. Furthermore, this feature vector evolves during the movement performance so that a complete movement is fully represented by a matrix M of size DxT, whose element M(i, j"}
{"full_text": "Despite the amount of savings available and the money supply managed by financial institutions, significant market failures and the failure of carbon pricing strategies prevent sufficient financing of the transition, notably through bank credit. Aware of the links between natural, monetary and productive aggregates, we propose the development of ''eco-systemic'' prudential policies by exposing the interdependence between macro, micro and environmental prudential measures. These would be based on a reorientation of corporate accounting standards towards the concept of socio-environmental solvency, notably the CARE-TDL model (integration of human and natural capital alongside financial capital on the liabilities side of the balance sheet). In an ecosystemic framework, this solvency of virtuous companies would compensate in accounting terms for the lack of financial solvency. The State would then be the guarantor in order to facilitate their access to financing, also reduced by Basel III and Solvency II. This policy develops a system of reallocation of financing capacities from non-virtuous companies to the most virtuous ones with public guarantees, aiming to reduce the debt ratio while increasing green investments, with monetary policies of rates but also of volumes and ratios differentiated according to the types of assets and the greening of bank balance sheets, and finally forms of public-private partnership. Facilitating the financing of green companies would green capital but increase it, partly neutralising the positive environmental impact. It is therefore necessary to limit the credit expansion of ''brown'' companies. This would reduce risky operations and favour less leveraged investments more connected to the real economy, reducing systemic financial risk. -- The Agenda 2030 Policy Briefs series (PoCFiN Kedge Business School - SDSN France - Institut Rousseau) mobilises economists and practitioners to identify an agenda of economic and financial reforms to achieve the 2030 Agenda, at territorial, national and supranational levels. These are published after peer review.", "prompt": "Despite the amount of savings available and the money supply managed by financial institutions, significant market failures and the failure of carbon pricing strategies prevent sufficient financing of the transition, notably through bank credit. Aware of the links between natural, monetary and productive aggregates, we propose the development of ''eco-systemic'' prudential policies", "orig": "by exposing the interdependence between macro, micro and environmental prudential measures. These would be based on a reorientation of corporate accounting standards towards the concept of socio-environmental solvency, notably the CARE-TDL model (integration of human and natural capital alongside financial capital on the liabilities side of the balance sheet", "generated": "Despite the amount of savings available and the money supply managed by financial institutions, significant market failures and the failure of carbon pricing strategies prevent sufficient financing of the transition, notably through bank credit. Aware of the links between natural, monetary and productive aggregates, we propose the development of 'eco-systemic' prudential policies"}
{"full_text": "With the large and increasing volume of textual data, automated methods for identifying significant topics to classify textual documents have received a growing interest. While many efforts have been made in this direction, it still remains a real challenge. Moreover, the issue is even more complex as full texts are not always freely available. Then, using only partial information to annotate these documents is promising but remains a very ambitious issue. MethodsWe propose two classification methods: a k-nearest neighbours (kNN)-based approach and an explicit semantic analysis (ESA)-based approach. Although the kNN-based approach is widely used in text classification, it needs to be improved to perform well in this specific classification problem which deals with partial information. Compared to existing kNN-based methods, our method uses classical Machine Learning (ML) algorithms for ranking the labels. Additional features are also investigated in order to improve the classifiers' performance. In addition, the combination of several learning algorithms with various techniques for fixing the number of relevant topics is performed. On the other hand, ESA seems promising for this classification task as it yielded interesting results in related issues, such as semantic relatedness computation between texts and text classification. Unlike existing works, which use ESA for enriching the bag-of-words approach with additional knowledge-based features, our ESA-based method builds a standalone classifier. Furthermore, we investigate if the results of this method could be useful as a complementary feature of our kNN-based approach.ResultsExperimental evaluations performed on large standard annotated datasets, provided by the BioASQ organizers, show that the kNN-based method with the Random Forest learning algorithm achieves good performances compared with the current state-of-the-art methods, reaching a competitive f-measure of 0.55% while the ESA-based approach surprisingly yielded reserved results.ConclusionsWe have proposed simple classification methods suitable to annotate textual documents using only partial information. They are therefore adequate for large multi-label classification and particularly in the biomedical domain. Thus, our work contributes to the extraction of relevant information from unstructured documents in order to facilitate their automated processing. Consequently, it could be used for various purposes, including document indexing, information retrieval, etc.", "prompt": "With the large and increasing volume of textual data, automated methods for identifying significant topics to classify textual documents have received a growing interest. While many efforts have been made in this direction, it still remains a real challenge. Moreover, the issue is even more complex as full texts are not always freely available. Then, using", "orig": "only partial information to annotate these documents is promising but remains a very ambitious issue. MethodsWe propose two classification methods: a k-nearest neighbours (kNN)-based approach and an explicit semantic analysis (ESA)-based approach. Although the kNN-based approach is widely used in text classification, it needs to be improved to", "generated": "With the large and increasing volume of textual data, automated methods for identifying significant topics to classify textual documents have received a growing interest. While many efforts have been made in this direction, it still remains a real challenge. Moreover, the issue is even more complex as full texts are not always freely available. Then, using"}
{"full_text": "Context: An island parser reads an input text and builds the parse (or abstract syntax) tree of only the programming constructs of interest in the text. These constructs are called islands and the rest of the text is called water, which the parser ignores and skips over. Since an island parser does not have to parse all the details of the input, it is often easy to develop but still useful enough for a number of software engineering tools. When a parser generator is used, the developer can implement an island parser by just describing a small number of grammar rules, for example, in Parsing Expression Grammar (PEG). Inquiry: In practice, however, the grammar rules are often complicated since the developer must define the water inside the island; otherwise, the island parsing will not reduce the total number of grammar rules. When describing the grammar rules for such water, the developer must consider other rules and enumerate a set of symbols, which we call alternative symbols. Due to this difficulty, island parsing seems to be not widely used today despite its usefulness in many applications. Approach: This paper proposes the lake symbols for addressing this difficulty in developing an island parser. It also presents an extension to PEG for supporting the lake symbols. The lake symbols automate the enumeration of the alternative symbols for the water inside an island. The paper proposes an algorithm for translating the extended PEG to the normal PEG, which can be given to an existing parser generator based on PEG. Knowledge: The user can use lake symbols to define water without specifying each alternative symbol. Our algorithms can calculate all alternative symbols for a lake symbol, based on where the lake symbol is used in the grammar. Grounding: We implemented a parser generator accepting our extended PEG and implemented 36 island parsers for Java and 20 island parsers for Python. Our experiments show that the lake symbols reduce 42 % of grammar rules for Java and 89 % of rules for Python on average, excluding the case where islands are expressions. Importance: This work eases the use of island parsing. Lake symbols enable the user to define the water inside the island simpler than before. Defining water inside the island is essential to apply island parsing for practical programming languages.", "prompt": "Context: An island parser reads an input text and builds the parse (or abstract syntax) tree of only the programming constructs of interest in the text. These constructs are called islands and the rest of the text is called water, which the parser ignores and skips over. Since an island parser does not have to parse all", "orig": "the details of the input, it is often easy to develop but still useful enough for a number of software engineering tools. When a parser generator is used, the developer can implement an island parser by just describing a small number of grammar rules, for example, in Parsing Expression Grammar (PEG). Inquiry: In practice, however", "generated": "An island parser reads an input text and builds the parse (or abstract syntax) tree of only the programming constructs of interest in the text. These constructs are called islands and the rest of the text is called water, which the parser ignores and skips over. Since an island parser does not have to parse all of the"}
{"full_text": "The mass balance of mountain glaciers is of interest for several applications (local hydrology or climate projections), and turbulent fluxes can be an important contributor to glacier surface mass balance during strong melting events. The underlying complex terrain leads to spatial heterogeneity and non-stationarity of turbulent fluxes. Due to the contribution of thermally-induced flows and gravity waves, exchange mechanisms are fully three-dimensional, instead of only vertical. Additionally, glaciers have their own distinct microclimate, governed by a down-glacier katabatic wind, which protects the glacier ice and interacts with the surrounding flows on multiple scales. In this study, we perform large-eddy simulations with the WRF model with dx=48 m to gain insight on the boundary-layer processes over an Alpine valley glacier, the Hintereisferner (HEF). We choose two case studies from a measurement campaign (August 2018) with different synoptic wind directions (South-West and North-West). Model evaluation with an array of eddy-covariance stations on the glacier tongue and surroundings reveals that WRF is able to simulate the general glacier boundary-layer structure. Under southwesterly airflow, the down-glacier wind is supported by the South-Western synoptic wind direction, a stable boundary layer is present over the ice surface, and local processes govern the turbulence kinetic energy production. Under northwesterly airflow, a cross-glacier valley flow and a breaking gravity wave lead strong turbulent mixing and to the subsequent erosion of the glacier boundary layer. Stationarity analyses of the sensible heat flux suggest non-stationary behaviour for both case study days, while non-stationarity is highest on the NW day during the gravity-wave event. These results suggest that the synoptic wind direction has, in addition to upstream topography and the atmospheric stability, a strong impact on whether a local glacier boundary layer can form or not, influencing whether a glacier is able to maintain its own microclimate.", "prompt": "The mass balance of mountain glaciers is of interest for several applications (local hydrology or climate projections), and turbulent fluxes can be an important contributor to glacier surface mass balance during strong melting events. The underlying complex terrain leads to spatial heterogeneity and non-stationarity of turbulent fluxes. Due to the contribution of", "orig": "thermally-induced flows and gravity waves, exchange mechanisms are fully three-dimensional, instead of only vertical. Additionally, glaciers have their own distinct microclimate, governed by a down-glacier katabatic wind, which protects the glacier ice and interacts with the surrounding flows on multiple scales. In this study, we perform large-ed", "generated": "The mass balance of mountain glaciers is of interest for several applications (local hydrology or climate projections), and turbulent fluxes can be an important contributor to glacier surface mass balance during strong melting events. The underlying complex terrain leads to spatial heterogeneity and non-stationarity of turbulent fluxes. Due to the contribution of"}
{"full_text": "In this paper, we examine the price dynamics of on-line art auctions of modern Indian art using functional data analysis. The purpose here is not just to understand what determines the final prices of art objects, but also the price movement during the entire auction. We identify several factors, such as artist characteristics (established or emerging artist; prior sales history), art characteristics (size; painting medium--canvas or paper), competition characteristics (current number of bidders; current number of bids) and auction design characteristics (opening bid; position of the lot in the auction), that explain the dynamics of price movement in an on-line art auction. We find that the effects on price vary over the duration of the auction, with some of these effects being stronger at the beginning of the auction (such as the opening bid and historical prices realized). In some cases, the rate of change in prices (velocity) increases at the end of the auction (for canvas paintings and paintings by established artists). Our analysis suggests that the opening bid is positively related to on-line auction price levels of art at the beginning of the auction, but its effect declines toward the end of the auction. The order in which the lots appear in an art auction is negatively related to the current price level, with this relationship decreasing toward the end of the auction. This implies that lots that appear earlier have higher current prices during the early part of the auction, but that effect diminishes by the end of the auction. Established artists show a positive relationship with the price level at the beginning of the auction. Reputation or popularity of the artists and their investment potential as assessed by previous history of sales are positively related to the price levels at the beginning of the auction. The medium (canvas or paper) of the painting does not show any relationship with art auction price levels, but the size of the painting is negatively related to the current price during the early part of the auction. Important implications for auction design are drawn from the analysis.", "prompt": "In this paper, we examine the price dynamics of on-line art auctions of modern Indian art using functional data analysis. The purpose here is not just to understand what determines the final prices of art objects, but also the price movement during the entire auction. We identify several factors, such as artist characteristics (established or", "orig": "emerging artist; prior sales history), art characteristics (size; painting medium--canvas or paper), competition characteristics (current number of bidders; current number of bids) and auction design characteristics (opening bid; position of the lot in the auction), that explain the dynamics of price movement in an on-line art auction. We", "generated": "In this paper, we examine the price dynamics of on-line art auctions of modern Indian art using functional data analysis. The purpose here is not just to understand what determines the final prices of art objects, but also the price movement during the entire auction. We identify several factors, such as artist characteristics (established or"}
{"full_text": "We consider a class of mean field games in which the agents interact through both their states and controls, and we focus on situations in which a generic agent tries to adjust her speed (control) to an average speed (the average is made in a neighborhood in the state space). In such cases, the monotonicity assumptions that are frequently made in the theory of mean field games do not hold, and uniqueness cannot be expected in general. Such model lead to systems of forward-backward nonlinear nonlocal parabolic equations; the latter are supplemented with various kinds of boundary conditions, in particular Neumann-like boundary conditions stemming from reflection conditions on the underlying controled stochastic processes. The present work deals with numerical approximations of the above mentioned systems. After describing the finite difference scheme, we propose an iterative method for solving the systems of nonlinear equations that arise in the discrete setting; it combines a continuation method, Newton iterations and inner loops of a bigradient like solver. The numerical method is used for simulating two examples. We also make experiments on the behaviour of the iterative algorithm when the parameters of the model vary. The theory of mean field games, (MFGs for short), aims at studying deterministic or stochastic differential games (Nash equilibria) as the number of agents tends to infinity. It supposes that the rational agents are indistinguishable and individually have a negligible influence on the game, and that each individual strategy is influenced by some averages of quantities depending on the states (or the controls as in the present work) of the other agents. MFGs have been introduced in the pioneering works of J-M. Lasry and P-L. Lions [17, 18, 19]. Independently and at approximately the same time, the notion of mean field games arose in the engineering literature, see the works of M.Y. Huang, P.E. Caines and R.Malham{\\'e} [14, 15]. The present work deals with numerical approximations of mean field games in which the agents interact through both their states and controls; it follows a more theoretical work by the second author, [16], which is devoted to the mathematical analysis of the related systems of nonlocal partial differential equations. There is not much literature on MFGs in which the agents also interact through their controls, see [13, 12, 8, 10, 7, 16]. To stress the fact that the latter situation is considered, we will sometimes use the terminology mean field games of control and the acronym MFGC.", "prompt": "We consider a class of mean field games in which the agents interact through both their states and controls, and we focus on situations in which a generic agent tries to adjust her speed (control) to an average speed (the average is made in a neighborhood in the state space). In such cases, the monotonicity", "orig": "assumptions that are frequently made in the theory of mean field games do not hold, and uniqueness cannot be expected in general. Such model lead to systems of forward-backward nonlinear nonlocal parabolic equations; the latter are supplemented with various kinds of boundary conditions, in particular Neumann-like boundary conditions stemming from reflection conditions on the", "generated": "We consider a class of mean field games in which the agents interact through both their states and controls, and we focus on situations in which a generic agent tries to adjust her speed (control) to an average speed (the average is made in a neighborhood in the state space). In such cases, the monotonicity"}
{"full_text": "The better developers can learn software tools, the faster they can start using them and the more efficiently they can later work with them. Tutorials are supposed to help here. While in the early days of computing, mostly text tutorials were available, nowadays software developers can choose among a huge number of tutorials for almost any popular software tool. However, only little research was conducted to understand how text tutorials differ from other tutorials, which tutorial types are preferred and, especially, which tutorial types yield the best learning experience in terms of efficiency and effectiveness, especially for programmers. To evaluate these questions, we converted an existing video tutorial for a novel software tool into a content-equivalent text tutorial. We then conducted an experiment in three groups where 42 undergraduate students from a software engineering course were commissioned to operate the software tool after using a tutorial: the first group was provided only with the video tutorial, the second group only with the text tutorial and the third group with both. In this context, the differences in terms of efficiency were almost negligible: We could observe that participants using only the text tutorial completed the tutorial faster than the participants with the video tutorial. However, the participants using only the video tutorial applied the learned content faster, achieving roughly the same bottom line performance. We also found that if both tutorial types are offered, participants prefer video tutorials for learning new content but text tutorials for looking up \"missed\" information. We mainly gathered our data through questionnaires and screen recordings and analyzed it with suitable statistical hypotheses tests. The data is available at [12]. Since producing tutorials requires effort, knowing with which type of tutorial learnability can be increased to which extent has an immense practical relevance. We conclude that in contexts similar to ours, while it would be ideal if software tool makers would offer both tutorial types, it seems more efficient to produce only text tutorials instead of a passive video tutorial - provided you manage to motivate your learners to use them.", "prompt": "The better developers can learn software tools, the faster they can start using them and the more efficiently they can later work with them. Tutorials are supposed to help here. While in the early days of computing, mostly text tutorials were available, nowadays software developers can choose among a huge number of tutorials for almost any popular", "orig": "software tool. However, only little research was conducted to understand how text tutorials differ from other tutorials, which tutorial types are preferred and, especially, which tutorial types yield the best learning experience in terms of efficiency and effectiveness, especially for programmers. To evaluate these questions, we converted an existing video tutorial for a novel software tool", "generated": "The better developers can learn software tools, the faster they can start using them and the more efficiently they can later work with them. Tutorials are supposed to help here. While in the early days of computing, mostly text tutorials were available, nowadays software developers can choose among a huge number of tutorials for almost any popular"}
{"full_text": "Bromochloro alkanes (BCAs) have been manufactured for use as flame retardants for decades and preliminary environmental risk screening suggests they are likely to behave similarly to polychlorinated alkanes (PCAs), subclasses of which are restricted as Stockholm Convention Persistent Organic Pollutants (POPs). BCAs have rarely been studied in the environment, though some evidence suggests they may migrate from treated-consumer materials into indoor dust, resulting in human exposure via inadvertent ingestion. In this study, BCA-C14 mixture standards were synthesized and used to validate an analytical method. This method relies on chloride-enhanced liquid chromatography-electrospray ionization-Orbitrap-high resolution mass spectrometry (LC-ESI-Orbitrap-HRMS) and a novel CP-Seeker integration software package for homologue detection and integration. Dust sample preparation via ultrasonic extraction, acidified silica clean-up and fractionation on neutral silica cartridges was found to be suitable for BCAs, with absolute recovery of individual homologues averaging 66 to 78% and coefficients of variation $\\le$10% in replicated spiking experiments (n=3). In addition, a total of 59 indoor dust samples from six countries including Australia (n=10), Belgium (n=10), Colombia (n=10), Japan (n=10), Thailand (n=10) and the United States of America (n=9) were analysed for BCAs. BCAs were detected in seven samples from the USA, with carbon chain lengths of C8, C10, C12, C14, C16, C18, C24 to C28, C30 and C31 observed overall, though not detected in samples from any other countries. Bromination of detected homologues in the indoor dust samples ranged from Br1-4 as well as Br7, while chlorine numbers ranged from Cl2-11. BCA-C18 were the most frequently detected, observed in each of the USA samples, while the most prevalent halogenation degrees were homologues of Br2 and Cl4-5. Broad estimations of BCA concentrations in the dust samples indicated that levels may approach those of other flame retardants in at least some instances. These findings suggest that development of quantification strategies and further investigation of environmental occurrence and health implications are needed.", "prompt": "Bromochloro alkanes (BCAs) have been manufactured for use as flame retardants for decades and preliminary environmental risk screening suggests they are likely to behave similarly to polychlorinated alkanes (PCAs), subclasses of which are restricted as Stockholm Convention Persistent Organic Pollutants (POPs", "orig": "). BCAs have rarely been studied in the environment, though some evidence suggests they may migrate from treated-consumer materials into indoor dust, resulting in human exposure via inadvertent ingestion. In this study, BCA-C14 mixture standards were synthesized and used to validate an analytical method. This method relies on chloride-enhanced liquid", "generated": "Bromochloro alkanes (BCAs) have been manufactured for use as flame retardants for decades and preliminary environmental risk screening suggests they are likely to behave similarly to polychlorinated alkanes (PCAs), subclasses of which are restricted as Stockholm Convention Persistent Organic Pollutants (POP)."}
{"full_text": "Context: CoRoT-2b is one of the most anomalously large exoplanet known. Given its large mass, its large radius cannot be explained by standard evolution models. Interestingly, the planet's parent star is an active, rapidly rotating solar-like star with a large fraction (7 to 20%) of spots. Aims: We want to provide constraints on the properties of the star-planet system and understand whether the planet's inferred large size may be due to a systematic error on the inferred parameters, and if not, how it may be explained. Methods: We combine stellar and planetary evolution codes based on all available spectroscopic and photometric data to obtain self-consistent constraints on the system parameters. Results: We find no systematic error in the stellar modeling (including spots and stellar activity) that would yield the required ~10% reduction in size for the star and thus the planet. Two classes of solutions are found: the usual main sequence solution for the star yields for the planet a mass of 3.67+/-0.13 Mjup, a radius of 1.55+/-0.03 Rjup for an age that is at least 130Ma, and should be less than 500Ma given the star's fast rotation and significant activity. We identify another class of solutions on the pre-main sequence, in which case the planet's mass is 3.45\\pm 0.27 Mjup, its radius is 1.50+/-0.06 Rjup for an age between 30 and 40 Ma. These extremely young solutions provide the simplest explanation for the planet's size which can then be matched by a simple contraction from an initially hot, expanded state, provided the atmospheric opacities are increased by a factor ~3 compared to usual assumptions for solar compositions atmospheres. Other solutions imply in any case that the present inflated radius of CoRoT-2b is transient and the result of an event that occurred less than 20 Ma ago: a giant impact with another Jupiter-mass planet, or interactions with another object in the system which caused a significant rise of the eccentricity followed by the rapid circularization of its orbit. Conclusions: Additional observations of CoRoT-2 that could help understanding this system include searches for infrared excess and the presence of a debris disk and searches for additional companions. The determination of a complete infrared lightcurve including both the primary and secondary transits would also be extremely valuable to constrain the planet's atmospheric properties and to determine the planet-to-star radius ratio in a manner less vulnerable to systematic errors due to stellar activity.", "prompt": "Context: CoRoT-2b is one of the most anomalously large exoplanet known. Given its large mass, its large radius cannot be explained by standard evolution models. Interestingly, the planet's parent star is an active, rapidly rotating solar-like star with a large fraction (7 to 20%)", "orig": "of spots. Aims: We want to provide constraints on the properties of the star-planet system and understand whether the planet's inferred large size may be due to a systematic error on the inferred parameters, and if not, how it may be explained. Methods: We combine stellar and planetary evolution codes based on all available", "generated": "CoRoT-2b is one of the most anomalously large exoplanet known. CoRoT-2b is one of the most anomalously large exoplanet known. CoRoT-2b is one of the most anomalously large exoplanet known. CoRoT-2b"}
{"full_text": "In the present contribution, we investigate first-order nonlinear systems of partial differential equations which are constituted of two parts: a system of conservation laws and non-conservative first order terms. Whereas the theory of first-order systems of conservation laws is well established and the conditions for the existence of supplementary conservation laws, and more specifically of an entropy supplementary conservation law for smooth solutions, well known, there exists so far no general extension to obtain such supplementary conservation laws when non-conservative terms are present. We propose a framework in order to extend the existing theory and show that the presence of non-conservative terms somewhat complexifies the problem since numerous combinations of the conservative and non-conservative terms can lead to a supplementary conservation law. We then identify a restricted framework in order to design and analyze physical models of complex fluid flows by means of computer algebra and thus obtain the entire ensemble of possible combination of conservative and non-conservative terms with the objective of obtaining specifically an entropy supplementary conservation law. The theory as well as developed computer algebra tool are then applied to a Baer-Nunziato two-phase flow model and to a multicomponent plasma fluid model. The first one is a first-order fluid model, with non-conservative terms impacting on the linearly degenerate field and requires a closure since there is no way to derive interfacial quantities from averaging principles and we need guidance in order to close the pressure and velocity of the interface and the thermodynamics of the mixture. The second one involves first order terms for the heavy species coupled to second order terms for the electrons, the non-conservative terms impact the genuinely nonlinear fields and the model can be rigorously derived from kinetic theory. We show how the theory allows to recover the whole spectrum of closures obtained so far in the literature for the two-phase flow system as well as conditions when one aims at extending the thermodynamics and also applies to the plasma case, where we recover the usual entropy supplementary equation, thus assessing the effectiveness and scope of the proposed theory.", "prompt": "In the present contribution, we investigate first-order nonlinear systems of partial differential equations which are constituted of two parts: a system of conservation laws and non-conservative first order terms. Whereas the theory of first-order systems of conservation laws is well established and the conditions for the existence of supplementary conservation laws, and more specifically of", "orig": "an entropy supplementary conservation law for smooth solutions, well known, there exists so far no general extension to obtain such supplementary conservation laws when non-conservative terms are present. We propose a framework in order to extend the existing theory and show that the presence of non-conservative terms somewhat complexifies the problem since numerous combinations of the", "generated": "In the present contribution, we investigate first-order nonlinear systems of partial differential equations which are constituted of two parts: a system of conservation laws and non-conservative first-order terms. Whereas the theory of first-order systems of conservation laws is well established and the conditions for the existence of supplementary conservation laws, and more specifically of"}
{"full_text": "Recent research has shown that surprisingly rich models of human activity can be learned from GPS (positional) data. However, most effort to date has concentrated on modeling single individuals or statistical properties of groups of people. Moreover, prior work focused solely on modeling actual successful executions (and not failed or attempted executions) of the activities of interest. We, in contrast, take on the task of understanding human interactions, attempted interactions, and intentions from noisy sensor data in a fully relational multi-agent setting. We use a real-world game of capture the flag to illustrate our approach in a well-defined domain that involves many distinct cooperative and competitive joint activities. We model the domain using Markov logic, a statistical-relational language, and learn a theory that jointly denoises the data and infers occurrences of high-level activities, such as a player capturing an enemy. Our unified model combines constraints imposed by the geometry of the game area, the motion model of the players, and by the rules and dynamics of the game in a probabilistically and logically sound fashion. We show that while it may be impossible to directly detect a multi-agent activity due to sensor noise or malfunction, the occurrence of the activity can still be inferred by considering both its impact on the future behaviors of the people involved as well as the events that could have preceded it. Further, we show that given a model of successfully performed multi-agent activities, along with a set of examples of failed attempts at the same activities, our system automatically learns an augmented model that is capable of recognizing success and failure, as well as goals of peoples actions with high accuracy. We compare our approach with other alternatives and show that our unified model, which takes into account not only relationships among individual players, but also relationships among activities over the entire length of a game, although more computationally costly, is significantly more accurate. Finally, we demonstrate that explicitly modeling unsuccessful attempts boosts performance on other important recognition tasks.", "prompt": "Recent research has shown that surprisingly rich models of human activity can be learned from GPS (positional) data. However, most effort to date has concentrated on modeling single individuals or statistical properties of groups of people. Moreover, prior work focused solely on modeling actual successful executions (and not failed or attempted executions) of the", "orig": "activities of interest. We, in contrast, take on the task of understanding human interactions, attempted interactions, and intentions from noisy sensor data in a fully relational multi-agent setting. We use a real-world game of capture the flag to illustrate our approach in a well-defined domain that involves many distinct cooperative and competitive joint activities.", "generated": "Recent research has shown that surprisingly rich models of human activity can be learned from GPS (positional) data. However, most effort to date has concentrated on modeling single individuals or statistical properties of groups of people. Moreover, prior work focused solely on modeling actual successful executions (and not failed or attempted executions) of these"}
{"full_text": "Technologies such as aerial photogrammetry allow production of 3D topographic data including complex environments such as urban areas. Therefore, it is possible to create High Resolution (HR) Digital Elevation Models (DEM) incorporating thin above ground elements influencing overland flow paths. Even though this category of big data has a high level of accuracy, there are still errors in measurements and hypothesis under DEM elaboration. Moreover, operators look for optimizing spatial discretization resolution in order to improve flood models computation time. Errors in measurement, errors in DEM generation, and operator choices for inclusion of this data within 2D hydraulic model, might influence results of flood models simulations. These errors and hypothesis may influence significantly flood modelling results variability. The purpose of this study is to investigate uncertainties related to (i) the own error of high resolution topographic data, and (ii) the modeller choices when including topographic data in hydraulic codes. The aim is to perform a Global Sensitivity Analysis (GSA) which goes through a Monte-Carlo uncertainty propagation, to quantify impact of uncertainties, followed by a Sobol' indices computation, to rank influence of identified parameters on result variability. A process using a coupling of an environment for parametric computation (Prom{\\'e}th{\\'e}e) and a code relying on 2D shallow water equations (FullSWOF 2D) has been developed (P-FS tool). The study has been performed over the lower part of the Var river valley using the estimated hydrograph of 1994 flood event. HR topographic data has been made available for the study area, which is 17.5 km 2 , by Nice municipality. Three uncertain parameters were studied: the measurement error (var. E), the level of details of above-ground element representation in DEM (buildings, sidewalks, etc.) (var. S), and the spatial discretization resolution (grid cell size for regular mesh) (var. R). Parameter var. E follows a probability density function, whereas parameters var. S and var. R. are discrete operator choices. Combining these parameters, a database of 2, 000 simulations has been produced using P-FS tool implemented on a high performance computing structure. In our study case, the output of interest is the maximal", "prompt": "Technologies such as aerial photogrammetry allow production of 3D topographic data including complex environments such as urban areas. Therefore, it is possible to create High Resolution (HR) Digital Elevation Models (DEM) incorporating thin above ground elements influencing overland flow paths. Even though this category of big data has", "orig": "a high level of accuracy, there are still errors in measurements and hypothesis under DEM elaboration. Moreover, operators look for optimizing spatial discretization resolution in order to improve flood models computation time. Errors in measurement, errors in DEM generation, and operator choices for inclusion of this data within 2D hydraulic model, might influence", "generated": "Technologies such as aerial photogrammetry allow production of 3D topographic data including complex environments such as urban areas. Therefore, it is possible to create High Resolution (HR) Digital Elevation Models (DEM) incorporating thin above ground elements influencing overland flow paths. Even though this category of big data has"}
{"full_text": "We tackle the problem of planning in nondeterministic domains, by presenting a new approach to conformant planning. Conformant planning is the problem of finding a sequence of actions that is guaranteed to achieve the goal despite the nondeterminism of the domain. Our approach is based on the representation of the planning domain as a finite state automaton. We use Symbolic Model Checking techniques, in particular Binary Decision Diagrams, to compactly represent and efficiently search the automaton. In this paper we make the following contributions. First, we present a general planning algorithm for conformant planning, which applies to fully nondeterministic domains, with uncertainty in the initial condition and in action effects. The algorithm is based on a breadth-first, backward search, and returns conformant plans of minimal length, if a solution to the planning problem exists, otherwise it terminates concluding that the problem admits no conformant solution. Second, we provide a symbolic representation of the search space based on Binary Decision Diagrams (BDDs), which is the basis for search techniques derived from symbolic model checking. The symbolic representation makes it possible to analyze potentially large sets of states and transitions in a single computation step, thus providing for an efficient implementation. Third, we present CMBP (Conformant Model Based Planner), an efficient implementation of the data structures and algorithm described above, directly based on BDD manipulations, which allows for a compact representation of the search layers and an efficient implementation of the search steps. Finally, we present an experimental comparison of our approach with the state-of-the-art conformant planners CGP, QBFPLAN and GPT. Our analysis includes all the planning problems from the distribution packages of these systems, plus other problems defined to stress a number of specific factors. Our approach appears to be the most effective: CMBP is strictly more expressive than QBFPLAN and CGP and, in all the problems where a comparison is possible, CMBP outperforms its competitors, sometimes by orders of magnitude.", "prompt": "We tackle the problem of planning in nondeterministic domains, by presenting a new approach to conformant planning. Conformant planning is the problem of finding a sequence of actions that is guaranteed to achieve the goal despite the nondeterminism of the domain. Our approach is based on the representation of the planning domain as", "orig": "a finite state automaton. We use Symbolic Model Checking techniques, in particular Binary Decision Diagrams, to compactly represent and efficiently search the automaton. In this paper we make the following contributions. First, we present a general planning algorithm for conformant planning, which applies to fully nondeterministic domains, with", "generated": "We tackle the problem of planning in nondeterministic domains, by presenting a new approach to conformant planning. Conformant planning is the problem of finding a sequence of actions that is guaranteed to achieve the goal despite the nondeterminism of the domain. Our approach is based on the representation of the planning domain as"}
{"full_text": "The aim of this paper is to give new insights about families of integrable systems lifting a Hamiltonian $S^1$-space. Specifically, we study one-parameter families $(M^4,\\omega,F_t=(J,H_t))_{0 \\leq t \\leq 1}$ of systems with a fixed Hamiltonian $S^1$-space $(M,\\omega,J)$ and which are semitoric for certain values of the parameter $t$, with a focus on such families in which one singular point undergoes a Hamiltonian-Hopf bifurcation (also called nodal trade in the context of semitoric systems, and more generally almost toric fibrations). Beyond semitoric systems, we also study families containing hypersemitoric systems, and we investigate the local theory of a nodal trade. Building on and generalizing the ideas of a previous paper, we show how such families can be used to find explicit semitoric systems with certain desired invariants (bundled in the marked semitoric polygon). This allows us to make progress on the semitoric minimal model program by understanding and coming up with explicit systems for each strictly minimal type (i.e. those not admitting any toric or semitoric type blowdown). In order to obtain these systems, we develop strategies for constructing and understanding explicit examples of semitoric (and hypersemitoric) systems in general. One strategy we make use of is to start from a well-understood system (such as a toric system) and to explicitly induce Hamiltonian-Hopf bifurcations to produce focus-focus singular points. This is an expanded version of the technique used in the aforementioned previous paper, in order to apply it to semitoric systems which include non-trivial isotropy spheres in the underlying $S^1$-space (i.e. $\\mathbb{Z}_k$-spheres), which occurs in several of the strictly minimal systems. In particular, we give an explicit one-parameter family of systems on $\\mathbb{CP}^2$ which transitions between being of toric type, semitoric type, and hypersemitoric type depending on the value of the parameter. We study this system at each stage, computing the marked semitoric polygon of the semitoric system and determining several properties of the hypersemitoric system, including the existence of a unique flap and two parabolic orbits. Furthermore, we study the transitions between these stages. We also come up with new explicit semitoric systems on all Hirzebruch surfaces which, together with the previous systems and the systems already contained in the literature, gives an explicit model for every type of strictly minimal system. Moreover, we show how to obtain every strictly minimal system by applying sequences of alternating toric type blowups and blowdowns to simple explicit systems. In particular, we obtain that every strictly minimal semitoric system is part of a family $(M,\\omega,F_t=(J,H_t))$ which is semitoric for all but a finite number of values of $t$, called a semitoric family.", "prompt": "The aim of this paper is to give new insights about families of integrable systems lifting a Hamiltonian $S^1$-space. Specifically, we study one-parameter families $(M^4,\\omega,F_t=(J,H_t))_{0 \\leq t \\leq 1}$ of systems with", "orig": "a fixed Hamiltonian $S^1$-space $(M,\\omega,J)$ and which are semitoric for certain values of the parameter $t$, with a focus on such families in which one singular point undergoes a Hamiltonian-Hopf bifurcation (also called nodal trade in the context of", "generated": "The aim of this paper is to give new insights about families of integrable systems lifting a Hamiltonian $S^1$-space. Specifically, we study one-parameter families $(M^4,\\omega,F_t=(J,H_t))_{0 \\leq t \\leq 1}$ of systems with"}
{"full_text": "In this paper, we introduce a new learning strategy based on a seminal idea of Mojirsheibani (1999, 2000, 2002a, 2002b), who proposed a smart method for combining several classifiers, relying on a consensus notion. In many aggregation methods, the prediction for a new observation x is computed by building a linear or convex combination over a collection of basic estimators r1(x),. .. , rm(x) previously calibrated using a training data set. Mojirsheibani proposes to compute the prediction associated to a new observation by combining selected outputs of the training examples. The output of a training example is selected if some kind of consensus is observed: the predictions computed for the training example with the different machines have to be \"similar\" to the prediction for the new observation. This approach has been recently extended to the context of regression in Biau et al. (2016). In the original scheme, the agreement condition is actually required to hold for all individual estimators, which appears inadequate if there is one bad initial estimator. In practice, a few disagreements are allowed ; for establishing the theoretical results, the proportion of estimators satisfying the condition is required to tend to 1. In this paper, we propose an alternative procedure, mixing the previous consensus ideas on the predictions with the Euclidean distance computed between entries. This may be seen as an alternative approach allowing to reduce the effect of a possibly bad estimator in the initial list, using a constraint on the inputs. We prove the consistency of our strategy in classification and in regression. We also provide some numerical experiments on simulated and real data to illustrate the benefits of this new aggregation method. On the whole, our practical study shows that our method may perform much better than the original combination technique, and, in particular, exhibit far less variance. We also show on simulated examples that this procedure mixing inputs and outputs is still robust to high dimensional inputs.", "prompt": "In this paper, we introduce a new learning strategy based on a seminal idea of Mojirsheibani (1999, 2000, 2002a, 2002b), who proposed a smart method for combining several classifiers, relying on a consensus notion. In many aggregation methods, the prediction for", "orig": "a new observation x is computed by building a linear or convex combination over a collection of basic estimators r1(x),..., rm(x) previously calibrated using a training data set. Mojirsheibani proposes to compute the prediction associated to a new observation by combining selected outputs of the training examples. The output", "generated": "In this paper, we introduce a new learning strategy based on a seminal idea of Mojirsheibani (1999, 2000, 2002a, 2002b), who proposed a smart method for combining several classifiers, relying on a consensus notion. In many aggregation methods, the prediction focuses"}
{"full_text": "Voting is a general method for aggregating the preferences of multiple agents. Each agent ranks all the possible alternatives, and based on this, an aggregate ranking of the alternatives (or at least a winning alternative) is produced. However, when there are many alternatives, it is impractical to simply ask agents to report their complete preferences. Rather, the agents preferences, or at least the relevant parts thereof, need to be elicited. This is done by asking the agents a (hopefully small) number of simple queries about their preferences, such as comparison queries, which ask an agent to compare two of the alternatives. Prior work on preference elicitation in voting has focused on the case of unrestricted preferences. It has been shown that in this setting, it is sometimes necessary to ask each agent (almost) as many queries as would be required to determine an arbitrary ranking of the alternatives. In contrast, in this paper, we focus on single-peaked preferences. We show that such preferences can be elicited using only a linear number of comparison queries, if either the order with respect to which preferences are single-peaked is known, or at least one other agents complete preferences are known. We show that using a sublinear number of queries does not suffice. We also consider the case of cardinally single-peaked preferences. For this case, we show that if the alternatives cardinal positions are known, then an agents preferences can be elicited using only a logarithmic number of queries; however, we also show that if the cardinal positions are not known, then a sublinear number of queries does not suffice. We present experimental results for all elicitation algorithms. We also consider the problem of only eliciting enough information to determine the aggregate ranking, and show that even for this more modest objective, a sublinear number of queries per agent does not suffice for known ordinal or unknown cardinal positions. Finally, we discuss whether and how these techniques can be applied when preferences are almost single-peaked.", "prompt": "Voting is a general method for aggregating the preferences of multiple agents. Each agent ranks all the possible alternatives, and based on this, an aggregate ranking of the alternatives (or at least a winning alternative) is produced. However, when there are many alternatives, it is impractical to simply ask agents to report", "orig": "their complete preferences. Rather, the agents preferences, or at least the relevant parts thereof, need to be elicited. This is done by asking the agents a (hopefully small) number of simple queries about their preferences, such as comparison queries, which ask an agent to compare two of the alternatives. Prior work on preference", "generated": "Voting is a general method for aggregating the preferences of multiple agents. Each agent ranks all the possible alternatives, and based on this, an aggregate ranking of the alternatives (or at least a winning alternative) is produced. However, when there are many alternatives, it is impractical to simply ask agents to report"}
{"full_text": "Massive stars are of interest as progenitors of super novae, i.e. neutron stars and black holes, which can be sources of gravitational waves. Recent population synthesis models can predict neutron star and gravitational wave observations but deal with a fixed super nova rate or an assumed initial mass function for the population of massive stars. Here we investigate those massive stars, which are supernova progenitors, i.e. with O and early B type stars, and also all super giants within 3kpc. We restrict our sample to those massive stars detected both in 2MASS and observed by Hipparcos, i.e. only those stars with parallax and precise photometry. To determine the luminosities we calculated the extinctions from published multi-colour photometry, spectral types, luminosity class, all corrected for multiplicity and recently revised Hipparcos distances. We use luminosities and temperatures to estimate the masses and ages of these stars using different models from different authors. Having estimated the luminosities of all our stars within 3kpc, in particular for all O- and early B-type stars, we have determined the median and mean luminosities for all spectral types for luminosity classes I, III, and V. Our luminosity values for super giants deviate from earlier results: Previous work generally overestimates distances and luminosities compared to our data, this is likely due to Hipparcos parallaxes (generally more accurate and larger than previous ground-based data) and the fact that many massive stars have recently been resolved into multiples of lower masses and luminosities. From luminosities and effective temperatures we derived masses and ages using mass tracks and isochrones from different authors. From masses and ages we estimated lifetimes and derived a lower limit for the supernova rate of ~20 events/Myr averaged over the next 10 Myrs within 600 pc from the sun. These data are then used to search for areas in the sky with higher likelihood for a supernova or gravitational wave event (like OB associations).", "prompt": "Massive stars are of interest as progenitors of super novae, i.e. neutron stars and black holes, which can be sources of gravitational waves. Recent population synthesis models can predict neutron star and gravitational wave observations but deal with a fixed super nova rate or an assumed initial mass function for the population of massive stars", "orig": ". Here we investigate those massive stars, which are supernova progenitors, i.e. with O and early B type stars, and also all super giants within 3kpc. We restrict our sample to those massive stars detected both in 2MASS and observed by Hipparcos, i.e. only those", "generated": "Massive stars are of interest as progenitors of super novae, i.e. neutron stars and black holes, which can be sources of gravitational waves. Recent population synthesis models can predict neutron star and gravitational wave observations but deal with a fixed super nova rate or an assumed initial mass function for the population of massive stars"}
{"full_text": "We present multi-component photoionization models allowing for local density inhomogeneities in the NLR to interpret the emission line spectra of Seyfert 2 galaxies. This approach leads to a successful match of a large set of line intensities from the UV to the NIR. In particular, the hitherto elusive NIR features [SIII]9062+9531 as well as high-ionization lines like [FeVII] are consistently fitted. The predictions of CIII] and CIV are considerably improved. From the detailed analysis of single-component photoionization models we derive the minimal radial extent of the NLR and the necessary span in density. Furthermore, we determine constraints on suggestions made about the role of matter-bounded clouds, and on proposed explanations for large [OIII]4363/5007 ratios (the so-called `temperature problem'), and assess the usability of some emission-line ratios as indicators of the ionization parameter. We find that a systematic variation of the cloud column densities in a population of matter-bounded clouds is inconsistent with the trends and correlations exhibited by the emission lines in the diagnostic diagrams. Concerning the temperature problem, the only possibility that leads to an overall consistency with the strengths of all other observed emission lines is subsolar metal abundances (as compared to e.g. the presence of dust, the existence of a high-density component, or matter-bounded clouds). In addition, the consequences of the presence of (Galactic-ISM-like) dust internal to the clouds were investigated. These models alleviate the [OIII]-ratio problem but did not lead to overall consistent fits. In our final model series, the NLR is composed of a mixture of metal-depleted (0.5 x solar) clouds with a radius-independent range in densities (10^2 to 10^5 cm^-3) distributed over a range of distances from the nucleus (galactocentric radii from at least $\\sim$ 10^{20} cm to 10^{21.5} cm, for $Q_{tot} = 10^{54}$ s^{-1}). In order to encompass the observed range of each line intensity relative to H$\\beta$, it turns out to be necessary to vary the spectral energy distribution incident on the clouds, qualitatively confirming the findings of Ho et al. (1993). We found a successful continuum sequence by adding an increasing contribution of a hot black body ($T \\approx 200000$ K) to a steep powerlaw ($\\alpha_{uv-x} \\approx -2$). These continua imply that low and high-excitation objects differ in the strength but not in the basic shape of the EUV bump.", "prompt": "We present multi-component photoionization models allowing for local density inhomogeneities in the NLR to interpret the emission line spectra of Seyfert 2 galaxies. This approach leads to a successful match of a large set of line intensities from the UV to the NIR. In particular, the hitherto", "orig": "elusive NIR features [SIII]9062+9531 as well as high-ionization lines like [FeVII] are consistently fitted. The predictions of CIII] and CIV are considerably improved. From the detailed analysis of single-component photoionization models we derive the minimal radial extent of the NLR and", "generated": "We present multi-component photoionization models allowing for local density inhomogeneities in the NLR to interpret the emission line spectra of Seyfert 2 galaxies. This approach leads to a successful match of a large set of line intensities from the UV to the NIR. In particular, the hitherto"}
{"full_text": "A highly sheared dense aqueous suspension of granular cornstarch particles displays rich nonlinear rheology. We had previously demonstrated the growth and onset of interfacial instabilities when shear-thinning cornstarch suspensions were displaced by a Newtonian fluid, and had suggested methods to maximise displacement efficiency [Palak, R. Sathayanath, S. K. Kalpathy and R. Bandyopadhyay, Colloids Surf. A Physicochem. Eng. Asp., 629 (2021) 127405]. In the present work, we explore the miscible displacement of a dense aqueous cornstarch suspension in its discontinuous shear-thickening regime in a quasi-two-dimensional radial Hele-Shaw cell. We systematically study the growth kinetics of the inner interface between water and the cornstarch suspension, and also of the outer interface between the suspension and air. In addition to the growth of interfacial instabilities at the inner interface, we observe a transient withdrawal of the suspension and the formation of fingering instabilities at the outer interface. We demonstrate that these `reverse fingering' instabilities are extremely sensitive to the injection flow rate of water, the gap of the Hele-Shaw cell and the concentration of the displaced cornstarch suspension, {and emerge irrespective of immiscibility between the fluid pair. We believe that as the cornstarch suspension dilates due to the high shear rate imposed by the displacing fluid, the outer suspension-air interface responds with a restoring force, resulting in the penetration of air into the suspension and the formation of reverse fingers. We note that the growth of reverse fingers significantly reduces the displacement efficiency of the suspension. Finally, we demonstrate a correlation in the growth of inner and outer interfacial patterns by computing the velocity with which stresses propagate in the confined dense suspension. Our findings are useful in understanding the flow of granular materials through constrained geometries and can be extended to study stress propagation in shear-thickening materials due to a sudden imposition of high shear rate, such as in impact behaviour.", "prompt": "A highly sheared dense aqueous suspension of granular cornstarch particles displays rich nonlinear rheology. We had previously demonstrated the growth and onset of interfacial instabilities when shear-thinning cornstarch suspensions were displaced by a Newtonian fluid, and had suggested methods to maximise displacement efficiency [Pal", "orig": "ak, R. Sathayanath, S. K. Kalpathy and R. Bandyopadhyay, Colloids Surf. A Physicochem. Eng. Asp., 629 (2021) 127405]. In the present work, we explore the miscible displacement of a dense", "generated": "A highly sheared dense aqueous suspension of granular cornstarch particles displays rich nonlinear rheology. We had previously demonstrated the growth and onset of interfacial instabilities when shear-thinning cornstarch suspensions were displaced by a Newtonian fluid, and had suggested methods to maximise displacement efficiency [Pal"}
{"full_text": "In this paper we propagate a large deviations approach for proving limit theory for (generally) multivariate time series with heavy tails. We make this notion precise by introducing regularly varying time series. We provide general large deviation results for functionals acting on a sample path and vanishing in some neighborhood of the origin. We study a variety of such functionals, including large deviations of random walks, their suprema, the ruin functional, and further derive weak limit theory for maxima, point processes, cluster functionals and the tail empirical process. One of the main results of this paper concerns bounds for the ruin probability in various heavy-tailed models including GARCH, stochastic volatility models and solutions to stochastic recurrence equations. 1. Preliminaries and basic motivation In the last decades, a lot of efforts has been put into the understanding of limit theory for dependent sequences, including Markov chains (Meyn and Tweedie [42]), weakly dependent sequences (Dedecker et al. [21]), long-range dependent sequences (Doukhan et al. [23], Samorodnitsky [54]), empirical processes (Dehling et al. [22]) and more general structures (Eberlein and Taqqu [25]), to name a few references. A smaller part of the theory was devoted to limit theory under extremal dependence for point processes, maxima, partial sums, tail empirical processes. Resnick [49, 50] started a systematic study of the relations between the convergence of point processes, sums and maxima, see also Resnick [51] for a recent account. He advocated the use of multivariate regular variation as a flexible tool to describe heavy-tail phenomena combined with advanced continuous mapping techniques. For example, maxima and sums are understood as functionals acting on an underlying point process, if the point process converges these functionals converge as well and their limits are described in terms of the points of the limiting point process. Davis and Hsing [13] recognized the power of this approach for limit theory of point processes, maxima, sums, and large deviations for dependent regularly varying processes, i.e., stationary sequences whose finite-dimensional distributions are regularly varying with the same index. Before [13], limit theory for particular regularly varying stationary sequences was studied for the sample mean, maxima, sample autocovariance and autocorrelation functions of linear and bilinear processes with iid regularly varying noise and extreme value theory was considered for regularly varying ARCH processes and solutions to stochastic recurrence equation, see Rootz\\'en [53], Davis and 1991 Mathematics Subject Classification. Primary 60F10, 60G70, secondary 60F05. Key words and phrases. Large deviation principle, regularly varying processes, central limit theorem, ruin probabilities, GARCH.", "prompt": "In this paper we propagate a large deviations approach for proving limit theory for (generally) multivariate time series with heavy tails. We make this notion precise by introducing regularly varying time series. We provide general large deviation results for functionals acting on a sample path and vanishing in some neighborhood of the origin. We", "orig": "study a variety of such functionals, including large deviations of random walks, their suprema, the ruin functional, and further derive weak limit theory for maxima, point processes, cluster functionals and the tail empirical process. One of the main results of this paper concerns bounds for the ruin probability in various heavy-tailed", "generated": "In this paper we propagate a large deviations approach for proving limit theory for (generally) multivariate time series with heavy tails. We make this notion precise by introducing regularly varying time series. We provide general large deviation results for functionals acting on a sample path and vanishing in some neighborhood of the origin."}
{"full_text": "Gas behavior in systems at microscale has been receiving significant attention from researchers in the last two decades [1-4]. Today, there is an enhanced emphasis on developing new experimental techniques to capture the local temperature profiles in gases at rarefied conditions. The main underlying reason behind this focus is the interesting physics exhibited by gases at these rarefied conditions, especially in the transition regime. There is the onset of local thermodynamic disequilibrium, which manifests as velocity slip and temperature jump [1-4] at the wall. However, there is limited experimental evidence on understanding these aforementioned phenomena. With the advances in experimental facilities, it is today possible, at least in principle, to map the local temperature profiles in gases at rarefied conditions. Molecular tagging approach is one such technique which has shown the potential to map the temperature profile in low pressure conditions [5]. In molecular tagging approach, a very small percentage of tracer molecules are introduced into the gas of interest, referred as carrier gas. In gas flow studies, the typical tracers employed are acetone and biacetyl. These tracer molecules, assumed to be in equilibrium with the carrier gas, are excited with a source of energy at a specific wavelength, typically a laser. The excited molecules are unstable and tend to de-excite in a radiative and non-radiative manner, which is manifested as fluorescence and phosphorescence. Following the deformation with time of a tagged line permits to obtain the flow velocity. In addition, the dependence of the phosphorescence and fluorescence intensity to the gas temperature could also allow to use this technique for local temperature measurements. The objective of this study is to develop an experimental setup capable of simultaneously mapping the wall and fluid near-wall temperatures with the final goal to measure temperature jump at the wall when rarefied conditions are reached. The originality of this setup shown in Figure 1 is to couple surface temperature measurements using an infrared camera with Molecular Tagging Thermometry (MTT) for gas temperature measurements. The bottom wall of the channel will be made of Sapphire substrate of 650 $\\mu$m thickness coated with a thin film of Indium Tin Oxide (ITO). The average roughness of this ITO layer is about 3 nm. The top wall of the channel will be made of SU8 and bonded with the bottom wall with a layer of PDMS. The channel will be filled in with acetone vapor,", "prompt": "Gas behavior in systems at microscale has been receiving significant attention from researchers in the last two decades [1-4]. Today, there is an enhanced emphasis on developing new experimental techniques to capture the local temperature profiles in gases at rarefied conditions. The main underlying reason behind this focus is the interesting physics exhibited by", "orig": "gases at these rarefied conditions, especially in the transition regime. There is the onset of local thermodynamic disequilibrium, which manifests as velocity slip and temperature jump [1-4] at the wall. However, there is limited experimental evidence on understanding these aforementioned phenomena. With the advances in experimental facilities, it", "generated": "Gas behavior in systems at microscale has been receiving significant attention from researchers in the last two decades [1-4]. Today, there is an enhanced emphasis on developing new experimental techniques to capture the local temperature profiles in gases at rarefied conditions. The main underlying reason behind this focus is the interesting physics exhibited by"}
{"full_text": "Solar flare prediction is a central problem in space weather forecasting and has captivated the attention of a wide spectrum of researchers due to recent advances in both remote sensing as well as machine learning and deep learning approaches. The experimental findings based on both machine and deep learning models reveal significant performance improvements for task specific datasets. Along with building models, the practice of deploying such models to production environments under operational settings is a more complex and often time-consuming process which is often not addressed directly in research settings. We present a set of new heuristic approaches to train and deploy an operational solar flare prediction system for $\\geq$M1.0-class flares with two prediction modes: full-disk and active region-based. In full-disk mode, predictions are performed on full-disk line-of-sight magnetograms using deep learning models whereas in active region-based models, predictions are issued for each active region individually using multivariate time series data instances. The outputs from individual active region forecasts and full-disk predictors are combined to a final full-disk prediction result with a meta-model. We utilized an equal weighted average ensemble of two base learners' flare probabilities as our baseline meta learner and improved the capabilities of our two base learners by training a logistic regression model. The major findings of this study are: (i) We successfully coupled two heterogeneous flare prediction models trained with different datasets and model architecture to predict a full-disk flare probability for next 24 hours, (ii) Our proposed ensembling model, i.e., logistic regression, improves on the predictive performance of two base learners and the baseline meta learner measured in terms of two widely used metrics True Skill Statistic (TSS) and Heidke Skill core (HSS), and (iii) Our result analysis suggests that the logistic regression-based ensemble (Meta-FP) improves on the full-disk model (base learner) by $\\sim9\\%$ in terms TSS and $\\sim10\\%$ in terms of HSS. Similarly, it improves on the AR-based model (base learner) by $\\sim17\\%$ and $\\sim20\\%$ in terms of TSS and HSS respectively. Finally, when compared to the baseline meta model, it improves on TSS by $\\sim10\\%$ and HSS by $\\sim15\\%$.", "prompt": "Solar flare prediction is a central problem in space weather forecasting and has captivated the attention of a wide spectrum of researchers due to recent advances in both remote sensing as well as machine learning and deep learning approaches. The experimental findings based on both machine and deep learning models reveal significant performance improvements for task specific datasets. Along with", "orig": "building models, the practice of deploying such models to production environments under operational settings is a more complex and often time-consuming process which is often not addressed directly in research settings. We present a set of new heuristic approaches to train and deploy an operational solar flare prediction system for $\\geq$M1.0-class flares", "generated": "Solar flare prediction is a central problem in space weather forecasting and has captivated the attention of a wide spectrum of researchers due to recent advances in both remote sensing as well as machine learning and deep learning approaches. The experimental findings based on both machine and deep learning models reveal significant performance improvements for task-specific datasets. Along with"}
{"full_text": "With the growing importance of corporate viral marketing campaigns on online social networks, the interest in studies of influence propagation through networks is higher than ever. In a viral marketing campaign, a firm initially targets a small set of pioneers and hopes that they would influence a sizeable fraction of the population by diffusion of influence through the network. In general, any marketing campaign might fail to go viral in the first try. As such, it would be useful to have some guide to evaluate the effectiveness of the campaign and judge whether it is worthy of further resources, and in case the campaign has potential, how to hit upon a good pioneer who can make the campaign go viral. In this paper, we present a diffusion model developed by enriching the generalized random graph (a.k.a. configuration model) to provide insight into these questions. We offer the intuition behind the results on this model, rigorously proved in Blaszczyszyn & Gaurav(2013), and illustrate them here by taking examples of random networks having prototypical degree distributions - Poisson degree distribution, which is commonly used as a kind of benchmark, and Power Law degree distribution, which is normally used to approximate the real-world networks. On these networks, the members are assumed to have varying attitudes towards propagating the information. We analyze three cases, in particular - (1) Bernoulli transmissions, when a member influences each of its friend with probability p; (2) Node percolation, when a member influences all its friends with probability p and none with probability 1-p; (3) Coupon-collector transmissions, when a member randomly selects one of his friends K times with replacement. We assume that the configuration model is the closest approximation of a large online social network, when the information available about the network is very limited. The key insight offered by this study from a firm's perspective is regarding how to evaluate the effectiveness of a marketing campaign and do cost-benefit analysis by collecting relevant statistical data from the pioneers it selects. The campaign evaluation criterion is informed by the observation that if the parameters of the underlying network and the campaign effectiveness are such that the campaign can indeed reach a significant fraction of the population, then the set of good pioneers also forms a significant fraction of the population. Therefore, in such a case, the firms can even adopt the naive strategy of repeatedly picking and targeting some number of pioneers at random from the population. With this strategy, the probability of them picking a good pioneer will increase geometrically fast with the number of tries.", "prompt": "With the growing importance of corporate viral marketing campaigns on online social networks, the interest in studies of influence propagation through networks is higher than ever. In a viral marketing campaign, a firm initially targets a small set of pioneers and hopes that they would influence a sizeable fraction of the population by diffusion of influence through the network", "orig": ". In general, any marketing campaign might fail to go viral in the first try. As such, it would be useful to have some guide to evaluate the effectiveness of the campaign and judge whether it is worthy of further resources, and in case the campaign has potential, how to hit upon a good pioneer who can make the", "generated": "With the growing importance of corporate viral marketing campaigns on online social networks, the interest in studies of influence propagation through networks is higher than ever. In a viral marketing campaign, a firm initially targets a small set of pioneers and hopes that they would influence a sizeable fraction of the population by diffusion of influence through the network"}
{"full_text": "This PhD thesis is devoted to deterministic study of the turbulence in the Navier- Stokes equations. The thesis is divided in four independent chapters.The first chapter involves a rigorous discussion about the energy's dissipation law, proposed by theory of the turbulence K41, in the deterministic setting of the homogeneous and incompressible Navier-Stokes equations, with a stationary external force (the force only depends of the spatial variable) and on the whole space R3. The energy's dissipation law, also called the Kolmogorov's dissipation law, characterizes the energy's dissipation rate (in the form of heat) of a turbulent fluid and this law was developed by A.N. Kolmogorov in 1941. However, its deduction (which uses mainly tools of statistics) is not fully understood until our days and then an active research area consists in studying this law in the rigorous framework of the Navier-Stokes equations which describe in a mathematical way the fluids motion and in particular the movement of turbulent fluids. In this setting, the purpose of this chapter is to highlight the fact that if we consider the Navier-Stokes equations on R3 then certain physical quantities, necessary for the study of the Kolmogorov's dissipation law, have no a rigorous definition and then to give a sense to these quantities we suggest to consider the Navier-Stokes equations with an additional damping term. In the framework of these damped equations, we obtain some estimates for the energy's dissipation rate according to the Kolmogorov's dissipation law.In the second chapter we are interested in study the stationary solutions of the damped Navier- Stokes introduced in the previous chapter. These stationary solutions are a particular type of solutions which do not depend of the temporal variable and their study is motivated by the fact that we always consider the Navier-Stokes equations with a stationary external force. In this chapter we study two properties of the stationary solutions : the first property concerns the stability of these solutions where we prove that if we have a control on the external force then all non stationary solution (with depends of both spatial and temporal variables) converges toward a stationary solution. The second property concerns the decay in spatial variable of the stationary solutions. These properties of stationary solutions are a consequence of the damping term introduced in the Navier-Stokes equations.In the third chapter we still study the stationary solutions of Navier-Stokes equations but now we consider the classical equations (without any additional damping term). The purpose of this chapter is to study an other problem related to the deterministic description of the turbulence : the frequency decay of the stationary solutions. Indeed, according to the K41 theory, if the fluid is in a laminar setting then the stationary solutions of the Navier-Stokes equations must exhibit a exponential frequency decay which starts at lows frequencies. But, if the fluid is in a turbulent setting then this exponential frequency decay must be observed only at highs frequencies. In this chapter, using some Fourier analysis tools, we give a precise description of this exponential frequency decay in the laminar and in the turbulent setting.In the fourth and last chapter we return to the stationary solutions of the classical Navier-Stokes equations and we study the uniqueness of these solutions in the particular case without any external force. Following some ideas of G. Seregin, we study the uniqueness of these solutions first in the framework of Lebesgue spaces of and then in the a general framework of Morrey spaces.", "prompt": "This PhD thesis is devoted to deterministic study of the turbulence in the Navier- Stokes equations. The thesis is divided in four independent chapters.The first chapter involves a rigorous discussion about the energy's dissipation law, proposed by theory of the turbulence K41, in the deterministic setting of the homogeneous and incompressible Nav", "orig": "ier-Stokes equations, with a stationary external force (the force only depends of the spatial variable) and on the whole space R3. The energy's dissipation law, also called the Kolmogorov's dissipation law, characterizes the energy's dissipation rate (in the form of heat) of", "generated": "This PhD thesis is devoted to deterministic study of the turbulence in the Navier-Stokes equations. The thesis is divided in four independent chapters. The first chapter involves a rigorous discussion about the energy's dissipation law, proposed by theory of the turbulence K41, in the deterministic setting of the homogeneous and incompressible"}
{"full_text": "The celebrated H\\\"{o}rmander condition is a sufficient (and nearly necessary) condition for a second-order linear Kolmogorov partial differential equation (PDE) with smooth coefficients to be hypoelliptic. As a consequence, the solutions of Kolmogorov PDEs are smooth at all positive times if the coefficients of the PDE are smooth and satisfy H\\\"{o}rmander's condition even if the initial function is only continuous but not differentiable. First-order linear Kolmogorov PDEs with smooth coefficients do not have this smoothing effect but at least preserve regularity in the sense that solutions are smooth if their initial functions are smooth. In this article, we consider the intermediate regime of nonhypoelliptic second-order Kolmogorov PDEs with smooth coefficients. The main observation of this article is that there exist counterexamples to regularity preservation in that case. More precisely, we give an example of a second-order linear Kolmogorov PDE with globally bounded and smooth coefficients and a smooth initial function with compact support such that the unique globally bounded viscosity solution of the PDE is not even locally H\\\"{o}lder continuous. From the perspective of probability theory, the existence of this example PDE has the consequence that there exists a stochastic differential equation (SDE) with globally bounded and smooth coefficients and a smooth function with compact support which is mapped by the corresponding transition semigroup to a function which is not locally H\\\"{o}lder continuous. In other words, degenerate noise can have a roughening effect. A further implication of this loss of regularity phenomenon is that numerical approximations may converge without any arbitrarily small polynomial rate of convergence to the true solution of the SDE. More precisely, we prove for an example SDE with globally bounded and smooth coefficients that the standard Euler approximations converge to the exact solution of the SDE in the strong and numerically weak sense, but at a rate that is slower then any power law.", "prompt": "The celebrated H\\\"{o}rmander condition is a sufficient (and nearly necessary) condition for a second-order linear Kolmogorov partial differential equation (PDE) with smooth coefficients to be hypoelliptic. As a consequence, the solutions of Kolmogorov PDEs are", "orig": "smooth at all positive times if the coefficients of the PDE are smooth and satisfy H\\\"{o}rmander's condition even if the initial function is only continuous but not differentiable. First-order linear Kolmogorov PDEs with smooth coefficients do not have this smoothing effect but at least preserve regular", "generated": "The celebrated H\\\"{o}rmander condition is a sufficient (and nearly necessary) condition for a second-order linear Kolmogorov partial differential equation (PDE) with smooth coefficients to be hypoelliptic. As a consequence, the solutions of Kolmogorov PDEs are"}
{"full_text": "Graphs and various graph-like combinatorial structures, such as preorders and hypergraphs, are ubiquitous in programming. This paper focuses on representing graphs in a purely functional programming language like Haskell. There are several existing approaches; one of the most recently developed ones is the \"algebraic graphs\" approach (2017). It uses an algebraic data type to represent graphs and has attracted users, including from industry, due to its emphasis on equational reasoning and making a common class of bugs impossible by eliminating internal invariants. The previous formulation of algebraic graphs did not support edge labels, which was a serious practical limitation. In this paper, we redesign the main algebraic data type and remove this limitation. We follow a fairly standard approach of parameterising a data structure with a semiring of edge labels. The new formulation is both more general and simpler: the two operations for composing graphs used in the previous work can now be obtained from a single operation by fixing the semiring parameter to zero and one, respectively. By instantiating the new data type with different semirings, and working out laws for interpreting the resulting expression trees, we discover an unusual algebraic structure, which we call \"united monoids\", that is, a pair of monoids whose unit elements coincide. We believe that it is worth studying united monoids in their full generality, going beyond the graphs which prompted their discovery. To that end, we characterise united monoids with a minimal set of axioms, prove a few basic theorems, and discuss several notable examples. We validate the presented approach by implementing it in the open-source *algebraic-graphs* library. Our theoretical contributions are supported by proofs that are included in the paper and have also been machine-checked in Agda. By extending algebraic graphs with support for edge labels, we make them suitable for a much larger class of possible applications. By studying united monoids, we provide a theoretical foundation for further research in this area.", "prompt": "Graphs and various graph-like combinatorial structures, such as preorders and hypergraphs, are ubiquitous in programming. This paper focuses on representing graphs in a purely functional programming language like Haskell. There are several existing approaches; one of the most recently developed ones is the \"algebraic graphs\" approach (201", "orig": "7). It uses an algebraic data type to represent graphs and has attracted users, including from industry, due to its emphasis on equational reasoning and making a common class of bugs impossible by eliminating internal invariants. The previous formulation of algebraic graphs did not support edge labels, which was a serious practical limitation. In", "generated": "Graphs and various graph-like combinatorial structures, such as preorders and hypergraphs, are ubiquitous in programming. This paper focuses on representing graphs in a purely functional programming language like Haskell. There are several existing approaches; one of the most recently developed ones is the \"algebraic graphs\" approach (201"}
{"full_text": "Background Weight loss trajectories after bariatric surgery vary widely between individuals, and predicting weight loss before the operation remains challenging. We aimed to develop a model using machine learning to provide individual preoperative prediction of 5-year weight loss trajectories after surgery. Methods In this multinational retrospective observational study we enrolled adult participants (aged $\\ge$18 years) from ten prospective cohorts (including ABOS [NCT01129297], BAREVAL [NCT02310178], the Swedish Obese Subjects study, and a large cohort from the Dutch Obesity Clinic [Nederlandse Obesitas Kliniek]) and two randomised trials (SleevePass [NCT00793143] and SM-BOSS [NCT00356213]) in Europe, the Americas, and Asia, with a 5 year followup after Roux-en-Y gastric bypass, sleeve gastrectomy, or gastric band. Patients with a previous history of bariatric surgery or large delays between scheduled and actual visits were excluded. The training cohort comprised patients from two centres in France (ABOS and BAREVAL). The primary outcome was BMI at 5 years. A model was developed using least absolute shrinkage and selection operator to select variables and the classification and regression trees algorithm to build interpretable regression trees. The performances of the model were assessed through the median absolute deviation (MAD) and root mean squared error (RMSE) of BMI. Findings10 231 patients from 12 centres in ten countries were included in the analysis, corresponding to 30 602 patient-years. Among participants in all 12 cohorts, 7701 (75$\\bullet$3%) were female, 2530 (24$\\bullet$7%) were male. Among 434 baseline attributes available in the training cohort, seven variables were selected: height, weight, intervention type, age, diabetes status, diabetes duration, and smoking status. At 5 years, across external testing cohorts the overall mean MAD BMI was 2$\\bullet$8 kg/m${}^2$ (95% CI 2$\\bullet$6-3$\\bullet$0) and mean RMSE BMI was 4$\\bullet$7 kg/m${}^2$ (4$\\bullet$4-5$\\bullet$0), and the mean difference between predicted and observed BMI was-0$\\bullet$3 kg/m${}^2$ (SD 4$\\bullet$7). This model is incorporated in an easy to use and interpretable web-based prediction tool to help inform clinical decision before surgery. InterpretationWe developed a machine learning-based model, which is internationally validated, for predicting individual 5-year weight loss trajectories after three common bariatric interventions.", "prompt": "Background Weight loss trajectories after bariatric surgery vary widely between individuals, and predicting weight loss before the operation remains challenging. We aimed to develop a model using machine learning to provide individual preoperative prediction of 5-year weight loss trajectories after surgery. Methods In this multinational retrospective observational study we enrolled adult participants (aged $\\", "orig": "ge$18 years) from ten prospective cohorts (including ABOS [NCT01129297], BAREVAL [NCT02310178], the Swedish Obese Subjects study, and a large cohort from the Dutch Obesity Clinic [Nederlandse Obesitas Kliniek]) and two randomised trials (S", "generated": "Background Weight loss trajectories after bariatric surgery vary widely between individuals, and predicting weight loss before the operation remains challenging. We aimed to develop a model using machine learning to provide individual preoperative prediction of 5-year weight loss trajectories after surgery. \n\nMethods In this multinational retrospective observational study, we enrolled adult participants ("}
{"full_text": "Context. Classical Be stars are hot non-supergiant stars surrounded by a gaseous circumstellar disk that is responsible for the observed infrared-excess and emission lines. The phenomena involved in the disk formation still remain highly debated. Aims. To progress in the understanding of the physical process or processes responsible for the mass ejections and test the hypothesis that they depend on the stellar parameters, we initiated a survey on the circumstellar environment of the brightest Be stars. Methods. To achieve this goal, we used spectro-interferometry, the only technique that combines high spectral (R=12000) and high spatial ($\\theta_{\\rm min}$=4\\,mas) resolutions. Observations were carried out at the Paranal observatory with the VLTI/AMBER instrument. We concentrated our observations on the Br$\\gamma$ emission line to be able to study the kinematics within the circumstellar disk. Our sample is composed of eight bright classical Be stars : $\\alpha$ Col, $\\kappa$ CMa, $\\omega$ Car, p Car, $\\delta$ Cen, $\\mu$ Cen, $\\alpha$ Ara, and \\textit{o} Aqr. Results. We managed to determine the disk extension in the line and the nearby continuum for most targets. We also constrained the disk kinematics, showing that it is dominated by rotation with a rotation law close to the Keplerian one. Our survey also suggests that these stars are rotating at a mean velocity of V/V$_{\\rm c}$\\,=\\,0.82\\,$\\pm$\\,0.08. This corresponds to a rotational rate of $\\Omega/\\Omega_{\\rm c}$\\,=\\,0.95\\,$\\pm$\\,0.02 Conclusions. We did not detect any correlation between the stellar parameters and the structure of the circumstellar environment. Moreover, it seems that a simple model of a geometrically thin Keplerian disk can explain most of our spectrally resolved K-band data. Nevertheless, some small departures from this model have been detected for at least two objects (i.e, $\\kappa$ CMa and $\\alpha$ Col). Finally, our Be stars sample suggests that rotation is the main physical process driving the mass-ejection. Nevertheless, smaller effects from other mechanisms have to be taken into account to fully explain how the residual gravity is compensated.", "prompt": "Context. Classical Be stars are hot non-supergiant stars surrounded by a gaseous circumstellar disk that is responsible for the observed infrared-excess and emission lines. The phenomena involved in the disk formation still remain highly debated. Aims. To progress in the understanding of the physical process or processes responsible for the", "orig": "mass ejections and test the hypothesis that they depend on the stellar parameters, we initiated a survey on the circumstellar environment of the brightest Be stars. Methods. To achieve this goal, we used spectro-interferometry, the only technique that combines high spectral (R=12000) and high spatial ($\\theta_{", "generated": "Context. Classical Be stars are hot non-supergiant stars surrounded by a gaseous circumstellar disk that is responsible for the observed infrared-excess and emission lines. The phenomena involved in the disk formation still remain highly debated. Aims. To progress in the understanding of the physical process or processes responsible for the"}
{"full_text": "Ingram Olkin was born on July 23, 1924 in Waterbury, Connecticut. His family moved to New York in 1934 and he graduated from DeWitt Clinton High School in 1941. He served three years in the Air Force during World War II and obtained a B.S. in mathematics at the City College of New York in 1947. After receiving an M.A. in mathematical statistics from Columbia in 1949, he completed his graduate studies in the Department of Statistics at the University of North Carolina in 1951. His dissertation was written under the direction of S. N. Roy and Harold Hotelling. He joined the Department of Mathematics at Michigan State University in 1951 as an Assistant Professor, subsequently being promoted to Professor. In 1960, he took a position as Chair of the Department of Statistics at the University of Minnesota. He moved to Stanford University in 1961 to take a joint position as Professor of Statistics and Professor of Education; he was also Chair of the Department of Statistics from 1973--1976. In 2007, Ingram became Professor Emeritus. Ingram was Editor of the Annals of Mathematical Statistics (1971--1972) and served as the first editor of the Annals of Statistics from 1972--1974. He was a primary force in the founding of the Journal of Educational Statistics, for which he was also Associate Editor during 1977--1985. In 1984, he was President of the Institute of Mathematical Statistics. Among his many professional activities, he has served as Chair of the Committee of Presidents of Statistical Societies (COPSS), Chair of the Committee on Applied and Theoretical Statistics of the National Research Council, Chair of the Management Board of the American Education Research Association, and as Trustee for the National Institute of Statistical Sciences. He has been honored by the American Statistical Association (ASA) with a Wilks Medal (1992) and a Founder's Award (1992). The American Psychological Association gave him a Lifetime Contribution Award (1997) and he was elected to the National Academy of Education in 2005. He received the COPSS Elizabeth L. Scott Award in 1998 and delivered the R. A. Fisher Lecture in 2000. In 2003, the City University of New York gave him a Townsend Harris Medal. An author of 5 books, an editor of 10 books, and an author of more than 200 publications, Ingram has made major contributions to statistics and education. His research has focused on multivariate analysis, majorization and inequalities, distribution theory, and meta-analysis. A volume in celebration of Ingram's 65th birthday contains a brief biography and an interview [Gleser, Perlman, Press and Sampson (1989)]. Ingram was chosen in 1997 to participate in the American Statistical Association Distinguished Statistician Video Series and a videotaped conversation and a lecture (Olkin, 1997) are available from the ASA (1997, DS041, DS042).", "prompt": "Ingram Olkin was born on July 23, 1924 in Waterbury, Connecticut. His family moved to New York in 1934 and he graduated from DeWitt Clinton High School in 1941. He served three years in the Air Force during World War II and obtained a B.S.", "orig": "in mathematics at the City College of New York in 1947. After receiving an M.A. in mathematical statistics from Columbia in 1949, he completed his graduate studies in the Department of Statistics at the University of North Carolina in 1951. His dissertation was written under the direction of S. N. Roy", "generated": "Ingram Olkin was born on July 23, 1924 in Waterbury, Connecticut. His family moved to New York in 1934 and he graduated from DeWitt Clinton High School in 1941. He served three years in the Air Force during World War II and obtained a B.S."}
{"full_text": "Glass transition where viscosity of liquids increases dramatically upon decrease of temperature without any major change in structural properties, remains one of the most challenging problems in condensed matter physics (Cavagna, 2009; Berthier and Biroli, 2011) in spite of tremendous research efforts in last decades. On the other hand disordered freezing of spins in a magnetic materials with decreasing temperature, the so-called spin glass transition, is relatively better understood (Mezard, Parisi and Virasoro, 1987; Castellani and Cavagna, 2005). Previously found similarity between some spin glass models with the structural glasses (Kirkpatrick and Thirumalai, 1987; Kirkpatrick and Wolynes, 1987; Kirkpatrick and Wolynes, 1987; Franz and Parisi, 1999; Moore and Drossel, 2002) inspired development of theories of structural glasses (Kirkpatrick, Thirumalai and Wolynes, 1989; Barrat, Franz and Parisi, 1997; M\\'ezard and Parisi, 1999; Lubchenko and Wolynes, 2007; Biroli and Bouchaud, 2012) based on the scenario of spin glass transition. This scenario though looks very appealing is still far from being well established. One of the main differences between standard spin systems to molecular systems is the absence of quenched disorder and the presence of translational invariance: it often assumed that this difference is not relevant, but this conjecture is still far from being established. The quantities, which are well defined and characterized for spin models, are not easily calculable for molecular glasses due to the lack of quenched disorder which breaks the translational invariance in the system and the characterization of the similarity between the spin and the structural glass transition remained an elusive subject still now. In this study we introduced a model structural glass with built in quenched disorder which alleviates this main difference between the spin and molecular glasses thereby helping us to compare these two systems: the possibility of producing a good thermalization at rather low temperatures is one of the advantages of this model.", "prompt": "Glass transition where viscosity of liquids increases dramatically upon decrease of temperature without any major change in structural properties, remains one of the most challenging problems in condensed matter physics (Cavagna, 2009; Berthier and Biroli, 2011) in spite of tremendous research efforts in last decades. On", "orig": "the other hand disordered freezing of spins in a magnetic materials with decreasing temperature, the so-called spin glass transition, is relatively better understood (Mezard, Parisi and Virasoro, 1987; Castellani and Cavagna, 2005). Previously found similarity between some spin glass models with the", "generated": "Glass transition where viscosity of liquids increases dramatically upon decrease of temperature without any major change in structural properties, remains one of the most challenging problems in condensed matter physics (Cavagna, 2009; Berthier and Biroli, 2011) in spite of tremendous research efforts in last decades."}
{"full_text": "When can we compute the diameter of a graph in quasi linear time? We address this question for the class of {\\em split graphs}, that we observe to be the hardest instances for deciding whether the diameter is at most two. We stress that although the diameter of a non-complete split graph can only be either $2$ or $3$, under the Strong Exponential-Time Hypothesis (SETH) we cannot compute the diameter of an $n$-vertex $m$-edge split graph in less than quadratic time -- in the size $n+m$ of the input. Therefore it is worth to study the complexity of diameter computation on {\\em subclasses} of split graphs, in order to better understand the complexity border. Specifically, we consider the split graphs with bounded {\\em clique-interval number} and their complements, with the former being a natural variation of the concept of interval number for split graphs that we introduce in this paper. We first discuss the relations between the clique-interval number and other graph invariants such as the classic interval number of graphs, the treewidth, the {\\em VC-dimension} and the {\\em stabbing number} of a related hypergraph. Then, in part based on these above relations, we almost completely settle the complexity of diameter computation on these subclasses of split graphs: - For the $k$-clique-interval split graphs, we can compute their diameter in truly subquadratic time if $k={\\cal O}(1)$, and even in quasi linear time if $k=o(\\log{n})$ and in addition a corresponding ordering of the vertices in the clique is given. However, under SETH this cannot be done in truly subquadratic time for any $k = \\omega(\\log{n})$. - For the {\\em complements} of $k$-clique-interval split graphs, we can compute their diameter in truly subquadratic time if $k={\\cal O}(1)$, and even in time ${\\cal O}(km)$ if a corresponding ordering of the vertices in the stable set is given. Again this latter result is optimal under SETH up to polylogarithmic factors. Our findings raise the question whether a $k$-clique interval ordering can always be computed in quasi linear time. We prove that it is the case for $k=1$ and for some subclasses such as bounded-treewidth split graphs, threshold graphs and comparability split graphs. Finally, we prove that some important subclasses of split graphs -- including the ones mentioned above -- have a bounded clique-interval number.", "prompt": "When can we compute the diameter of a graph in quasi linear time? We address this question for the class of {\\em split graphs}, that we observe to be the hardest instances for deciding whether the diameter is at most two. We stress that although the diameter of a non-complete split graph can only be either $2", "orig": "$ or $3$, under the Strong Exponential-Time Hypothesis (SETH) we cannot compute the diameter of an $n$-vertex $m$-edge split graph in less than quadratic time -- in the size $n+m$ of the input. Therefore it is worth to study the complexity of diameter", "generated": "When can we compute the diameter of a graph in quasi linear time? We address this question for the class of {\\em split graphs}, that we observe to be the hardest instances for deciding whether the diameter is at most two. We stress that although the diameter of a non-complete split graph can only be either $2"}
{"full_text": "Mammography is a specific type of breast imaging that uses low-dose X-rays to detect cancer in early stage. During the exam, the women breast is compressed between two plates until a nearly uniform breast thickness is obtained. This technique improves image quality and reduces dose but can also be the source of discomfort and sometimes pain for the patient. Therefore, alternative techniques allowing reduced breast compression is of potential interest. The aim of this work is to develop a 3D biomechanical Finite Element (FE) breast model in order to analyze various breast compression strategies and their impact on image quality and radiation dose. Large breast deformations are simulated using this FE model with ANSYS software. A particular attention is granted to the computation of the residual stress in the model due to gravity and boundary conditions (thorax anatomy, position of the patient inside the MRI machine). Previously developed biomechanical breast models use a simplified breast anatomy by modeling adipose and fibroglandular tissues only (Rajagopal et al. in Wiley Interdiscip Rev: Syst Biol Med 2:293--304, 2010). However, breast reconstruction surgery has proven the importance of suspensory ligaments and breast fasciae on breast mechanics (Lockwood in Plast Reconstr Surg 103:1411--1420, 1999). We are therefore consider using a more realistic breast anatomy by including skin, muscles, and suspensory ligaments. The breast tissues are modeled as neo-Hookean materials. A physical correct modeling of the breast requires the knowledge of the stress-free breast configuration. Here, this undeformed shape (i.e., without any residual stress) is computed using the prediction--correction iterative scheme proposed by Eiben et al. (Ann of Biomed Eng 44:154--173, 2016). The unloading procedure uses the breast configuration in prone and supine position in order to find a unique displacement vector field induced by gravitational forces. The 3D breast geometry is reconstructed from MRI images that are segmented (Yushkevich et al. in Neuroimage 31:1116--1128, 2006) to differentiate the four main tissue types. The breast volume is discretized with a hexa-dominant FE meshing tool as a unique volume. Finally, the model is evaluated by comparing the estimated breast deformations under gravity load with the experimental ones measured in three body positions: prone, supine, and oblique supine.", "prompt": "Mammography is a specific type of breast imaging that uses low-dose X-rays to detect cancer in early stage. During the exam, the women breast is compressed between two plates until a nearly uniform breast thickness is obtained. This technique improves image quality and reduces dose but can also be the source of discomfort and sometimes", "orig": "pain for the patient. Therefore, alternative techniques allowing reduced breast compression is of potential interest. The aim of this work is to develop a 3D biomechanical Finite Element (FE) breast model in order to analyze various breast compression strategies and their impact on image quality and radiation dose. Large breast deformations are simulated", "generated": "Mammography is a specific type of breast imaging that uses low-dose X-rays to detect cancer in early stage. During the exam, the woman's breast is compressed between two plates until a nearly uniform breast thickness is obtained. This technique improves image quality and reduces dose but can also be the source of discomfort and"}
{"full_text": "The close-in planet orbiting GJ 436 presents a puzzling orbital eccentricity considering its very short orbital period. Given the age of the system, this planet should have been tidally circularized a long time ago. Many attempts to explain this were proposed in recent years, either involving abnormally weak tides, or the perturbing action of a distant companion. We address here the latter issue based on Kozai migration. We propose that GJ 436b was formerly located further away from the star and that it underwent a migration induced by a massive, inclined perturber via Kozai mechanism. In this context, the perturbations by the companion trigger high amplitude variations to GJ 436b that cause tides to act at periastron. Then the orbit tidally shrinks to reach its present day location. We numerically integrate the 3-body system including tides and General Relativity correction. We first show that starting from the present-day location of GJ 436b inevitably leads to damping the Kozai oscillations and to rapidly circularizing the planet. Conversely, starting from 5-10 times further away allows the onset of Kozai cycles. The tides act in peak eccentricity phases and reduce the semi-major axis of the planet. The net result is an evolution characterized by two phases: a first one with Kozai cycles and a slowly shrinking semi-major axis, and a second one once the planet gets out of the Kozai resonance characterized by a more rapid decrease. The timescale of this process appears in most cases much longer than the standard circularization time of the planet by a factor larger than 50. This model can provide a solution to the eccentricity paradox of GJ 436b. Depending on the various orbital configurations, it can take several Gyrs to GJ 436b to achieve a full orbital decrease and circularization. According to this scenario, we could be witnessing today the second phase of the scenario where the semi-major axis is already reduced while the eccentricity is still significant. We then explore the parameter space and derive in which conditions this model can be realistic given the age of the system. This yields constraints on the characteristics of the putative companion.", "prompt": "The close-in planet orbiting GJ 436 presents a puzzling orbital eccentricity considering its very short orbital period. Given the age of the system, this planet should have been tidally circularized a long time ago. Many attempts to explain this were proposed in recent years, either involving abnormally weak tides", "orig": ", or the perturbing action of a distant companion. We address here the latter issue based on Kozai migration. We propose that GJ 436b was formerly located further away from the star and that it underwent a migration induced by a massive, inclined perturber via Kozai mechanism. In this", "generated": "The close-in planet orbiting GJ 436 presents a puzzling orbital eccentricity considering its very short orbital period. Given the age of the system, this planet should have been tidally circularized a long time ago. Many attempts to explain this were proposed in recent years, either involving abnormally weak tidal interactions"}
{"full_text": "The pair-condensed unpolarized spin-$1/2$ Fermi gases have a collective excitation branch in their pair-breaking continuum (V.A. Andrianov, V.N. Popov, 1976). We study it at zero temperature, with the eigenenergy equation deduced from the linearized time-dependent BCS theory and extended analytically to the lower half complex plane through its branch cut, calculating both the dispersion relation and the spectral weights (quasiparticle residues) of the branch. In the case of BCS superconductors, so called because the effect of the ion lattice is replaced by a short-range electron-electron interaction, we also include the Coulomb interaction and we restrict ourselves to the weak coupling limit $\\Delta/\\mu\\to 0^+$ ($\\Delta$ is the order parameter, $\\mu $ the chemical potential) and to wavenumbers $q=O(1/\\xi)$ where $\\xi$ is the size of a pair; when the complex energy $z_q$ is expressed in units of $\\Delta$ and $q$ in units of $1/\\xi$, the branch follows a universal law insensitive to the Coulomb interaction. In the case of cold atoms in the BEC-BCS crossover, only a contact interaction remains, but the coupling strength $\\Delta/\\mu$ can take arbitrary values, and we study the branch at any wave number. At weak coupling, we predict three scales, that already mentioned $q\\approx 1/\\xi$, that $q\\approx(\\Delta/\\mu)^{-1/3}/\\xi$ where the real part of the dispersion relation has a minimum and that $q\\approx (\\mu/\\Delta)/\\xi\\approx k_{\\rm F}$ ($k_{\\rm F}$ is the Fermi wave number) where the branch reaches the edge of its existence domain. Near the point where the chemical potential vanishes on the BCS side, $\\mu/\\Delta\\to 0^+$, where $\\xi\\approx k_{\\rm F}$, we find two scales $q\\approx(\\mu/\\Delta)^{1/2}/\\xi$ and $q\\approx 1/\\xi$. In all cases, the branch has a limit $2\\Delta$ and a quadratic start at $q=0$. These results were obtained for $\\mu>0$, where the eigenenergy equation admits at least two branch points $\\epsilon_a(q)$ and $\\epsilon_b(q)$ on the positive real axis, and for an analytic continuation through the interval $[\\epsilon_a(q),\\epsilon_b(q)] $. We find new continuum branches by performing the analytic continuation through $[\\epsilon_b(q),+\\infty[$ or even, for $q$ low enough, where there is a third real positive branch point $\\epsilon_c(q)$, through $[\\epsilon_b(q),\\epsilon_c(q)]$ and $[\\epsilon_c(q),+\\infty[$. On the BEC side $\\mu<0$ not previously studied, where there is only one real positive branch point $ \\epsilon_a(q)$, we also find new collective excitation branches under the branch cut $[\\epsilon_a (q),+\\infty[$. For $\\mu>0$, some of these new branches have a low-wavenumber exotic hypoacoustic $z_q\\approx q^{3/2}$ or hyperacoustic $z_q\\approx q^{4/5}$ behavior. For $\\mu<0$, we find a hyperacoustic branch and a nonhypoacoustic branch, with a limit $2\\Delta$ and a purely real quadratic start at $q=0$ for $\\Delta/|\\mu|<0.222$.", "prompt": "The pair-condensed unpolarized spin-$1/2$ Fermi gases have a collective excitation branch in their pair-breaking continuum (V.A. Andrianov, V.N. Popov, 1976). We study it at zero temperature, with the eigenenergy equation deduced from the linearized", "orig": "time-dependent BCS theory and extended analytically to the lower half complex plane through its branch cut, calculating both the dispersion relation and the spectral weights (quasiparticle residues) of the branch. In the case of BCS superconductors, so called because the effect of the ion lattice is replaced by a", "generated": "The pair-condensed unpolarized spin-$1/2$ Fermi gases have a collective excitation branch in their pair-breaking continuum (V.A. Andrianov, V.N. Popov, 1976). We study it at zero temperature, with the eigenenergy equation deduced from the linearized"}
{"full_text": "Various programming tools, languages, and environments give programmers the impression of changing a program while it is running. This experience of liveness has been discussed for over two decades and a broad spectrum of research on this topic exists. Amongst others, this work has been carried out in the communities around three major ideas which incorporate liveness as an important aspect: live programming, exploratory programming, and live coding. While there have been publications on the focus of each particular community, the overall spectrum of liveness across these three communities has not been investigated yet. Thus, we want to delineate the variety of research on liveness. At the same time, we want to investigate overlaps and differences in the values and contributions between the three communities. Therefore, we conducted a literature study with a sample of 212 publications on the terms retrieved from three major indexing services. On this sample, we conducted a thematic analysis regarding the following aspects: motivation for liveness, application domains, intended outcomes of running a system, and types of contributions. We also gathered bibliographic information such as related keywords and prominent publications. Besides other characteristics the results show that the field of exploratory programming is mostly about technical designs and empirical studies on tools for general-purpose programming. In contrast, publications on live coding have the most variety in their motivations and methodologies with a majority being empirical studies with users. As expected, most publications on live coding are applied to performance art. Finally, research on live programming is mostly motivated by making programming more accessible and easier to understand, evaluating their tool designs through empirical studies with users. In delineating the spectrum of work on liveness, we hope to make the individual communities more aware of the work of the others. Further, by giving an overview of the values and methods of the individual communities, we hope to provide researchers new to the field of liveness with an initial overview.", "prompt": "Various programming tools, languages, and environments give programmers the impression of changing a program while it is running. This experience of liveness has been discussed for over two decades and a broad spectrum of research on this topic exists. Amongst others, this work has been carried out in the communities around three major ideas which incorporate", "orig": "liveness as an important aspect: live programming, exploratory programming, and live coding. While there have been publications on the focus of each particular community, the overall spectrum of liveness across these three communities has not been investigated yet. Thus, we want to delineate the variety of research on liveness. At the", "generated": "Various programming tools, languages, and environments give programmers the impression of changing a program while it is running. This experience of liveness has been discussed for over two decades and a broad spectrum of research on this topic exists. Amongst others, this work has been carried out in the communities around three major ideas which incorporate"}
{"full_text": "Implementing circular economy (CE) principles is increasingly recommended as a convenient solution to meet the goals of sustainable development. New tools are required to support practitioners, decision-makers and policy-makers towards more CE practices, as well as to monitor the effects of CE adoption. Worldwide, academics, industrialists and politicians all agree on the need to use CE-related measuring instruments to manage this transition at different systemic levels. In this context, a wide range of circularity indicators (C-indicators) has been developed in recent years. Yet, as there is not one single definition of the CE concept, it is of the utmost importance to know what the available indicators measure in order to use them properly. Indeed, through a systematic literature review-considering both academic and grey literature-55 sets of C-indicators, developed by scholars, consulting companies and governmental agencies, have been identified, encompassing different purposes, scopes, and potential usages. Inspired by existing taxonomies of eco-design tools and sustainability indicators, and in line with the CE characteristics, a classification of indicators aiming to assess, improve, monitor and communicate on the CE performance is proposed and discussed. In the developed taxonomy including 10 categories, C-indicators are differentiated regarding criteria such as the levels of CE implementation (e.g. micro, meso, macro), the CE loops (maintain, reuse, remanufacture, recycle), the performance (intrinsic, impacts), the perspective of circularity (actual, potential) they are taking into account, or their degree of transversality (generic, sector-specific). In addition, the database inventorying the 55 sets of C-indicators is linked to an Excel-based query tool to facilitate the selection of appropriate indicators according to the specific user's needs and requirements. This study enriches the literature by giving a first need-driven taxonomy of C-indicators, which is experienced on several use cases. It provides a synthesis and clarification to the emerging and must-needed research theme of C-indicators, and sheds some light on remaining key challenges like their effective uptake by industry. Eventually, limitations, improvement areas, as well as implications of the proposed taxonomy are intently addressed to guide future research on C-indicators and CE implementation.", "prompt": "Implementing circular economy (CE) principles is increasingly recommended as a convenient solution to meet the goals of sustainable development. New tools are required to support practitioners, decision-makers and policy-makers towards more CE practices, as well as to monitor the effects of CE adoption. Worldwide, academics, industrialists and politicians all agree on", "orig": "the need to use CE-related measuring instruments to manage this transition at different systemic levels. In this context, a wide range of circularity indicators (C-indicators) has been developed in recent years. Yet, as there is not one single definition of the CE concept, it is of the utmost importance to know what the", "generated": "Implementing circular economy (CE) principles is increasingly recommended as a convenient solution to meet the goals of sustainable development. New tools are required to support practitioners, decision-makers and policy-makers towards more CE practices, as well as to monitor the effects of CE adoption. Worldwide, academics, industrialists and politicians all agree on"}
{"full_text": "We define the prequantization of a symplectic Anosov diffeomorphism f:M-> M, which is a U(1) extension of the diffeomorphism f preserving an associated specific connection, and study the spectral properties of the associated transfer operator, called prequantum transfer operator. This is a model for the transfer operators associated to geodesic flows on negatively curved manifolds (or contact Anosov flows). We restrict the prequantum transfer operator to the N-th Fourier mode with respect to the U(1) action and investigate the spectral property in the limit N->infinity, regarding the transfer operator as a Fourier integral operator and using semi-classical analysis. In the main result, we show a \" band structure \" of the spectrum, that is, the spectrum is contained in a few separated annuli and a disk concentric at the origin. We show that, with the special (H\\\"older continuous) potential V0=1/2 log |det Df_x|_{E_u}|, the outermost annulus is the unit circle and separated from the other parts. For this, we use an extension of the transfer operator to the Grassmanian bundle. Using Atiyah-Bott trace formula, we establish the Gutzwiller trace formula with exponentially small reminder for large time. We show also that, for a potential V such that the outermost annulus is separated from the other parts, most of the eigenvalues in the outermost annulus concentrate on a circle of radius exp where <.> denotes the spatial average on M. The number of these eigenvalues is given by the \"Weyl law\", that is, N^d.Vol(M) with d=1/2. dim(M) in the leading order. We develop a semiclassical calculus associated to the prequantum operator by defining quantization of observables Op(psi) in an intrinsic way. We obtain that the semiclassical Egorov formula of quantum transport is exact. We interpret all these results from a physical point of view as the emergence of quantum dynamics in the classical correlation functions for large time. We compare these results with standard quantization (geometric quantization) in quantum chaos.", "prompt": "We define the prequantization of a symplectic Anosov diffeomorphism f:M-> M, which is a U(1) extension of the diffeomorphism f preserving an associated specific connection, and study the spectral properties of the associated transfer operator, called prequantum transfer operator. This", "orig": "is a model for the transfer operators associated to geodesic flows on negatively curved manifolds (or contact Anosov flows). We restrict the prequantum transfer operator to the N-th Fourier mode with respect to the U(1) action and investigate the spectral property in the limit N->infinity, regarding the", "generated": "We define the prequantization of a symplectic Anosov diffeomorphism f:M-> M, which is a U(1) extension of the diffeomorphism f preserving an associated specific connection, and study the spectral properties of the associated transfer operator, called prequantum transfer operator.\n\nThe"}
{"full_text": "Estimating the number $n$ of unseen species from a $k-$sample displaying only $p\\leq k$ distinct sampled species has received attention for long. It requires a model of species abundance together with a sampling model. We start with a discrete model of iid stochastic species abundances, each with Gibbs-Poisson distribution. A $k-$sample drawn from the $n-$species abundances vector is the one obtained while conditioning it on summing to $k$% . We discuss the sampling formulae (species occupancy distributions, frequency of frequencies) in this context. We then develop some aspects of the estimation of $n$ problem from the size $k$ of the sample and the observed value of $P_{n,k}$, the number of distinct sampled species. It is shown that it always makes sense to study these occupancy problems from a Gibbs-Poisson abundance model in the context of a population with infinitely many species. From this extension, a parameter $\\gamma $ naturally appears, which is a measure of richness or diversity of species. We rederive the sampling formulae for a population with infinitely many species, together with the distribution of the number $P_{k}$ of distinct sampled species. We investigate the estimation of $\\gamma $ problem from the sample size $k$ and the observed value of $P_{k}$. We then exhibit a large special class of Gibbs-Poisson distributions having the property that sampling from a discrete abundance model may equivalently be viewed as a sampling problem from a random partition of unity, now in the continuum. When $n$ is finite, this partition may be built upon normalizing $% n$ infinitely divisible iid positive random variables by its partial sum. It is shown that the sampling process in the continuum should generically be biased on the total length appearing in the latter normalization. A construction with size-biased sampling from the ranked normalized jumps of a subordinator is also supplied, would the problem under study present infinitely many species. We illustrate our point of view with many examples, some of which being new ones.", "prompt": "Estimating the number $n$ of unseen species from a $k-$sample displaying only $p\\leq k$ distinct sampled species has received attention for long. It requires a model of species abundance together with a sampling model. We start with a discrete model of iid stochastic species abundances, each with Gibbs", "orig": "-Poisson distribution. A $k-$sample drawn from the $n-$species abundances vector is the one obtained while conditioning it on summing to $k$%. We discuss the sampling formulae (species occupancy distributions, frequency of frequencies) in this context. We then develop some aspects of the estimation of", "generated": "Estimating the number $n$ of unseen species from a $k-$sample displaying only $p\\leq k$ distinct sampled species has received attention for long. It requires a model of species abundance together with a sampling model. We start with a discrete model of iid stochastic species abundances, each with Gibbs"}
{"full_text": "Model checking verifies that a model of a system satisfies a given property, and otherwise produces a counter-example explaining the violation. The verified properties are formally expressed in temporal logics. Some temporal logics, such as CTL, are branching: they allow to express facts about the whole computation tree of the model, rather than on each single linear computation. This branching aspect is even more critical when dealing with multi-modal logics, i.e. logics expressing facts about systems with several transition relations. A prominent example is CTLK, a logic that reasons about temporal and epistemic properties of multi-agent systems. In general, model checkers produce linear counter-examples for failed properties, composed of a single computation path of the model. But some branching properties are only poorly and partially explained by a linear counter-example. This paper proposes richer counter-example structures called tree-like annotated counter-examples (TLACEs), for properties in Action-Restricted CTL (ARCTL), an extension of CTL quantifying paths restricted in terms of actions labeling transitions of the model. These counter-examples have a branching structure that supports more complete description of property violations. Elements of these counter-examples are annotated with parts of the property to give a better understanding of their structure. Visualization and browsing of these richer counter-examples become a critical issue, as the number of branches and states can grow exponentially for deeply-nested properties. This paper formally defines the structure of TLACEs, characterizes adequate counter-examples w.r.t. models and failed properties, and gives a generation algorithm for ARCTL properties. It also illustrates the approach with examples in CTLK, using a reduction of CTLK to ARCTL. The proposed approach has been implemented, first by extending the NuSMV model checker to generate and export branching counter-examples, secondly by providing an interactive graphical interface to visualize and browse them.", "prompt": "Model checking verifies that a model of a system satisfies a given property, and otherwise produces a counter-example explaining the violation. The verified properties are formally expressed in temporal logics. Some temporal logics, such as CTL, are branching: they allow to express facts about the whole computation tree of the model, rather than", "orig": "on each single linear computation. This branching aspect is even more critical when dealing with multi-modal logics, i.e. logics expressing facts about systems with several transition relations. A prominent example is CTLK, a logic that reasons about temporal and epistemic properties of multi-agent systems. In general, model checkers", "generated": "Model checking verifies that a model of a system satisfies a given property, and otherwise produces a counter-example explaining the violation. The verified properties are formally expressed in temporal logics. Some temporal logics, such as CTL, are branching: they allow to express facts about the whole computation tree of the model, rather than"}
{"full_text": "Context. An essential facet of turbulence is the space-time intermittency of the cascade of energy that leads to coherent structures of high dissipation. Aims. In this work, we attempt to investigate systematically the physical nature of the intense dissipation regions in decaying isothermal magnetohydrodynamical (MHD) turbulence. Methods. We probe the turbulent dissipation with grid based simulations of compressible isothermal decaying MHD turbulence. We take unprecedented care at resolving and controlling dissipation: we design methods to locally recover the dissipation due to the numerical scheme. We locally investigate the geometry of the gradients of the fluid state variables. We develop a method to assess the physical nature of the largest gradients in simulations and to estimate their travelling velocity. Finally we investigate their statistics. Results. We find that intense dissipation regions mainly correspond to sheets: locally, density, velocity and magnetic fields vary primarily across one direction. We identify these highly dissipative regions as fast/slow shocks or Alfv{\\'e}n discontinuities (Parker sheets or rotational discontinuities). On these structures, we find the main deviation from 1D planar steady-state is mass loss in the plane of the structure. We investigate the effect of initial conditions which yield different imprints at early time on the relative distributions between these four categories. However, these differences fade out after about one turnover time, when they become dominated by weakly compressible Alfv{\\'e}n discontinuities. We show that the magnetic Prandtl number has little influence on the statistics of these discontinuities, but it controls the Ohmic vs viscous heating rates within them. Finally, we find the entrance characteristics of the structures (such as entrance velocity and magnetic pressure) are strongly correlated. Conclusions. These new methods allow to consider developed compressible turbulence as a statistical collection of intense dissipation structures. This can be used to post-process 3D turbulence with detailed 1D models apt for comparison with observations. It could also reveal useful as a framework to formulate new dynamical properties of turbulence.", "prompt": "Context. An essential facet of turbulence is the space-time intermittency of the cascade of energy that leads to coherent structures of high dissipation. Aims. In this work, we attempt to investigate systematically the physical nature of the intense dissipation regions in decaying isothermal magnetohydrodynamical (MHD", "orig": ") turbulence. Methods. We probe the turbulent dissipation with grid based simulations of compressible isothermal decaying MHD turbulence. We take unprecedented care at resolving and controlling dissipation: we design methods to locally recover the dissipation due to the numerical scheme. We locally investigate the geometry of the gradients of the fluid state", "generated": "Context. An essential facet of turbulence is the space-time intermittency of the cascade of energy that leads to coherent structures of high dissipation. Aims. In this work, we attempt to investigate systematically the physical nature of the intense dissipation regions in decaying isothermal magnetohydrodynamical (MHD"}
{"full_text": "The amount of data generated by numerical simulations in various scientific domains such as molecular dynamics, climate modeling, biology, or astrophysics, led to a fundamental redesign of application workflows. The throughput and the capacity of storage subsystems have not evolved as fast as the computing power in extreme-scale supercomputers. As a result, the classical post-hoc analysis of simulation outputs became highly inefficient. In-situ workflows have then emerged as a solution in which simulation and data analytics are intertwined through shared computing resources, thus lower latencies. Determining the best allocation, i.e., how many resources to allocate to each component of an in-situ workflow; and mapping, i.e., where and at which frequency to run the data analytics component, is a complex task whose performance assessment is crucial to the efficient execution of in-situ workflows. However, such a performance evaluation of different allocation and mapping strategies usually relies either on directly running them on the targeted execution environments, which can rapidly become extremely time-and resource-consuming, or on resorting to the simulation of simplified models of the components of an in-situ workflow, which can lack of realism. In both cases, the validity of the performance evaluation is limited. To address this issue, we introduce SIM-SITU, a framework for the faithful simulation of in-situ workflows. This framework builds on the SimGrid toolkit and benefits of several important features of this versatile simulation tool. We designed SIM-SITU to reflect the typical structure of in-situ workflows and thanks to its modular design, SIM-SITU has the necessary flexibility to easily and faithfully evaluate the behavior and performance of various allocation and mapping strategies for in-situ workflows. We illustrate the simulation capabilities of SIM-SITU on a Molecular Dynamics use case. We study the impact of different allocation and mapping strategies on performance and show how users can leverage SIM-SITU to determine interesting tradeoffs when designing their in-situ workflow.", "prompt": "The amount of data generated by numerical simulations in various scientific domains such as molecular dynamics, climate modeling, biology, or astrophysics, led to a fundamental redesign of application workflows. The throughput and the capacity of storage subsystems have not evolved as fast as the computing power in extreme-scale supercomputers. As a", "orig": "result, the classical post-hoc analysis of simulation outputs became highly inefficient. In-situ workflows have then emerged as a solution in which simulation and data analytics are intertwined through shared computing resources, thus lower latencies. Determining the best allocation, i.e., how many resources to allocate to each component of an in-s", "generated": "The amount of data generated by numerical simulations in various scientific domains such as molecular dynamics, climate modeling, biology, or astrophysics, led to a fundamental redesign of application workflows. The throughput and the capacity of storage subsystems have not evolved as fast as the computing power in extreme-scale supercomputers. As a"}
{"full_text": "Modern network sensors continuously produce enormous quantities of raw data that are beyond the capacity of human analysts. Cross-correlation of network sensors increases this challenge by enriching every network event with additional metadata. These large volumes of enriched network data present opportunities to statistically characterize network traffic and quickly answer a key question: \"What are the primary cyber characteristics of my network data?\" The Python GraphBLAS and PyD4M analysis frameworks enable anonymized statistical analysis to be performed quickly and efficiently on very large network data sets. This approach is tested using billions of anonymized network data samples from the largest Internet observatory (CAIDA Telescope) and tens of millions of anonymized records from the largest commercially available background enrichment capability (GreyNoise). The analysis confirms that most of the enriched variables follow expected heavy-tail distributions and that a large fraction of the network traffic is due to a small number of cyber activities. This information can simplify the cyber analysts' task by enabling prioritization of cyber activities based on statistical prevalence. -- Los sensores de red modernos producen enormes cantidades de datos sin procesar que est\\'an m\\'as all\\'a de la capacidad del an\\'alisis humano. Una correlaci\\'on cruzada de sensores de red se convierte en un desaf\\'io al enriquecer cada evento de red con metadatos adicionales. Estos grandes vol\\'umenes de datos de red enriquecidos presentan una oportunidad para caracterizar estad\\'isticamente el tr\\'afico de red y responder a la pregunta: \"?Cu\\'ales son las principales caracter\\'isticas cibern\\'eticas de mis datos de red?\" Los esquemas de an\\'alisis de Python GraphBLAS y D4M permiten realizar an\\'alisis estad\\'isticos an\\'onimos, r\\'apidos y eficientes en conjuntos grandes de datos de red. Este enfoque se prueba utilizando miles de millones de muestras de datos de red an\\'onimos del observatorio de Internet m\\'as grande (Telescopio CAIDA) y decenas de millones de registros an\\'onimos del fondo comercial con la mayor capacidad de enriquecimiento (GreyNoise). El an\\'alisis confirma que la mayor\\'ia de las variables enriquecidas siguen las distribuciones de cola pesada y que una gran fracci\\'on del tr\\'afico de red se debe a una peque\\`na cantidad de actividades cibern\\'eticas. Esta informaci\\'on puede simplificar la tarea de los analistas cibern\\'eticos al permitir la priorizaci\\'on de las actividades cibern\\'eticas en funci\\'on de la prevalencia estad\\'istica.", "prompt": "Modern network sensors continuously produce enormous quantities of raw data that are beyond the capacity of human analysts. Cross-correlation of network sensors increases this challenge by enriching every network event with additional metadata. These large volumes of enriched network data present opportunities to statistically characterize network traffic and quickly answer a key question: \"What are the", "orig": "primary cyber characteristics of my network data?\" The Python GraphBLAS and PyD4M analysis frameworks enable anonymized statistical analysis to be performed quickly and efficiently on very large network data sets. This approach is tested using billions of anonymized network data samples from the largest Internet observatory (CAIDA Telescope) and tens of", "generated": "Modern network sensors continuously produce enormous quantities of raw data that are beyond the capacity of human analysts. Cross-correlation of network sensors increases this challenge by enriching every network event with additional metadata. These large volumes of enriched network data present opportunities to statistically characterize network traffic and quickly answer a key question: \"What are the"}
{"full_text": "We define the equivariant Cox ring of a normal variety with algebraic group action. We study algebraic and geometric aspects of this object and show how it is related to the ordinary Cox ring. Then, we specialize to the case of normal rational varieties of complexity one under the action of a connected reductive group G. We show that the G-equivariant Cox ring is then a finitely generated integral normal G-algebra. Under a mild additional condition, we give a presentation by generators and relations of its subalgebra of U-invariants, where U is the unipotent part of a Borel subgroup of G. The ordinary Cox ring is also finitely generated and canonically isomorphic to the U-equivariant Cox ring, so that it inherits a canonical structure of U-algebra. Relying on a work of Hausen and Herppich, we prove that the subalgebra of U-invariants of the Cox ring is a finitely generated Cox ring of a variety of complexity one under the action of a torus. This yields in particular that this latter algebra is a complete intersection.Characterizing the log terminality of singularities in a finitely generated Cox ring is an interesting question, particularly since the work of Gongyo, Okawa, Sannai and Takagi characterizing varieties of Fano type via singularities of Cox rings ([13]). We provide a criterion of combinatorial nature for the Cox ring of an almost homogeneous G-variety of complexity one to have log terminal singularities.Iteration of Cox rings has been introduced by Arzhantsev, Braun, Hausen and Wrobel in [1]. For log terminal quasicones with a torus action of complexity one, they proved that the iteration sequence is finite with a finitely generated factorial master Cox ring. We prove that the iteration sequence is finite for equivariant and ordinary Cox rings of normal rational G-varieties of complexity one satisfying a mild additional condition (e.g. complete varieties or almost homogeneous varieties). In the almost homogeneous case, we prove that the equivariant and ordinary master Cox rings are finitely generated and factorial.", "prompt": "We define the equivariant Cox ring of a normal variety with algebraic group action. We study algebraic and geometric aspects of this object and show how it is related to the ordinary Cox ring. Then, we specialize to the case of normal rational varieties of complexity one under the action of a connected reductive group G", "orig": ". We show that the G-equivariant Cox ring is then a finitely generated integral normal G-algebra. Under a mild additional condition, we give a presentation by generators and relations of its subalgebra of U-invariants, where U is the unipotent part of a Borel subgroup of G.", "generated": "We define the equivariant Cox ring of a normal variety with algebraic group action. We study algebraic and geometric aspects of this object and show how it is related to the ordinary Cox ring. Then, we specialize to the case of normal rational varieties of complexity one under the action of a connected reductive group G"}
{"full_text": "We theoretically investigate the entangled states of an atomic ensemble that can be obtained via cavity-feedback, varying the atom-light coupling from weak to strong, and including a systematic treatment of decoherence. In the strong coupling regime for small atomic ensembles, the system is driven by cavity losses into a long-lived, highly-entangled many-body state that we characterize analytically. In the weak coupling regime for large ensembles, we find analytically the maximum spin squeezing that can be achieved by optimizing both the coupling and the atom number. This squeezing is fundamentally limited by spontaneous emission to a constant value, independent of the atom number. Harnessing entanglement in many-body systems is of fundamental interest [1] and is the key requirement for quantum enhanced technologies, in particular quantum metrology [2]. In this respect, many efforts have been devoted to prepare entangled states in atomic ensembles because of their high degree of coherence and their potential for precision measurement. Spin squeezed states as well as number states have been produced following methods based either on coherent evolution in the presence of a non-linearity in the atomic field [3--5], or on quantum non-demolition measurement [6--8]. Among methods of the first kind, cavity feedback [5, 9] is one of the most promising: it has already allowed for the creation of highly squeezed states [5] and the effective non-linearity introduced by the atom-cavity coupling can be easily switched off, making it very attractive for metrol-ogy applications. In this Letter, we analyze the entangled states that can be produced by cavity feedback in different coupling regimes from weak to strong, and derive the ultimate limits of the metrology gain, extending the optimization of squeezing to unexplored domains of parameters values. After optimization of both the coupling strength and the atom number, we find a maximum squeezing limit that depends only on the atomic structure.", "prompt": "We theoretically investigate the entangled states of an atomic ensemble that can be obtained via cavity-feedback, varying the atom-light coupling from weak to strong, and including a systematic treatment of decoherence. In the strong coupling regime for small atomic ensembles, the system is driven by cavity losses into a long-lived, highly-", "orig": "entangled many-body state that we characterize analytically. In the weak coupling regime for large ensembles, we find analytically the maximum spin squeezing that can be achieved by optimizing both the coupling and the atom number. This squeezing is fundamentally limited by spontaneous emission to a constant value, independent of the atom number.", "generated": "We theoretically investigate the entangled states of an atomic ensemble that can be obtained via cavity-feedback, varying the atom-light coupling from weak to strong, and including a systematic treatment of decoherence. In the strong coupling regime for small atomic ensembles, the system is driven by cavity losses into a long-lived, highly-"}
{"full_text": "Motivation. Association studies have been widely used to search for associations between common genetic variants observations and a given phenotype. However, it is now generally accepted that genes and environment must be examined jointly when estimating phenotypic variance. In this work we consider two types of biological markers: genotypic markers, which characterize an observation in terms of inherited genetic information, and metagenomic marker which are related to the environment. Both types of markers are available in their millions and can be used to characterize any observation uniquely. Objective. Our focus is on detecting interactions between groups of genetic and metagenomic markers in order to gain a better understanding of the complex relationship between environment and genome in the expression of a given phenotype. Contributions. We propose a novel approach for efficiently detecting interactions between complementary datasets in a high-dimensional setting with a reduced computational cost. The method, named SICOMORE, reduces the dimension of the search space by selecting a subset of supervariables in the two complementary datasets. These supervariables are given by a weighted group structure defined on sets of variables at different scales. A Lasso selection is then applied on each type of supervariable to obtain a subset of potential interactions that will be explored via linear model testing. Results. We compare SICOMORE with other approaches in simulations, with varying sample sizes, noise, and numbers of true interactions. SICOMORE exhibits convincing results in terms of recall, as well as competitive performances with respect to running time. The method is also used to detect interaction between genomic markers in Medicago truncatula and metagenomic markers in its rhizosphere bacterial community. Software availability. A R package is available, along with its documentation and associated scripts, allowing the reader to reproduce the results presented in the paper.", "prompt": "Motivation. Association studies have been widely used to search for associations between common genetic variants observations and a given phenotype. However, it is now generally accepted that genes and environment must be examined jointly when estimating phenotypic variance. In this work we consider two types of biological markers: genotypic markers, which characterize", "orig": "an observation in terms of inherited genetic information, and metagenomic marker which are related to the environment. Both types of markers are available in their millions and can be used to characterize any observation uniquely. Objective. Our focus is on detecting interactions between groups of genetic and metagenomic markers in order to gain a better understanding", "generated": "Motivation. Association studies have been widely used to search for associations between common genetic variants observations and a given phenotype. However, it is now generally accepted that genes and environment must be examined jointly when estimating phenotypic variance. In this work we consider two types of biological markers: genotypic markers, which characterize"}
{"full_text": "Between 1998 and 2004, the planning community has seen vast progress in terms of the sizes of benchmark examples that domain-independent planners can tackle successfully. The key technique behind this progress is the use of heuristic functions based on relaxing the planning task at hand, where the relaxation is to assume that all delete lists are empty. The unprecedented success of such methods, in many commonly used benchmark examples, calls for an understanding of what classes of domains these methods are well suited for. In the investigation at hand, we derive a formal background to such an understanding. We perform a case study covering a range of 30 commonly used STRIPS and ADL benchmark domains, including all examples used in the first four international planning competitions. We *prove* connections between domain structure and local search topology -- heuristic cost surface properties -- under an idealized version of the heuristic functions used in modern planners. The idealized heuristic function is called h^+, and differs from the practically used functions in that it returns the length of an *optimal* relaxed plan, which is NP-hard to compute. We identify several key characteristics of the topology under h^+, concerning the existence/non-existence of unrecognized dead ends, as well as the existence/non-existence of constant upper bounds on the difficulty of escaping local minima and benches. These distinctions divide the (set of all) planning domains into a taxonomy of classes of varying h^+ topology. As it turns out, many of the 30 investigated domains lie in classes with a relatively easy topology. Most particularly, 12 of the domains lie in classes where FFs search algorithm, provided with h^+, is a polynomial solving mechanism. We also present results relating h^+ to its approximation as implemented in FF. The behavior regarding dead ends is provably the same. We summarize the results of an empirical investigation showing that, in many domains, the topological qualities of h^+ are largely inherited by the approximation. The overall investigation gives a rare example of a successful analysis of the connections between typical-case problem structure, and search performance. The theoretical investigation also gives hints on how the topological phenomena might be automatically recognizable by domain analysis techniques. We outline some preliminary steps we made into that direction.", "prompt": "Between 1998 and 2004, the planning community has seen vast progress in terms of the sizes of benchmark examples that domain-independent planners can tackle successfully. The key technique behind this progress is the use of heuristic functions based on relaxing the planning task at hand, where the relaxation is to assume that all delete lists", "orig": "are empty. The unprecedented success of such methods, in many commonly used benchmark examples, calls for an understanding of what classes of domains these methods are well suited for. In the investigation at hand, we derive a formal background to such an understanding. We perform a case study covering a range of 30 commonly used STRIPS", "generated": "...operations are free, allowing for very efficient planning systems that can tackle much larger instances. However, with the advent of the web and the growth of the planning community, more complex and realistic planning problems started to appear, which could not be tackled by these domain-independent planners. These planners were mainly concerned with finding the"}
{"full_text": "Context: Meta programming consists for a large part of matching, analyzing, and transforming syntax trees. Many meta programming systems process abstract syntax trees, but this requires intimate knowledge of the structure of the data type describing the abstract syntax. As a result, meta programming is error-prone, and meta programs are not resilient to evolution of the structure of such ASTs, requiring invasive, fault-prone change to these programs. Inquiry: Concrete syntax patterns alleviate this problem by allowing the meta programmer to match and create syntax trees using the actual syntax of the object language. Systems supporting concrete syntax patterns, however, require a concrete grammar of the object language in their own formalism. Creating such grammars is a costly and error-prone process, especially for realistic languages such as Java and C++. Approach: In this paper we present Concretely, a technique to extend meta programming systems with pluggable concrete syntax patterns, based on external, black box parsers. We illustrate Concretely in the context of Rascal, an open-source meta programming system and language workbench, and show how to reuse existing parsers for Java, JavaScript, and C++. Furthermore, we propose Tympanic, a DSL to declaratively map external AST structures to Rascal's internal data structures. Tympanic allows implementors of Concretely to solve the impedance mismatch between object-oriented class hierarchies in Java and Rascal's algebraic data types. Both the algebraic data type and AST marshalling code is automatically generated. Knowledge: The conceptual architecture of Concretely and Tympanic supports the reuse of pre-existing, external parsers, and their AST representation in meta programming systems that feature concrete syntax patterns for matching and constructing syntax trees. As such this opens up concrete syntax pattern matching for a host of realistic languages for which writing a grammar from scratch is time consuming and error-prone, but for which industry-strength parsers exist in the wild. Grounding: We evaluate Concretely in terms of source lines of code (SLOC), relative to the size of the AST data type and marshalling code. We show that for real programming languages such as C++ and Java, adding support for concrete syntax patterns takes an effort only in the order of dozens of SLOC. Similarly, we evaluate Tympanic in terms of SLOC, showing an order of magnitude of reduction in SLOC compared to manual implementation of the AST data types and marshalling code. Importance: Meta programming has applications in reverse engineering, reengineering, source code analysis, static analysis, software renovation, domain-specific language engineering, and many others. Processing of syntax trees is central to all of these tasks. Concrete syntax patterns improve the practice of constructing meta programs. The combination of Concretely and Tympanic has the potential to make concrete syntax patterns available with very little effort, thereby improving and promoting the application of meta programming in the general software engineering context.", "prompt": "Context: Meta programming consists for a large part of matching, analyzing, and transforming syntax trees. Many meta programming systems process abstract syntax trees, but this requires intimate knowledge of the structure of the data type describing the abstract syntax. As a result, meta programming is error-prone, and meta programs are not resilient to", "orig": "evolution of the structure of such ASTs, requiring invasive, fault-prone change to these programs. Inquiry: Concrete syntax patterns alleviate this problem by allowing the meta programmer to match and create syntax trees using the actual syntax of the object language. Systems supporting concrete syntax patterns, however, require a concrete grammar of the object language", "generated": "Meta programming consists for a large part of matching, analyzing, and transforming syntax trees. Many meta programming systems process abstract syntax trees, but this requires intimate knowledge of the structure of the data type describing the abstract syntax. As a result, meta programming is error-prone, and meta programs are not resilient to the dynamic"}
{"full_text": "Many current large-scale multiagent team implementations can be characterized as following the belief-desire-intention (BDI) paradigm, with explicit representation of team plans. Despite their promise, current BDI team approaches lack tools for quantitative performance analysis under uncertainty. Distributed partially observable Markov decision problems (POMDPs) are well suited for such analysis, but the complexity of finding optimal policies in such models is highly intractable. The key contribution of this article is a hybrid BDI-POMDP approach, where BDI team plans are exploited to improve POMDP tractability and POMDP analysis improves BDI team plan performance. Concretely, we focus on role allocation, a fundamental problem in BDI teams: which agents to allocate to the different roles in the team. The article provides three key contributions. First, we describe a role allocation technique that takes into account future uncertainties in the domain; prior work in multiagent role allocation has failed to address such uncertainties. To that end, we introduce RMTDP (Role-based Markov Team Decision Problem), a new distributed POMDP model for analysis of role allocations. Our technique gains in tractability by significantly curtailing RMTDP policy search; in particular, BDI team plans provide incomplete RMTDP policies, and the RMTDP policy search fills the gaps in such incomplete policies by searching for the best role allocation. Our second key contribution is a novel decomposition technique to further improve RMTDP policy search efficiency. Even though limited to searching role allocations, there are still combinatorially many role allocations, and evaluating each in RMTDP to identify the best is extremely difficult. Our decomposition technique exploits the structure in the BDI team plans to significantly prune the search space of role allocations. Our third key contribution is a significantly faster policy evaluation algorithm suited for our BDI-POMDP hybrid approach. Finally, we also present experimental results from two domains: mission rehearsal simulation and RoboCupRescue disaster rescue simulation.", "prompt": "Many current large-scale multiagent team implementations can be characterized as following the belief-desire-intention (BDI) paradigm, with explicit representation of team plans. Despite their promise, current BDI team approaches lack tools for quantitative performance analysis under uncertainty. Distributed partially observable Markov decision problems (POMDPs)", "orig": "are well suited for such analysis, but the complexity of finding optimal policies in such models is highly intractable. The key contribution of this article is a hybrid BDI-POMDP approach, where BDI team plans are exploited to improve POMDP tractability and POMDP analysis improves BDI team plan performance", "generated": "Many current large-scale multiagent team implementations can be characterized as following the belief-desire-intention (BDI) paradigm, with explicit representation of team plans. Despite their promise, current BDI team approaches lack tools for quantitative performance analysis under uncertainty. Distributed partially observable Markov decision problems (POMDPs)"}
{"full_text": "Recently developed spatial capture-recapture (SCR) models represent a major advance over traditional capture-recapture (CR) models because they yield explicit estimates of animal density instead of population size within an unknown area. Furthermore, unlike nonspatial CR methods, SCR models account for heterogeneity in capture probability arising from the juxtaposition of animal activity centers and sample locations. Although the utility of SCR methods is gaining recognition, the requirement that all individuals can be uniquely identified excludes their use in many contexts. In this paper, we develop models for situations in which individual recognition is not possible, thereby allowing SCR concepts to be applied in studies of unmarked or partially marked populations. The data required for our model are spatially referenced counts made on one or more sample occasions at a collection of closely spaced sample units such that individuals can be encountered at multiple locations. Our approach includes a spatial point process for the animal activity centers and uses the spatial correlation in counts as information about the number and location of the activity centers. Camera-traps, hair snares, track plates, sound recordings, and even point counts can yield spatially correlated count data, and thus our model is widely applicable. A simulation study demonstrated that while the posterior mean exhibits frequentist bias on the order of 5-10% in small samples, the posterior mode is an accurate point estimator as long as adequate spatial correlation is present. Marking a subset of the population substantially increases posterior precision and is recommended whenever possible. We applied our model to avian point count data collected on an unmarked population of the northern parula (Parula americana) and obtained a density estimate (posterior mode) of 0.38 (95% CI: 0.19-1.64) birds/ha. Our paper challenges sampling and analytical conventions in ecology by demonstrating that neither spatial independence nor individual recognition is needed to estimate population density - rather, spatial dependence can be informative about individual distribution and density.", "prompt": "Recently developed spatial capture-recapture (SCR) models represent a major advance over traditional capture-recapture (CR) models because they yield explicit estimates of animal density instead of population size within an unknown area. Furthermore, unlike nonspatial CR methods, SCR models account for heterogeneity in capture probability arising from the juxtaposition", "orig": "of animal activity centers and sample locations. Although the utility of SCR methods is gaining recognition, the requirement that all individuals can be uniquely identified excludes their use in many contexts. In this paper, we develop models for situations in which individual recognition is not possible, thereby allowing SCR concepts to be applied in studies of unmarked or", "generated": "Recently developed spatial capture-recapture (SCR) models represent a major advance over traditional capture-recapture (CR) models because they yield explicit estimates of animal density instead of population size within an unknown area. Furthermore, unlike nonspatial CR methods, SCR models account for heterogeneity in capture probability arising from the juxtaposition"}
{"full_text": "One of the major terminological forces driving ICT integration in research today is that of \"big data.\" While the phrase sounds inclusive and integrative, \"big data\" approaches are highly selective, excluding input that cannot be effectively structured, represented, or digitised. Data of this complex sort is precisely the kind that human activity produces, but the technological imperative to enhance signal through the reduction of noise does not accommodate this richness. Data and the computational approaches that facilitate \"big data\" have acquired a perceived objectivity that belies their curated, malleable, reactive, and performative nature. In an input environment where anything can \"be data\" once it is entered into the system as \"data,\" data cleaning and processing, together with the metadata and information architectures that structure and facilitate our cultural archives acquire a capacity to delimit what data are. This engenders a process of simplification that has major implications for the potential for future innovation within research environments that depend on rich material yet are increasingly mediated by digital technologies. This paper presents the preliminary findings of the European-funded KPLEX (Knowledge Complexity) project which investigates the delimiting effect digital mediation and datafication has on rich, complex cultural data. The paper presents a systematic review of existing implicit definitions of data, elaborating on the implications of these definitions and highlighting the ways in which metadata and computational technologies can restrict the interpretative potential of data. It sheds light on the gap between analogue or augmented digital practices and fully computational ones, and the strategies researchers have developed to deal with this gap. The paper proposes a reconceptualisation of data as it is functionally employed within digitally-mediated research so as to incorporate and acknowledge the richness and complexity of our source materials.", "prompt": "One of the major terminological forces driving ICT integration in research today is that of \"big data.\" While the phrase sounds inclusive and integrative, \"big data\" approaches are highly selective, excluding input that cannot be effectively structured, represented, or digitised. Data of this complex sort is precisely the kind that human", "orig": "activity produces, but the technological imperative to enhance signal through the reduction of noise does not accommodate this richness. Data and the computational approaches that facilitate \"big data\" have acquired a perceived objectivity that belies their curated, malleable, reactive, and performative nature. In an input environment where anything can \"be", "generated": "One of the major terminological forces driving ICT integration in research today is that of \"big data.\" While the phrase sounds inclusive and integrative, \"big data\" approaches are highly selective, excluding input that cannot be effectively structured, represented, or digitised. Data of this complex sort is precisely the kind that humans"}
{"full_text": "With the recognized crisis of credibility in scientific research, there is a growth of reproducibility studies in computer science, and although existing surveys have reviewed reproducibility from various perspectives, especially very specific technological issues, they do not address the author-publisher relationship in the publication of reproducible computational scientific articles. This aspect requires significant attention because it is the basis for reliable research. We have found a large gap between the reproducibility-oriented practices, journal policies, recommendations, publisher artifact Description/Evaluation guidelines, submission guides, technological reproducibility evolution, and its effective adoption to contribute to tackling the crisis. We conducted a narrative survey, a comprehensive overview and discussion identifying the mutual efforts required from Authors, Journals, and Technological actors to achieve reproducibility research. The relationship between authors and scientific journals in their mutual efforts to jointly improve the reproducibility of scientific results is analyzed. Eventually, we propose recommendations for the journal policies, as well as a unified and standardized Reproducibility Guide for the submission of scientific articles for authors. The main objective of this work is to analyze the implementation and experiences of reproducibility policies, techniques and technologies, standards, methodologies, software, and data management tools required for scientific reproducible publications. Also, the benefits and drawbacks of such an adoption, as well as open challenges and promising trends, to propose possible strategies and efforts to mitigate the identified gaps. To this purpose, we analyzed 200 scientific articles, surveyed 16 Computer Science journals, and systematically classified them according to reproducibility strategies, technologies, policies, code citation, and editorial business. We conclude there is still a reproducibility gap in scientific publications, although at the same time also the opportunity to reduce this gap with the joint effort of authors, publishers, and technological providers.", "prompt": "With the recognized crisis of credibility in scientific research, there is a growth of reproducibility studies in computer science, and although existing surveys have reviewed reproducibility from various perspectives, especially very specific technological issues, they do not address the author-publisher relationship in the publication of reproducible computational scientific articles. This aspect", "orig": "requires significant attention because it is the basis for reliable research. We have found a large gap between the reproducibility-oriented practices, journal policies, recommendations, publisher artifact Description/Evaluation guidelines, submission guides, technological reproducibility evolution, and its effective adoption to contribute to tackling the crisis. We conducted a narrative survey,", "generated": "With the recognized crisis of credibility in scientific research, there is a growth of reproducibility studies in computer science, and although existing surveys have reviewed reproducibility from various perspectives, especially very specific technological issues, they do not address the author-publisher relationship in the publication of reproducible computational scientific articles. This aspect"}
{"full_text": "We study the 2D full Ginzburg-Landau energy with a periodic rapidly oscillating, discontinuous and [strongly] diluted pinning term using a perturbative argument. This energy models the state of an heterogeneous type II supercon-ductor submitted to a magnetic field. We calculate the value of the first critical field which links the presence of vorticity defects with the intensity of the applied magnetic field. Then we prove a standard dependance of the quantized vorticity defects with the intensity of the applied field. Our study includes the case of a London solution having several minima. The macroscopic location of the vor-ticity defects is understood with the famous Bethuel-Brezis-H{\\'e}lein renormalized energy. The mesoscopic location, i.e., the arrangement of the vorticity defects around the minima of the London solution, is the same than in the homogenous case. The microscopic location is exactly the same than in the heterogeneous case without magnetic field. We also compute the value of secondary critical fields that increment the quantized vorticity. 1. Introdution This article studies the pinning phenomenon in type-II superconducting composites. Superconductivity is a property that appears in certain materials cooled below a critical temperature. These materials are called superconductors. Superconductivity is characterized by a total absence of electrical resistance and a perfect diamagnetism. Unfortunately, when the imposed conditions are too intense, superconductivity is destroyed in certain areas of the material called vorticity defects. We are interested in type II superconductors which are characterized by the fact that the vorticity defects first appear in small areas. Their number increases with the intensity of the conditions imposed until filling the material. For example, when the intensity h ex of an applied magnetic field exceeds a first threshold, the first vorticity defects appear: the magnetic field begins to penetrate the superconductor. The penetration is done along thin wires and may move resulting an energy dissipation. These motions may be limited by trapping the vorticity defects in small areas. The behavior of a superconductor is modeled by minimizers of a Ginzburg-Landau type energy. In order to study the presence of traps for the vorticity defects we consider an energy including a pinning term that models impurities in the supercon-ductor. These impurities would play the role of traps for the vorticity defects. We are thus lead to the subject of this article: the type-II superconducting composites with impurities.", "prompt": "We study the 2D full Ginzburg-Landau energy with a periodic rapidly oscillating, discontinuous and [strongly] diluted pinning term using a perturbative argument. This energy models the state of an heterogeneous type II supercon-ductor submitted to a magnetic field. We calculate the value", "orig": "of the first critical field which links the presence of vorticity defects with the intensity of the applied magnetic field. Then we prove a standard dependance of the quantized vorticity defects with the intensity of the applied field. Our study includes the case of a London solution having several minima. The macroscopic location", "generated": "We study the 2D full Ginzburg-Landau energy with a periodic rapidly oscillating, discontinuous and strongly diluted pinning term using a perturbative argument. This energy models the state of an heterogeneous type II superconductor submitted to a magnetic field. We calculate the values of the critical current"}
{"full_text": "This book deals with functions allowing to express the dissimilarity (discrepancy) between two data fields or ''divergence functions'' with the aim of applications to linear inverse problems. Most of the divergences found in the litterature are used in the field of information theory to quantify the difference between two probability density functions, that is between positive data whose sum is equal to one. In such context, they take a simplified form that is not adapted to the problems considered here, in which the data fields are non-negative but with a sum not necessarily equal to one. In a systematic way, we reconsider the classical divergences and we give their forms adapted to inverse problems. To this end, we will recall the methods allowing to build such divergences, and propose some generalizations. The resolution of inverse problems implies systematically the minimisation of a divergence between physical measurements and a model depending of the unknown parameters. In the context image reconstruction, the model is generally linear and the constraints that must be taken into account are the non-negativity as well as (if necessary) the sum constraint of the unknown parameters. To take into account in a simple way the sum constraint, we introduce the class of scale invariant or affine invariant divergences. Such divergences remains unchanged when the model parameters are multiplied by a constant positive factor. We show the general properties of the invariance factors, and we give some interesting characteristics of such divergences.An extension of such divergences allows to obtain the property of invariance with respect to both the arguments of the divergences; this characteristic can be used to introduce the smoothness regularization of inverse problems, that is a regularisation in the sense of Tikhonov.We then develop in a last step, minimisation methods of the divergences subject to non-negativity and sum constraints on the solution components. These methods are founded on the Karush-Kuhn-Tucker conditions that must be fulfilled at the optimum. The Tikhonov regularization is considered in these methods.Chapter 11 associated with Appendix 9 deal with the application to the NMF, while Chapter 12 is dedicated to the Blind Deconvolution problem.In these two chapters, the interest of the scale invariant divergences is highlighted.", "prompt": "This book deals with functions allowing to express the dissimilarity (discrepancy) between two data fields or ''divergence functions'' with the aim of applications to linear inverse problems. Most of the divergences found in the litterature are used in the field of information theory to quantify the difference between", "orig": "two probability density functions, that is between positive data whose sum is equal to one. In such context, they take a simplified form that is not adapted to the problems considered here, in which the data fields are non-negative but with a sum not necessarily equal to one. In a systematic way, we reconsider the classical diver", "generated": "This book deals with functions allowing to express the dissimilarity (discrepancy) between two data fields or ''divergence functions'' with the aim of applications to linear inverse problems. Most of the divergences found in the litterature are used in the field of information theory to quantify the difference between"}
{"full_text": "In the presence of a missing response, reweighting the complete case subsample by the inverse of nonmissing probability is both intuitive and easy to implement. When the population totals of some auxiliary variables are known and when the inclusion probabilities are known by design, survey statisticians have developed calibration methods for improving efficiencies of the inverse probability weighting estimators and the methods can be applied to missing data analysis. Model-based calibration has been proposed in the survey sampling literature, where multidimensional auxiliary variables are first summarized into a predictor function from a working regression model. Usually, one working model is being proposed for each parameter of interest and results in different sets of calibration weights for estimating different parameters. This paper considers calibration using multiple working regression models for estimating a single or multiple parameters. Contrary to a common belief that overfitting hurts efficiency, we present three rather unexpected results. First, when the missing probability is correctly specified and multiple working regression models for the conditional mean are posited, calibration enjoys an oracle property: the same semiparametric efficiency bound is attained as if the true outcome model is known in advance. Second, when the missing data mechanism is misspecified, calibration can still be a consistent estimator when any one of the outcome regression models is correctly specified. Third, a common set of calibration weights can be used to improve efficiency in estimating multiple parameters of interest and can simultaneously attain semiparametric efficiency bounds for all parameters of interest. We provide connections of a wide class of calibration estimators, constructed based on generalized empirical likelihood, to many existing estimators in biostatistics, econometrics and survey sampling and perform simulation studies to show that the finite sample properties of calibration estimators conform well with the theoretical results being studied.", "prompt": "In the presence of a missing response, reweighting the complete case subsample by the inverse of nonmissing probability is both intuitive and easy to implement. When the population totals of some auxiliary variables are known and when the inclusion probabilities are known by design, survey statisticians have developed calibration methods for improving efficiencies of the", "orig": "inverse probability weighting estimators and the methods can be applied to missing data analysis. Model-based calibration has been proposed in the survey sampling literature, where multidimensional auxiliary variables are first summarized into a predictor function from a working regression model. Usually, one working model is being proposed for each parameter of interest and results in different sets", "generated": "In the presence of a missing response, reweighting the complete case subsample by the inverse of nonmissing probability is both intuitive and easy to implement. When the population totals of some auxiliary variables are known and when the inclusion probabilities are known by design, survey statisticians have developed calibration methods for improving efficiencies of this"}
{"full_text": "The interplay between viscoelasticity and inertia in dilute polymer solutions at high deformation rates can result in inertio-elastic instabilities. The nonlinear evolution of these instabilities generates a state of turbulence with significantly different spatio-temporal features compared to Newtonian turbulence, termed elasto-inertial turbulence (EIT). We explore EIT by studying the dynamics of a submerged planar jet of a dilute aqueous polymer solution injected into a quiescent tank of water using a combination of schlieren imaging and laser Doppler velocimetry (LDV). We show how fluid elasticity has a nonmonotonic effect on the jet stability depending on its magnitude, creating two distinct regimes in which elastic effects can either destabilize or stabilize the jet. In agreement with linear stability analyses of viscoelastic jets, an inertio-elastic shear-layer instability emerges near the edge of the jet for small levels of elasticity, independent of bulk undulations in the fluid column. The growth of this disturbance mode destabilizes the flow, resulting in a turbulence transition at lower Reynolds numbers and closer to the nozzle compared to the conditions required for the transition to turbulence in a Newtonian jet. Increasing the fluid elasticity merges the shear-layer instability into a bulk instability of the jet column. In this regime, elastic tensile stresses generated in the shear layer act as an \"elastic membrane\" that partially stabilizes the flow, retarding the transition to turbulence to higher levels of inertia and greater distances from the nozzle. In the fully turbulent state far from the nozzle, planar viscoelastic jets exhibit unique spatio-temporal features associated with EIT. The time-averaged angle of jet spreading, an Eulerian measure of the degree of entrainment, and the centerline velocity of the jets both evolve self-similarly with distance from the nozzle. The autocovariance of the schlieren images in the fully turbulent region of the jets shows coherent structures that are elongated in the streamwise direction, consistent with the suppression of streamwise vortices by elastic stresses. These coherent structures give a higher spectral energy to small frequency modes in EIT characterized by LDV measurements of the velocity fluctuations at the jet centerline. Finally, our LDV measurements reveal a frequency spectrum characterized by a $-3$ power-law exponent, different from the well-known $-5/3$ power-law exponent characteristic of Newtonian turbulence.", "prompt": "The interplay between viscoelasticity and inertia in dilute polymer solutions at high deformation rates can result in inertio-elastic instabilities. The nonlinear evolution of these instabilities generates a state of turbulence with significantly different spatio-temporal features compared to Newtonian turbulence, termed elasto-inertial turbulence (", "orig": "EIT). We explore EIT by studying the dynamics of a submerged planar jet of a dilute aqueous polymer solution injected into a quiescent tank of water using a combination of schlieren imaging and laser Doppler velocimetry (LDV). We show how fluid elasticity has a nonmonotonic", "generated": "The interplay between viscoelasticity and inertia in dilute polymer solutions at high deformation rates can result in inertio-elastic instabilities. The nonlinear evolution of these instabilities generates a state of turbulence with significantly different spatio-temporal features compared to Newtonian turbulence, termed elasto-inertial turbulence."}
{"full_text": "In a field of research about general reasoning mechanisms, it is essential to have appropriate benchmarks. Ideally, the benchmarks should reflect possible applications of the developed technology. In AI Planning, researchers more and more tend to draw their testing examples from the benchmark collections used in the International Planning Competition (IPC). In the organization of (the deterministic part of) the fourth IPC, IPC-4, the authors therefore invested significant effort to create a useful set of benchmarks. They come from five different (potential) real-world applications of planning: airport ground traffic control, oil derivative transportation in pipeline networks, model-checking safety properties, power supply restoration, and UMTS call setup. Adapting and preparing such an application for use as a benchmark in the IPC involves, at the time, inevitable (often drastic) simplifications, as well as careful choice between, and engineering of, domain encodings. For the first time in the IPC, we used compilations to formulate complex domain features in simple languages such as STRIPS, rather than just dropping the more interesting problem constraints in the simpler language subsets. The article explains and discusses the five application domains and their adaptation to form the PDDL test suites used in IPC-4. We summarize known theoretical results on structural properties of the domains, regarding their computational complexity and provable properties of their topology under the h+ function (an idealized version of the relaxed plan heuristic). We present new (empirical) results illuminating properties such as the quality of the most wide-spread heuristic functions (planning graph, serial planning graph, and relaxed plan), the growth of propositional representations over instance size, and the number of actions available to achieve each fact; we discuss these data in conjunction with the best results achieved by the different kinds of planners participating in IPC-4.", "prompt": "In a field of research about general reasoning mechanisms, it is essential to have appropriate benchmarks. Ideally, the benchmarks should reflect possible applications of the developed technology. In AI Planning, researchers more and more tend to draw their testing examples from the benchmark collections used in the International Planning Competition (IPC). In the organization of (", "orig": "the deterministic part of) the fourth IPC, IPC-4, the authors therefore invested significant effort to create a useful set of benchmarks. They come from five different (potential) real-world applications of planning: airport ground traffic control, oil derivative transportation in pipeline networks, model-checking safety properties, power supply restoration, and", "generated": "In a field of research about general reasoning mechanisms, it is essential to have appropriate benchmarks. Ideally, the benchmarks should reflect possible applications of the developed technology. In AI Planning, researchers more and more tend to draw their testing examples from the benchmark collections used in the International Planning Competition (IPC). In the organization of IPC"}
{"full_text": "In the frame of the test of NISP instrument for ESA Euclid mission, the question was raised to perform a metrology measurement of different components during the thermal vacuum test of NISP instrument. NISP will be tested at Laboratoire d'Astrophysique de Marseille (LAM) in ERIOS chamber under vacuum and thermal conditions in order to qualify the instrument in its operating environment and to perform the final acceptance test before delivery to the payload. One of the main objectives of the test campaign will be the measurement of the focus position of NISP image plane with respect to the EUCLID object plane. To simulate the EUCLID object plane, a telescope simulator with a very well know focal distance will be installed in front of NISP into ERIOS chamber. We need to measure at cold and vacuum the position of reflectors installed on NISP instrument and the telescope simulator. From these measurements, we will provide at operational temperature the measurement of references frames set on the telescope simulator and NISP, the knowledge of the coordinates of the object point source provided by the telescope simulator and the measurement of the angle between the telescope simulator optical axis and NISP optical axis. In this context, we have developed a metrology method based on the use of a laser tracker to measure the position of the reflectors inside ERIOS. The laser tracker is installed outside the vacuum chamber and measure through a curved window reflectors put inside the chamber either at ambient pressure or vacuum pressure. Several tests campaigns have been done at LAM to demonstrate the measurement performance with this configuration. Using a well know reflectors configuration, we show that it is possible to correct the laser tracker measurement from the window disturbances and from the vacuum impact. A corrective term is applied to the data and allows retrieving the real coordinates of the reflectors with a bias lower than 30$\\mu$m, which is lower than the laser tracker measurement uncertainties estimated at 60$\\mu$m. No additional error term of the laser tracker measurement is observed when using the laser tracker with the curved window and in vacuum, comparing with a classical use of the laser tracker. With these test campaign, we have been able to demonstrate the possibility to use a laser tracker to measure in real time during a vacuum thermal test the position of different mechanical parts into a vacuum chamber with an accuracy better than 60$\\mu$m.", "prompt": "In the frame of the test of NISP instrument for ESA Euclid mission, the question was raised to perform a metrology measurement of different components during the thermal vacuum test of NISP instrument. NISP will be tested at Laboratoire d'Astrophysique de Marseille (LAM) in ERIOS", "orig": "chamber under vacuum and thermal conditions in order to qualify the instrument in its operating environment and to perform the final acceptance test before delivery to the payload. One of the main objectives of the test campaign will be the measurement of the focus position of NISP image plane with respect to the EUCLID object plane. To simulate the", "generated": "In the frame of the test of NISP instrument for ESA Euclid mission, the question was raised to perform a metrology measurement of different components during the thermal vacuum test of NISP instrument. NISP will be tested at Laboratoire d'Astrophysique de Marseille (LAM) in ERIOS"}
{"full_text": "In this paper, a few novel data hiding techniques are proposed. These techniques are improvements over the classical LSB data hiding technique and the Fibonacci LSB data-hiding technique proposed by Battisti et al. \\cite{r1}. The classical LSB technique is the simplest, but using this technique it is possible to embed only in first few bit-planes, since image quality becomes drastically distorted when embedding in higher bit-planes. Battisti et al. \\cite{r1} proposed an improvement over this by using Fibonacci decomposition technique and generating a different set of virtual bit-planes all together, thereby increasing the number of bit-planes. In this paper, first we mathematically model and generalize this particular approach of virtual bit-plane generation. Then we propose two novel embedding techniques, both of which are special-cases of our generalized model. The first embedding scheme is based on decomposition of a number (pixel-value) in sum of prime numbers, while the second one is based on decomposition in sum of natural numbers. Each of these particular representations generates a different set of (virtual) bit-planes altogether, suitable for embedding purposes. They not only allow one to embed secret message in higher bit-planes but also do it without much distortion, with a much better stego-image quality, in a reliable and secured manner, guaranteeing efficient retrieval of secret message. A comparative performance study between the classical Least Significant Bit (LSB) method, the data hiding technique using Fibonacci -p-Sequence decomposition and our proposed schemes has been done. Theoretical analysis indicates that image quality of the stego-image hidden by the technique using Fibonacci decomposition improves against simple LSB substitution method, while the same using the prime decomposition method improves drastically against that using Fibonacci decomposition technique, and finally the natural number decomposition method is a further improvement against that using prime decomposition technique. Also, optimality for the last technique is proved. For both of our data-hiding techniques, the experimental results show that, the stego-image is visually indistinguishable from the original cover image.", "prompt": "In this paper, a few novel data hiding techniques are proposed. These techniques are improvements over the classical LSB data hiding technique and the Fibonacci LSB data-hiding technique proposed by Battisti et al. \\cite{r1}. The classical LSB technique is the simplest, but using this technique it is possible to embed only", "orig": "in first few bit-planes, since image quality becomes drastically distorted when embedding in higher bit-planes. Battisti et al. \\cite{r1} proposed an improvement over this by using Fibonacci decomposition technique and generating a different set of virtual bit-planes all together, thereby increasing the number of bit-planes.", "generated": "In this paper, a few novel data hiding techniques are proposed. These techniques are improvements over the classical LSB data hiding technique and the Fibonacci LSB data-hiding technique proposed by Battisti et al. \\cite{r1}. The classical LSB technique is the simplest, but using this technique it is possible to embed only"}
{"full_text": "Do our physics curricula provide the appropriate data management competences in a world where data are considered a crucial resource and substantial funding is available for building a national research data infrastructure (German: Nationale Forschungsdateninfrastuktur = NFDI)? Although basic data evaluation and systematic documentation are practiced when students first come into contact with data, particularly in experimental physics lab courses, they do not meet the increasing demands of research and professional practice to deal with the analysis of huge datasets. In many cases, the problem starts with the fact that there is no consensus on a suitable entry-level programming language. At the Department of Physics at Friedrich-Alexander-Universit\\\"at Erlangen-N\\\"urnberg, we have made minor adjustments to the physics curriculum in recent years, which we present in this article. We placed data management competences early in the bachelor curriculum, which has resulted in considerable advantages throughout the further course of studies. The authors feel that students are quickly moving into the fast lane in data management; we can already see in our research groups that they are becoming a driving force towards modern research data management. -- Vermitteln unsere Physikcurricula die passenden Datenverarbeitungskompetenzen in einer Welt, in der Daten als entscheidende Ressource betrachtet werden und erhebliche F\\\"ordermittel f\\\"ur eine Nationale Forschungsdateninfrastruktur (NFDI) bereitstehen? Beim Erstkontakt mit Daten, also insbesondere in den Praktika der Experimentalphysik, werden zwar elementare Datenevaluation und systematische Dokumentation einge\\\"ubt, diese gen\\\"ugen aber nicht den steigenden Anspr\\\"uchen der Forschung und der Berufspraxis, sich zunehmend mit der Analyse gro{\\ss}er Datenmengen zu befassen. Es scheitert oft schon an einem Konsens \\\"uber eine geeignete Einstiegsprogrammiersprache. Am Department Physik der Friedrich-Alexander-Universit\\\"at Erlangen-N\\\"urnberg haben wir in den letzten Jahren kleinere Anpassungen im Physikcurriculum vorgenommen, die wir in diesem Artikel vorstellen. Datenkompetenz wurde fr\\\"uh im Bachelorstudium platziert, woraus sich erhebliche Vorteile f\\\"ur den weiteren Studienverlauf ergeben haben. Die Autoren k\\\"onnen sich des Eindrucks nicht erwehren, dass die Studierenden in puncto Datenkompetenz schnell auf die \\\"Uberholspur gehen; wir sehen bereits jetzt in unseren Arbeitsgruppen, dass sie sich als treibende Kr\\\"afte hin zu einem modernen Forschungsdatenmanagement erweisen.", "prompt": "Do our physics curricula provide the appropriate data management competences in a world where data are considered a crucial resource and substantial funding is available for building a national research data infrastructure (German: Nationale Forschungsdateninfrastuktur = NFDI)? Although basic data evaluation and systematic documentation are practiced when students", "orig": "first come into contact with data, particularly in experimental physics lab courses, they do not meet the increasing demands of research and professional practice to deal with the analysis of huge datasets. In many cases, the problem starts with the fact that there is no consensus on a suitable entry-level programming language. At the Department of Physics at", "generated": "Do our physics curricula provide the appropriate data management competences in a world where data are considered a crucial resource and substantial funding is available for building a national research data infrastructure (German: Nationale Forschungsdateninfrastuktur = NFDI)? Although basic data evaluation and systematic documentation are practiced when students"}
{"full_text": "The majority of the proteins encoded in the genomes of eukaryotes contain more than one domain. Reasons for high prevalence of multi-domain proteins in various organisms have been attributed to higher stability and functional and folding advantages over single-domain proteins. Despite these advantages, many proteins are composed of only one domain while their homologous domains are part of multi-domain proteins. In the study presented here, differences in the properties of protein domains in single-domain and multi-domain systems and their influence on functions are discussed. We studied 20 pairs of identical protein domains, which were crystallized in two forms (a) tethered to other proteins domains and (b) tethered to fewer protein domains than (a) or not tethered to any protein domain. Results suggest that tethering of domains in multi-domain proteins influences the structural, dynamic and energetic properties of the constituent protein domains. 50% of the protein domain pairs show significant structural deviations while 90% of the protein domain pairs show differences in dynamics and 12% of the residues show differences in the energetics. To gain further insights on the influence of tethering on the function of the domains, 4 pairs of homologous protein domains, where one of them is a full-length single-domain protein and the other protein domain is a part of a multi-domain protein, were studied. Analyses showed that identical and structurally equivalent functional residues show differential dynamics in homologous protein domains, though comparable dynamics between in-silico generated chimera protein and multi-domain proteins were observed. From these observations, the differences observed in the functions of homologous proteins could be attributed to the presence of tethered domain. Overall, we conclude that tethered domains in multi-domain proteins not only provide stability or folding advantages but also influence pathways resulting in differences in function or regulatory properties.", "prompt": "The majority of the proteins encoded in the genomes of eukaryotes contain more than one domain. Reasons for high prevalence of multi-domain proteins in various organisms have been attributed to higher stability and functional and folding advantages over single-domain proteins. Despite these advantages, many proteins are composed of only one domain while their homolog", "orig": "ous domains are part of multi-domain proteins. In the study presented here, differences in the properties of protein domains in single-domain and multi-domain systems and their influence on functions are discussed. We studied 20 pairs of identical protein domains, which were crystallized in two forms (a) tethered to other proteins domains and", "generated": "The majority of the proteins encoded in the genomes of eukaryotes contain more than one domain. Reasons for high prevalence of multi-domain proteins in various organisms have been attributed to higher stability and functional and folding advantages over single-domain proteins. Despite these advantages, many proteins are composed of only one domain while their homolog"}
{"full_text": "Context. The direct detection of exoplanets with high-contrast imaging requires advanced data processing methods to disentangle potential planetary signals from bright quasi-static speckles. Among them, angular differential imaging (ADI) permits potential planetary signals with a known rotation rate to be separated from instrumental speckles that are either statics or slowly variable. The method presented in this paper, called ANDROMEDA for ANgular Differential OptiMal Exoplanet Detection Algorithm is based on a maximum likelihood approach to ADI and is used to estimate the position and the flux of any point source present in the field of view. Aims. In order to optimize and experimentally validate this previously proposed method, we applied ANDROMEDA to real VLT/NaCo data. In addition to its pure detection capability, we investigated the possibility of defining simple and efficient criteria for automatic point source extraction able to support the processing of large surveys. Methods. To assess the performance of the method, we applied ANDROMEDA on VLT/NaCo data of TYC-8979-1683-1 which is surrounded by numerous bright stars and on which we added synthetic planets of known position and flux in the field. In order to accommodate the real data properties, it was necessary to develop additional pre-processing and post-processing steps to the initially proposed algorithm. We then investigated its skill in the challenging case of a well-known target, $\\beta$ Pictoris, whose companion is close to the detection limit and we compared our results to those obtained by another method based on principal component analysis (PCA). Results. Application on VLT/NaCo data demonstrates the ability of ANDROMEDA to automatically detect and characterize point sources present in the image field. We end up with a robust method bringing consistent results with a sensitivity similar to the recently published algorithms, with only two parameters to be fine tuned. Moreover, the companion flux estimates are not biased by the algorithm parameters and do not require a posteriori corrections. Conclusions. ANDROMEDA is an attractive alternative to current standard image processing methods that can be readily applied to on-sky data.", "prompt": "Context. The direct detection of exoplanets with high-contrast imaging requires advanced data processing methods to disentangle potential planetary signals from bright quasi-static speckles. Among them, angular differential imaging (ADI) permits potential planetary signals with a known rotation rate to be separated from instrumental speckles that are either", "orig": "statics or slowly variable. The method presented in this paper, called ANDROMEDA for ANgular Differential OptiMal Exoplanet Detection Algorithm is based on a maximum likelihood approach to ADI and is used to estimate the position and the flux of any point source present in the field of view. Aims.", "generated": "Context. The direct detection of exoplanets with high-contrast imaging requires advanced data processing methods to disentangle potential planetary signals from bright quasi-static speckles. Among them, angular differential imaging (ADI) permits potential planetary signals with a known rotation rate to be separated from instrumental speckles that are either"}
{"full_text": "This paper shortly summarises the experimental results obtained since 2011 by a large European academic consortium for the scientific conceptualisation, the definition of the technical requirements, the generation of experimental data, and the validation of a numerical code, for the Pulsating Heat Pipes (PHP) experiment on the International Space Station (ISS). The PHP is a passive, wickless thermal device, whereby a two-phase fluid, forming liquid plugs and vapour slugs, moves with a pulsating or circulating motion inside a meandering tube or channel. The PHP may have a very broad range of geometries (flat, tubular, 3D structured), it can dissipate heat from large areas, and it can be suitable for high power applications with low/medium heat fluxes. PHP functioning is based on the capillary effect, which provides the existence of liquid plugs completely filling the channel cross-section, in a way that any expansion or contraction of the vapour slugs will naturally generate a movement of the fluid along the channel axis. For this, it is important that the channel has a cross-section size below a given threshold, which depends on the liquid surface tension and (for a static fluid) on the gravity acceleration. In space, when only residual accelerations are acting, such a static size threshold is virtually infinite, while a finite dynamic threshold exists even in the absence of gravity. The concept of a ''Space PHP'' was originally developed in 2014 by the team, and from then 17 Parabolic Flight Campaigns (PFC) and 3 Sounding Rocket (SR) experiments have been carried out to generate the data for the preparation of an experiment targeting a Low Earth Orbit (LEO) mission. Both a tubular and a flat plate PHP have been successfully tested in reduced gravity and on ground, by using different combinations of fluids and building materials. The need for having an experiment on a LEO environment is mainly because, during a PFC, only 22sec of reduced gravity are possible, which is a period below the characteristic time for reaching a steady state condition for almost all of the tested devices. Instead, a steady state was reached using the SR campaigns: in this case however, only one experimental condition was achievable, and long-duration data of the PHP performance still remains beyond reach. Several measurement methodologies have been used to characterise the Space PHP, like infrared analysis, high-speed camera visualisation techniques, with data processed with different techniques, from wavelets to inverse heat transfer problem solution. The results clearly showed that PHPs are very interesting for space applications due to their simplicity of construction, the capacity to transfer heat up to several hundred watts, a high power/weight ratio, their geometrical adaptability, and, in particular, the Space PHP will be a breakthrough technology for space thermal management.", "prompt": "This paper shortly summarises the experimental results obtained since 2011 by a large European academic consortium for the scientific conceptualisation, the definition of the technical requirements, the generation of experimental data, and the validation of a numerical code, for the Pulsating Heat Pipes (PHP) experiment on the International Space Station (", "orig": "ISS). The PHP is a passive, wickless thermal device, whereby a two-phase fluid, forming liquid plugs and vapour slugs, moves with a pulsating or circulating motion inside a meandering tube or channel. The PHP may have a very broad range of geometries (flat, tubular, 3", "generated": "This paper shortly summarises the experimental results obtained since 2011 by a large European academic consortium for the scientific conceptualisation, the definition of the technical requirements, the generation of experimental data, and the validation of a numerical code, for the Pulsating Heat Pipes (PHP) experiment on the International Space Station."}
{"full_text": "Many complex systems are modeled through complex networks whose analysis reveals typical topological properties. Amongst those, the community structure is one of the most studied. Many methods are proposed to detect communities, not only in plain, but also in attributed, directed or even dynamic networks. A community structure takes the form of a partition of the node set, which must then be characterized relatively to the properties of the studied system. We propose a method to support such a characterization task. We define a sequence-based representation of networks, combining temporal information, topological measures, and nodal attributes. We then characterize communities using the most representative emerging sequential patterns of its nodes. This also allows detecting unusual behavior in a community. We describe an empirical study of a network of scientific collaborations.---De nombreux syst\\`emes complexes sont \\'etudi\\'es via l'analyse de r\\'eseaux dits complexes ayant des propri\\'et\\'es topologiques typiques. Parmi cellesci, les structures de communaut\\'es sont particuli\\`erement \\'etudi\\'ees. De nombreuses m\\'ethodes permettent de les d\\'etecter, y compris dans des r\\'eseaux contenant des attributs nodaux, des liens orient\\'es ou \\'evoluant dans le temps. La d\\'etection prend la forme d'une partition de l'ensemble des noeuds, qu'il faut ensuite caract\\'eriser relativement au syst\\`eme mod\\'elis\\'e. Nous travaillons sur l'assistance \\`a cette t\\^ache de caract\\'erisation. Nous proposons une repr\\'esentation des r\\'eseaux sous la forme de s\\'equences de descripteurs de noeuds, qui combinent les informations temporelles, les mesures topologiques, et les valeurs des attributs nodaux. Les communaut\\'es sont caract\\'eris\\'ees au moyen des motifs s\\'equentiels \\'emergents les plus repr\\'esentatifs issus de leurs noeuds. Ceci permet notamment la d\\'etection de comportements inhabituels au sein d'une communaut\\'e. Nous d\\'ecrivons une \\'etude empirique sur un r\\'eseau de collaboration scientifique.", "prompt": "Many complex systems are modeled through complex networks whose analysis reveals typical topological properties. Amongst those, the community structure is one of the most studied. Many methods are proposed to detect communities, not only in plain, but also in attributed, directed or even dynamic networks. A community structure takes the form of a partition", "orig": "of the node set, which must then be characterized relatively to the properties of the studied system. We propose a method to support such a characterization task. We define a sequence-based representation of networks, combining temporal information, topological measures, and nodal attributes. We then characterize communities using the most representative emerging sequential patterns of", "generated": "Many complex systems are modeled through complex networks whose analysis reveals typical topological properties. Amongst those, the community structure is one of the most studied. Many methods are proposed to detect communities, not only in plain, but also in attributed, directed or even dynamic networks. A community structure takes the form of a partition"}
{"full_text": "A long time ago, Brochard and de Gennes predicted the possibility of significantly decreasing the critical magnetic field of the Fredericksz transition (the magnetic Fredericksz threshold) in a mixture of nematic liquid crystals and ferromagnetic particles, the so-called ferronematics. This phenomenon has rarely been measured, usually due to soft homeotropic anchoring induced at the nanoparticle surface. Here we present an optical study of the magnetic Fredericksz transition combined with a light scattering study of the classical nematic liquid crystal, 5CB, doped with 6 nm diameter magnetic and non-magnetic nanoparticles. Surprisingly, for both nanoparticles, we observe at room temperature a net decrease of the threshold field of the Fredericksz transition at low nanoparticle concentrations, which appears associated with a coating of the nanoparticles by a brush of polydimethylsiloxane copolymer chains inducing planar anchoring of the director on the nanoparticle surface. Moreover the magnetic Fredericksz threshold exhibits non-monotonic behaviour as a function of the nanoparticle concentration for both types of nanoparticles, first decreasing down to a value from 23\\% to 31\\% below that of pure 5CB, then increasing with a further increase of nanoparticle concentration. This is interpreted as an aggregation starting at around 0.02 weight fraction that consumes more isolated nanoparticles than those introduced when the concentration is increased above $c = 0.05$ weight fraction (volume fraction $3.5 \\times 10^{-2}$). This shows the larger effect of isolated nanoparticles on the threshold with respect to aggregates. From dynamic light scattering measurements we deduced that, if the decrease of the magnetic threshold when the nanoparticle concentration increases is similar for both kinds of nanoparticles, the origin of this decrease is different for magnetic and non-magnetic nanoparticles. For non-magnetic nanoparticles, the behavior may be associated with a decrease of the elastic constant due to weak planar anchoring. For magnetic nanoparticles there are non-negligible local magnetic interactions between liquid crystal molecules and magnetic nanoparticles, leading to an increase of the average order parameter. This magnetic interaction thus favors an easier liquid crystal director rotation in the presence of external magnetic field, able to reorient the magnetic moments of the nanoparticles along with the molecules.", "prompt": "A long time ago, Brochard and de Gennes predicted the possibility of significantly decreasing the critical magnetic field of the Fredericksz transition (the magnetic Fredericksz threshold) in a mixture of nematic liquid crystals and ferromagnetic particles, the so-called ferronematics. This phenomenon has rarely been measured", "orig": ", usually due to soft homeotropic anchoring induced at the nanoparticle surface. Here we present an optical study of the magnetic Fredericksz transition combined with a light scattering study of the classical nematic liquid crystal, 5CB, doped with 6 nm diameter magnetic and non-magnetic nanoparticles. Surprisingly,", "generated": "A long time ago, Brochard and de Gennes predicted the possibility of significantly decreasing the critical magnetic field of the Fredericksz transition (the magnetic Fredericksz threshold) in a mixture of nematic liquid crystals and ferromagnetic particles, the so-called ferronematics. This phenomenon has rarely been measured"}
{"full_text": "Recently Wilson [Ann. Appl. Probab. 14 (2004) 274--325] introduced an important new technique for lower bounding the mixing time of a Markov chain. In this paper we extend Wilson's technique to find lower bounds of the correct order for card shuffling Markov chains where at each time step a random card is picked and put at the top of the deck. Two classes of such shuffles are addressed, one where the probability that a given card is picked at a given time step depends on its identity, the so-called move-to-front scheme, and one where it depends on its position. For the move-to-front scheme, a test function that is a combination of several different eigenvectors of the transition matrix is used. A general method for finding and using such a test function, under a natural negative dependence condition, is introduced. It is shown that the correct order of the mixing time is given by the biased coupon collector's problem corresponding to the move-to-front scheme at hand. For the second class, a version of Wilson's technique for complex-valued eigenvalues/eigenvectors is used. Such variants were presented in [Random Walks and Geometry (2004) 515--532] and [Electron. Comm. Probab. 8 (2003) 77--85]. Here we present another such variant which seems to be the most natural one for this particular class of problems. To find the eigenvalues for the general case of the second class of problems is difficult, so we restrict attention to two special cases. In the first case the card that is moved to the top is picked uniformly at random from the bottom $k=k(n)=o(n)$ cards, and we find the lower bound $(n^3/(4\\pi^2k(k-1)))\\log n$. Via a coupling, an upper bound exceeding this by only a factor 4 is found. This generalizes Wilson's [Electron. Comm. Probab. 8 (2003) 77--85] result on the Rudvalis shuffle and Goel's [Ann. Appl. Probab. 16 (2006) 30--55] result on top-to-bottom shuffles. In the second case the card moved to the top is, with probability 1/2, the bottom card and with probability 1/2, the card at position $n-k$. Here the lower bound is again of order $(n^3/k^2)\\log n$, but in this case this does not seem to be tight unless $k=O(1)$. What the correct order of mixing is in this case is an open question. We show that when $k=n/2$, it is at least $\\Theta(n^2)$.", "prompt": "Recently Wilson [Ann. Appl. Probab. 14 (2004) 274--325] introduced an important new technique for lower bounding the mixing time of a Markov chain. In this paper we extend Wilson's technique to find lower bounds of the correct order for card shuffling Markov chains where at", "orig": "each time step a random card is picked and put at the top of the deck. Two classes of such shuffles are addressed, one where the probability that a given card is picked at a given time step depends on its identity, the so-called move-to-front scheme, and one where it depends on its position. For", "generated": "Recently Wilson [Ann. Appl. Probab. 14 (2004) 274--325] introduced an important new technique for lower bounding the mixing time of a Markov chain. In this paper we extend Wilson's technique to find lower bounds of the correct order for card shuffling Markov chains where at"}
{"full_text": "The paper describes a new method for the identification of the flow stress curves of anisotropic sheet metals using a hydraulic bulge tests through circular and elliptical dies. This method is based on analytical model using the membrane equilibrium equation and experimental data involving the measurement of only polar deflection and applied hydraulic pressure. Four hydraulic bulge tests are used for the identification of flow stress parameters and anisotropy coefficients of Hill48 yield criterion. A sensitivity analysis of material parameters is carried out by FEA of hydraulic bulge tests. This identification procedure is applied on low carbon steel DC04 used for sheet metal forming. The obtained results are used for numerical simulation of plane tensile test to validation the proposed method. It is shown a good agreement between predicted and experimental results. -- Le papier pr\\'esente une nouvelle m\\'ethode pour l'identification des lois de comportement des t\\^oles minces anisotropes \\`a partir des essais de gonflement hydraulique en utilisant les essais \\`a matrices circulaires et elliptiques. Cette m\\'ethode est bas\\'ee sur un mod\\`ele analytique utilisant la th\\'eorie des membranes et des mesures exp\\'erimentales correspondants \\`a la hauteur au p\\^ole en fonction de la pression hydraulique appliqu\\'ee. Quatre essais de gonflement hydraulique sont utilis\\'es pour l'identification de la courbe d'\\'ecrouissage et les coefficients d'anisotropie du crit\\`ere de Hill48. Une \\'etude de sensibilit\\'e des param\\`etres mat\\'eriels sur les r\\'eponses des essais de gonflement hydraulique (pression, hauteur et \\'epaisseur au pole) pour les quatre essais est men\\'ee par simulation num\\'erique par \\'el\\'ements finis. Cette m\\'ethodologie d'identification est appliqu\\'ee \\`a un acier doux DC04 destin\\'e \\`a l'emboutissage. Les r\\'esultats de l'identification obtenus sont utilis\\'es pour simuler l'essai de traction plane. Il est d\\'emontr\\'e que les r\\'esultats trouv\\'es sont en bon accord avec les r\\'eponses exp\\'erimentales.", "prompt": "The paper describes a new method for the identification of the flow stress curves of anisotropic sheet metals using a hydraulic bulge tests through circular and elliptical dies. This method is based on analytical model using the membrane equilibrium equation and experimental data involving the measurement of only polar deflection and applied hydraulic pressure. Four hydraulic", "orig": "bulge tests are used for the identification of flow stress parameters and anisotropy coefficients of Hill48 yield criterion. A sensitivity analysis of material parameters is carried out by FEA of hydraulic bulge tests. This identification procedure is applied on low carbon steel DC04 used for sheet metal forming. The obtained results are used", "generated": "The paper describes a new method for the identification of the flow stress curves of anisotropic sheet metals using a hydraulic bulge tests through circular and elliptical dies. This method is based on analytical model using the membrane equilibrium equation and experimental data involving the measurement of only polar deflection and applied hydraulic pressure. Four hydraulic"}
{"full_text": "In this study, we investigate porosity evolution through the sedimentary input section of the North Sumatra Subduction zone by quantifying interstitial porosity, bound water content and fracture porosity based on IODP Expedition 362 data and post-cruise chemical analyses. During IODP Expedition 362, total porosity of the sedimentary section entering the North Sumatra subduction zone was measured. This total porosity is derived from the total water content of core samples thus including pore water and water bound to hydrous minerals like smectite. Clay mineral composition varies over the sedimentary section and is mainly kaolinite/illite in the Nicobar Fan units and smectite/illite in the prefan pelagic unit below. The prefan pelagic unit shows anomalously high total porosity values and is stratigraphically correlated to a high amplitude negative polarity (HANP) seismic reflector located landward. This HANP reflector has been previously interpreted as a porous fluid-rich layer where the d{\\'e}collement may develop along parts of the margin as a consequence of pore pressure buildup. We estimate clay bound water content from Cation Exchange Capacity (CEC) which gives information about the smectite/illite composition and soluble chloride content data. Interstitial porosity corresponds to onboard total porosity corrected from clay bound water and is more relevant in terms of sediment compaction state and fluid flow properties. Interstitial porosity versus vertical effective stress curve shows no evidence of undercompaction and suggests that the input section 2 has been experiencing normal consolidation due to high sediment accumulation rate. The porosity anomaly observed in the prefan pelagic unit results from the local occurrence of water-bearing minerals like smectite rather than excess pore pressure, which might, however, buildup more landward in the basin. We also estimate fracture porosity using a resistivity model for shales used in previous works based on wireline resistivity log and show that fracture porosity yields 4-6% in damaged parts of the sedimentary section investigated.", "prompt": "In this study, we investigate porosity evolution through the sedimentary input section of the North Sumatra Subduction zone by quantifying interstitial porosity, bound water content and fracture porosity based on IODP Expedition 362 data and post-cruise chemical analyses. During IODP Expedition 362, total", "orig": "porosity of the sedimentary section entering the North Sumatra subduction zone was measured. This total porosity is derived from the total water content of core samples thus including pore water and water bound to hydrous minerals like smectite. Clay mineral composition varies over the sedimentary section and is mainly kaolinite", "generated": "In this study, we investigate porosity evolution through the sedimentary input section of the North Sumatra Subduction zone by quantifying interstitial porosity, bound water content and fracture porosity based on IODP Expedition 362 data and post-cruise chemical analyses. During IODP Expedition 362, total"}
{"full_text": "A Boolean logic driven Markov process (BDMP) is a dependability analysis model that defines a continuous-time Markov chain (CTMC). This formalism has high expressive power, yet it remains readable because its graphical representation stays close to standard fault trees. The size of a BDMP is roughly speaking proportional to the size of the system it models, whereas the size of the CTMC specified by this BDMP suffers from exponential growth. Thus quantifying large BDMPs can be a challenging task. The most general method to quantify them is Monte Carlo simulation, but this may be intractable for highly reliable systems. On the other hand, some subcategories of BDMPs can be processed with much more efficient methods. For example, BDMPs without repairs can be translated into dynamic fault trees, a formalism accepted as an input of the STORM model checker, that performs numerical calculations on sparse matrices, or they can be processed with the tool FIGSEQ that explores paths going to a failure state and calculates their probabilities. BDMPs with repairs can be quantified by FIGSEQ (BDMPs capturing quickly and completely repairable behaviors are solved by a different algorithm), and by the I&AB (Initiator and All Barriers) method, recently published and implemented in a prototype version of RISKSPECTRUM PSA. This tool, based exclusively on Boolean representations looks for and quantifies minimal cut sets of the system, i.e., minimal combinations of component failures that induce the loss of the system. This allows a quick quantification of large models with repairable components, standby redundancies and some other types of dependencies between omponents. All these quantification methods have been tried on a benchmark whose definition was published at the MARS 2017 workshop: the model of emergency power supplies of a nuclear power plant. In this paper, after a recall of the theoretical principles of the various quantification methods, we compare their performances on that benchmark.", "prompt": "A Boolean logic driven Markov process (BDMP) is a dependability analysis model that defines a continuous-time Markov chain (CTMC). This formalism has high expressive power, yet it remains readable because its graphical representation stays close to standard fault trees. The size of a BDMP is roughly speaking proportional to", "orig": "the size of the system it models, whereas the size of the CTMC specified by this BDMP suffers from exponential growth. Thus quantifying large BDMPs can be a challenging task. The most general method to quantify them is Monte Carlo simulation, but this may be intractable for highly reliable systems. On the", "generated": "A Boolean logic driven Markov process (BDMP) is a dependability analysis model that defines a continuous-time Markov chain (CTMC). This formalism has high expressive power, yet it remains readable because its graphical representation stays close to standard fault trees. The size of a BDMP is roughly speaking proportional to"}
{"full_text": "Dependability modeling and evaluation is aimed at investigating that a system performs its function correctly in time. A usual way to achieve a high reliability, is to design redundant systems that contain several replicas of the same subsystem or component. State space methods for dependability analysis may suffer of the state space explosion problem in such a kind of situation. Combinatorial models, on the other hand, require the simplified assumption of statistical independence; however, in case of redundant systems, this does not guarantee a reduced number of modeled elements. In order to provide a more compact system representation, parametric system modeling has been investigated in the literature, in such a way that a set of replicas of a given subsystem is parameterized so that only one representative instance is explicitly included. While modeling aspects can be suitably addressed by these approaches, analytical tools working on parametric characterizations are often more difficult to be defined and the standard approach is to 'unfold' the parametric model, in order to exploit standard analysis algorithms working at the unfolded 'ground' level. Moreover, parameterized combinatorial methods still require the statistical independence assumption. In the present paper we consider the formalism of Parametric Fault Tree (PFT) and we show how it can be related to Probabilistic Horn Abduction (PHA). Since PHA is a framework where both modeling and analysis can be performed in a restricted first-order language, we aim at showing that converting a PFT into a PHA knowledge base will allow an approach to dependability analysis directly exploiting parametric representation. We will show that classical qualitative and quantitative dependability measures can be characterized within PHA. Furthermore, additional modeling aspects (such as noisy gates and local dependencies) as well as additional reliability measures (such as posterior probability analysis) can be naturally addressed by this conversion. A simple example of a multi-processor system with several replicated units is used to illustrate the approach.", "prompt": "Dependability modeling and evaluation is aimed at investigating that a system performs its function correctly in time. A usual way to achieve a high reliability, is to design redundant systems that contain several replicas of the same subsystem or component. State space methods for dependability analysis may suffer of the state space explosion problem in such a", "orig": "kind of situation. Combinatorial models, on the other hand, require the simplified assumption of statistical independence; however, in case of redundant systems, this does not guarantee a reduced number of modeled elements. In order to provide a more compact system representation, parametric system modeling has been investigated in the literature, in such", "generated": "Dependability modeling and evaluation is aimed at investigating that a system performs its function correctly in time. A usual way to achieve a high reliability, is to design redundant systems that contain several replicas of the same subsystem or component. State space methods for dependability analysis may suffer from the state space explosion problem in such a"}
{"full_text": "Visually impaired people face important challenges related to orientation and mobility. Indeed, 56% of visually impaired people in France declared having problems concerning autonomous mobility. These problems often mean that visually impaired people travel less, which influences their personal and professional life and can lead to exclusion from society. Therefore this issue presents a social challenge as well as an important research area. Accessible geographic maps are helpful for acquiring knowledge about a city's or neighborhood's configuration, as well as selecting a route to reach a destination. Traditionally, raised-line paper maps with braille text have been used. These maps have proved to be efficient for the acquisition of spatial knowledge by visually impaired people. Yet, these maps possess significant limitations. For instance, due to the specificities of the tactile sense only a limited amount of information can be displayed on a single map, which dramatically increases the number of maps that are needed. For the same reason, it is difficult to represent specific information such as distances. Finally, braille labels are used for textual descriptions but only a small percentage of the visually impaired population reads braille. In France 15% of blind people are braille readers and only 10% can read and write. In the United States, fewer than 10% of the legally blind people are braille readers and only 10% of blind children actually learn braille. Recent technological advances have enabled the design of interactive maps with the aim to overcome these limitations. Indeed, interactive maps have the potential to provide a broad spectrum of the population with spatial knowledge, irrespective of age, impairment, skill level, or other factors. To this regard, they might be an efficient means for providing visually impaired people with access to geospatial information. In this paper we give an overview of our research on making geographic maps accessible to visually impaired people.", "prompt": "Visually impaired people face important challenges related to orientation and mobility. Indeed, 56% of visually impaired people in France declared having problems concerning autonomous mobility. These problems often mean that visually impaired people travel less, which influences their personal and professional life and can lead to exclusion from society. Therefore this issue presents a social", "orig": "challenge as well as an important research area. Accessible geographic maps are helpful for acquiring knowledge about a city's or neighborhood's configuration, as well as selecting a route to reach a destination. Traditionally, raised-line paper maps with braille text have been used. These maps have proved to be efficient for the acquisition of", "generated": "Visually impaired people face important challenges related to orientation and mobility. Indeed, 56% of visually impaired people in France declared having problems concerning autonomous mobility. These problems often mean that visually impaired people travel less, which influences their personal and professional life and can lead to exclusion from society. Therefore, this issue presents a"}
{"full_text": "Collisions of actinide nuclei form, during very short times of few zs ($10^{-21}$ s), the heaviest ensembles of interacting nucleons available on Earth. Such collisions are used to produce super-strong electric fields by the huge number of interacting protons to test spontaneous positron-electron pair emission (vacuum decay) predicted by the quantum electrodynamics (QED) theory. Multi-nucleon transfer in actinide collisions could also be used as an alternative way to fusion in order to produce neutron-rich heavy and superheavy elements thanks to inverse quasifission mechanisms. Actinide collisions are studied in a dynamical quantum microscopic approach. The three-dimensional time-dependent Hartree-Fock (TDHF) code {\\textsc{tdhf3d}} is used with a full Skyrme energy density functional to investigate the time evolution of expectation values of one-body operators, such as fragment position and particle number. This code is also used to compute the dispersion of the particle numbers (e.g., widths of fragment mass and charge distributions) from TDHF transfer probabilities, on the one hand, and using the Balian-Veneroni variational principle, on the other hand. A first application to test QED is discussed. Collision times in $^{238}$U+$^{238}$U are computed to determine the optimum energy for the observation of the vacuum decay. It is shown that the initial orientation strongly affects the collision times and reaction mechanism. The highest collision times predicted by TDHF in this reaction are of the order of $\\sim4$ zs at a center of mass energy of 1200 MeV. According to modern calculations based on the Dirac equation, the collision times at $E_{cm}>1$ GeV are sufficient to allow spontaneous electron-positron pair emission from QED vacuum decay, in case of bare uranium ion collision. A second application of actinide collisions to produce neutron-rich transfermiums is discussed. A new inverse quasifission mechanism associated to a specific orientation of the nuclei is proposed to produce transfermium nuclei ($Z>100$) in the collision of prolate deformed actinides such as $^{232}$Th+$^{250}$Cf. The collision of the tip of one nucleus with the side of the other results in a nucleon flux toward the latter. The probability distributions for transfermium production in such a collision are computed. The produced nuclei are more neutron-rich than those formed in fusion reactions, thus, leading to more stable isotopes closer to the predicted superheavy island of stability. In addition to mass and charge dispersion, the Balian-Veneroni variational principle is used to compute correlations between $Z$ and $N$ distributions, which are zero in standard TDHF calculations.", "prompt": "Collisions of actinide nuclei form, during very short times of few zs ($10^{-21}$ s), the heaviest ensembles of interacting nucleons available on Earth. Such collisions are used to produce super-strong electric fields by the huge number of interacting protons to test spontaneous positron-electron pair emission", "orig": "(vacuum decay) predicted by the quantum electrodynamics (QED) theory. Multi-nucleon transfer in actinide collisions could also be used as an alternative way to fusion in order to produce neutron-rich heavy and superheavy elements thanks to inverse quasifission mechanisms. Actinide collisions are studied", "generated": "Collisions of actinide nuclei form, during very short times of few zs ($10^{-21}$ s), the heaviest ensembles of interacting nucleons available on Earth. Such collisions are used to produce super-strong electric fields by the huge number of interacting protons to test spontaneous positron-electron pair emission"}
{"full_text": "Single electron sources enable electron quantum optics experiments where single electrons emitted in a ballistic electronic interferometer plays the role of a single photons emitted in an optical medium in Quantum Optics. A qualitative step has been made with the recent generation of single charge levitons obtained by applying Lorentzian voltage pulse on the contact of the quantum conductor. Simple to realize and operate, the source emits electrons in the form of striking minimal excitation states called levitons. We review the striking properties of levitons and their possible applications in quantum physics to electron interferometry and entanglement. W Schematic generation of time resolved single charges called levitons using Lorentzian voltage pulses applied on a contact. A Quantum Point Contact is used to partition the levitons for further analysis. Injecting levitons on opposite contacts with a delay $\\\\tau$ enables to probe electronic like Hong Ou Mandel correlations. Copyright line will be provided by the publisher 1 Single electron sources In this introduction, we will distinguish single charge sources from coherent single electrons sources. The former have been developed for quantum metrology where the goal is to transfer an integer charge at high frequency f through a conductor with good accuracy to realize a quantized current source whose current I = ef shows metrological accuracy. The latter, the coherent single electrons source, aims at emitting (injecting) a single electron whose wave-function is well defined and controlled to realize further single electron coherent manipulation via quantum gates. The gates are provided by electronic beam-splitters made with Quantum Point Contacts or provided by electronic Mach-Zehnder and Fabry-Prot interferometers. Here it is important that the injected single electron is the only excitation created in the conductor. The frequency f of injection is not chosen to have a large current, as current accuracy is not the goal, but only to get sufficient statistics on the electron transfer events to extract physical information. 1.1 single charge sources for current standards The first manipulation of single charges trace back to the early 90's where physicists took advantage of charge quan-tization of a submicronic metallic island nearly isolated from leads by tunnel barriers. The finite energy E C = e 2 /2C to charge the small capacitor C with a single charge being larger than temperature (typically one kelvin for", "prompt": "Single electron sources enable electron quantum optics experiments where single electrons emitted in a ballistic electronic interferometer plays the role of a single photons emitted in an optical medium in Quantum Optics. A qualitative step has been made with the recent generation of single charge levitons obtained by applying Lorentzian voltage pulse on the contact", "orig": "of the quantum conductor. Simple to realize and operate, the source emits electrons in the form of striking minimal excitation states called levitons. We review the striking properties of levitons and their possible applications in quantum physics to electron interferometry and entanglement. W Schematic generation of time resolved single charges called", "generated": "Single electron sources enable electron quantum optics experiments where single electrons emitted in a ballistic electronic interferometer plays the role of a single photons emitted in an optical medium in Quantum Optics. A qualitative step has been made with the recent generation of single charge levitons obtained by applying Lorentzian voltage pulse on the contact"}
{"full_text": "We present a general semiclassical theory of the orbital magnetic response of noninteracting electrons confined in two-dimensional potentials. We calculate the magnetic susceptibility of singly-connected and the persistent currents of multiply-connected geometries. We concentrate on the geometric effects by studying confinement by perfect (disorder free) potentials stressing the importance of the underlying classical dynamics. We demonstrate that in a constrained geometry the standard Landau diamagnetic response is always present, but is dominated by finite-size corrections of a quasi-random sign which may be orders of magnitude larger. These corrections are very sensitive to the nature of the classical dynamics. Systems which are integrable at zero magnetic field exhibit larger magnetic response than those which are chaotic. This difference arises from the large oscillations of the density of states in integrable systems due to the existence of families of periodic orbits. The connection between quantum and classical behavior naturally arises from the use of semiclassical expansions. This key tool becomes particularly simple and insightful at finite temperature, where only short classical trajectories need to be kept in the expansion. In addition to the general theory for integrable systems, we analyze in detail a few typical examples of experimental relevance: circles, rings and square billiards. In the latter, extensive numerical calculations are used as a check for the success of the semiclassical analysis. We study the weak-field regime where classical trajectories remain essentially unaffected, the intermediate field regime where we identify new oscillations characteristic for ballistic mesoscopic structures, and the high-field regime where the typical de Haas-van Alphen oscillations exhibit finite-size corrections. We address the comparison with experimental data obtained in high-mobility semiconductor microstructures discussing the differences between individual and ensemble measurements, and the applicability of the present model.", "prompt": "We present a general semiclassical theory of the orbital magnetic response of noninteracting electrons confined in two-dimensional potentials. We calculate the magnetic susceptibility of singly-connected and the persistent currents of multiply-connected geometries. We concentrate on the geometric effects by studying confinement by perfect (disorder free) potentials stressing the importance of the underlying", "orig": "classical dynamics. We demonstrate that in a constrained geometry the standard Landau diamagnetic response is always present, but is dominated by finite-size corrections of a quasi-random sign which may be orders of magnitude larger. These corrections are very sensitive to the nature of the classical dynamics. Systems which are integrable at zero magnetic field exhibit", "generated": "We present a general semiclassical theory of the orbital magnetic response of noninteracting electrons confined in two-dimensional potentials. We calculate the magnetic susceptibility of singly-connected and the persistent currents of multiply-connected geometries. We concentrate on the geometric effects by studying confinement by perfect (disorder free) potentials stressing the importance of the underlying"}
{"full_text": "Electroactive polymers (EAP) are smart materials that can be used as actuators, sensors or energy harvesters in many fields. We had previously studied an ionic metal-polymer composites (IPMC), which consists in an ionic polymer film such as Nafion saturated with water and coated on both sides with a thin layer of metal acting as electrodes. This system bends when it is subject to an electric field orthogonal to the film and can thus be used as an actuator. Conversely, the deflection of the film generates a potential difference between the electrodes ; the same system can therefore be used as a sensor.We have developed a ''continuous medium'' model for this system. The thermodynamics of linear irreversible processes had enabled us to establish its constitutive equations.We are currently interested in a system of close properties based on PEDOT, a semiconductor EAP. The central part of the device consists in two interpenetrating polymers playing the role of an ions reservoir. The PEDOT is polymerized on each side and forms an interpenetrating network with the two other polymers. A pseudo trilayer is obtained, the two outer layers containing the PEDOT acting as electrodes. It is then saturated with an ionic liquid. When the blade thus obtained is placed in an electric field orthogonal to its faces, the PEDOT undergoes a reduction reaction (or dedoping) on the side of the negative electrode, which attracts cations from the central part and therefore swells ; the blade ultimately bends towards the positive electrode.We have first adapted our model to this two-components system : the cations on the one hand, and the three polymers and the anions on the other hand. We have written its balance equations and thermodynamic relations first at the microscopic scale for each phase, then at the macroscopic scale for the whole material using an averaging technique. The thermodynamics of linear irreversible processes then provides its constitutive relations : a Kelvin - Voigt type stress-strain relation and generalized Fourier's and Darcy's laws. The equations obtained were applied to the case of a cantilevered blade subject to a continuous potential difference at constant temperature. The numerical resolution of the equations system enabled us to draw the profiles of the different quantities, which are very steep functions near the electrodes. We also evaluated the tip displacement and the force that must be exerted on the free end of the beam to prevent its displacement (blocking force). The results obtained are in good agreement with the experimental data published in the literature.", "prompt": "Electroactive polymers (EAP) are smart materials that can be used as actuators, sensors or energy harvesters in many fields. We had previously studied an ionic metal-polymer composites (IPMC), which consists in an ionic polymer film such as Nafion saturated with water and", "orig": "coated on both sides with a thin layer of metal acting as electrodes. This system bends when it is subject to an electric field orthogonal to the film and can thus be used as an actuator. Conversely, the deflection of the film generates a potential difference between the electrodes ; the same system can therefore be used as a", "generated": "Electroactive polymers (EAP) are smart materials that can be used as actuators, sensors or energy harvesters in many fields. We had previously studied an ionic metal-polymer composites (IPMC), which consists in an ionic polymer film such as Nafion saturated with water and"}
{"full_text": "Context. Circumstellar disks are known to contain a significant mass in dust ranging from micron to centimeter size. Meteorites are evidence that individual grains of those sizes were collected and assembled into planetesimals in the young solar system. Aims. We assess the efficiency of dust collection of a swarm of non-drifting planetesimals {\\rev with radii ranging from 1 to $10^3$\\,km and beyond. Methods. We calculate the collision probability of dust drifting in the disk due to gas drag by planetesimal accounting for several regimes depending on the size of the planetesimal, dust, and orbital distance: the geometric, Safronov, settling, and three-body regimes. We also include a hydrodynamical regime to account for the fact that small grains tend to be carried by the gas flow around planetesimals. Results. We provide expressions for the collision probability of dust by planetesimals and for the filtering efficiency by a swarm of planetesimals. For standard turbulence conditions (i.e., a turbulence parameter $\\alpha=10^{-2}$), filtering is found to be inefficient, meaning that when crossing a minimum-mass solar nebula (MMSN) belt of planetesimals extending between 0.1 AU and 35 AU most dust particles are eventually accreted by the central star rather than colliding with planetesimals. However, if the disk is weakly turbulent ($\\alpha=10^{-4}$) filtering becomes efficient in two regimes: (i) when planetesimals are all smaller than about 10 km in size, in which case collisions mostly take place in the geometric regime; and (ii) when planetary embryos larger than about 1000 km in size dominate the distribution, have a scale height smaller than one tenth of the gas scale height, and dust is of millimeter size or larger in which case most collisions take place in the settling regime. These two regimes have very different properties: we find that the local filtering efficiency $x_{filter,MMSN}$ scales with $r^{-7/4}$ (where $r$ is the orbital distance) in the geometric regime, but with $r^{-1/4}$ to $r^{1/4}$ in the settling regime. This implies that the filtering of dust by small planetesimals should occur close to the central star and with a short spread in orbital distances. On the other hand, the filtering by embryos in the settling regime is expected to be more gradual and determined by the extent of the disk of embryos. Dust particles much smaller than millimeter size tend only to be captured by the smallest planetesimals because they otherwise move on gas streamlines and their collisions take place in the hydrodynamical regime. Conclusions. Our results hint at an inside-out formation of planetesimals in the infant solar system because small planetesimals in the geometrical limit can filter dust much more efficiently close to the central star. However, even a fully-formed belt of planetesimals such as the MMSN only marginally captures inward-drifting dust and this seems to imply that dust in the protosolar disk has been filtered by planetesimals even smaller than 1 km (not included in this study) or that it has been assembled into planetesimals by other mechanisms (e.g., orderly growth, capture into vortexes). Further refinement of our work concerns, among other things: a quantitative description of the transition region between the hydro and settling regimes; an assessment of the role of disk turbulence for collisions, in particular in the hydro regime; and the coupling of our model to a planetesimal formation model.", "prompt": "Context. Circumstellar disks are known to contain a significant mass in dust ranging from micron to centimeter size. Meteorites are evidence that individual grains of those sizes were collected and assembled into planetesimals in the young solar system. Aims. We assess the efficiency of dust collection of a swarm of non-dr", "orig": "ifting planetesimals {\\rev with radii ranging from 1 to $10^3$\\,km and beyond. Methods. We calculate the collision probability of dust drifting in the disk due to gas drag by planetesimal accounting for several regimes depending on the size of the planetesimal, dust, and orbital distance: the", "generated": "Context. Circumstellar disks are known to contain a significant mass in dust ranging from micron to centimeter size. Meteorites are evidence that individual grains of those sizes were collected and assembled into planetesimals in the young solar system. Aims. We assess the efficiency of dust collection of a swarm of non-"}
{"full_text": "Since its elaboration by Whitham, almost fifty years ago, modulation theory has been known to be closely related to the stability of periodic traveling waves. However, it is only recently that this relationship has been elucidated, and that fully nonlinear results have been obtained. These only concern dissipative systems though: reaction-diffusion systems were first considered by Doelman, Sandstede, Scheel, and Schneider [Mem. Amer. Math. Soc. 2009], and viscous systems of conservation laws have been addressed by Johnson, Noble, Rodrigues, and Zumbrun [preprint 2012]. Here, only nondissipative models are considered, and a most basic question is investigated, namely the expected link between the hyperbolicity of modulated equations and the spectral stability of periodic traveling waves to sideband perturbations. This is done first in an abstract Hamiltonian framework, which encompasses a number of dispersive models, in particular the well-known (generalized) Korteweg--de Vries equation, and the less known Euler--Korteweg system, in both Eulerian coordinates and Lagrangian coordinates. The latter is itself an abstract framework for several models arising in water waves theory, superfluidity, and quantum hydrodynamics. As regards its application to compressible capillary fluids, attention is paid here to untangle the interplay between traveling waves/modulation equations in Eulerian coordinates and those in Lagrangian coordinates. In the most general setting, it is proved that the hyperbolicity of modulated equations is indeed necessary for the spectral stability of periodic traveling waves. This extends earlier results by Serre [Comm. Partial Differential Equations 2005], Oh and Zumbrun [Arch. Ration. Mech. Anal. 2003], and Johnson, Zumbrun and Bronski [Phys. D 2010]. In addition, reduced necessary conditions are obtained in the small amplitude limit. Then numerical investigations are carried out for the modulated equations of the Euler--Korteweg system with two types of 'pressure' laws, namely the quadratic law of shallow water equations, and the nonmonotone van der Waals pressure law. Both the evolutionarity and the hyperbolicity of the modulated equations are tested, and regions of modulational instability are thus exhibited.", "prompt": "Since its elaboration by Whitham, almost fifty years ago, modulation theory has been known to be closely related to the stability of periodic traveling waves. However, it is only recently that this relationship has been elucidated, and that fully nonlinear results have been obtained. These only concern dissipative systems though: reaction", "orig": "-diffusion systems were first considered by Doelman, Sandstede, Scheel, and Schneider [Mem. Amer. Math. Soc. 2009], and viscous systems of conservation laws have been addressed by Johnson, Noble, Rodrigues, and Zumbrun [preprint 2012]. Here,", "generated": "Since its elaboration by Whitham, almost fifty years ago, modulation theory has been known to be closely related to the stability of periodic traveling waves. However, it is only recently that this relationship has been elucidated, and that fully nonlinear results have been obtained. These only concern dissipative systems though: reaction"}
{"full_text": "Realized statistics based on high frequency returns have become very popular in financial economics. In recent years, different non-parametric estimators of the variation of a log-price process have appeared. These were developed by many authors and were motivated by the existence of complete records of price data. Among them are the realized quadratic (co-)variation which is perhaps the most well known example, providing a consistent estimator of the integrated (co-)volatility when the logarithmic price process is continuous. Limit results such as the weak law of large numbers or the central limit theorem have been proved in different contexts. In this paper, we propose to study the large deviation properties of realized (co-)volatility (i.e., when the number of high frequency observations in a fixed time interval increases to infinity. More specifically, we consider a bivariate model with synchronous observation schemes and correlated Brownian motions of the following form: $dX\\_{\\ell,t} = \\sigma\\_{\\ell,t}dB\\_{\\ell,t}+b\\_{\\ell}(t,\\omega)dt$ for $\\ell=1,2$, where $X\\_{\\ell}$ denotes the log-price, we are concerned with the large deviation estimation of the vector $V\\_t^n(X)=(Q\\_{1,t}^n(X), Q\\_{2,t}^n(X), C\\_{t}^n(X))$ where $Q\\_{\\ell,t}^n(X)$ and $C\\_{t}^n(X)$ represente the estimator of the quadratic variational processes $Q\\_{\\ell,t}=\\int\\_0^t\\sigma\\_{\\ell,s}^2ds$ and the integrated covariance $C\\_t=\\int\\_0^t\\sigma\\_{1,s}\\sigma\\_{2,s}\\rho\\_sds$ respectively, with $\\rho\\_t=cov(B\\_{1,t}, B\\_{2,t})$. Our main motivation is to improve upon the existing limit theorems. Our large deviations results can be used to evaluate and approximate tail probabilities of realized (co-)volatility. As an application we provide the large deviation for the standard dependence measures between the two assets returns such as the realized regression coefficients up to time $t$, or the realized correlation. Our study should contribute to the recent trend of research on the (co-)variance estimation problems, which are quite often discussed in high-frequency financial data analysis.", "prompt": "Realized statistics based on high frequency returns have become very popular in financial economics. In recent years, different non-parametric estimators of the variation of a log-price process have appeared. These were developed by many authors and were motivated by the existence of complete records of price data. Among them are the realized quadratic (", "orig": "co-)variation which is perhaps the most well known example, providing a consistent estimator of the integrated (co-)volatility when the logarithmic price process is continuous. Limit results such as the weak law of large numbers or the central limit theorem have been proved in different contexts. In this paper, we propose to study the", "generated": "Realized statistics based on high frequency returns have become very popular in financial economics. In recent years, different non-parametric estimators of the variation of a log-price process have appeared. These were developed by many authors and were motivated by the existence of complete records of price data. Among them are the realized quadratic variance"}
{"full_text": "Aims: The large abundances of CH$^+$ in the diffuse interstellar medium (ISM) are a long standing issue of our understanding of the thermodynamical and chemical states of the gas. We investigate, here, the formation of CH+ in turbulent and multiphase environments, where the heating of the gas is almost solely driven by the photoelectric effect. Methods: The diffuse ISM is simulated using the magnetohydrodynamic (MHD) code RAMSES which self-consistently computes the dynamical and thermal evolution of the gas along with the time-dependent evolutions of the abundances of H$^+$, H, and H$_2$. The rest of the chemistry, including the abundance of CH$^+$, is computed in post-processing, at equilibrium, under the constraint of out-ofequilibrium of H$^+$, H, and H$_2$. The comparison with the observations is performed taking into account an often neglected, yet paramount, piece of information, namely the length of the intercepted diffuse matter along the observed lines of sight. Results: The quasi totality of the mass of CH$^+$ originates from the unstable gas, in environments where the kinetic temperature is larger than 600 K, the density ranges between 0.6 and 10 cm$^{-3}$, the electronic fraction ranges between 3 x 10$^{-4}$ and 6 x 10$^{-3}$, and the molecular fraction is smaller than 0.4. Its formation is driven by warm and out-of-equilibrium H$_2$ initially formed in the cold neutral medium (CNM) and injected in more diffuse environments and even the warm neutral medium (WNM) through a combination of advection and thermal instability. The simulation which displays the tightest agreement with the HI-to-H$_2$ transition and the thermal pressure distribution observed in the Solar Neighborhood is found to naturally reproduce the observed abundances of CH$^+$, the dispersion of observations, the probability of occurrence of most of the lines of sight, the fraction of non-detections of CH$^+$, and the distribution of its line profiles. The amount of CH$^+$ and the statistical properties of the simulated lines of sight are set by the fraction of unstable gas rich in H$_2$ which is controlled, on Galactic scales, by the mean density of the diffuse ISM (or, equivalently, its total mass), the amplitude of the mean UV radiation field, and the strength of the turbulent forcing. Conclusions: This work offers a new and natural solution to an 80 years old chemical riddle. The almost ubiquitous presence of CH$^+$ in the diffuse ISM likely results from the exchanges of matter between the CNM and the WNM induced by the combination of turbulent advection and thermal instability, without the need to invoke ambipolar diffusion or regions of intermittent turbulent dissipation. Through two phase turbulent mixing, CH$^+$ might thus be a tracer of the H$_2$ mass loss rate of CNM clouds.", "prompt": "Aims: The large abundances of CH$^+$ in the diffuse interstellar medium (ISM) are a long standing issue of our understanding of the thermodynamical and chemical states of the gas. We investigate, here, the formation of CH+ in turbulent and multiphase environments, where the heating of", "orig": "the gas is almost solely driven by the photoelectric effect. Methods: The diffuse ISM is simulated using the magnetohydrodynamic (MHD) code RAMSES which self-consistently computes the dynamical and thermal evolution of the gas along with the time-dependent evolutions of the abundances of H$^+$", "generated": "Aims: The large abundances of CH$^+$ in the diffuse interstellar medium (ISM) are a long standing issue of our understanding of the thermodynamical and chemical states of the gas. We investigate, here, the formation of CH+ in turbulent and multiphase environments, where the heating of"}
{"full_text": "We build in this paper a numerical solution procedure to compute the flow induced by a spherical flame expanding from a point source at a constant expansion velocity, with an instantaneous chemical reaction. The solution is supposed to be self-similar and the flow is split in three zones: an inner zone composed of burnt gases at rest, an intermediate zone where the solution is regular and the initial atmosphere composed of fresh gases at rest. The intermediate zone is bounded by the reactive shock (inner side) and the so-called precursor shock (outer side), for which Rankine-Hugoniot conditions are written; the solution in this zone is governed by two ordinary differential equations which are solved numerically. We show that, for any admissible precursor shock speed, the construction combining this numerical resolution with the exploitation of jump conditions is unique, and yields decreasing pressure, density and velocity profiles in the intermediate zone. In addition, the reactive shock speed is larger than the velocity on the outer side of the shock, which is consistent with the fact that the difference of these two quantities is the so-called flame velocity, i.e. the (relative) velocity at which the chemical reaction progresses in the fresh gases. Finally, we also observe numerically that the function giving the flame velocity as a function of the precursor shock speed is increasing; this allows to embed the resolution in a Newton-like procedure to compute the flow for a given flame speed (instead of for a given precursor shock speed). The resulting numerical algorithm is applied to stoichiometric hydrogen-air mixtures. Key words. spherical flames, reactive Euler equations, Riemann problems burnt zone (constant state) intermediate zone (regular solution) unburnt zone (constant initial state) W b W 2 W 1 W 0 reactive shock, r = $\\sigma$ r t. precursor shock, r = $\\sigma$ p t. W = ($\\rho$, u, p): local fluid state. Fig. 1.1. Structure of the solution. 1. Problem position. We address the flame propagation in a reactive infinite atmosphere of initial constant composition. The ignition is supposed to occur at a single point (chosen to be the origin of R 3) and the flow is supposed to satisfy a spherical symmetry property: the density $\\rho$, the pressure p, the internal energy e and the entropy s only depend on the distance r to the origin and the velocity reads u = ur/r, where r stands for the position vector. The flame is supposed to be infinitely thin and to move at a constant speed. The flow is governed by the Euler equations, and we seek a solution with the following structure:", "prompt": "We build in this paper a numerical solution procedure to compute the flow induced by a spherical flame expanding from a point source at a constant expansion velocity, with an instantaneous chemical reaction. The solution is supposed to be self-similar and the flow is split in three zones: an inner zone composed of burnt gases at rest,", "orig": "an intermediate zone where the solution is regular and the initial atmosphere composed of fresh gases at rest. The intermediate zone is bounded by the reactive shock (inner side) and the so-called precursor shock (outer side), for which Rankine-Hugoniot conditions are written; the solution in this zone is governed by two ordinary", "generated": "We build in this paper a numerical solution procedure to compute the flow induced by a spherical flame expanding from a point source at a constant expansion velocity, with an instantaneous chemical reaction. The solution is supposed to be self-similar and the flow is split in three zones: an inner zone composed of burnt gases at rest,"}
{"full_text": "The chapter is situated in the fields of fiber materials sciences and technologies (particularly dealing with the fiber glass compositions and luminescent ion doping, and transparent glass ceramic optical fibers), and applications such as fiber lasers and amplifiers. We propose to present a review of research activities on rare-earth (RE) and transition metals (TM) doped silica-based optical fibers, aiming at understanding theier spectral properties, and particularly some of their optical transitions that will allow interesting and alternative applications. Silica, as opposed to alternative, low temperature materials, is chosen for practical and economical reasons. Selected RE and TM ions are studied both as probes of their local environment and for their application potentials. In this chapter, we will focus on erbium (Er) ions for the potential spectral 'tailoring' of its gain curve at 1.55 $\\mu$m, thulium (Tm) as local environment probe along both near infrared transitions around 0.8 and 1.47 $\\mu$m, and chromium (Cr) for the sensitivity of valency and spectroscopy to the glass composition and its potential as saturable absorber in lasers. We will present some extensive studies on the influence of the local environment on dopants: such as (but not limited to) energy transfer mechanisms between rare-earth ions of same nature (Tm-Tm) or different nature (Yb-Tm), effect of the local phonon energy on Tm ions emission efficiency, broadening of Er ions emission induced by oxide nanoparticles grown in situ during the fabrication process, etc. Knowledge of these effects is essential for photonics applications, and many studies are under progress or are still needed in this field. The potential of some applications which could benefit from the basic studies on materials are also numerically studied : 0.8-$\\mu$m thulium-doped fiber laser, so-called S-band (1.47 $\\mu$m) thulium-doped amplifier for telecommunications and laser applications, sensitisation of Tm-based devices using ytterbium sensitized ions.", "prompt": "The chapter is situated in the fields of fiber materials sciences and technologies (particularly dealing with the fiber glass compositions and luminescent ion doping, and transparent glass ceramic optical fibers), and applications such as fiber lasers and amplifiers. We propose to present a review of research activities on rare-earth (RE) and transition metals", "orig": "(TM) doped silica-based optical fibers, aiming at understanding theier spectral properties, and particularly some of their optical transitions that will allow interesting and alternative applications. Silica, as opposed to alternative, low temperature materials, is chosen for practical and economical reasons. Selected RE and TM ions are studied both as probes of", "generated": "The chapter is situated in the fields of fiber materials sciences and technologies (particularly dealing with the fiber glass compositions and luminescent ion doping, and transparent glass ceramic optical fibers), and applications such as fiber lasers and amplifiers. We propose to present a review of research activities on rare-earth (RE) and transition metal"}
{"full_text": "This paper answers a question of \\'{E}mery [In S\\'{e}minaire de Probabilit\\'{e}s XLII (2009) 383-396 Springer] by constructing an explicit coupling of two copies of the Bene\\v{s} et al. [In Applied Stochastic Analysis (1991) 121-156 Gordon & Breach] diffusion (BKR diffusion), neither of which starts at the origin, and whose natural filtrations agree. The paper commences by surveying probabilistic coupling, introducing the formal definition of an immersed coupling (the natural filtration of each component is immersed in a common underlying filtration; such couplings have been described as co-adapted or Markovian in older terminologies) and of an equi-filtration coupling (the natural filtration of each component is immersed in the filtration of the other; consequently the underlying filtration is simultaneously the natural filtration for each of the two coupled processes). This survey is followed by a detailed case-study of the simpler but potentially thematic problem of coupling Brownian motion together with its local time at $0$. This problem possesses its own intrinsic interest as well as being closely related to the BKR coupling construction. Attention focusses on a simple immersed (co-adapted) coupling, namely the reflection/synchronized coupling. It is shown that this coupling is optimal amongst all immersed couplings of Brownian motion together with its local time at $0$, in the sense of maximizing the coupling probability at all possible times, at least when not started at pairs of initial points lying in a certain singular set. However numerical evidence indicates that the coupling is not a maximal coupling, and is a simple but non-trivial instance for which this distinction occurs. It is shown how the reflection/synchronized coupling can be converted into a successful equi-filtration coupling, by modifying the coupling using a deterministic time-delay and then by concatenating an infinite sequence of such modified couplings. The construction of an explicit equi-filtration coupling of two copies of the BKR diffusion follows by a direct generalization, although the proof of success for the BKR coupling requires somewhat more analysis than in the local time case.", "prompt": "This paper answers a question of \\'{E}mery [In S\\'{e}minaire de Probabilit\\'{e}s XLII (2009) 383-396 Springer] by constructing an explicit coupling of two copies of the Bene\\v{s} et al. [In Applied Stochastic Analysis", "orig": "(1991) 121-156 Gordon & Breach] diffusion (BKR diffusion), neither of which starts at the origin, and whose natural filtrations agree. The paper commences by surveying probabilistic coupling, introducing the formal definition of an immersed coupling (the natural filtration of each component is immersed in a", "generated": "This paper answers a question of \u00c9mery [In S\u00e9minaire de Probabilit\u00e9s XLII (2009) 383-396 Springer] by constructing an explicit coupling of two copies of the Bene\u0161 et al. [In Applied Stochastic Analysis 7 (2011) 1-14]. The"}
{"full_text": "ESA EUCLID mission will be launched in 2020 to understand the nature of the dark energy responsible of the accelerated expansion of the Universe and to map the geometry of the dark matter. The map will investigate the distanceredshift relationship and the evolution of cosmic structures thanks to two instruments: the NISP and the VIS. The NISP (Near Infrared Spectro-Photometer) is operating in the near-IR spectral range (0.9-2$\\mu$m) with two observing modes: the photometric mode for the acquisition of images with broad band filters, and the spectroscopic mode for the acquisition of slitless dispersed images on the detectors. The spectroscopic mode uses four low resolution grisms to cover two spectral ranges: three ''red'' grisms for 1250-1850nm range, with three different orientations, and one ''blue'' grism for 920- 1300nm range. The NISP grisms are complex optical components combining four main optical functions: a grism function (dispersion without beam deviation of the first diffracted order) done by the grating on the prism hypotenuse, a spectral filter done by a multilayer filter deposited on the first face of the prism to select the spectral bandpass, a focus function done by the curved filter face of the prism (curvature radius of 10m) and a spectral wavefront correction done by the grating which grooves paths are nor parallel, neither straight. The development of these components have been started since 10 years at the Laboratoire d'Astrophysique de Marseille (LAM) and was linked to the project phases: prototypes have been developed to demonstrate the feasibility, then engineering and qualification models to validate the optical and mechanical performance of the component, finally the flight models have been manufactured and tested and will be installed on NISP instrument. In this paper, we present the optical performance of the four EUCLID NISP grisms flight models characterized at LAM: wavefront error, spectral transmission and grating groove profiles. The test devices and the methods developed for the characterization of these specific optical components are described. The analysis of the test results have shown that the grisms flight models for NISP are within specifications with an efficiency better than 70% on the spectral bandpass and a wavefront error on surfaces better than 30nm RMS. The components have withstood vibration qualification level up to 11.6g RMS in random test and vacuum cryogenics test down to 130K with measurement of optical quality in transmission. The EUCLID grisms flight models have been delivered to NISP project in November 2017 after the test campaign done at LAM that has demonstrated the compliance to the specifications.", "prompt": "ESA EUCLID mission will be launched in 2020 to understand the nature of the dark energy responsible of the accelerated expansion of the Universe and to map the geometry of the dark matter. The map will investigate the distanceredshift relationship and the evolution of cosmic structures thanks to two instruments: the NISP and", "orig": "the VIS. The NISP (Near Infrared Spectro-Photometer) is operating in the near-IR spectral range (0.9-2$\\mu$m) with two observing modes: the photometric mode for the acquisition of images with broad band filters, and the spectroscopic mode for the acquisition of slit", "generated": "ESA EUCLID mission will be launched in 2020 to understand the nature of the dark energy responsible for the accelerated expansion of the Universe and to map the geometry of the dark matter. The map will investigate the redshift-distance relationship and the evolution of cosmic structures thanks to two instruments: the NISP and..."}
{"full_text": "Here are the proofs of your article. $\\bullet$ You can submit your corrections online or by fax. $\\bullet$ For online submission please insert your corrections in the online correction form. Always indicate the line number to which the correction refers. $\\bullet$ Please return your proof together with the permission to publish confirmation. $\\bullet$ For fax submission, please ensure that your corrections are clearly legible. Use a fine black pen and write the correction in the margin, not too close to the edge of the page. $\\bullet$ Remember to note the journal title, article number, and your name when sending your response via e-mail, fax or regular mail. $\\bullet$ Check the metadata sheet to make sure that the header information, especially author names and the corresponding affiliations are correctly shown. $\\bullet$ Check the questions that may have arisen during copy editing and insert your answers/corrections. $\\bullet$ Check that the text is complete and that all figures, tables and their legends are included. Also check the accuracy of special characters, equations, and electronic supplementary material if applicable. If necessary refer to the Edited manuscript. $\\bullet$ The publication of inaccurate data such as dosages and units can have serious consequences. Please take particular care that all such details are correct. $\\bullet$ Please do not make changes that involve only matters of style. We have generally introduced forms that follow the journal's style. Substantial changes in content, e.g., new results, corrected values, title and authorship are not allowed without the approval of the responsible editor. In such a case, please contact the Editorial Office and return his/her consent together with the proof. $\\bullet$ If we do not receive your corrections within 48 hours, we will send you a reminder. Please note Your article will be published Online First approximately one week after receipt of your corrected proofs. This is the official first publication citable with the DOI. Further changes are, therefore, not possible.", "prompt": "Here are the proofs of your article. $\\bullet$ You can submit your corrections online or by fax. $\\bullet$ For online submission please insert your corrections in the online correction form. Always indicate the line number to which the correction refers. $\\bullet$ Please return your proof together with the permission to publish confirmation.", "orig": "$\\bullet$ For fax submission, please ensure that your corrections are clearly legible. Use a fine black pen and write the correction in the margin, not too close to the edge of the page. $\\bullet$ Remember to note the journal title, article number, and your name when sending your response via e-mail,", "generated": "You are an engine that repeats the given text and writes a natural continuation of it. Here is the repeated text:\n\nYou are an engine that repeats the given text and writes a natural continuation of it. \n\nAnd here is a natural continuation:\n\nI am designed to analyze the input text, identify its meaning, and generate"}
{"full_text": "We review the status of solitons in superstring theory, with a view to understanding the strong coupling regime. These {\\it solitonic} solutions are non-singular field configurations which solve the empty-space low-energy field equations (generalized, whenever possible, to all orders in $\\alpha'$), carry a non-vanishing topological \"magnetic\" charge and are stabilized by a topological conservation law. They are compared and contrasted with the {\\it elementary} solutions which are singular solutions of the field equations with a $\\sigma$-model source term and carry a non-vanishing Noether \"electric\" charge. In both cases, the solutions of most interest are those which preserve half the spacetime supersymmetries and saturate a Bogomol'nyi bound. They typically arise as the extreme mass=charge limit of more general two-parameter solutions with event horizons. We also describe the theory {\\it dual} to the fundamental string for which the roles of elementary and soliton solutions are interchanged. In ten spacetime dimensions, this dual theory is a superfivebrane and this gives rise to a string/fivebrane duality conjecture according to which the fivebrane may be regarded as fundamental in its own right, with the strongly coupled string corresponding to the weakly coupled fivebrane and vice-versa. After compactification to four spacetime dimensions, the fivebrane appears as a magnetic monopole or a dual string according as it wraps around five or four of the compactified dimensions. This gives rise to a four-dimensional string/string duality conjecture which subsumes a Montonen-Olive type duality in that the magnetic monopoles of the fundamental string correspond to the electric winding states of the dual string. This leads to a {\\it duality of dualities} whereby under string/string duality the the strong/weak coupling $S$-duality trades places with the minimum/maximum length $T$-duality. Since these magnetic monopoles are extreme black holes, a prediction of $S$-duality is that the corresponding electric massive states of the fundamental string are also extreme black holes.", "prompt": "We review the status of solitons in superstring theory, with a view to understanding the strong coupling regime. These {\\it solitonic} solutions are non-singular field configurations which solve the empty-space low-energy field equations (generalized, whenever possible, to all orders in $\\alpha'$), carry a", "orig": "non-vanishing topological \"magnetic\" charge and are stabilized by a topological conservation law. They are compared and contrasted with the {\\it elementary} solutions which are singular solutions of the field equations with a $\\sigma$-model source term and carry a non-vanishing Noether \"electric\" charge", "generated": "We review the status of solitons in superstring theory, with a view to understanding the strong coupling regime. These {\\it solitonic} solutions are non-singular field configurations which solve the empty-space low-energy field equations (generalized, whenever possible, to all orders in $\\alpha'$), carry a"}
{"full_text": "\"Topological Quantum Chemistry (TQC) links the chemical and symmetry structure of a given material with its topological properties. This field tabulates the data of the 10398 real-space atomic limits of materials, and solves the compatibility relations of electronic bands in momentum space. A material that is not an atomic limit or whose bands do not satisfy the compatibility relations, is a topological insulator/semimetal. We use TQC to find the topological stoichiometric non-magnetic, \"high-quality'' materials in the world. We develop several code additions to VASP which can compute all characters of all symmetries at all high-symmetry points in the Brillouin Zone (BZ). Using TQC we then develop codes to check which materials in ICSD are topological. Out of 26938 stoichiometric materials in our filtered ICSD database, we find 2861 topological insulators (TI) and 2936 topological semimetals (2505 and 2560 non-f electron, respectively). Our method is uniquely capable to show that none of the TI's found exhibit fragile topology. We partition the topological materials in different physical classes. For the majority of the 5797 \"high-quality'' topological material, we compute: the topological class (equivalence classes of TQC elementary band representations -- equivalent to the topological index), the symmetry(ies) that protects the topological class, the representations at high symmetry points and the direct gap (for insulators), and the topological index. For topological semimetals we then compute whether the system becomes a topological insulator (whose index/class we compute) upon breaking symmetries -- useful for experiments. 2152 more TI's are obtained in this way. For almost all 5065 non-f-electron topological materials, we provide the electronic band structures, allowing the identification of quantitative properties (gaps, velocities). Remarkably, our exhaustive results show that a large proportion ( ~ 24% !) of all materials in nature are topological (confirmed by calculations of \"low-quality'' materials). We confirm the topology of several new materials by Wilson loop calculations. We added an open-source code and end-user button on the Bilbao Crystallographic Server (BCS) which checks the topology of any material. We comment on the chemistry of each compound and sample part of the \"low-quality'' ICSD data to find more materials.\"", "prompt": "\"Topological Quantum Chemistry (TQC) links the chemical and symmetry structure of a given material with its topological properties. This field tabulates the data of the 10398 real-space atomic limits of materials, and solves the compatibility relations of electronic bands in momentum space. A material that is not an atomic limit", "orig": "or whose bands do not satisfy the compatibility relations, is a topological insulator/semimetal. We use TQC to find the topological stoichiometric non-magnetic, \"high-quality'' materials in the world. We develop several code additions to VASP which can compute all characters of all symmetries", "generated": "Topological Quantum Chemistry (TQC) links the chemical and symmetry structure of a given material with its topological properties. This field tabulates the data of the 10398 real-space atomic limits of materials, and solves the compatibility relations of electronic bands in momentum space. A material that is not an atomic limit exhibits"}
{"full_text": "Graphs are commonly used to characterise interactions between objects of interest. Because they are based on a straightforward formalism, they are used in many scientific fields from computer science to historical sciences. In this paper, we give an introduction to some methods relying on graphs for learning. This includes both unsupervised and supervised methods. Unsupervised learning algorithms usually aim at visualising graphs in latent spaces and/or clustering the nodes. Both focus on extracting knowledge from graph topologies. While most existing techniques are only applicable to static graphs, where edges do not evolve through time, recent developments have shown that they could be extended to deal with evolving networks. In a supervised context, one generally aims at inferring labels or numerical values attached to nodes using both the graph and, when they are available, node characteristics. Balancing the two sources of information can be challenging, especially as they can disagree locally or globally. In both contexts, supervised and un-supervised, data can be relational (augmented with one or several global graphs) as described above, or graph valued. In this latter case, each object of interest is given as a full graph (possibly completed by other characteristics). In this context, natural tasks include graph clustering (as in producing clusters of graphs rather than clusters of nodes in a single graph), graph classification, etc. 1 Real networks One of the first practical studies on graphs can be dated back to the original work of Moreno [51] in the 30s. Since then, there has been a growing interest in graph analysis associated with strong developments in the modelling and the processing of these data. Graphs are now used in many scientific fields. In Biology [54, 2, 7], for instance, metabolic networks can describe pathways of biochemical reactions [41], while in social sciences networks are used to represent relation ties between actors [66, 56, 36, 34]. Other examples include powergrids [71] and the web [75]. Recently, networks have also been considered in other areas such as geography [22] and history [59, 39]. In machine learning, networks are seen as powerful tools to model problems in order to extract information from data and for prediction purposes. This is the object of this paper. For more complete surveys, we refer to [28, 62, 49, 45]. In this section, we introduce notations and highlight properties shared by most real networks. In Section 2, we then consider methods aiming at extracting information from a unique network. We will particularly focus on clustering methods where the goal is to find clusters of vertices. Finally, in Section 3, techniques that take a series of networks into account, where each network is", "prompt": "Graphs are commonly used to characterise interactions between objects of interest. Because they are based on a straightforward formalism, they are used in many scientific fields from computer science to historical sciences. In this paper, we give an introduction to some methods relying on graphs for learning. This includes both unsupervised and supervised", "orig": "methods. Unsupervised learning algorithms usually aim at visualising graphs in latent spaces and/or clustering the nodes. Both focus on extracting knowledge from graph topologies. While most existing techniques are only applicable to static graphs, where edges do not evolve through time, recent developments have shown that they could be extended to deal with evolving", "generated": "Graphs are commonly used to characterise interactions between objects of interest. Because they are based on a straightforward formalism, they are used in many scientific fields from computer science to historical sciences. In this paper, we give an introduction to some methods relying on graphs for learning. This includes both unsupervised and supervised"}
{"full_text": "Context: Actor-based programming languages offer many essential features for developing modern distributed reactive systems. These systems exploit the actor model's isolation property to fulfill their performance and scalability demands. Unfortunately, the reliance of the model on isolation as its most fundamental property requires programmers to express complex interaction patterns between their actors to be expressed manually in terms of complex combinations of messages sent between the isolated actors. Inquiry: In the last three decades, several language design proposals have been introduced to reduce the complexity that emerges from describing said interaction and coordination of actors. We argue that none of these proposals is satisfactory in order to express the many complex interaction patterns between actors found in modern reactive distributed systems. Approach: We describe seven smart home automation scenarios (in which an actor represents every smart home appliance) to motivate the support by actor languages for five radically different types of message synchronization patterns, which are lacking in modern distributed actor-based languages. Fortunately, these five types of synchronisation patterns have been studied extensively by the Complex Event Processing (CEP) community. Our paper describes how such CEP patterns are elegantly added to an actor-based programming language. Knowledge: Based on our findings, we propose an extension of the single-message matching paradigm of contemporary actor-based languages in order to support a multiple-message matching way of thinking in the same way as proposed by CEP languages. Our proposal thus enriches the actor-model by ways of declaratively describing complex message combinations to which an actor can respond. Grounding: We base the problem-statement of the paper on an online poll in the home automation community that has motivated the real need for the CEP-based synchronisation operators between actors proposed in the paper. Furthermore, we implemented a DSL -- called Sparrow -- that supports said operators and we argue quantitatively (in terms of LOC and in terms of a reduction of the concerns that have to be handled by programmers) that the DSL outperforms existing approaches. Importance: This work aims to provide a set of synchronization operators that help actor-based languages to handle the complex interaction required by modern reactive distributed systems. To the best of our knowledge, our proposal is the first one to add advanced CEP synchronization operators to the -- relatively simplistic single-message based matching -- mechanisms of most actor-based languages.", "prompt": "Context: Actor-based programming languages offer many essential features for developing modern distributed reactive systems. These systems exploit the actor model's isolation property to fulfill their performance and scalability demands. Unfortunately, the reliance of the model on isolation as its most fundamental property requires programmers to express complex interaction patterns between their actors to be expressed manually in", "orig": "terms of complex combinations of messages sent between the isolated actors. Inquiry: In the last three decades, several language design proposals have been introduced to reduce the complexity that emerges from describing said interaction and coordination of actors. We argue that none of these proposals is satisfactory in order to express the many complex interaction patterns between actors found in", "generated": "Actor-based programming languages offer many essential features for developing modern distributed reactive systems. These systems exploit the actor model's isolation property to fulfill their performance and scalability demands. Unfortunately, the reliance of the model on isolation as its most fundamental property requires programmers to express complex interaction patterns between their actors to be expressed manually in order to"}
{"full_text": "**Context** Context-free grammars are widely used for language prototyping and implementation. They allow formalizing the syntax of domain-specific or general-purpose programming languages concisely and declaratively. However, the natural and concise way of writing a context-free grammar is often ambiguous. Therefore, grammar formalisms support extensions in the form of *declarative disambiguation rules* to specify operator precedence and associativity, solving ambiguities that are caused by the subset of the grammar that corresponds to expressions. **Inquiry** Implementing support for declarative disambiguation within a parser typically comes with one or more of the following limitations in practice: a lack of parsing performance, or a lack of modularity (i.e., disallowing the composition of grammar fragments of potentially different languages). The latter subject is generally addressed by scannerless generalized parsers. We aim to equip scannerless generalized parsers with novel disambiguation methods that are inherently performant, without compromising the concerns of modularity and language composition. **Approach** In this paper, we present a novel low-overhead implementation technique for disambiguating deep associativity and priority conflicts in scannerless generalized parsers with lightweight data-dependency. **Knowledge** Ambiguities with respect to operator precedence and associativity arise from combining the various operators of a language. While *shallow conflicts* can be resolved efficiently by one-level tree patterns, *deep conflicts* require more elaborate techniques, because they can occur arbitrarily nested in a tree. Current state-of-the-art approaches to solving deep priority conflicts come with a severe performance overhead. **Grounding** We evaluated our new approach against state-of-the-art declarative disambiguation mechanisms. By parsing a corpus of popular open-source repositories written in Java and OCaml, we found that our approach yields speedups of up to 1.73x over a grammar rewriting technique when parsing programs with deep priority conflicts--with a modest overhead of 1-2 % when parsing programs without deep conflicts. **Importance** A recent empirical study shows that deep priority conflicts are indeed wide-spread in real-world programs. The study shows that in a corpus of popular OCaml projects on Github, up to 17 % of the source files contain deep priority conflicts. However, there is no solution in the literature that addresses efficient disambiguation of deep priority conflicts, with support for modular and composable syntax definitions.", "prompt": "**Context** Context-free grammars are widely used for language prototyping and implementation. They allow formalizing the syntax of domain-specific or general-purpose programming languages concisely and declaratively. However, the natural and concise way of writing a context-free grammar is often ambiguous. Therefore, grammar formalisms support extensions in", "orig": "the form of *declarative disambiguation rules* to specify operator precedence and associativity, solving ambiguities that are caused by the subset of the grammar that corresponds to expressions. **Inquiry** Implementing support for declarative disambiguation within a parser typically comes with one or more of the following limitations", "generated": "**Context** Context-free grammars are widely used for language prototyping and implementation. They allow formalizing the syntax of domain-specific or general-purpose programming languages concisely and declaratively. However, the natural and concise way of writing a context-free grammar is often ambiguous. Therefore, grammar formalisms support extensions in"}
{"full_text": "Context: Roblox Studio lets millions of creators build interactive experiences by programming in a variant of Lua called Luau. The creators form a broad group, ranging from novices writing their first script to professional developers; thus, Luau must support a wide audience. As part of its efforts to support all kinds of programmers, Luau includes an optional, gradual type system and goes to great lengths to minimize false positive errors. Inquiry: Since Luau is currently being used by many creators, we want to collect data to improve the language and, in particular, the type system. The standard way to collect data is to deploy client-side telemetry; however, we cannot scrape personal data or proprietary information, which means we cannot collect source code fragments, error messages, or even filepaths. The research questions are thus about how to conduct telemetry that is not invasive and obtain insights from it about type errors. Approach: We designed and implemented a pseudonymized, randomly-sampling telemetry system for Luau. Telemetry records include a timestamp, a session id, a reason for sending, and a numeric summary of the most recent type analyses. This information lets us study type errors over time without revealing private data. We deployed the system in Roblox Studio during Spring 2023 and collected over 1.5 million telemetry records from over 340,000 sessions. Knowledge: We present several findings about Luau, all of which suggest that telemetry is an effective way to study type error pragmatics. One of the less-surprising findings is that opt-in gradual types are unpopular: there is an 100x gap between the number of untyped Luau sessions and the number of typed ones. One surprise is that the strict mode for type analysis is overly conservative about interactions with data assets. A reassuring finding is that type analysis rarely hits its internal limits on problem size. Grounding: Our findings are supported by a dataset of over 1.5 million telemetry records. The data and scripts for analyzing it are available in an artifact. Importance: Beyond the immediate benefits to Luau, our findings about types and type errors have implications for adoption and ergonomics in other gradual languages such as TypeScript, Elixir, and Typed Racket. Our telemetry design is of broad interest, as it reports on type errors without revealing sensitive information.", "prompt": "Context: Roblox Studio lets millions of creators build interactive experiences by programming in a variant of Lua called Luau. The creators form a broad group, ranging from novices writing their first script to professional developers; thus, Luau must support a wide audience. As part of its efforts to support all kinds of programmers", "orig": ", Luau includes an optional, gradual type system and goes to great lengths to minimize false positive errors. Inquiry: Since Luau is currently being used by many creators, we want to collect data to improve the language and, in particular, the type system. The standard way to collect data is to deploy client-side telemetry", "generated": "Roblox Studio lets millions of creators build interactive experiences by programming in a variant of Lua called Luau. The creators form a broad group, ranging from novices writing their first script to professional developers; thus, Luau must support a wide audience. As part of its efforts to support all kinds of programmers, the"}
{"full_text": "Muscle tissue drives nearly all movement in the animal kingdom, providing power, mobility, and dexterity. Technologies for measuring muscle tissue motion, such as sonomicrometry, fluoromicrometry, and ultrasound, have significantly advanced our understanding of biomechanics. Yet, the field lacks the ability to monitor muscle tissue motion for animal behavior outside the lab. Towards addressing this issue, we previously introduced magnetomicrometry, a method that uses magnetic beads to wirelessly monitor muscle tissue length changes, and we validated magnetomicrometry via tightly-controlled in situ testing. In this study we validate the accuracy of magnetomicrometry against fluoromicrometry during untethered running in an in vivo turkey model. We demonstrate real-time muscle tissue length tracking of the freely-moving turkeys executing various motor activities, including ramp ascent and descent, vertical ascent and descent, and free roaming movement. Given the demonstrated capacity of magnetomicrometry to track muscle movement in untethered animals, we feel that this technique will enable new scientific explorations and an improved understanding of muscle function. -- -- El tejido muscular es el motor de casi todos los movimientos del reino animal, ya que proporciona fuerza, movilidad y destreza. Las tecnolog\\'ias para medir el movimiento del tejido muscular, como la sonomicrometr\\'ia, la fluoromicrometr\\'ia y el ultrasonido, han avanzado considerablemente la comprensi\\'on de la biomec\\'anica. Sin embargo, este campo carece de la capacidad de rastrear el movimiento del tejido muscular en el comportamiento animal fuera del laboratorio. Para abordar este problema, presentamos previamente la magnetomicrometr\\'ia, un m\\'etodo que utiliza peque\\~nos imanes para rastrear de forma inal\\'ambrica los cambios de longitud del tejido muscular, y validamos la magnetomicrometr\\'ia mediante pruebas estrechamente controladas in situ. En este estudio validamos la precisi\\'on de la magnetomicrometr\\'ia en comparaci\\'on con la fluoromicrometr\\'ia usando un modelo de pavo in vivo mientras corre libremente. Demostramos el rastreo en tiempo real de la longitud del tejido muscular de los pavos que se mueven libremente ejecutando varias actividades motoras, incluyendo el ascenso y el descenso en rampa, el ascenso y el descenso vertical, y el movimiento libre. Dada la capacidad demostrada de la magnetomicrometr\\'ia para rastrear el movimiento muscular en animales en un contexto m\\'ovil, creemos que esta t\\'ecnica permitir\\'a nuevas exploraciones cient\\'ificas y una mejor comprensi\\'on de la funci\\'on muscular.", "prompt": "Muscle tissue drives nearly all movement in the animal kingdom, providing power, mobility, and dexterity. Technologies for measuring muscle tissue motion, such as sonomicrometry, fluoromicrometry, and ultrasound, have significantly advanced our understanding of biomechanics. Yet, the field lacks the ability to monitor muscle", "orig": "tissue motion for animal behavior outside the lab. Towards addressing this issue, we previously introduced magnetomicrometry, a method that uses magnetic beads to wirelessly monitor muscle tissue length changes, and we validated magnetomicrometry via tightly-controlled in situ testing. In this study we validate the accuracy of magnetomicrometry against", "generated": "Muscle tissue drives nearly all movement in the animal kingdom, providing power, mobility, and dexterity. Technologies for measuring muscle tissue motion, such as sonomicrometry, fluoromicrometry, and ultrasound, have significantly advanced our understanding of biomechanics. Yet, the field lacks the ability to monitor muscle"}
{"full_text": "We explore how to improve the hybrid model description of the particles originating from the wake that a jet produced in a heavy ion collision leaves in the droplet of quark-gluon plasma (QGP) through which it propagates, using linearized hydrodynamics on a background Bjorken flow. Jet energy and momentum loss described by the hybrid model become currents sourcing linearized hydrodynamics. By solving the linearized hydrodynamic equations numerically, we investigate the development of the wake in the dynamically evolving droplet of QGP, study the effect of viscosity, scrutinize energy-momentum conservation, and check the validity of the linear approximation. We find that linearized hydrodynamics works better in the viscous case because diffusive modes damp the energy-momentum perturbation produced by the jet. We calculate the distribution of particles produced from the jet wake by using the Cooper-Frye prescription and find that both the transverse momentum spectrum and the distribution of particles in azimuthal angle are similar in shape in linearized hydrodynamics and in the hybrid model. Their normalizations are different because the momentum-rapidity distribution in the linearized hydrodynamics analysis is more spread out, due to sound modes. Since the Bjorken flow has no transverse expansion, we explore the effect of transverse flow by using local boosts to add it into the Cooper-Frye formula. After including the effects of transverse flow in this way, the transverse momentum spectrum becomes harder: more particles with transverse momenta bigger than $2$ GeV are produced than in the hybrid model. Although we defer implementing this analysis in a jet Monte Carlo, as would be needed to make quantitative comparisons to data, we gain a qualitative sense of how the jet wake may modify jet observables by computing proxies for two example observables: the lost energy recovered in a cone of varying open angle, and the fragmentation function. We find that linearized hydrodynamics with transverse flow effects added improves the description of the jet wake in the hybrid model in just the way that comparison to data indicates is needed. Our study illuminates a path to improving the description of the wake in the hybrid model, highlighting the need to take into account the effects of both transverse flow and the broadening of the energy-momentum perturbation in spacetime rapidity on particle production.", "prompt": "We explore how to improve the hybrid model description of the particles originating from the wake that a jet produced in a heavy ion collision leaves in the droplet of quark-gluon plasma (QGP) through which it propagates, using linearized hydrodynamics on a background Bjorken flow. Jet energy", "orig": "and momentum loss described by the hybrid model become currents sourcing linearized hydrodynamics. By solving the linearized hydrodynamic equations numerically, we investigate the development of the wake in the dynamically evolving droplet of QGP, study the effect of viscosity, scrutinize energy-momentum conservation, and check the validity of the", "generated": "We explore how to improve the hybrid model description of the particles originating from the wake that a jet produced in a heavy ion collision leaves in the droplet of quark-gluon plasma (QGP) through which it propagates, using linearized hydrodynamics on a background Bjorken flow. Jet energy"}
{"full_text": "We measure the large-scale real-space power spectrum P(k) using luminous red galaxies (LRGs) in the Sloan Digital Sky Survey (SDSS) and use this measurement to sharpen constraints on cosmological parameters from the Wilkinson Microwave Anisotropy Probe (WMAP). We employ a matrix-based power spectrum estimation method using Pseudo-Karhunen-Loeve eigenmodes, producing uncorrelated minimum-variance measurements in 20 k-bands of both the clustering power and its anisotropy due to redshift-space distortions, with narrow and well-behaved window functions in the range 0.01h/Mpc < k < 0.2h/Mpc. Results from the LRG and main galaxy samples are consistent, with the former providing higher signal-to-noise. Our results are robust to omitting angular and radial density fluctuations and are consistent between different parts of the sky. They provide a striking confirmation of the predicted large-scale LCDM power spectrum. Combining only SDSS LRG and WMAP data places robust constraints on many cosmological parameters that complement prior analyses of multiple data sets. The LRGs provide independent cross-checks on Om and the baryon fraction in good agreement with WMAP. Within the context of flat LCDM models, our LRG measurements complement WMAP by sharpening the constraints on the matter density, the neutrino density and the tensor amplitude by about a factor of two, giving Omega_m=0.24+-0.02 (1 sigma), sum m_nu < 0.9 eV (95%) and r<0.3 (95%). Baryon oscillations are clearly detected and provide a robust measurement of the comoving distance to the median survey redshift z=0.35 independent of curvature and dark energy properties. Within the LCDM framework, our power spectrum measurement improves the evidence for spatial flatness, sharpening the curvature constraint Omega_tot=1.05+-0.05 from WMAP alone to Omega_tot=1.003+-0.010. Assuming Omega_tot=1, the equation of state parameter is constrained to w=-0.94+-0.09, indicating the potential for more ambitious future LRG measurements to provide precision tests of the nature of dark energy. All these constraints are essentially independent of scales k>0.1h/Mpc and associated nonlinear complications, yet agree well with more aggressive published analyses where nonlinear modeling is crucial.", "prompt": "We measure the large-scale real-space power spectrum P(k) using luminous red galaxies (LRGs) in the Sloan Digital Sky Survey (SDSS) and use this measurement to sharpen constraints on cosmological parameters from the Wilkinson Microwave Anisotropy Probe (WMAP). We employ a matrix-based power spectrum estimation", "orig": "method using Pseudo-Karhunen-Loeve eigenmodes, producing uncorrelated minimum-variance measurements in 20 k-bands of both the clustering power and its anisotropy due to redshift-space distortions, with narrow and well-behaved window functions in the range 0.01", "generated": "We measure the large-scale real-space power spectrum P(k) using luminous red galaxies (LRGs) in the Sloan Digital Sky Survey (SDSS) and use this measurement to sharpen constraints on cosmological parameters from the Wilkinson Microwave Anisotropy Probe (WMAP). We employ a matrix-based power spectrum estimation"}
{"full_text": "We present Spitzer IRS mid-infrared (5-35 micron) spectra of a complete flux-limited sample (> 3 mJy at 8 micron) of young stellar object (YSO) candidates selected on the basis of their infrared colors in the Serpens Molecular Cloud. Spectra of 147 sources are presented and classified. Background stars (with slope consistent with a reddened stellar spectrum and silicate features in absorption), galaxies (with redshifted PAH features) and a planetary nebula (with high ionization lines) amount to 22% of contamination in this sample, leaving 115 true YSOs. Sources with rising spectra and ice absorption features, classified as embedded Stage I protostars, amount to 18% of the sample. The remaining 82% (94) of the disk sources are analyzed in terms of spectral energy distribution shapes, PAHs and silicate features. The presence, strength and shape of these silicate features are used to infer disk properties for these systems. About 8% of the disks have 30/13 micron flux ratios consistent with cold disks with inner holes or gaps, and 3% of the disks show PAH emission. Comparison with models indicates that dust grains in the surface of these disks have sizes of at least a few \\mu\\m. The 20 micron silicate feature is sometimes seen in absence of the 10 micron feature, which may be indicative of very small holes in these disks. No significant difference is found in the distribution of silicate feature shapes and strengths between sources in clusters and in the field. Moreover, the results in Serpens are compared with other well-studied samples: the c2d IRS sample distributed over 5 clouds and a large sample of disks in the Taurus star-forming region. The remarkably similar distributions of silicate feature characteristics in samples with different environment and median ages - if significant - imply that the dust population in the disk surface results from an equilibrium between dust growth and destructive collision processes that are maintained over a few million years for any YSO population irrespective of environment.", "prompt": "We present Spitzer IRS mid-infrared (5-35 micron) spectra of a complete flux-limited sample (> 3 mJy at 8 micron) of young stellar object (YSO) candidates selected on the basis of their infrared colors in the Serpens Molecular Cloud. Spectra of 147", "orig": "sources are presented and classified. Background stars (with slope consistent with a reddened stellar spectrum and silicate features in absorption), galaxies (with redshifted PAH features) and a planetary nebula (with high ionization lines) amount to 22% of contamination in this sample, leaving 115 true YSO", "generated": "We present Spitzer IRS mid-infrared (5-35 micron) spectra of a complete flux-limited sample (> 3 mJy at 8 micron) of young stellar object (YSO) candidates selected on the basis of their infrared colors in the Serpens Molecular Cloud. Spectra of 147"}
{"full_text": "The epitaxial growth of anatase (001) films deposited by pulsed laser deposition (PLD) and molecular beam epitaxy (MBE) on SrTiO3 (001) (STO) single crystals has been studied using X-ray diffraction and surface sensitivity UHV techniques. The evolution of the strain represented by the microstrain and the change of the in-plane and out-of-plane lattice parameters with film growth temperature, the effect of the annealing temperature and the influence of the oxygen content of the film have been investigated.The out-of-plane lattice strain shows a compressive (-0.2%) or expansive (+0.3%) behavior, in the range 600-900$^\\circ$C, for temperatures below or above 700$^\\circ$C, respectively. The in-plane lattice parameters, as well as the cell volume of the film, remain under compression over the entire temperature range explored.PLD films grow into square islands that align with the surface lattice directions of the STO substrate. The maximum size of these islands is reached at growth temperatures close to 875-925$^\\circ$C. Film annealing at temperatures of 800$^\\circ$C or higher melts the islands into flat terraces. Larger terraces are reached at high annealing temperatures of 925$^\\circ$C for extended periods of 12 hours. This procedure allows flat surface terrace sizes of up to 650 nm to be achieved.The crystalline quality achieved in anatase films prepared by PLD or MBE growth methods is similar. The two-step anatase growth process used during the synthesis of the films with both methods: film growth and post-annealing treatment in oxygen or air at ambient pressure, using temperature and time as key parameters, allows to control the surface terrace size and stoichiometry of the films, as well as the anatase/rutile intermixing rates at sufficiently high temperatures. This growth process could allow the substitution of their equivalent single crystals. The range of applicability of these films would include their use as structural and electronic model systems, or in harsh experimental conditions due to their low production cost.", "prompt": "The epitaxial growth of anatase (001) films deposited by pulsed laser deposition (PLD) and molecular beam epitaxy (MBE) on SrTiO3 (001) (STO) single crystals has been studied using X-ray diffraction and surface sensitivity UHV techniques. The evolution", "orig": "of the strain represented by the microstrain and the change of the in-plane and out-of-plane lattice parameters with film growth temperature, the effect of the annealing temperature and the influence of the oxygen content of the film have been investigated.The out-of-plane lattice strain shows a compressive (-0.2%) or expansive (+", "generated": "The epitaxial growth of anatase (001) films deposited by pulsed laser deposition (PLD) and molecular beam epitaxy (MBE) on SrTiO3 (001) (STO) single crystals has been studied using X-ray diffraction and surface sensitivity UHV techniques. The evolution"}
{"full_text": "In classical General Relativity, the way to exhibit the equations for the gravitational waves is based on two \"tricks\" allowing to transform the Einstein equations after linearizing them over the Minkowski metric. With specific notations used in the study of {\\it Lie pseudogroups} of transformations of an $n$-dimensional manifold, let $\\Omega=({\\Omega}\\_{ij}={\\Omega}\\_{ji})$ be a perturbation of the non-degenerate metric $\\omega=({\\omega}\\_{ij}={\\omega}\\_{ji})$ with $det(\\omega)\\neq 0$ and call ${\\omega}^{-1}=({\\omega}^{ij}={\\omega}^{ji})$ the inverse matrix appearing in the Dalembertian operator $\\Box = {\\omega}^{ij}d\\_{ij}$. The first idea is to introduce the linear transformation ${\\bar{\\Omega}}\\_{ij}={\\Omega}\\_{ij}-\\frac{1}{2}{\\omega}\\_{ij}tr(\\Omega)$ where $tr(\\Omega)={\\omega}^{ij}{\\Omega}\\_{ij}$ is the {\\it trace} of $\\Omega$, which is invertible when $n\\geq 3$. The second important idea is to notice that the composite second order linearized Einstein operator $\\bar{\\Omega} \\rightarrow \\Omega \\rightarrow E=(E\\_{ij}=R\\_{ij} - \\frac{1}{2}{\\omega}\\_{ij}tr(R))$ where $\\Omega \\rightarrow R=(R\\_{ij}=R\\_{ji})$ is the linearized Ricci operator with trace $tr(R)={\\omega}^{ij}R\\_{ij}$ is reduced to $\\Box {\\bar{\\Omega}}\\_{ij}$ when ${\\omega}^{rs}d\\_{ri}{\\bar{\\Omega}}\\_{sj}=0$. The purpose of this short but striking paper is to revisit these two results in the light of the {\\it differential duality} existing in Algebraic Analysis, namely a mixture of differential geometry and homological agebra, providing therefore a totally different interpretation. In particular, we prove that the above operator $\\bar{\\Omega} \\rightarrow E$ is nothing else than the formal adjoint of the Ricci operator $\\Omega \\rightarrow R$ and that the map $\\Omega \\rightarrow \\bar{\\Omega}$ is just the formal adjoint (transposed) of the defining tensor map $R \\rightarrow E$. Accordingly, the Cauchy operator (stress equations) can be directly parametrized by the formal adjoint of the Ricci operator and the Einstein operator is no longer needed.", "prompt": "In classical General Relativity, the way to exhibit the equations for the gravitational waves is based on two \"tricks\" allowing to transform the Einstein equations after linearizing them over the Minkowski metric. With specific notations used in the study of {\\it Lie pseudogroups} of transformations of an $n", "orig": "$-dimensional manifold, let $\\Omega=({\\Omega}\\_{ij}={\\Omega}\\_{ji})$ be a perturbation of the non-degenerate metric $\\omega=({\\omega}\\_{ij}={\\omega}\\_{ji})$ with $det(\\omega)\\neq 0$", "generated": "In classical General Relativity, the way to exhibit the equations for the gravitational waves is based on two \"tricks\" allowing to transform the Einstein equations after linearizing them over the Minkowski metric. With specific notations used in the study of {\\it Lie pseudogroups} of transformations of an arbitrary R"}
{"full_text": "In recent years genetic algorithms have emerged as a useful tool for the heuristic solution of complex discrete optimisation problems. In particular there has been considerable interest in their use in tackling problems arising in the areas of scheduling and timetabling. However, the classical genetic algorithm paradigm is not well equipped to handle constraints and successful implementations usually require some sort of modification to enable the search to exploit problem specific knowledge in order to overcome this shortcoming. This paper is concerned with the development of a family of genetic algorithms for the solution of a nurse rostering problem at a major UK hospital. The hospital is made up of wards of up to 30 nurses. Each ward has its own group of nurses whose shifts have to be scheduled on a weekly basis. In addition to fulfilling the minimum demand for staff over three daily shifts, nurses' wishes and qualifications have to be taken into account. The schedules must also be seen to be fair, in that unpopular shifts have to be spread evenly amongst all nurses, and other restrictions, such as team nursing and special conditions for senior staff, have to be satisfied. The basis of the family of genetic algorithms is a classical genetic algorithm consisting of n-point crossover, single-bit mutation and a rank-based selection. The solution space consists of all schedules in which each nurse works the required number of shifts, but the remaining constraints, both hard and soft, are relaxed and penalised in the fitness function. The talk will start with a detailed description of the problem and the initial implementation and will go on to highlight the shortcomings of such an approach, in terms of the key element of balancing feasibility, i.e. covering the demand and work regulations, and quality, as measured by the nurses' preferences. A series of experiments involving parameter adaptation, niching, intelligent weights, delta coding, local hill climbing, migration and special selection rules will then be outlined and it will be shown how a series of these enhancements were able to eradicate these difficulties. Results based on several months' real data will be used to measure the impact of each modification, and to show that the final algorithm is able to compete with a tabu search approach currently employed at the hospital. The talk will conclude with some observations as to the overall quality of this approach to this and similar problems.", "prompt": "In recent years genetic algorithms have emerged as a useful tool for the heuristic solution of complex discrete optimisation problems. In particular there has been considerable interest in their use in tackling problems arising in the areas of scheduling and timetabling. However, the classical genetic algorithm paradigm is not well equipped to handle constraints and successful implementations", "orig": "usually require some sort of modification to enable the search to exploit problem specific knowledge in order to overcome this shortcoming. This paper is concerned with the development of a family of genetic algorithms for the solution of a nurse rostering problem at a major UK hospital. The hospital is made up of wards of up to 30 nurses", "generated": "In recent years genetic algorithms have emerged as a useful tool for the heuristic solution of complex discrete optimisation problems. In particular there has been considerable interest in their use in tackling problems arising in the areas of scheduling and timetabling. However, the classical genetic algorithm paradigm is not well equipped to handle constraints and successful implementation"}
{"full_text": "This book starts from the basic questions that had been raised by the founders of Economic theory, Smith, Ricardo, and Marx: what makes the value of commodities, what are production, exchange, money and incomes like profits, wages and rents. The answers that these economists had provided were mostly wrong, above all by defining the equivalence of commodities at the level of exchange, but also because of a confusion made between values and prices, and wrong views of what production really is and the role of fixed capital. Using the mathematical theory of measurement and the physical theory of dimensional analysis, this book provides a coherent theory of value based on an equivalence relation not at the level of exchange, but of production. Indeed exchange is considered here as an equivalence relation between money and a monetary price, and not between commodities, modern monetary theory having demonstrated that money is not a commodity. The book rejects the conception of production as a surplus, which owes much to Sraffa's theory of production prices, and is shown to be severely flawed. It founds the equivalence of commodities at the level of a production process considered as a transformation process. It rehabilitates the labor theory of value, based on the connection between money and labor due the monetary payment of wages, which allows the homogenization of various kinds of concrete labor into abstract labor. It shows that value is then a dimension of commodities and that this dimension is time, i.e. the time of physics. On this background, the book shows that the calculation of values for all commodities is always possible, even in the case of joint production, and that there cannot be any commodity residue left by this calculation. As a further step, this book provides a coherent theory of the realization of the product, which occurs in the circulation process. Using an idea - the widow's cruse - introduced by Keynes in his Treatise on Money, it brings to light the mechanism behind the transformation of money values into money prices and of surplus-value into profits and other transfer incomes, ensuring the formation of monetary profits. The book sheds some light on the rate of profit, its determinants and its evolution, showing in particular the paramount importance of capitalist consumption as one of its main determinants. In passing it explains the reasons why in the real world there is a multiplicity of profit rates. Finally, it allows to solve in a precise and illustrated way the problems raised by the Marxist law of the tendency of the rate of profit to fall. Most of the results obtained translate into principles, the first ones being truly basic, the following ones less basic, but all of them being fundamental. All in all, this book might provide the first building blocks to develop a full-fledged and scientific economic theory to many fellow economists, critical of neo-classical theory, but who have not yet dicovered the bases of a complete and coherent alternative.", "prompt": "This book starts from the basic questions that had been raised by the founders of Economic theory, Smith, Ricardo, and Marx: what makes the value of commodities, what are production, exchange, money and incomes like profits, wages and rents. The answers that these economists had provided were mostly wrong, above all by defining", "orig": "the equivalence of commodities at the level of exchange, but also because of a confusion made between values and prices, and wrong views of what production really is and the role of fixed capital. Using the mathematical theory of measurement and the physical theory of dimensional analysis, this book provides a coherent theory of value based on an equivalence relation", "generated": "This book starts from the basic questions that had been raised by the founders of Economic theory, Smith, Ricardo, and Marx: what makes the value of commodities, what are production, exchange, money and incomes like profits, wages and rents. The answers that these economists had provided were mostly wrong, above all by defining"}
{"full_text": "We propose an automatic video inpainting algorithm which relies on the optimisation of a global, patch-based functional. Our algorithm is able to deal with a variety of challenging situations which naturally arise in video inpainting, such as the correct reconstruction of dynamic textures, multiple moving objects and moving background. Furthermore, we achieve this in an order of magnitude less execution time with respect to the state-of-the-art. We are also able to achieve good quality results on high definition videos. Finally, we provide specific algorithmic details to make implementation of our algorithm as easy as possible. The resulting algorithm requires no segmentation or manual input other than the definition of the inpainting mask, and can deal with a wider variety of situations than is handled by previous work. 1. Introduction. Advanced image and video editing techniques are increasingly common in the image processing and computer vision world, and are also starting to be used in media entertainment. One common and difficult task closely linked to the world of video editing is image and video \" inpainting \". Generally speaking, this is the task of replacing the content of an image or video with some other content which is visually pleasing. This subject has been extensively studied in the case of images, to such an extent that commercial image inpainting products destined for the general public are available, such as Photoshop's \" Content Aware fill \" [1]. However, while some impressive results have been obtained in the case of videos, the subject has been studied far less extensively than image inpainting. This relative lack of research can largely be attributed to high time complexity due to the added temporal dimension. Indeed, it has only very recently become possible to produce good quality inpainting results on high definition videos, and this only in a semi-automatic manner. Nevertheless, high-quality video inpainting has many important and useful applications such as film restoration, professional post-production in cinema and video editing for personal use. For this reason, we believe that an automatic, generic video inpainting algorithm would be extremely useful for both academic and professional communities.", "prompt": "We propose an automatic video inpainting algorithm which relies on the optimisation of a global, patch-based functional. Our algorithm is able to deal with a variety of challenging situations which naturally arise in video inpainting, such as the correct reconstruction of dynamic textures, multiple moving objects and moving background. Furthermore, we achieve this", "orig": "in an order of magnitude less execution time with respect to the state-of-the-art. We are also able to achieve good quality results on high definition videos. Finally, we provide specific algorithmic details to make implementation of our algorithm as easy as possible. The resulting algorithm requires no segmentation or manual input other than the definition of", "generated": "We propose an automatic video inpainting algorithm which relies on the optimisation of a global, patch-based functional. Our algorithm is able to deal with a variety of challenging situations which naturally arise in video inpainting, such as the correct reconstruction of dynamic textures, multiple moving objects and moving background. Furthermore, we achieve this"}
{"full_text": "Let $\\mathbf{f} = \\left(f_1, \\dots, f_p\\right) $ be a polynomial tuple in $\\mathbb{Q}[z_1, \\dots, z_n]$ and let $d = \\max_{1 \\leq i \\leq p} \\deg f_i$. We consider the problem of computing the set of asymptotic critical values of the polynomial mapping, with the assumption that this mapping is dominant, $\\mathbf{f}: z \\in \\mathbb{K}^n \\to (f\\_1(z), \\dots, f\\_p(z)) \\in \\mathbb{K}^p$ where $\\mathbb{K}$ is either $\\mathbb{R}$ or $\\mathbb{C}$. This is the set of values $c$ in the target space of $\\mathbf{f}$ such that there exists a sequence of points $(\\mathbf{x}_i)_{i\\in \\mathbb{N}}$ for which $\\mathbf{f}(\\mathbf{x}_i)$ tends to $c$ and $\\|\\mathbf{x}_i\\| \\kappa {\\rm d} \\mathbf{f}(\\mathbf{x}_i))$ tends to $0$ when $i$ tends to infinity where ${\\rm d} \\mathbf{f}$ is the differential of $\\mathbf{f}$ and $\\kappa$ is a function measuring the distance of a linear operator to the set of singular linear operators from $\\mathbb{K}^n$ to $\\mathbb{K}^p$. Computing the union of the classical and asymptotic critical values allows one to put into practice generalisations of Ehresmann's fibration theorem. This leads to natural and efficient applications in polynomial optimisation and computational real algebraic geometry. Going back to previous works by Kurdyka, Orro and Simon, we design new algorithms to compute asymptotic critical values. Through randomisation, we introduce new geometric characterisations of asymptotic critical values. This allows us to dramatically reduce the complexity of computing such values to a cost that is essentially $O(d^{2n(p+1)})$ arithmetic operations in $\\mathbb{Q}$. We also obtain tighter degree bounds on a hypersurface containing the asymptotic critical values, showing that the degree is at most $p^{n-p+1}(d-1)^{n-p}(d+1)^{p}$. Next, we show how to apply these algorithms to unconstrained polynomial optimisation problems and the problem of computing sample points per connected component of a semi-algebraic set defined by a single inequality/inequation. We report on the practical capabilities of our implementation of this algorithm. It shows how the practical efficiency surpasses the current state-of-the-art algorithms for computing asymptotic critical values by tackling examples that were previously out of reach.", "prompt": "Let $\\mathbf{f} = \\left(f_1, \\dots, f_p\\right) $ be a polynomial tuple in $\\mathbb{Q}[z_1, \\dots, z_n]$ and let $d = \\max_{1 \\leq i \\leq p} \\", "orig": "deg f_i$. We consider the problem of computing the set of asymptotic critical values of the polynomial mapping, with the assumption that this mapping is dominant, $\\mathbf{f}: z \\in \\mathbb{K}^n \\to (f\\_1(z), \\dots, f\\_p(z))", "generated": "Here is the repeated text:\nLet $\\mathbf{f} = \\left(f_1, \\dots, f_p\\right) $ be a polynomial tuple in $\\mathbb{Q}[z_1, \\dots, z_n]$ and let $d = \\max_{1 \\leq i"}
{"full_text": "The software supply chain is becoming a widespread analogy to designate the series of steps taken to go from source code published by developers to executables running on the users? computers. A security vulnerability in any of these steps puts users at risk, and evidence shows that attacks on the supply chain are becoming more common. The consequences of an attack on the software supply chain can be tragic in a society that relies on many interconnected software systems, and this has led research interest as well as governmental incentives for supply chain security to rise. GNU Guix is a software deployment tool and software distribution that supports provenance tracking, reproducible builds, and reproducible software environments. Unlike many software distributions, it consists exclusively of source code: it provides a set of package definitions that describe how to build code from source. Together, these properties set it apart from many deployment tools that center on the distribution of binaries. This paper focuses on one research question: how can Guix and similar systems allow users to securely update their software? Guix source code is distributed using the Git version control system; updating Guix-installed software packages means, first, updating the local copy of the Guix source code. Prior work on secure software updates focuses on systems very different from Guix -- systems such as Debian, Fedora, or PyPI where updating consists in fetching metadata about the latest binary artifacts available -- and is largely inapplicable in the context of Guix. By contrast, the main threats for Guix are attacks on its source code repository, which could lead users to run inauthentic code or to downgrade their system. Deployment tools that more closely resemble Guix, from Nix to Portage, either lack secure update mechanisms or suffer from shortcomings. Our main contribution is a model and tool to authenticate new Git revisions. We further show how, building on Git semantics, we build protections against downgrade attacks and related threats. We explain implementation choices. This work has been deployed in production two years ago, giving us insight on its actual use at scale every day. The Git checkout authentication at its core is applicable beyond the specific use case of Guix, and we think it could benefit to developer teams that use Git. As attacks on the software supply chain appear, security research is now looking at every link of the supply chain. Secure updates are one important aspect of the supply chain, but this paper also looks at the broader context: how Guix models and implements the supply chain, from upstream source code to binaries running on computers. While much recent work focuses on attestation -- certifying each link of the supply chain -- Guix takes a more radical approach: enabling independent verification of each step, building on reproducible builds, \"bootstrappable\" builds, and provenance tracking. The big picture shows how Guix can be used as the foundation of secure software supply chains.", "prompt": "The software supply chain is becoming a widespread analogy to designate the series of steps taken to go from source code published by developers to executables running on the users? computers. A security vulnerability in any of these steps puts users at risk, and evidence shows that attacks on the supply chain are becoming more common. The consequences", "orig": "of an attack on the software supply chain can be tragic in a society that relies on many interconnected software systems, and this has led research interest as well as governmental incentives for supply chain security to rise. GNU Guix is a software deployment tool and software distribution that supports provenance tracking, reproducible builds, and reproducible", "generated": "The software supply chain is becoming a widespread analogy to designate the series of steps taken to go from source code published by developers to executables running on the users' computers. A security vulnerability in any of these steps puts users at risk, and evidence shows that attacks on the supply chain are becoming more common. The consequences"}
{"full_text": "Along the design process of interactive system many intermediate artefacts (such as user interface prototypes, task models describing user work and activities, dialog models specifying system behavior, interaction models describing user interactions {\\ldots}) are created, tested, revised and improved until the development team produces a validated version of the full-fledged system. Indeed, to build interactive systems there is a need to use multiple artefacts/models (as they provide a complementary view). However, relevant information for describing the design solution and/or supporting design decisions (such as rational about the design, decisions made, recommendations, etc.) is not explicitly capturable in the models/artefacts, hence the need for annotations. Multi-artefacts approaches usually argue that a given information should only be present in one artefact to avoid duplication and increase maintainability of the artefacts. Nonetheless, annotations created on one artefact are usually relevant to other artefacts/models. So that, there is a need for tools and techniques to coordinate annotations across artefacts/models which is the contribution of the present work. In this paper, we propose a model-based approach that was conceived to handle annotations in a systematic way along the development process of interactive systems. As part of the solution, we propose an annotation model built upon the W3C's Web Annotation Data Model. The feasibility of the approach is demonstrated by means of a tool suite featuring a plugin, which has been deployed and tested over the multi-artefacts. The overall approach is illustrated on the design of an interactive cockpit application performing two design iterations. The contribution brings two main benefits for interactive systems engineering: i) it presents a generic pattern for integrating information in multiple usually heterogenous artefacts throughout the design process of interactive systems; and ii) it highlights the need for tools helping to rationalize and to document the various artefacts and the related decisions made during interactive systems design. CCS CONCEPTS $\\bullet$ Human-centered computing $\\bullet$ Human computer interaction (HCI)", "prompt": "Along the design process of interactive system many intermediate artefacts (such as user interface prototypes, task models describing user work and activities, dialog models specifying system behavior, interaction models describing user interactions {\\ldots}) are created, tested, revised and improved until the development team produces a validated version of the full-fledged system", "orig": ". Indeed, to build interactive systems there is a need to use multiple artefacts/models (as they provide a complementary view). However, relevant information for describing the design solution and/or supporting design decisions (such as rational about the design, decisions made, recommendations, etc.) is not explicitly capturable in the models/artef", "generated": "Along the design process of interactive system many intermediate artefacts (such as user interface prototypes, task models describing user work and activities, dialog models specifying system behavior, interaction models describing user interactions {\\ldots}) are created, tested, revised and improved until the development team produces a validated version of the full-fledged system"}
{"full_text": "The individual data collected throughout patient follow-up constitute crucial information for assessing the risk of a clinical event, and eventually for adapting a therapeutic strategy. Joint models and landmark models have been proposed to compute individual dynamic predictions from repeated measures to one or two markers. However, they hardly extend to the case where the complete patient history includes much more repeated markers possibly. Our objective was thus to propose a solution for the dynamic prediction of a health event that may exploit repeated measures of a possibly large number of markers. We combined a landmark approach extended to endogenous markers history with machine learning methods adapted to survival data. Each marker trajectory is modeled using the information collected up to landmark time, and summary variables that best capture the individual trajectories are derived. These summaries and additional covariates are then included in different prediction methods. To handle a possibly large dimensional history, we rely on machine learning methods adapted to survival data, namely regularized regressions and random survival forests, to predict the event from the landmark time, and we show how they can be combined into a superlearner. Then, the performances are evaluated by cross-validation using estimators of Brier Score and the area under the Receiver Operating Characteristic curve adapted to censored data. We demonstrate in a simulation study the benefits of machine learning survival methods over standard survival models, especially in the case of numerous and/or nonlinear relationships between the predictors and the event. We then applied the methodology in two prediction contexts: a clinical context with the prediction of death for patients with primary biliary cholangitis, and a public health context with the prediction of death in the general elderly population at different ages. Our methodology, implemented in R, enables the prediction of an event using the entire longitudinal patient history, even when the number of repeated markers is large. Although introduced with mixed models for the repeated markers and methods for a single right censored time-to-event, our method can be used with any other appropriate modeling technique for the markers and can be easily extended to competing risks setting.", "prompt": "The individual data collected throughout patient follow-up constitute crucial information for assessing the risk of a clinical event, and eventually for adapting a therapeutic strategy. Joint models and landmark models have been proposed to compute individual dynamic predictions from repeated measures to one or two markers. However, they hardly extend to the case where the complete patient history", "orig": "includes much more repeated markers possibly. Our objective was thus to propose a solution for the dynamic prediction of a health event that may exploit repeated measures of a possibly large number of markers. We combined a landmark approach extended to endogenous markers history with machine learning methods adapted to survival data. Each marker trajectory is modeled using the information", "generated": "The individual data collected throughout patient follow-up constitute crucial information for assessing the risk of a clinical event, and eventually for adapting a therapeutic strategy. Joint models and landmark models have been proposed to compute individual dynamic predictions from repeated measures to one or two markers. However, they hardly extend to the case where the complete patient history"}
{"full_text": "Complex networks are a powerful modeling tool, allowing the study of countless real-world systems. They have been used in very different domains such as computer science, biology, sociology, management, etc. Authors have been trying to characterize them using various measures such as degree distribution, transitivity or average distance. Their goal is to detect certain properties such as the small-world or scale-free properties. Previous works have shown some of these properties are present in many different systems, while others are characteristic of certain types of systems only. However, each one of these studies generally focuses on a very small number of topological measures and networks. In this work, we aim at using a more systematic approach. We first constitute a dataset of 152 publicly available networks, spanning over 7 different domains. We then process 14 different topological measures to characterize them in the most possible complete way. Finally, we apply standard data mining tools to analyze these data. A cluster analysis reveals it is possible to obtain two significantly distinct clusters of networks, corresponding roughly to a bisection of the domains modeled by the networks. On these data, the most discriminant measures are density, modularity, average degree and transitivity, and at a lesser extent, closeness and edgebetweenness centralities.Abstract--Complex networks are a powerful modeling tool, allowing the study of countless real-world systems. They have been used in very different domains such as computer science, biology, sociology, management, etc. Authors have been trying to characterize them using various measures such as degree distribution, transitivity or average distance. Their goal is to detect certain properties such as the small-world or scale-free properties. Previous works have shown some of these properties are present in many different systems, while others are characteristic of certain types of systems only. However, each one of these studies generally focuses on a very small number of topological measures and networks. In this work, we aim at using a more systematic approach. We first constitute a dataset of 152 publicly available networks, spanning over 7 different domains. We then process 14 different topological measures to characterize them in the most possible complete way. Finally, we apply standard data mining tools to analyze these data. A cluster analysis reveals it is possible to obtain two significantly distinct clusters of networks, corresponding roughly to a bisection of the domains modeled by the networks. On these data, the most discriminant measures are density, modularity, average degree and transitivity, and at a lesser extent, closeness and edgebetweenness centralities.", "prompt": "Complex networks are a powerful modeling tool, allowing the study of countless real-world systems. They have been used in very different domains such as computer science, biology, sociology, management, etc. Authors have been trying to characterize them using various measures such as degree distribution, transitivity or average distance. Their goal is to", "orig": "detect certain properties such as the small-world or scale-free properties. Previous works have shown some of these properties are present in many different systems, while others are characteristic of certain types of systems only. However, each one of these studies generally focuses on a very small number of topological measures and networks. In this work,", "generated": "Complex networks are a powerful modeling tool, allowing the study of countless real-world systems. They have been used in very different domains such as computer science, biology, sociology, management, etc. Authors have been trying to characterize them using various measures such as degree distribution, transitivity or average distance. Their goal is to"}
{"full_text": "The thesis describes a logical formalization of natural-language database interfacing. We assume the existence of a ``natural language engine'' capable of mediating between surface linguistic string and their representations as ``literal'' logical forms: the focus of interest will be the question of relating ``literal'' logical forms to representations in terms of primitives meaningful to the underlying database engine. We begin by describing the nature of the problem, and show how a variety of interface functionalities can be considered as instances of a type of formal inference task which we call ``Abductive Equivalential Translation'' (AET); functionalities which can be reduced to this form include answering questions, responding to commands, reasoning about the completeness of answers, answering meta-questions of type ``Do you know...'', and generating assertions and questions. In each case, a ``linguistic domain theory'' (LDT) $\\Gamma$ and an input formula $F$ are given, and the goal is to construct a formula with certain properties which is equivalent to $F$, given $\\Gamma$ and a set of permitted assumptions. If the LDT is of a certain specified type, whose formulas are either conditional equivalences or Horn-clauses, we show that the AET problem can be reduced to a goal-directed inference method. We present an abstract description of this method, and sketch its realization in Prolog. The relationship between AET and several problems previously discussed in the literature is discussed. In particular, we show how AET can provide a simple and elegant solution to the so-called ``Doctor on Board'' problem, and in effect allows a ``relativization'' of the Closed World Assumption. The ideas in the thesis have all been implemented concretely within the SRI CLARE project, using a real projects and payments database. The LDT for the example database is described in detail, and examples of the types of functionality that can be achieved within the example domain are presented.", "prompt": "The thesis describes a logical formalization of natural-language database interfacing. We assume the existence of a ``natural language engine'' capable of mediating between surface linguistic string and their representations as ``literal'' logical forms: the focus of interest will be the question of relating ``literal'' logical forms to representations in terms of", "orig": "primitives meaningful to the underlying database engine. We begin by describing the nature of the problem, and show how a variety of interface functionalities can be considered as instances of a type of formal inference task which we call ``Abductive Equivalential Translation'' (AET); functionalities which can be reduced to this form include answering questions", "generated": "The thesis describes a logical formalization of natural-language database interfacing. We assume the existence of a ``natural language engine'' capable of mediating between surface linguistic string and their representations as ``literal'' logical forms: the focus of interest will be the question of relating ``literal'' logical forms to representations in terms of"}
{"full_text": "Let $(\\{X_i(t)\\}_{i\\in \\mathbb{Z}^d})_{t\\geq 0}$ be the system of interacting diffusions on $[0,\\infty)$ defined by the following collection of coupled stochastic differential equations: \\begin{eqnarray}dX_i(t)=\\sum\\limits_{j\\in \\mathbb{Z}^d}a(i,j)[X_j(t)-X_i(t)] dt+\\sqrt{bX_i(t)^2} dW_i(t), \\eqntext{i\\in \\mathbb{Z}^d,t\\geq 0.}\\end{eqnarray} Here, $a(\\cdot,\\cdot)$ is an irreducible random walk transition kernel on $\\mathbb{Z}^d\\times \\mathbb{Z}^d$, $b\\in (0,\\infty)$ is a diffusion parameter, and $(\\{W_i(t)\\}_{i\\in \\mathbb{Z}^d})_{t\\geq 0}$ is a collection of independent standard Brownian motions on $\\mathbb{R}$. The initial condition is chosen such that $\\{X_i(0)\\}_{i\\in \\mathbb{Z}^d}$ is a shift-invariant and shift-ergodic random field on $[0,\\infty)$ with mean $\\Theta\\in (0,\\infty)$ (the evolution preserves the mean). We show that the long-time behavior of this system is the result of a delicate interplay between $a(\\cdot,\\cdot)$ and $b$, in contrast to systems where the diffusion function is subquadratic. In particular, let $\\hat{a}(i,j)={1/2}[a(i,j)+a(j,i)]$, $i,j\\in \\mathbb{Z}^d$, denote the symmetrized transition kernel. We show that: (A) If $\\hat{a}(\\cdot,\\cdot)$ is recurrent, then for any $b>0$ the system locally dies out. (B) If $\\hat{a}(\\cdot,\\cdot)$ is transient, then there exist $b_*\\geq b_2>0$ such that: (B1)d The system converges to an equilibrium $\\nu_{\\Theta}$ (with mean $\\Theta$) if $0<b<b_*$. (B2) The system locally dies out if $b>b_*$. (B3) $\\nu_{\\Theta}$ has a finite 2nd moment if and only if $0<b<b_2$. (B4) The 2nd moment diverges exponentially fast if and only if $b>b_2$. The equilibrium $\\nu_{\\Theta}$ is shown to be associated and mixing for all $0<b<b_*$. We argue in favor of the conjecture that $b_*>b_2$. We further conjecture that the system locally dies out at $b=b_*$. For the case where $a(\\cdot,\\cdot)$ is symmetric and transient we further show that: (C) There exists a sequence $b_2\\geq b_3\\geq b_4\\geq ... >0$ such that: (C1) $\\nu_{\\Theta}$ has a finite $m$th moment if and only if $0<b<b_m$. (C2) The $m$th moment diverges exponentially fast if and only if $b>b_m$. (C3) $b_2\\leq (m-1)b_m<2$. uad(C4) $\\lim_{m\\to\\infty}(m-1)b_m=c=\\sup_{m\\geq 2}(m-1)b_m$. The proof of these results is based on self-duality and on a representation formula through which the moments of the components are related to exponential moments of the collision local time of random walks. Via large deviation theory, the latter lead to variational expressions for $b_*$ and the $b_m$'s, from which sharp bounds are deduced. The critical value $b_*$ arises from a stochastic representation of the Palm distribution of the system. The special case where $a(\\cdot,\\cdot)$ is simple random walk is commonly referred to as the parabolic Anderson model with Brownian noise. This case was studied in the memoir by Carmona and Molchanov [Parabolic Anderson Problem and Intermittency (1994) Amer. Math. Soc., Providence, RI], where part of our results were already established.", "prompt": "Let $(\\{X_i(t)\\}_{i\\in \\mathbb{Z}^d})_{t\\geq 0}$ be the system of interacting diffusions on $[0,\\infty)$ defined by the following collection of coupled stochastic differential equations: \\begin{eqnarray}d", "orig": "X_i(t)=\\sum\\limits_{j\\in \\mathbb{Z}^d}a(i,j)[X_j(t)-X_i(t)] dt+\\sqrt{bX_i(t)^2} dW_i(t), \\eqntext{i\\in \\mathbb{Z}^d", "generated": "I'm not sure what you're referring to but I'll give it a try. It seems like a mathematical equation related to stochastic processes.\n\nThe system of interacting diffusions on $[0,\\infty)$ defined by the following collection of coupled stochastic differential equations:\n\n\\begin{eqnarray}dX_i"}
{"full_text": "Soft supramolecular materials are promising for the design of innovative and highly tunable adhesives. These materials are composed of polymer chains functionalized by strongly interacting moieties, sometimes called \"stickers\". In order to systematically investigate the effect of the presence of associative groups on the debonding properties of a supramolecular adhesive, a series of supramolecular model systems has been characterized by probe-tack tests. These model materials, composed of linear and low dispersity poly(butylacrylate) chains functionalized in the middle by a single tri-urea sticker, are able to self-associate by six hydrogen bonds and range in molecular weight (M n) between 5 and 85 kg/mol. The linear rheology and the nanostructure of the same materials (called \"PnBA3U\") was the object of a previous study 1,2. At room temperature, the association of polymers via hydrogen bonds induces the formation of rod-like aggregates structured into bundles for M n \\textless{} 40kg/mol and the behavior of a soft elastic material was observed (G'\\textgreater{}\\textgreater{}G \"and G'~$\\omega$ 0). For higher M n , the filaments were randomly oriented and polymers displayed a crossover towards viscous behavior although terminal relaxation was not reached in the experimental frequency window. All these materials show however similar adhesive properties characterized by a cohesive mode of failure and low debonding energies (W adh \\textless{}40J/m 2 for a debonding speed of 100$\\mu$m/s). The debonding mechanisms observed during the adhesion tests have been investigated in detail with an Image tools analysis developed by our group 3. The measure of the projected area covered by cavities growing in the adhesive layer during debonding can be used to estimate the true stress in the walls of the cavities and thus, to characterize the in-situ large strain deformation of the thin layer during the adhesion test itself. This analysis revealed in particular that the PnBA3U materials with M n \\textless{} 40 kg/mol soften very markedly at large deformation like yield stress fluids, explaining the low adhesion energies measured for these viscoelastic gels. 2", "prompt": "Soft supramolecular materials are promising for the design of innovative and highly tunable adhesives. These materials are composed of polymer chains functionalized by strongly interacting moieties, sometimes called \"stickers\". In order to systematically investigate the effect of the presence of associative groups on the debonding properties of a sup", "orig": "ramolecular adhesive, a series of supramolecular model systems has been characterized by probe-tack tests. These model materials, composed of linear and low dispersity poly(butylacrylate) chains functionalized in the middle by a single tri-urea sticker, are able to self-associate by six hydrogen", "generated": "Soft supramolecular materials are promising for the design of innovative and highly tunable adhesives. These materials are composed of polymer chains functionalized by strongly interacting moieties, sometimes called \"stickers\". In order to systematically investigate the effect of the presence of associative groups on the debonding properties of a sup"}
{"full_text": "Magnetoencephalography (MEG) and electroencephalogra-phy (EEG) are non-invasive modalities that measure the weak electromagnetic fields generated by neural activity. Inferring the location of the current sources that generated these magnetic fields is an ill-posed inverse problem known as source imaging. When considering a group study, a baseline approach consists in carrying out the estimation of these sources independently for each subject. The ill-posedness of each problem is typically addressed using sparsity promoting regularizations. A straightforward way to define a common pattern for these sources is then to average them. A more advanced alternative relies on a joint localization of sources for all subjects taken together, by enforcing some similarity across all estimated sources. An important advantage of this approach is that it consists in a single estimation in which all measurements are pooled together, making the inverse problem better posed. Such a joint estimation poses however a few challenges, notably the selection of a valid regularizer that can quantify such spatial similarities. We propose in this work a new procedure that can do so while taking into account the geometrical structure of the cortex. We call this procedure Minimum Wasserstein Estimates (MWE). The benefits of this model are twofold. First, joint inference allows to pool together the data of different brain geometries, accumulating more spatial information. Second, MWE are defined through Optimal Transport (OT) metrics which provide a tool to model spatial proximity between cortical sources of different subjects, hence not enforcing identical source location in the group. These benefits allow MWE to be more accurate than standard MEG source localization techniques. To support these claims, we perform source localization on realistic MEG simulations based on forward operators derived from MRI scans. On a visual task dataset, we demonstrate how MWE infer neural patterns similar to functional Magnetic Resonance Imaging (fMRI) maps.", "prompt": "Magnetoencephalography (MEG) and electroencephalogra-phy (EEG) are non-invasive modalities that measure the weak electromagnetic fields generated by neural activity. Inferring the location of the current sources that generated these magnetic fields is an ill-posed inverse problem known as source imaging", "orig": ". When considering a group study, a baseline approach consists in carrying out the estimation of these sources independently for each subject. The ill-posedness of each problem is typically addressed using sparsity promoting regularizations. A straightforward way to define a common pattern for these sources is then to average them. A more advanced alternative relies", "generated": "Magnetoencephalography (MEG) and electroencephalography (EEG) are non-invasive modalities that measure the weak electromagnetic fields generated by neural activity. Inferring the location of the current sources that generated these magnetic fields is an ill-posed inverse problem known as source imaging. This"}
{"full_text": "Purpose: The main purpose of this study was to assess the reliability of shape and heterogeneity features in both Positron Emission Tomography (PET) and low-dose Computed Tomography (CT) components of PET/CT. A secondary objective was to investigate the impact of image quantization.Material and methods: A Health Insurance Portability and Accountability Act -compliant secondary analysis of deidentified prospectively acquired PET/CT test-retest datasets of 74 patients from multi-center Merck and ACRIN trials was performed. Metabolically active volumes were automatically delineated on PET with Fuzzy Locally Adaptive Bayesian algorithm. 3DSlicerTM was used to semi-automatically delineate the anatomical volumes on low-dose CT components. Two quantization methods were considered: a quantization into a set number of bins (quantizationB) and an alternative quantization with bins of fixed width (quantizationW). Four shape descriptors, ten first-order metrics and 26 textural features were computed. Bland-Altman analysis was used to quantify repeatability. Features were subsequently categorized as very reliable, reliable, moderately reliable and poorly reliable with respect to the corresponding volume variability. Results: Repeatability was highly variable amongst features. Numerous metrics were identified as poorly or moderately reliable. Others were (very) reliable in both modalities, and in all categories (shape, 1st-, 2nd- and 3rd-order metrics). Image quantization played a major role in the features repeatability. Features were more reliable in PET with quantizationB, whereas quantizationW showed better results in CT.Conclusion: The test-retest repeatability of shape and heterogeneity features in PET and low-dose CT varied greatly amongst metrics. The level of repeatability also depended strongly on the quantization step, with different optimal choices for each modality. The repeatability of PET and low-dose CT features should be carefully taken into account when selecting metrics to build multiparametric models.", "prompt": "Purpose: The main purpose of this study was to assess the reliability of shape and heterogeneity features in both Positron Emission Tomography (PET) and low-dose Computed Tomography (CT) components of PET/CT. A secondary objective was to investigate the impact of image quantization.Material and methods", "orig": ": A Health Insurance Portability and Accountability Act -compliant secondary analysis of deidentified prospectively acquired PET/CT test-retest datasets of 74 patients from multi-center Merck and ACRIN trials was performed. Metabolically active volumes were automatically delineated on PET with Fuzzy Locally Adaptive Bayesian algorithm.", "generated": "Purpose: The main purpose of this study was to assess the reliability of shape and heterogeneity features in both Positron Emission Tomography (PET) and low-dose Computed Tomography (CT) components of PET/CT. A secondary objective was to investigate the impact of image quantization.\n\nMaterial and"}
{"full_text": "We will study algebraic hyper-surfaces on the real unit sphere $\\mathcal S^{n-1}$ given by an homogeneous polynomial of degree d in n variables with the view point, rarely exploited, of Euclidian geometry using Bombieri's scalar product and norm. This view point is mostly present in works about the topology of random hyper-surfaces \\cite{ShubSmale93, GayetWelschinger14}. Our first result (lemma \\ref{distgen} page \\pageref{distgen}) is a formula for the distance $\\dist(P,\\Delta)$ of a polynomial to the {\\em real discriminant} $\\Delta$, i.e. the set of polynomials with a real singularity on the sphere. This formula is given for any distance coming from a scalar product on the vector space of polynomials. Then, we concentrate on Bombieri scalar product and its remarkable properties. For instance we establish a combinatoric formula for the scalar product of two products of linear-forms (lemma \\ref{scalarlinear} page \\pageref{scalarlinear}) which allows to give a (new ?) proof of the invariance of Bombieri's norm by composition with the orthogonal group. These properties yield a simple formula for the distance in theorem \\ref{bombineqdist} page \\pageref{bombineqdist} from which we deduce the following inequality: $$\\dist(P, \\Delta) \\leq \\min_{x \\hbox{ critical point of } P \\hbox{ on } \\mathcal S^{n-1}} |P(x)|$$ The definition \\ref{maindef} page \\pageref{maindef} classifies in two categories the ways to make a polynomial singular to realise the distance to the discriminant. Then, we show, in theorem \\ref{extremal} page \\pageref{extremal}, that one of the category is forbidden in the case of an {\\em extremal} hyper-surfaces (i.e. with maximal Betti numbers). This implies as a corollary \\ref{bombeqdistbis} (page \\pageref{bombeqdistbis}) that the above inequality becomes an equality is that case. The main result in this paper concerns extremal hyper-surfaces $P=0$ that maximise the distance to the discriminant (with $\\|P\\| = 1$). They are very remarkable objects which enjoy properties similar to those of quadratic forms: they are linear combination of powers of linear forms $x \\mapsto \\langle x | u_i \\rangle^d$ where the vectors $u_i$ are the critical points of $P$ on $\\mathcal S^{n-1}$ corresponding to the least positive critical value of $|P|$. This is corollary \\ref{comblinbis} page \\pageref{comblinbis} of a similar theorem \\ref{comblin} page \\pageref{comblin} for all algebraic hyper-surfaces. The next section is devoted to homogeneous polynomials in $2$ variables. We prove that a polynomial of degree $d$ with $2d$ regularly spaced roots on the unit circle is a local maximum of the distance to the discriminant among polynomials with the same norm and number of roots. We conjecture that it is a global maximum and that the polynomial of degree $d$ with $2r$ regularly spaced roots on the unit circle is also a similar global maximum when $d < r \\leq 2d$. This claim is supported by the fact that we were able to prove the consequence of this together with corollary \\ref{comblinbis} which yields to interesting trigonometric identities that we could not find somewhere else (proposition \\ref{trigo} page \\pageref{trigo}). We also obtain metric information about algebraic hyper-surfaces. First, in the case of extremal hyper-surface, we give an upper bound (theorem \\ref{bandwidth} page \\pageref{bandwidth}) on the length of an integral curve of the gradient of $P$ in the band where $|P|$ is less that the least positive critical value of $|P|$. Then, a general lower bound on the size and distance between the connected components of the zero locus of $P$ (corollary \\ref{sphereinside} and theorem \\ref{distancebetween}). The last section will present experimental results among which are five extremal sextic curves far from the discriminant. These are obtained by very long running numerical optimisation (many months) some of which are not terminated.", "prompt": "We will study algebraic hyper-surfaces on the real unit sphere $\\mathcal S^{n-1}$ given by an homogeneous polynomial of degree d in n variables with the view point, rarely exploited, of Euclidian geometry using Bombieri's scalar product and norm. This view point is mostly present in works", "orig": "about the topology of random hyper-surfaces \\cite{ShubSmale93, GayetWelschinger14}. Our first result (lemma \\ref{distgen} page \\pageref{distgen}) is a formula for the distance $\\dist(P,\\Delta)$ of a polynomial to the {\\em real", "generated": "We will study algebraic hyper-surfaces on the real unit sphere $\\mathcal S^{n-1}$ given by an homogeneous polynomial of degree d in n variables with the view point, rarely exploited, of Euclidian geometry using Bombieri's scalar product and norm. This view point is mostly present in work"}
{"full_text": "Multiple outcomes, both continuous and discrete, are routinely gathered on subjects in longitudinal studies and during routine clinical follow-up in general. To motivate our work, we consider a longitudinal study on patients with primary biliary cirrhosis (PBC) with a continuous bilirubin level, a discrete platelet count and a dichotomous indication of blood vessel malformations as examples of such longitudinal outcomes. An apparent requirement is to use all the outcome values to classify the subjects into groups (e.g., groups of subjects with a similar prognosis in a clinical setting). In recent years, numerous approaches have been suggested for classification based on longitudinal (or otherwise correlated) outcomes, targeting not only traditional areas like biostatistics, but also rapidly evolving bioinformatics and many others. However, most available approaches consider only continuous outcomes as a basis for classification, or if noncontinuous outcomes are considered, then not in combination with other outcomes of a different nature. Here, we propose a statistical method for clustering (classification) of subjects into a prespecified number of groups with a priori unknown characteristics on the basis of repeated measurements of several longitudinal outcomes of a different nature. This method relies on a multivariate extension of the classical generalized linear mixed model where a mixture distribution is additionally assumed for random effects. We base the inference on a Bayesian specification of the model and simulation-based Markov chain Monte Carlo methodology. To apply the method in practice, we have prepared ready-to-use software for use in R (http://www.R-project.org). We also discuss evaluation of uncertainty in the classification and also discuss usage of a recently proposed methodology for model comparison - the selection of a number of clusters in our case - based on the penalized posterior deviance proposed by Plummer [Biostatistics 9 (2008) 523-539].", "prompt": "Multiple outcomes, both continuous and discrete, are routinely gathered on subjects in longitudinal studies and during routine clinical follow-up in general. To motivate our work, we consider a longitudinal study on patients with primary biliary cirrhosis (PBC) with a continuous bilirubin level, a discrete platelet count and a", "orig": "dichotomous indication of blood vessel malformations as examples of such longitudinal outcomes. An apparent requirement is to use all the outcome values to classify the subjects into groups (e.g., groups of subjects with a similar prognosis in a clinical setting). In recent years, numerous approaches have been suggested for classification based on longitudinal", "generated": "Multiple outcomes, both continuous and discrete, are routinely gathered on subjects in longitudinal studies and during routine clinical follow-up in general. To motivate our work, we consider a longitudinal study on patients with primary biliary cirrhosis (PBC) with a continuous bilirubin level, a discrete platelet count and a"}
{"full_text": "The electromagnetic fields of linear radiating systems working without dispersive and dissipative losses are analyzed both in the time and the frequency domains. In the case of the time domain radiating system, the parameter studied is the action, A, associated with the radiation. The action is defined as the product of the energy and the duration of the radiation. In the case of the frequency domain radiating system, which produces radiation in bursts of duration T/2 where T is the period of oscillation, the parameter studied is the energy, U, dissipated in a single burst of radiation of duration T/2. In this paper, we have studied how A and U vary as a function of the charge associated with the current in the radiating system and the ratio of the length of the radiating system and its radius. We have observed remarkable results when this ratio is equal to the ratio of the radius of the universe to the Bohr radius. In the case of the time domain radiating system, we have observed that when the charge associated with the current propagating along the radiator reaches the electronic charge, the action associated with the radiation reduces to h/2*pi where h is the Planck constant. In the case of the frequency domain radiating system, we have observed that as the magnitude of the oscillating charge reduces to the electronic charge, the energy dissipated in a single burst of radiation reduces to h*v, where v is the frequency of oscillation. Interestingly, all these results are based purely on classical electrodynamics and general relativity. The importance of the findings is discussed. In particular, the fact that the minimum free charge that exists in nature is the electronic charge, is shown for the first time to be a direct consequence of the photonic nature of the electromagnetic fields. Furthermore, the presented findings allow to derive for the first time an expression for the dark energy density of the universe in terms of the other fundamental constants in nature, the prediction of which is consistent with experimental observations. This Equation, which combines together the dark energy, electronic charge and mass, speed of light, gravitational constant and Planck constant, creates a link between classical field theories (i.e., classical electrodynamics and general relativity) and quantum mechanics.", "prompt": "The electromagnetic fields of linear radiating systems working without dispersive and dissipative losses are analyzed both in the time and the frequency domains. In the case of the time domain radiating system, the parameter studied is the action, A, associated with the radiation. The action is defined as the product of the energy and", "orig": "the duration of the radiation. In the case of the frequency domain radiating system, which produces radiation in bursts of duration T/2 where T is the period of oscillation, the parameter studied is the energy, U, dissipated in a single burst of radiation of duration T/2. In this paper, we", "generated": "The electromagnetic fields of linear radiating systems working without dispersive and dissipative losses are analyzed both in the time and the frequency domains. In the case of the time domain radiating system, the parameter studied is the action, A, associated with the radiation. The action is defined as the product of the energy and"}
{"full_text": "This document presents the statistical methods used to process low-level measurements in the presence of noise. These methods can be classical or Bayesian. The question is placed in the general framework of the problem of nuisance parameters, one of the canonical problems of statistical inference. By using a simple criterion proposed by Bolstad (2007), it is possible to define statistically significant results during a measurement process (act of measuring in the vocabulary of metrology). This result is similar for a classic paradigm (called ``frequentist'') or Bayesian: the presence of zero in the interval considered (confidence or credibility). It is shown that in the case of homoskedastic Gaussians, the commonly used results are found. The case of Poisson distributions is then considered. In the case of heteroscedastic Gaussians, which is that of radioactivity measurement, we can consider them as Poisson laws in the limit of large counts. The results are different from those commonly used, and in particular those from standards (ISO 11929). Their statistical performances, characterized by simulation, are better and are well verified experimentally. This is confirmed theoretically by the use of the Neyman-Pearson lemma which makes it possible to formally determine the statistical tests with the best performances. These results also make it possible to understand the paradox of the possible divergence of the detection limit. It is also formally shown that the confidence intervals thus calculated by getting rid of the nuisance parameter according to established methods result in the commonly used confidence interval. To our knowledge, this constitutes the first formal derivation of these confidence intervals.This method is based on keeping the measurement results whether they are significant or not (not censoring them). This is recommended in several standards or documents, is compatible with the ISO 11929 standard and is in line with recent proposals in the field of statistics. On the other hand, all the information necessary to determine whether a measurement result is significant or not remains available. The conservation and restitution of all results is currently applied in the USA. The textbook case of the WIPP incident makes it possible to ensure favorable public perception.The implications and applications of this method in different fields are finally discussed.", "prompt": "This document presents the statistical methods used to process low-level measurements in the presence of noise. These methods can be classical or Bayesian. The question is placed in the general framework of the problem of nuisance parameters, one of the canonical problems of statistical inference. By using a simple criterion proposed by Bolstad (2007),", "orig": "it is possible to define statistically significant results during a measurement process (act of measuring in the vocabulary of metrology). This result is similar for a classic paradigm (called ``frequentist'') or Bayesian: the presence of zero in the interval considered (confidence or credibility). It is shown that in the case of hom", "generated": "This document presents the statistical methods used to process low-level measurements in the presence of noise. These methods can be classical or Bayesian. The question is placed in the general framework of the problem of nuisance parameters, one of the canonical problems of statistical inference. By using a simple criterion proposed by Bolstad (2007),"}
{"full_text": "Jerome H. Friedman was born in Yreka, California, USA, on December 29, 1939. He received his high school education at Yreka High School, then spent two years at Chico State College before transferring to the University of California at Berkeley in 1959. He completed an undergraduate degree in physics in 1962 and a Ph.D. in high-energy particle physics in 1968 and was a post-doctoral research physicist at the Lawrence Berkeley Laboratory during 1968-1972. In 1972, he moved to Stanford Linear Accelerator Center (SLAC) as head of the Computation Research Group, retaining this position until 2006. In 1981, he was appointed half time as Professor in the Department of Statistics, Stanford University, remaining half time with his SLAC appointment. He has held visiting appointments at CSIRO in Sydney, CERN and the Department of Statistics at Berkeley, and has had a very active career as a commercial consultant. Jerry became Professor Emeritus in the Department of Statistics in 2007. Apart from some 30 publications in high-energy physics early in his career, Jerry has published over 70 research articles and books in statistics and computer science, including co-authoring the pioneering books Classification and Regression Trees and The Elements of Statistical Learning. Many of his publications have hundreds if not thousands of citations (e.g., the CART book has over 21,000). Much of his software is incorporated in commercial products, including at least one popular search engine. Many of his methods and algorithms are essential inclusions in modern statistical and data mining packages. Honors include the following: the Rietz Lecture (1999) and the Wald Lectures (2009); election to the American Academy of Arts and Sciences (2005) and the US National Academy of Sciences (2010); a Fellow of the American Statistical Association; Paper of the Year (JASA 1980, 1985; Technometrics 1998, 1992); Statistician of the Year (ASA, Chicago Chapter, 1999); ACM Data Mining Lifetime Innovation Award (2002), Emanuel & Carol Parzen Award for Statistical Innovation (2004); Noether Senior Lecturer (American Statistical Association, 2010); and the IEEE Computer Society Data Mining Research Contribution Award (2012).", "prompt": "Jerome H. Friedman was born in Yreka, California, USA, on December 29, 1939. He received his high school education at Yreka High School, then spent two years at Chico State College before transferring to the University of California at Berkeley in 1959. He completed", "orig": "an undergraduate degree in physics in 1962 and a Ph.D. in high-energy particle physics in 1968 and was a post-doctoral research physicist at the Lawrence Berkeley Laboratory during 1968-1972. In 1972, he moved to Stanford Linear Accelerator Center (SLAC) as head", "generated": "Jerome H. Friedman was born in Yreka, California, USA, on December 29, 1939. He received his high school education at Yreka High School, then spent two years at Chico State College before transferring to the University of California at Berkeley in 1959. He completed"}
{"full_text": "The spinor representation is developed for conformal immersions of Riemann surfaces into space. We adapt the approach of Dennis Sullivan, which treats a spin structure on a Riemann surface M as a complex line bundle S whose square is the canonical line bundle K=T(M). Given a conformal immersion of M into \\bbR^3, the unique spin strucure on S^2 pulls back via the Gauss map to a spin structure S on M, and gives rise to a pair of smooth sections (s_1,s_2) of S. Conversely, any pair of sections of S generates a (possibly periodic) conformal immersion of M under a suitable integrability condition, which for a minimal surface is simply that the spinor sections are meromorphic. A spin structure S also determines (and is determined by) the regular homotopy class of the immersion by way of a \\bbZ_2-quadratic form q_S. We present an analytic expression for the Arf invariant of q_S, which decides whether or not the correponding immersion can be deformed to an embedding. The Arf invariant also turns out to be an obstruction, for example, to the existence of certain complete minimal immersions. The later parts of this paper use the spinor representation to investigate minimal surfaces with embedded planar ends. In general, we show for a spin structure S on a compact Riemann surface M with punctures at P that the space of all such (possibly periodic) minimal immersions of M\\setminus P into \\bbR^3 (upto homothety) is the the product of S^1\\times H^3 with the Grassmanian of 2-planes in a complex vector space \\calK of meromorphic sections of S. An important tool -- a skew-symmetric form \\Omega defined by residues of a certain meromorphic quadratic differential on M -- lets us compute how \\calK varies as M and P are varied. Then we apply this to determine the moduli spaces of planar-ended minimal spheres and real projective planes, and also to construct a new family of minimal tori and a minimal Klein bottle with 4 ends. These surfaces compactify in S^3 to yield surfaces critical for the \\Moebius invariant squared mean curvature functional W. On the other hand, Robert Bryant has shown all W-critical spheres and real projective planes arise this way. Thus we find at the same time the moduli spaces of W-critical spheres and real projective planes via the spinor representation.", "prompt": "The spinor representation is developed for conformal immersions of Riemann surfaces into space. We adapt the approach of Dennis Sullivan, which treats a spin structure on a Riemann surface M as a complex line bundle S whose square is the canonical line bundle K=T(M). Given a conformal immersion of M", "orig": "into \\bbR^3, the unique spin strucure on S^2 pulls back via the Gauss map to a spin structure S on M, and gives rise to a pair of smooth sections (s_1,s_2) of S. Conversely, any pair of sections of S generates a (possibly periodic", "generated": "The spinor representation is developed for conformal immersions of Riemann surfaces into space. We adapt the approach of Dennis Sullivan, which treats a spin structure on a Riemann surface M as a complex line bundle S whose square is the canonical line bundle K=T(M). Given a conformal immersion of M"}
{"full_text": "Introduction: The amount of data generated by original research is growing exponentially. Publicly releasing them is recommended to comply with the Open Science principles. However, data collected from human participants cannot be released as-is without raising privacy concerns. Fully synthetic data represent a promising answer to this challenge. This approach is explored by the French Centre de Recherche en {\\'E}pid{\\'e}miologie et Sant{\\'e} des Populations in the form of a synthetic data generation framework based on Classification and Regression Trees and an original distance-based filtering. The goal of this work was to develop a refined version of this framework and to assess its risk-utility profile with empirical and formal tools, including novel ones developed for the purpose of this evaluation.Materials and Methods: Our synthesis framework consists of four successive steps, each of which is designed to prevent specific risks of disclosure. We assessed its performance by applying two or more of these steps to a rich epidemiological dataset. Privacy and utility metrics were computed for each of the resulting synthetic datasets, which were further assessed using machine learning approaches.Results: Computed metrics showed a satisfactory level of protection against attribute disclosure attacks for each synthetic dataset, especially when the full framework was used. Membership disclosure attacks were formally prevented without significantly altering the data. Machine learning approaches showed a low risk of success for simulated singling out and linkability attacks. Distributional and inferential similarity with the original data were high with all datasets.Discussion: This work showed the technical feasibility of generating publicly releasable synthetic data using a multi-step framework. Formal and empirical tools specifically developed for this demonstration are a valuable contribution to this field. Further research should focus on the extension and validation of these tools, in an effort to specify the intrinsic qualities of alternative data synthesis methods.Conclusion: By successfully assessing the quality of data produced using a novel multi-step synthetic data generation framework, we showed the technical and conceptual soundness of the Open-CESP initiative, which seems ripe for full-scale implementation.", "prompt": "Introduction: The amount of data generated by original research is growing exponentially. Publicly releasing them is recommended to comply with the Open Science principles. However, data collected from human participants cannot be released as-is without raising privacy concerns. Fully synthetic data represent a promising answer to this challenge. This approach is explored by the French", "orig": "Centre de Recherche en {\\'E}pid{\\'e}miologie et Sant{\\'e} des Populations in the form of a synthetic data generation framework based on Classification and Regression Trees and an original distance-based filtering. The goal of this work was to develop a refined version of this framework and to assess", "generated": "Introduction: The amount of data generated by original research is growing exponentially. Publicly releasing them is recommended to comply with the Open Science principles. However, data collected from human participants cannot be released as-is without raising privacy concerns. Fully synthetic data represent a promising answer to this challenge. This approach is explored by the French"}
{"full_text": "Bilateral trade, a fundamental topic in economics, models the problem of intermediating between two strategic agents, a seller and a buyer, willing to trade a good for which they hold private valuations. Despite the simplicity of this problem, a classical result by Myerson and Satterthwaite (1983) affirms the impossibility of designing a mechanism which is simultaneously efficient, incentive compatible, individually rational, and budget balanced. This impossibility result fostered an intense investigation of meaningful trade-offs between these desired properties. Much work has focused on approximately efficient fixed-price mechanisms, i.e., Blumrosen and Dobzinski (2014; 2016), Colini-Baldeschi et al. (2016), which have been shown to fully characterize strong budget balanced and ex-post individually rational direct revelation mechanisms. All these results, however, either assume some knowledge on the priors of the seller/buyer valuations, or a black box access to some samples of the distributions, as in D{\\\"u}tting et al. (2021). In this paper, we cast for the first time the bilateral trade problem in a regret minimization framework over rounds of seller/buyer interactions, with no prior knowledge on the private seller/buyer valuations. Our main contribution is a complete characterization of the regret regimes for fixed-price mechanisms with different models of feedback and private valuations, using as benchmark the best fixed price in hindsight. More precisely, we prove the following bounds on the regret: $\\bullet$ $\\widetilde{\\Theta}(\\sqrt{T})$ for full-feedback (i.e., direct revelation mechanisms); $\\bullet$ $\\widetilde{\\Theta}(T^{2/3})$ for realistic feedback (i.e., posted-price mechanisms) and independent seller/buyer valuations with bounded densities; $\\bullet$ $\\Theta(T)$ for realistic feedback and seller/buyer valuations with bounded densities; $\\bullet$ $\\Theta(T)$ for realistic feedback and independent seller/buyer valuations; $\\bullet$ $\\Theta(T)$ for the adversarial setting.", "prompt": "Bilateral trade, a fundamental topic in economics, models the problem of intermediating between two strategic agents, a seller and a buyer, willing to trade a good for which they hold private valuations. Despite the simplicity of this problem, a classical result by Myerson and Satterthwaite (1983)", "orig": "affirms the impossibility of designing a mechanism which is simultaneously efficient, incentive compatible, individually rational, and budget balanced. This impossibility result fostered an intense investigation of meaningful trade-offs between these desired properties. Much work has focused on approximately efficient fixed-price mechanisms, i.e., Blumrosen and Dobzinski", "generated": "Bilateral trade, a fundamental topic in economics, models the problem of intermediating between two strategic agents, a seller and a buyer, willing to trade a good for which they hold private valuations. Despite the simplicity of this problem, a classical result by Myerson and Satterthwaite (1983)"}
{"full_text": "Richard Olshen was born in Portland, Oregon, on May 17, 1942. Richard spent his early years in Chevy Chase, Maryland, but has lived most of his life in California. He received an A.B. in Statistics at the University of California, Berkeley, in 1963, and a Ph.D. in Statistics from Yale University in 1966, writing his dissertation under the direction of Jimmie Savage and Frank Anscombe. He served as Research Staff Statistician and Lecturer at Yale in 1966-1967. Richard accepted a faculty appointment at Stanford University in 1967, and has held tenured faculty positions at the University of Michigan (1972-1975), the University of California, San Diego (1975-1989), and Stanford University (since 1989). At Stanford, he is Professor of Health Research and Policy (Biostatistics), Chief of the Division of Biostatistics (since 1998) and Professor (by courtesy) of Electrical Engineering and of Statistics. At various times, he has had visiting faculty positions at Columbia, Harvard, MIT, Stanford and the Hebrew University. Richard's research interests are in statistics and mathematics and their applications to medicine and biology. Much of his work has concerned binary tree-structured algorithms for classification, regression, survival analysis and clustering. Those for classification and survival analysis have been used with success in computer-aided diagnosis and prognosis, especially in cardiology, oncology and toxicology. He coauthored the 1984 book Classification and Regression Trees (with Leo Brieman, Jerome Friedman and Charles Stone) which gives motivation, algorithms, various examples and mathematical theory for what have come to be known as CART algorithms. The approaches to tree-structured clustering have been applied to problems in digital radiography (with Stanford EE Professor Robert Gray) and to HIV genetics, the latter work including studies on single nucleotide polymorphisms, which has helped to shed light on the presence of hypertension in certain subpopulations of women.", "prompt": "Richard Olshen was born in Portland, Oregon, on May 17, 1942. Richard spent his early years in Chevy Chase, Maryland, but has lived most of his life in California. He received an A.B. in Statistics at the University of California, Berkeley, in 1963, and", "orig": "a Ph.D. in Statistics from Yale University in 1966, writing his dissertation under the direction of Jimmie Savage and Frank Anscombe. He served as Research Staff Statistician and Lecturer at Yale in 1966-1967. Richard accepted a faculty appointment at Stanford University in 1967, and", "generated": "Richard Olshen was born in Portland, Oregon, on May 17, 1942. Richard spent his early years in Chevy Chase, Maryland, but has lived most of his life in California. He received an A.B. in Statistics at the University of California, Berkeley, in 1963, and"}
{"full_text": "The aggregation of microarray datasets originating from different studies is still a difficult open problem. Currently, best results are generally obtained by the so-called meta-analysis approach, which aggregates results from individual datasets, instead of analyzing aggre-gated datasets. In order to tackle such aggregation problems, it is necessary to correct for interstudy variability prior to aggregation. The goal of this paper is to present a new approach for microarray datasets merging, based upon explicit modeling of interstudy variability and gene variability. We develop and demonstrate a new algorithm for microarray datasets merging. The underlying model assumes normally distributed intrinsic gene expressions, distorted by a study-dependent nonlinear transformation, and study dependent (normally distributed) observation noise. The algorithm addresses both parameter estimation (the parameters being gene expression means and variances, observation noise variances and the nonlinear transformations) and data adjustment, and yields as a result adjusted datasets suitable for aggregation. The method is validated on two case studies. The first one concerns E. Coli expression data, artificially distorted by given nonlinear transformations and additive observation noise. The proposed method is able to correct for the distortion, and yields adjusted datasets from which the relevant biological effects can be recovered, as shown by a standard differential analysis. The second case study concerns the aggregation of two real prostate cancer datasets. After adjustment using the proposed algorithm, a differential analysis performed on adjusted datasets yields a larger number of differentially expressed genes (between control and tumor data). The proposed method has been implemented using the statistical software R 1, and Bioconductor packages 2. The source code (valid for merging two datasets), as well as the datasets used for the validation, and some complementary results, are made available on the web site", "prompt": "The aggregation of microarray datasets originating from different studies is still a difficult open problem. Currently, best results are generally obtained by the so-called meta-analysis approach, which aggregates results from individual datasets, instead of analyzing aggre-gated datasets. In order to tackle such aggregation problems, it is necessary to correct for inter", "orig": "study variability prior to aggregation. The goal of this paper is to present a new approach for microarray datasets merging, based upon explicit modeling of interstudy variability and gene variability. We develop and demonstrate a new algorithm for microarray datasets merging. The underlying model assumes normally distributed intrinsic gene expressions, distorted by a study-dependent nonlinear", "generated": "The aggregation of microarray datasets originating from different studies is still a difficult open problem. Currently, best results are generally obtained by the so-called meta-analysis approach, which aggregates results from individual datasets, instead of analyzing aggregated datasets. In order to tackle such aggregation problems, it is necessary to correct for inter-platform variation,"}
{"full_text": "Context. Tens of light hydrides and small molecules have now been detected over several hundreds sight lines sampling the diffuse interstellar medium (ISM) in both the Solar neighbourhood and the inner Galactic disk. They provide unprecedented statistics on the first steps of chemistry in the diffuse gas. Aims. These new data confirm the limitations of the traditional chemical pathways driven by the UV photons and the cosmic rays (CR) and the need for additional energy sources, such as turbulent dissipation, to open highly endoenergetic formation routes. The goal of the present paper is to further investigate the link between specific species and the properties of the turbulent cascade in particular its space-time intermittency. Methods. We have analysed ten different atomic and molecular species in the framework of the updated model of turbulent dissipation regions (TDR). We study the influence on the abundances of these species of parameters specific to chemistry (density, UV field, and CR ionisation rate) and those linked to turbulence (the average turbulent dissipation rate, the dissipation timescale, and the ion neutral velocity drift in the regions of dissipation). Results. The most sensitive tracers of turbulent dissipation are the abundances of CH+ and SH+, and the column densities of the J = 3, 4, 5 rotational levels of H2 . The abundances of CO, HCO+, and the intensity of the 158 $\\mu$m [CII] emission line are significantly enhanced by turbulent dissipation. The vast diversity of chemical pathways allows the independent determinations of free parameters never estimated before: an upper limit to the average turbulent dissipation rate, $\\overline{\\varepsilon}$ < 10$^{-23}$ erg cm$^{-3}$ s$^{-1}$ for $n_H$=20 cm$^{-3}$, from the CH+ abundance; an upper limit to the ion-neutral velocity drift, $u_{in}$ < 3.5 km s$^{-1}$, from the SH+ to CH+ abundance ratio; and a range of dissipation timescales, 100 < $\\tau_V$ < 1000 yr, from the CO to HCO+ abundance ratio. For the first time, we reproduce the large abundances of CO observed on diffuse lines of sight, and we show that CO may be abundant even in regions with UV-shieldings as low as $5 \\times 10^{-3}$ mag. The best range of parameters also reproduces the abundance ratios of OH, C2H, and H2O to HCO+ and are consistent with the known properties of the turbulent cascade in the Galactic diffuse ISM. Conclusions. Our results disclose an unexpected link between the dissipation of turbulence and the emergence of molecular richness in the diffuse ISM. Some species, such as CH+ or SH+, turn out to be unique tracers of the energy trail in the ISM. In spite of some degeneracy, the properties of the turbulent cascade, down to dissipation, can be captured through specific molecular abundances.", "prompt": "Context. Tens of light hydrides and small molecules have now been detected over several hundreds sight lines sampling the diffuse interstellar medium (ISM) in both the Solar neighbourhood and the inner Galactic disk. They provide unprecedented statistics on the first steps of chemistry in the diffuse gas. Aims. These new data confirm", "orig": "the limitations of the traditional chemical pathways driven by the UV photons and the cosmic rays (CR) and the need for additional energy sources, such as turbulent dissipation, to open highly endoenergetic formation routes. The goal of the present paper is to further investigate the link between specific species and the properties of the turbulent", "generated": "Context. Tens of light hydrides and small molecules have now been detected over several hundreds sight lines sampling the diffuse interstellar medium (ISM) in both the Solar neighbourhood and the inner Galactic disk. They provide unprecedented statistics on the first steps of chemistry in the diffuse gas. Aims. These new data confirm"}
{"full_text": "We review the importance of recent UV observations of solar system targets and discuss the need for further measurements, instrumentation and laboratory work in the coming decade. In the past decade, numerous important advances have been made in solar system science using ultraviolet (UV) spectroscopic techniques. Formerly used nearly exclusively for studies of giant planet atmospheres, planetary exospheres and cometary emissions, UV imaging spectroscopy has recently been more widely applied. The geyser-like plume at Saturn's moon Enceladus was discovered in part as a result of UV stellar occultation observations, and this technique was used to characterize the plume and jets during the entire Cassini mission. Evidence for a similar style of activity has been found at Jupiter's moon Europa using Hubble Space Telescope (HST) UV emission and absorption imaging. At other moons and small bodies throughout the solar system, UV spectroscopy has been utilized to search for activity, probe surface composition, and delineate space weathering effects; UV photometric studies have been used to uncover regolith structure. Insights from UV imaging spectroscopy of solar system surfaces have been gained largely in the last 1-2 decades, including studies of surface composition, space weathering effects (e.g. radiolytic products) and volatiles on asteroids (e.g. [2][39][48][76][84]), the Moon (e.g. [30][46][49]), comet nuclei (e.g. [85]) and icy satellites (e.g. [38][41-44][45][47][65]). The UV is sensitive to some species, minor contaminants and grain sizes often not detected in other spectral regimes. In the coming decade, HST observations will likely come to an end. New infrastructure to bolster future UV studies is critically needed. These needs include both developmental work to help improve future UV observations and laboratory work to help interpret spacecraft data. UV instrumentation will be a critical tool on missions to a variety of targets in the coming decade, especially for the rapidly expanding application of UV reflectance investigations of atmosphereless bodies.", "prompt": "We review the importance of recent UV observations of solar system targets and discuss the need for further measurements, instrumentation and laboratory work in the coming decade. In the past decade, numerous important advances have been made in solar system science using ultraviolet (UV) spectroscopic techniques. Formerly used nearly exclusively for studies of giant", "orig": "planet atmospheres, planetary exospheres and cometary emissions, UV imaging spectroscopy has recently been more widely applied. The geyser-like plume at Saturn's moon Enceladus was discovered in part as a result of UV stellar occultation observations, and this technique was used to characterize the plume and", "generated": "We review the importance of recent UV observations of solar system targets and discuss the need for further measurements, instrumentation and laboratory work in the coming decade. In the past decade, numerous important advances have been made in solar system science using ultraviolet (UV) spectroscopic techniques. Formerly used nearly exclusively for studies of giant"}
{"full_text": "Mapping forest aboveground biomass (AGB) has become an important task, particularly for the reporting of carbon stocks and changes. AGB can be mapped using synthetic aperture radar data (SAR) or passive optical data. However, these data are insensitive to high AGB levels (\\textgreater{}150 Mg/ha, and \\textgreater{}300 Mg/ha for P-band), which are commonly found in tropical forests. Studies have mapped the rough variations in AGB by combining optical and environmental data at regional and global scales. Nevertheless, these maps cannot represent local variations in AGB in tropical forests. In this paper, we hypothesize that the problem of misrepresenting local variations in AGB and AGB estimation with good precision occurs because of both methodological limits (signal saturation or dilution bias) and a lack of adequate calibration data in this range of AGB values. We test this hypothesis by developing a calibrated regression model to predict variations in high AGB values (mean \\textgreater{}300 Mg/ha) in French Guiana by a methodological approach for spatial extrapolation with data from the optical geoscience laser altimeter system (GLAS), forest inventories, radar, optics, and environmental variables for spatial inter-and extrapolation. Given their higher point count, GLAS data allow a wider coverage of AGB values. We find that the metrics from GLAS footprints are correlated with field AGB estimations (R 2 =0.54, RMSE=48.3 Mg/ha) with no bias for high values. First, predictive models, including remote-sensing, environmental variables and spatial correlation functions, allow us to obtain \"wall-to-wall\" AGB maps over French Guiana with an RMSE for the in situ AGB estimates of ~51 Mg/ha and R${}^2$=0.48 at a 1-km grid size. We conclude that a calibrated regression model based on GLAS with dependent environmental data can produce good AGB predictions even for high AGB values if the calibration data fit the AGB range. We also demonstrate that small temporal and spatial mismatches between field data and GLAS footprints are not a problem for regional and global calibrated regression models because field data aim to predict large and deep tendencies in AGB variations from environmental gradients and do not aim to represent high but stochastic and temporally limited variations from forest dynamics. Thus, we advocate including a greater variety of data, even if less precise and shifted, to better represent high AGB values in global models and to improve the fitting of these models for high values.", "prompt": "Mapping forest aboveground biomass (AGB) has become an important task, particularly for the reporting of carbon stocks and changes. AGB can be mapped using synthetic aperture radar data (SAR) or passive optical data. However, these data are insensitive to high AGB levels (\\textgreater{}150 Mg/ha", "orig": ", and \\textgreater{}300 Mg/ha for P-band), which are commonly found in tropical forests. Studies have mapped the rough variations in AGB by combining optical and environmental data at regional and global scales. Nevertheless, these maps cannot represent local variations in AGB in tropical forests. In this paper, we hypo", "generated": "Mapping forest aboveground biomass (AGB) has become an important task, particularly for the reporting of carbon stocks and changes. AGB can be mapped using synthetic aperture radar data (SAR) or passive optical data. However, these data are insensitive to high AGB levels (>150 Mg/ha. Recent advancements"}
{"full_text": "Far-field optical microscopy using focused light is an important tool in a number of scientific disciplines including chemical, (bio)physical and biomedical research, particularly with respect to the study of living cells and organisms. Unfortunately, the applicability of the optical microscope is limited, since the diffraction of light imposes limitations on the spatial resolution of the image. Consequently the details of, for example, cellular protein distributions, can be visualized only to a certain extent. Fortunately, recent years have witnessed the development of 'super-resolution' far-field optical microscopy (nanoscopy) techniques such as stimulated emission depletion (STED), ground state depletion (GSD), reversible saturated optical (fluorescence) transitions (RESOLFT), photoactivation localization microscopy (PALM), stochastic optical reconstruction microscopy (STORM), structured illumination microscopy (SIM) or saturated structured illumination microscopy (SSIM), all in one way or another addressing the problem of the limited spatial resolution of far-field optical microscopy. While SIM achieves a two-fold improvement in spatial resolution compared to conventional optical microscopy, STED, RESOLFT, PALM/STORM, or SSIM have all gone beyond, pushing the limits of optical image resolution to the nanometer scale. Consequently, all super-resolution techniques open new avenues of biomedical research. Because the field is so young, the potential capabilities of different super-resolution microscopy approaches have yet to be fully explored, and uncertainties remain when considering the best choice of methodology. Thus, even for experts, the road to the future is sometimes shrouded in mist. The super-resolution optical microscopy roadmap of Journal of Physics D: Applied Physics addresses this need for clarity. It provides guidance to the outstanding questions through a collection of short review articles from experts in the field, giving a thorough discussion on the concepts underlying super-resolution optical microscopy, the potential of different approaches, the importance of label optimization (such as reversible photoswitchable proteins) and applications in which these methods will have a significant impact.", "prompt": "Far-field optical microscopy using focused light is an important tool in a number of scientific disciplines including chemical, (bio)physical and biomedical research, particularly with respect to the study of living cells and organisms. Unfortunately, the applicability of the optical microscope is limited, since the diffraction of light imposes limitations on the spatial", "orig": "resolution of the image. Consequently the details of, for example, cellular protein distributions, can be visualized only to a certain extent. Fortunately, recent years have witnessed the development of'super-resolution' far-field optical microscopy (nanoscopy) techniques such as stimulated emission depletion (STED), ground state depletion (G", "generated": "resolution that can be achieved. Diffraction is the bending of light around the edges of objects that are comparable in size to or smaller than the wavelength of the light used. The diffraction limit, also known as Abbe's diffraction limit, restricts the size of the details that can be resolved, or distinguished"}
{"full_text": "Despite the rapid progression in our understanding of neutrinos over the last half century, much is left unknown about their properties. This leaves neutrinos as the most promising portal for Beyond Standard Model (BSM) physics, and neutrinos have already provided fruitful surprises. A number of neutrino experiments in the last three decades have observed anomalous oscillation signals consistent with a mass-squared splitting of $\\Delta m^2 \\sim 1\\ \\textrm{eV}^2$, motivating the existence and search for sterile neutrinos. On the other hand, other experiments have failed to see such a signal. In this thesis, we present two analyses. The first is an update to the sterile neutrino global fits with the inclusion of recent experimental data. We find that the 3+1 model provides a better fit to the global data set compared to the null, with an improvement of $\\Delta \\chi^2 = 51$ with the addition of only 3 degrees of freedom, corresponding to $6.6\\sigma$. While a substantial improvement, we also find a irreconcilable tension between the data sets of $5.1\\sigma$, calculated using the parameter goodness-of-fit test. This motivates the exploration of expanded models: a 3+2 model, and a 3+1+Decay model. In the 3+2 model, we find negligible improvement to the fit, and an even worse tension of $5.5\\sigma$. In the more exotic 3+1+Decay model, we find the tension reduced to $3.6\\sigma$. While a substantial improvement compared to the 3+1 model with the introduction of only one additional parameter, the tension is still too large to assuage concerns. The second analysis is the results of an expanded IceCube sterile neutrino search. A previous sterile neutrino search found no evidence for sterile neutrinos, finding a p-value of 8%. Of the three sterile mixing angles, $\\theta_{14}, \\theta_{24}$, and $\\theta_{34}$, only $\\theta_{24}$ was fitted for, as $\\theta_{14}$ was negligible and $\\theta_{34} = 0$ was considered a conservative assumption. We present results of an analysis where we include $\\theta_{34}$ to the fitted model. Both a frequentist and Bayesian analysis were conducted, with fits done in terms of the mass-squared splitting $\\Delta m_{41}^2$ and the mixing matrix parameters $|U_{\\mu4}|^2$ and $|U_{\\tau4}|^2$. The frequentist analysis finds a best fit at $\\Delta m_{41}^2 = 5.0\\ \\textrm{eV}^2$, $|U_{\\mu4}|^2 = 0.04$, and $|U_{\\tau4}|^2 = 0.006$, with a p-value of 5.2% assuming Wilks' Theorem with 3 degrees of freedom. Pseudoexperiments are indicating a smaller p-value 2.7%. The Bayesian analysis finds a similar best fit point at $\\Delta m_{41}^2 = 5.0\\ \\textrm{eV}^2$, $|U_{\\mu4}|^2 = 0.02$, and $|U_{\\tau4}|^2 = 0.006$, with a Bayes factor indicating a ``Very Strong'' preference for this sterile hypothesis over the null hypothesis.", "prompt": "Despite the rapid progression in our understanding of neutrinos over the last half century, much is left unknown about their properties. This leaves neutrinos as the most promising portal for Beyond Standard Model (BSM) physics, and neutrinos have already provided fruitful surprises. A number of neutrino experiments in the last three decades", "orig": "have observed anomalous oscillation signals consistent with a mass-squared splitting of $\\Delta m^2 \\sim 1\\ \\textrm{eV}^2$, motivating the existence and search for sterile neutrinos. On the other hand, other experiments have failed to see such a signal. In this thesis", "generated": "Despite the rapid progression in our understanding of neutrinos over the last half century, much is left unknown about their properties. This leaves neutrinos as the most promising portal for Beyond Standard Model (BSM) physics, and neutrinos have already provided fruitful surprises. A number of neutrino experiments in the last three decades"}
{"full_text": "We have shown, in a series of articles, that a classical description of a large number of economic agents can be replaced by a statistical fields formalism. To better understand the accumulation and allocation of capital among different sectors, the present paper applies this statistical fields description to a large number of heterogeneous agents divided into two groups. The first group is composed of a large number of firms in different sectors that collectively own the entire physical capital. The second group, investors, holds the entire financial capital and allocates it between firms across sectors according to investment preferences, expected returns, and stock prices variations on financial markets. In return, firms pay dividends to their investors. Financial capital is thus a function of dividends and stock valuations, whereas physical capital is a function of the total capital allocated by the financial sector. Whereas our previous work focused on the background fields that describe potential long-term equilibria, here we compute the transition functions of individual agents and study their probabilistic dynamics in the background field, as a function of their initial state. We show that capital accumulation depends on various factors. The probability associated with each firm's trajectories is the result of several contradictory effects: the firm tends to shift towards sectors with the greatest long-term return, but must take into account the impact of its shift on its attractiveness for investors throughout its trajectory. Since this trajectory depends largely on the average capital of transition sectors, a firm's attractiveness during its relocation depends on the relative level of capital in those sectors. Thus, an under-capitalized firm reaching a high-capital sector will experience a loss of attractiveness, and subsequently, in investors. Moreover, the firm must also consider the effects of competition in the intermediate sectors. An under-capitalized firm will tend to be ousted out towards sectors with lower average capital, while an over-capitalized firm will tend to shift towards higher averagecapital sectors. For investors, capital allocation depends on their short and long-term returns. These returns are not independent: in the short-term, returns are composed of both the firm's dividends and the increase in its stock prices. In the long-term, returns are based on the firm's growth expectations, but also, indirectly, on expectations of higher stock prices. Investors' capital allocation directly depends on the volatility of stock prices and {\\ldots}rms'dividends. Investors will tend to reallocate their capital to maximize their short and long-term returns. The higher their level of capital, the stronger the reallocation will be.", "prompt": "We have shown, in a series of articles, that a classical description of a large number of economic agents can be replaced by a statistical fields formalism. To better understand the accumulation and allocation of capital among different sectors, the present paper applies this statistical fields description to a large number of heterogeneous agents divided into two groups", "orig": ". The first group is composed of a large number of firms in different sectors that collectively own the entire physical capital. The second group, investors, holds the entire financial capital and allocates it between firms across sectors according to investment preferences, expected returns, and stock prices variations on financial markets. In return, firms pay dividends", "generated": "We have shown, in a series of articles, that a classical description of a large number of economic agents can be replaced by a statistical fields formalism. To better understand the accumulation and allocation of capital among different sectors, the present paper applies this statistical fields description to a large number of heterogeneous agents divided into two groups"}
{"full_text": "The face of simulation-based training has greatly evolved, with the most recent tools giving the ability to create virtual environments that rival realism. At first glance, it might appear that what the training sector needs is the most realistic simulators possible, but traditional simulators are not necessarily the most efficient or practical training tools. With all that these new technologies have to offer; the challenge is to go back to the core of training needs and identify the right vector of sensory cues that will most effectively enable training mode in the human brain. Bigger and Pricier doesn't necessarily mean better. Simulation with cross-reality content (XR), which by definition encompasses virtual reality (VR), mixed reality (MR), and augmented reality (AR), is the most practical solution for deploying any kind of simulation-based training. The authors of this paper (a teacher and a technology expert) share their experiences and expose XR-specific best practices to maximize learning transfer. ABOUT THE AUTHORS Sebastien Loze : Starting his career in the modeling and simulation community more than 15 years ago, S{\\'e}bastien has focused on learning about the latest simulation innovations and sharing information on how experts have solved their challenges. He worked on the COTS integration at CAE and the Presagis focusing on Simulation and Visualization products. More recently, Sebastien put together simulation and training teams and strategies for emerging companies like CM Labs and D-BOX. He is now the Simulations Industry Manager at Epic Games, focusing on helping companies develop real-time solutions for simulation-based training. Philippe Lepinard: Former military helicopter pilot and simulation officer, Philippe L{\\'e}pinard is now an associate professor at the University of Paris-Est Cr{\\'e}teil (UPEC). His research is focusing on playful learning and training through simulation. He is one of the founding members of the French simulation association.", "prompt": "The face of simulation-based training has greatly evolved, with the most recent tools giving the ability to create virtual environments that rival realism. At first glance, it might appear that what the training sector needs is the most realistic simulators possible, but traditional simulators are not necessarily the most efficient or practical training tools. With", "orig": "all that these new technologies have to offer; the challenge is to go back to the core of training needs and identify the right vector of sensory cues that will most effectively enable training mode in the human brain. Bigger and Pricier doesn't necessarily mean better. Simulation with cross-reality content (XR), which by", "generated": "The rise of simulation-based training has brought about a new era of immersive and effective learning experiences. The face of simulation-based training has greatly evolved, with the most recent tools giving the ability to create virtual environments that rival realism. At first glance, it might appear that what the training sector needs is the most realistic sim"}
{"full_text": "This paper reports experiments on the shear transient response of concentrated non-Brownian suspensions. The shear viscosity of the suspensions is measured using a wide-gap Couette rheometer equipped with a Particle Image Velocimetry (PIV) device that allows measuring the velocity field. The suspensions made of PMMA particles (31$\\mu$m in diameter) suspended in a Newtonian index- and density-matched liquid are transparent enough to allow an accurate measurement of the local velocity for particle concentrations as high as 50%. In the wide-gap Couette cell, the shear induced particle migration is evidenced by the measurement of the time evolution of the flow profile. A peculiar radial zone in the gap is identified where the viscosity remains constant. At this special location, the local particle volume fraction is taken to be the mean particle concentration. The local shear transient response of the suspensions when the shear flow is reversed is measured at this point where the particle volume fraction is well defined. The local rheological measurements presented here confirm the macroscopic measurements of Gadala-Maria and Acrivos (1980). After shear reversal, the viscosity undergoes a step-like reduction, decreases slower and passes through a minimum before increasing again to reach a plateau. Upon varying the particle concentration, we have been able to show that the minimum and the plateau viscosities do not obey the same scaling law with respect to the particle volume fraction. These experimental results are consistent with the scaling predicted by Mills and Snabre (2009) and with the results of numerical simulation performed on random suspensions [Sierou and Brady (2001)]. The minimum seems to be associated with the viscosity of an isotropic suspension, or at least of a suspension whose particles do not interact through non-hydrodynamic forces, while the plateau value would correspond to the viscosity of a suspension structured by the shear where the non-hydrodynamic forces play a crucial role.", "prompt": "This paper reports experiments on the shear transient response of concentrated non-Brownian suspensions. The shear viscosity of the suspensions is measured using a wide-gap Couette rheometer equipped with a Particle Image Velocimetry (PIV) device that allows measuring the velocity field. The suspensions made of PMMA", "orig": "particles (31$\\mu$m in diameter) suspended in a Newtonian index- and density-matched liquid are transparent enough to allow an accurate measurement of the local velocity for particle concentrations as high as 50%. In the wide-gap Couette cell, the shear induced particle migration is evidenced by the measurement of the time evolution", "generated": "This paper reports experiments on the shear transient response of concentrated non-Brownian suspensions. The shear viscosity of the suspensions is measured using a wide-gap Couette rheometer equipped with a Particle Image Velocimetry (PIV) device that allows measuring the velocity field. The suspensions made of PMM"}
{"full_text": "Purpose. It has been reported clinically that rupture or dissections in thoracic aortic aneurysms (TAA) often occur due to hypertension which may be modelled with sudden increase of peripheral resistance, inducing acute changes of blood volumes in the aorta. There is clinical evidence that more compliant aneurysms are less prone to rupture as they can sustain such changes of volume. The aim of the current paper is to verify this paradigm by evaluating computationally the role played by the variation of peripheral resistance and the impact of aortic stiffness onto peak wall stress in ascending TAA. Methods. Fluid-Structure Interaction (FSI) analyses were performed using patient-specific geometries and boundary conditions derived from 4D MRI datasets acquired on a patient. Blood was assumed incompressible and was treated as a non-Newtonian fluid using the Carreau model while the wall mechanical properties were obtained from the bulge inflation tests carried out in vitro after surgical repair. The Navier Stokes equations were solved in ANSYS Fluent. The Arbitrary Lagrangian Eulerian formulation was used to account for the wall deformations. At the interface between the solid domain and the fluid domain, the fluid pressure was transferred to the wall and the displacement of the wall was transferred to the fluid. The two systems were connected by the System Coupling component which controls the solver execution of fluid and solid simulations in ANSYS. Fluid and solid domains were solved sequentially starting from the fluid simulations. Results. Distributions of blood flow, wall shear stress and wall stress were evaluated in the ascending thoracic aorta using the FSI analyses. We always observed a significant flow eccentricity in the simulations, in very good agreement with velocity profiles measured using 4D MRI. The results also showed significant increase of peak wall stress due to the increase of peripheral resistance and aortic stiffness. In the worst case scenario, the largest peripheral resistance (10 10 kg.s.m-4) and stiffness (10 MPa) resulted in a maximal principal stress equal to 702 kPa, whereas it was only 77 kPa in normal conditions. Conclusions. This is the first time that the risk of rupture of an aTAA is quantified in case of the combined effects of hypertension and aortic stiffness increase. Our findings suggest that a stiffer TAA may have the most altered distribution of wall stress and an acute change of peripheral vascular resistance could significantly increase the risk of rupture for a stiffer aneurysm.", "prompt": "Purpose. It has been reported clinically that rupture or dissections in thoracic aortic aneurysms (TAA) often occur due to hypertension which may be modelled with sudden increase of peripheral resistance, inducing acute changes of blood volumes in the aorta. There is clinical evidence that more compliant aneurys", "orig": "ms are less prone to rupture as they can sustain such changes of volume. The aim of the current paper is to verify this paradigm by evaluating computationally the role played by the variation of peripheral resistance and the impact of aortic stiffness onto peak wall stress in ascending TAA. Methods. Fluid-Structure Interaction (FSI", "generated": "Purpose. It has been reported clinically that rupture or dissections in thoracic aortic aneurysms (TAA) often occur due to hypertension which may be modelled with sudden increase of peripheral resistance, inducing acute changes of blood volumes in the aorta. There is clinical evidence that more compliant aneurys"}
{"full_text": "We present a new convenient and efficient \"grafting from\" method to obtain well defined polystyrene (PS) silica nanoparticles. The method, based on Nitroxide-Mediated Polymerization (NMP), consists to bind covalently the alkoxyamine, which acts as initiator controller agent, at the silica nanoparticles surface in two steps. The first step is a reaction between the aminopropylsilane and the silica particles in order to functionalize the particles surface with amino group. In a second step, the initiating-controlling alkoxyamine moiety is introduced via an over grafting reaction between the amino group and the N-hydroxysuccinimide based MAMA-SG1 activated ester. To simplify both their chemical transformation and the polymerization step, the native silica particles, initially dispersed in water, have been transferred in an organic solvent, the dimethylacetamide, which is also a good solvent for the polystyrene. The synthesis parameters have been optimized for grafting density, conversion rates, and synthesis reproducibility while keeping the colloidal stability and to avoid any aggregation of silica particles induced by the inter-particles interaction evolution during the synthesis. After synthesis, the final grafted objects have been purified and the non-grafted polymer chains formed in the solvent have been washed out by ultra filtration. Then the particles have been studied using Small angle Neutron Scattering (SANS) coupled to neutron contrast variation method. To optimize the contrast conditions, both hydrogenated and deuterated monomers have been used for the synthesis. A refined fitting analysis based on the comparison on two models, a basic core-shell and the Gaussian Pedersen model, enables us to fit nicely the experimental data for both the hydrogenated and deuterated grafted case. Differences are seen between grafting of normal or deuterated chains which can be due to monomer reactivity or to neutron contrast effect variations. The synthesis and the characterization method established in this work constitute a robust and reproducible way to design well defined grafted polymer nanoparticles. These objects will be incorporated in polymer matrices in a further step to create Nanocomposites for polymer reinforcement.", "prompt": "We present a new convenient and efficient \"grafting from\" method to obtain well defined polystyrene (PS) silica nanoparticles. The method, based on Nitroxide-Mediated Polymerization (NMP), consists to bind covalently the alkoxyamine, which acts as initiator controller agent", "orig": ", at the silica nanoparticles surface in two steps. The first step is a reaction between the aminopropylsilane and the silica particles in order to functionalize the particles surface with amino group. In a second step, the initiating-controlling alkoxyamine moiety is introduced via an over grafting reaction", "generated": "We present a new convenient and efficient \"grafting from\" method to obtain well defined polystyrene (PS) silica nanoparticles. The method, based on Nitroxide-Mediated Polymerization (NMP), consists to bind covalently the alkoxyamine, which acts as initiator controller agent"}
{"full_text": "Thanks to recent advances, AI Planning has become the underlying technique for several applications. Figuring prominently among these is automated Web Service Composition (WSC) at the \"capability\" level, where services are described in terms of preconditions and effects over ontological concepts. A key issue in addressing WSC as planning is that ontologies are not only formal vocabularies; they also axiomatize the possible relationships between concepts. Such axioms correspond to what has been termed \"integrity constraints\" in the actions and change literature, and applying a web service is essentially a belief update operation. The reasoning required for belief update is known to be harder than reasoning in the ontology itself. The support for belief update is severely limited in current planning tools. Our first contribution consists in identifying an interesting special case of WSC which is both significant and more tractable. The special case, which we term \"forward effects\", is characterized by the fact that every ramification of a web service application involves at least one new constant generated as output by the web service. We show that, in this setting, the reasoning required for belief update simplifies to standard reasoning in the ontology itself. This relates to, and extends, current notions of \"message-based\" WSC, where the need for belief update is removed by a strong (often implicit or informal) assumption of \"locality\" of the individual messages. We clarify the computational properties of the forward effects case, and point out a strong relation to standard notions of planning under uncertainty, suggesting that effective tools for the latter can be successfully adapted to address the former. Furthermore, we identify a significant sub-case, named \"strictly forward effects\", where an actual compilation into planning under uncertainty exists. This enables us to exploit off-the-shelf planning tools to solve message-based WSC in a general form that involves powerful ontologies, and requires reasoning about partial matches between concepts. We provide empirical evidence that this approach may be quite effective, using Conformant-FF as the underlying planner.", "prompt": "Thanks to recent advances, AI Planning has become the underlying technique for several applications. Figuring prominently among these is automated Web Service Composition (WSC) at the \"capability\" level, where services are described in terms of preconditions and effects over ontological concepts. A key issue in addressing WSC as planning is that", "orig": "ontologies are not only formal vocabularies; they also axiomatize the possible relationships between concepts. Such axioms correspond to what has been termed \"integrity constraints\" in the actions and change literature, and applying a web service is essentially a belief update operation. The reasoning required for belief update is known to be", "generated": "Thanks to recent advances, AI Planning has become the underlying technique for several applications. Figuring prominently among these is automated Web Service Composition (WSC) at the \"capability\" level, where services are described in terms of preconditions and effects over ontological concepts. A key issue in addressing WSC as planning is that"}
{"full_text": "Seismic waves due to strong earthquakes propagating in surficial soil layers may both reduce soil stiffness and increase the energy dissipation into the soil. To investigate seismic wave amplification in such cases, past studies have been devoted to one-directional shear wave propagation in a soil column (1D-propagation) considering one motion component only (1C-polarization). Three independent purely 1C computations may be performed ('1D-1C' approach) and directly superimposed in the case of weak motions (linear behaviour). This research aims at studying local site effects by considering seismic wave propagation in a 1-D soil profile accounting for the influence of the 3-D loading path and non-linear hysteretic behaviour of the soil. In the proposed '1D-3C' approach, the three components (3C-polarization) of the incident wave are simultaneously propagated into a horizontal multilayered soil. A 3-D non-linear constitutive relation for the soil is implemented in the framework of the Finite Element Method in the time domain. The complex rheology of soils is modelled by mean of a multisurface cyclic plasticity model of the Masing-Prandtl-Ishlinskii-Iwan type. The great advantage of this choice is that the only data needed to describe the model is the modulus reduction curve. A parametric study is carried out to characterize the changes in the seismic motion of the surficial layers due to both incident wavefield properties and soil non-linearities. The numerical simulations show a seismic response depending on several parameters such as polarization of seismic waves, material elastic and dynamic properties, as well as on the impedance contrast between layers and frequency content and oscillatory character of the input motion. The 3-D loading path due to the 3C-polarization leads to multi-axial stress interaction that reduces soil strength and increases non-linear effects. The non-linear behaviour of the soil may have beneficial or detrimental effects on the seismic response at the free surface, depending on the energy dissipation rate. Free surface time histories, stress-strain hysteresis loops and in-depth profiles of octahedral stress and strain are estimated for each soil column. The combination of three separate 1D-1C non-linear analyses is compared to the proposed 1D-3C approach, evidencing the influence of the 3C-polarization and the 3-D loading path on strong seismic motions.", "prompt": "Seismic waves due to strong earthquakes propagating in surficial soil layers may both reduce soil stiffness and increase the energy dissipation into the soil. To investigate seismic wave amplification in such cases, past studies have been devoted to one-directional shear wave propagation in a soil column (1D-propagation) considering one", "orig": "motion component only (1C-polarization). Three independent purely 1C computations may be performed ('1D-1C' approach) and directly superimposed in the case of weak motions (linear behaviour). This research aims at studying local site effects by considering seismic wave propagation in a 1-D soil profile", "generated": "Seismic waves due to strong earthquakes propagating in surficial soil layers may both reduce soil stiffness and increase the energy dissipation into the soil. To investigate seismic wave amplification in such cases, past studies have been devoted to one-directional shear wave propagation in a soil column (1D-propagation) considering only"}
{"full_text": "Ultra-high energy cosmic rays (UHECRs) are atomic nuclei with energies over ten million times energies accessible to human-made particle accelerators. Evidence suggests that they originate from relatively nearby extragalactic sources, but the nature of the sources is unknown. We develop a multilevel Bayesian framework for assessing association of UHECRs and candidate source populations, and Markov chain Monte Carlo algorithms for estimating model parameters and comparing models by computing, via Chib's method, marginal likelihoods and Bayes factors. We demonstrate the framework by analyzing measurements of 69 UHECRs observed by the Pierre Auger Observatory (PAO) from 2004-2009, using a volume-complete catalog of 17 local active galactic nuclei (AGN) out to 15 megaparsecs as candidate sources. An early portion of the data (\"period 1,\" with 14 events) was used by PAO to set an energy cut maximizing the anisotropy in period 1; the 69 measurements include this \"tuned\" subset, and subsequent \"untuned\" events with energies above the same cutoff. Also, measurement errors are approximately summarized. These factors are problematic for independent analyses of PAO data. Within the context of \"standard candle\" source models (i.e., with a common isotropic emission rate), and considering only the 55 untuned events, there is no significant evidence favoring association of UHECRs with local AGN vs. an isotropic background. The highest-probability associations are with the two nearest, adjacent AGN, Centaurus A and NGC 4945. If the association model is adopted, the fraction of UHECRs that may be associated is likely nonzero but is well below 50%. Our framework enables estimation of the angular scale for deflection of cosmic rays by cosmic magnetic fields; relatively modest scales of $\\approx\\!3^{\\circ}$ to $30^{\\circ}$ are favored. Models that assign a large fraction of UHECRs to a single nearby source (e.g., Centaurus A) are ruled out unless very large deflection scales are specified a priori, and even then they are disfavored. However, including the period 1 data alters the conclusions significantly, and a simulation study supports the idea that the period 1 data are anomalous, presumably due to the tuning. Accurate and optimal analysis of future data will likely require more complete disclosure of the data.", "prompt": "Ultra-high energy cosmic rays (UHECRs) are atomic nuclei with energies over ten million times energies accessible to human-made particle accelerators. Evidence suggests that they originate from relatively nearby extragalactic sources, but the nature of the sources is unknown. We develop a multilevel Bayesian framework for assessing association of", "orig": "UHECRs and candidate source populations, and Markov chain Monte Carlo algorithms for estimating model parameters and comparing models by computing, via Chib's method, marginal likelihoods and Bayes factors. We demonstrate the framework by analyzing measurements of 69 UHECRs observed by the Pierre Auger Observatory (PAO", "generated": "Ultra-high energy cosmic rays (UHECRs) are atomic nuclei with energies over ten million times those accessible to human-made particle accelerators. Evidence suggests that they originate from relatively nearby extragalactic sources, but the nature of the sources is unknown. We develop a multilevel Bayesian framework for assessing association of"}
{"full_text": "Coalition formation is a fundamental type of interaction that involves the creation of coherent groupings of distinct, autonomous, agents in order to efficiently achieve their individual or collective goals. Forming effective coalitions is a major research challenge in the field of multi-agent systems. Central to this endeavour is the problem of determining which of the many possible coalitions to form in order to achieve some goal. This usually requires calculating a value for every possible coalition, known as the coalition value, which indicates how beneficial that coalition would be if it was formed. Once these values are calculated, the agents usually need to find a combination of coalitions, in which every agent belongs to exactly one coalition, and by which the overall outcome of the system is maximized. However, this coalition structure generation problem is extremely challenging due to the number of possible solutions that need to be examined, which grows exponentially with the number of agents involved. To date, therefore, many algorithms have been proposed to solve this problem using different techniques ranging from dynamic programming, to integer programming, to stochastic search all of which suffer from major limitations relating to execution time, solution quality, and memory requirements. With this in mind, we develop an anytime algorithm to solve the coalition structure generation problem. Specifically, the algorithm uses a novel representation of the search space, which partitions the space of possible solutions into sub-spaces such that it is possible to compute upper and lower bounds on the values of the best coalition structures in them. These bounds are then used to identify the sub-spaces that have no potential of containing the optimal solution so that they can be pruned. The algorithm, then, searches through the remaining sub-spaces very efficiently using a branch-and-bound technique to avoid examining all the solutions within the searched subspace(s). In this setting, we prove that our algorithm enumerates all coalition structures efficiently by avoiding redundant and invalid solutions automatically. Moreover, in order to effectively test our algorithm we develop a new type of input distribution which allows us to generate more reliable benchmarks compared to the input distributions previously used in the field. Given this new distribution, we show that for 27 agents our algorithm is able to find solutions that are optimal in 0.175% of the time required by the fastest available algorithm in the literature. The algorithm is anytime, and if interrupted before it would have normally terminated, it can still provide a solution that is guaranteed to be within a bound from the optimal one. Moreover, the guarantees we provide on the quality of the solution are significantly better than those provided by the previous state of the art algorithms designed for this purpose. For example, for the worst case distribution given 25 agents, our algorithm is able to find a 90% efficient solution in around 10% of time it takes to find the optimal solution.", "prompt": "Coalition formation is a fundamental type of interaction that involves the creation of coherent groupings of distinct, autonomous, agents in order to efficiently achieve their individual or collective goals. Forming effective coalitions is a major research challenge in the field of multi-agent systems. Central to this endeavour is the problem of determining which of", "orig": "the many possible coalitions to form in order to achieve some goal. This usually requires calculating a value for every possible coalition, known as the coalition value, which indicates how beneficial that coalition would be if it was formed. Once these values are calculated, the agents usually need to find a combination of coalitions, in which", "generated": "Coalition formation is a fundamental type of interaction that involves the creation of coherent groupings of distinct, autonomous, agents in order to efficiently achieve their individual or collective goals. Forming effective coalitions is a major research challenge in the field of multi-agent systems. Central to this endeavour is the problem of determining which of"}
{"full_text": "Developers need to make a constant effort to improve the quality of their code if they want to stay productive. Tools that highlight code locations that could benefit from refactoring are thus highly desirable. The most common name for such locations is \"bad code smell\". A number of tools offer such quality feedback and there is a substantial body of related research. However, all these tools, including those based on Machine Learning, still produce false positives. Every single false positive shown to the developer places a cognitive burden on her and should thus be avoided. The literature discusses the choice of metric thresholds, the general subjectivity of such a judgment and the relation to conscious design choices, \"design ideas\". To examine false positives and the relation between bad smells and design ideas, we designed and conducted an exploratory case study. While previous research presented a broad overview, we have chosen a narrow setting to reach for even deeper insights: The framework JHotDraw had been designed so thoughtfully that most smell warnings are expected to be false positives. Nevertheless, the \"Law of Good Style\", better known as the \"Law of Demeter\", is a rather restrictive design rule so that we still expected to find some potential bad smells, i.e. violations of this \"Law\". This combination led to 1215 potential smells of which at most 42 are true positives. We found generic as well as specific design ideas that were traded for the smell. Our confidence in that decision ranged from high enough to very high. We were surprised to realize that the smell definition itself required the formulation of constructive design ideas. Finally we found some smells to be the result of the limitation of the language and one could introduce auxiliary constructive design ideas to compensate for them. The decision whether a potential smell occurrence is actually a true positive was made very meticulously. For that purpose we took three qualities that the smell could affect negatively into account and we discussed the result of the recommended refactorings. If we were convinced that we had found a false positive, we described the relationships with design ideas. The realization that not only general design ideas but also specific design ideas have an influence on whether a potential smell is a true positive turns the problem of false positives from a scientific problem (\"What is the true definition of the smell?\") to a engineering problem (\"How can we incorporate design ideas into smell definitions?\"). We recommend to add adaptation points to the smell definitions. Higher layers may then adapt the smell for specific contexts. After adaptation the tool may continuously provide distinct and precise quality feedback, reducing the cognitive load for the developer and preventing habituation. Furthermore, the schema for the discussion of potential smells may be used to elaborate more sets of true and false smell occurrences. Finally, it follows that smell detection based on machine learning should also take signs of design ideas into account.", "prompt": "Developers need to make a constant effort to improve the quality of their code if they want to stay productive. Tools that highlight code locations that could benefit from refactoring are thus highly desirable. The most common name for such locations is \"bad code smell\". A number of tools offer such quality feedback and there is a", "orig": "substantial body of related research. However, all these tools, including those based on Machine Learning, still produce false positives. Every single false positive shown to the developer places a cognitive burden on her and should thus be avoided. The literature discusses the choice of metric thresholds, the general subjectivity of such a judgment and the relation", "generated": "Developers need to make a constant effort to improve the quality of their code if they want to stay productive. Tools that highlight code locations that could benefit from refactoring are thus highly desirable. The most common name for such locations is \"bad code smell\". A number of tools offer such quality feedback and there is a"}
{"full_text": "Reducing costs of manufactured products is one of the key issues of companies. Bar turning companies (mechanical subcontracting companies) are faced with the following dilemma: use a pull strategy or use a push strategy. Instinctively these companies produce more than demand required by customers. This strategy allows them to respond to requests forecasts and reduce their cost of changeover time. These companies make a bet on sales opportunities and think to realize an additional profit. We have tried to find in this study to provide elements to know the limits of this strategy. Our proposal focuses on developing a model to support the decision taking into account the mix of opportunities, economic constraints and mean constraints. This model features the particular importance of high rates of ownership and the risk of not selling. R\\'eduire les co\\^uts de revient des produits fabriqu\\'es est une des probl\\'ematiques essentielles des entreprises d'aujourd'hui. Les entreprises de d\\'ecolletage (entreprises de sous-traitance m\\'ecanique) sont confront\\'ees au dilemme suivant : produire juste la demande client ou produire plus. Instinctivement ces entreprises, dont les temps de changement de s\\'erie sont \\'elev\\'es, cherchent \\`a produire plus que la demande exig\\'ee par le client. Cette strat\\'egie leur permet de r\\'epondre \\`a des demandes pr\\'evisionnelles et r\\'eduire ainsi le co\\^ut de revient des produits. Ces entreprises r\\'ealisent un pari sur les opportunit\\'es de vente et pensent r\\'ealiser un gain suppl\\'ementaire en r\\'ealisant des stocks. Nous avons cherch\\'e dans cette \\'etude \\`a fournir des \\'el\\'ements de d\\'ecision pour conna\\^itre les limites de cette r\\`egle de gestion. Notre proposition porte sur le d\\'eveloppement d'un mod\\`ele d'aide \\`a la d\\'ecision prenant en consid\\'eration le mixte entre opportunit\\'es commerciales, contraintes \\'economiques et contraintes de moyen. Ce mod\\`ele souligne l'importance particuli\\`ere du taux de possession et du risque de non vente.", "prompt": "Reducing costs of manufactured products is one of the key issues of companies. Bar turning companies (mechanical subcontracting companies) are faced with the following dilemma: use a pull strategy or use a push strategy. Instinctively these companies produce more than demand required by customers. This strategy allows them to respond to", "orig": "requests forecasts and reduce their cost of changeover time. These companies make a bet on sales opportunities and think to realize an additional profit. We have tried to find in this study to provide elements to know the limits of this strategy. Our proposal focuses on developing a model to support the decision taking into account the mix of opportunities", "generated": "Reducing costs of manufactured products is one of the key issues of companies. Bar turning companies (mechanical subcontracting companies) are faced with the following dilemma: use a pull strategy or use a push strategy. Instinctively these companies produce more than demand required by customers. This strategy allows them to respond to"}
{"full_text": "The ejecta of the first probably very massive stars polluted the Big Bang primordial element mix with the first heavier elements. The resulting ultra metal-poor abundance distribution provided the initial conditions for the second stars of a wide range of initial masses reaching down to intermediate and low masses. The importance of these second stars for understanding the origin of the elements in the early universe are manifold. While the massive first stars have long vanished the second stars are still around and currently observed. They are the carriers of the information about the first stars, but they are also capable of nuclear production themselves. For example, in order to use ultra or extremely metal-poor stars as a probe for the r-process in the early universe a reliable model of the s-process in the second stars is needed. Eventually, the second stars may provide us with important clues on questions ranging from structure formation to how the stars actually make the elements, not only in the early but also in the present universe. In particular the C-rich extremely metal-poor stars, most of which show the s-process signature, are thought to be associated with chemical yields from the evolved giant phase of intermediate mass stars. Models of such AGB stars at extremely low metallicity now exist, and comparison with observation show important discrepancies, for example with regard to the synthesis of nitrogen. This may hint at burning and mixing aspects of extremely metal-poor evolved stars that are not yet included in the standard picture of evolution, as for example the hydrogen-ingestion flash. The second stars of intermediate mass may have also played an important role in the formation of heavy elements that form through slow neutron capture reaction chains (s-process). Comparison of models with observations reveal which aspects of the physics input and assumptions need to be improved. The s-process is a particularly useful diagnostic tool for probing the physical processes that are responsible for the creation of elements in stars, like for example rotation. As new observational techniques and strategies continue to penetrate the field, for example the multi-object spectroscopy, or the future spectroscopic surveys, the extremely metal-poor stars will play an increasingly important role to address some of the most fundamental and challenging, current questions of astronomy.", "prompt": "The ejecta of the first probably very massive stars polluted the Big Bang primordial element mix with the first heavier elements. The resulting ultra metal-poor abundance distribution provided the initial conditions for the second stars of a wide range of initial masses reaching down to intermediate and low masses. The importance of these second stars for understanding", "orig": "the origin of the elements in the early universe are manifold. While the massive first stars have long vanished the second stars are still around and currently observed. They are the carriers of the information about the first stars, but they are also capable of nuclear production themselves. For example, in order to use ultra or extremely metal-p", "generated": "The ejecta of the first probably very massive stars polluted the Big Bang primordial element mix with the first heavier elements. The resulting ultra metal-poor abundance distribution provided the initial conditions for the second stars of a wide range of initial masses reaching down to intermediate and low masses. The importance of these second stars for understanding"}
{"full_text": "In this paper we introduce the word {\\em fresco} to denote a monogenic geometric (a,b)-module. This \"basic object\" (generalized Brieskorn module with one generator) corresponds to the formal germ of the minimal filtered (regular) differential equation. Such an equation is satisfied by a relative de Rham cohomology class at a critical value of a holomorphic function on a smooth complex manifold. In [B.09] the first structure theorems are proved. Then in [B.10] we introduced the notion of {\\em theme} which corresponds in the \\ $[\\lambda]-$primitive case to frescos having a unique Jordan-H{\\\"o}lder sequence (a unique Jordan block for the monodromy). Themes correspond to asymptotic expansion of a given vanishing period, so to an image of a fresco in the module of asymptotic expansions. For a fixed relative de Rham cohomology class (for instance given by a smooth differential form $d-$closed and $df-$closed) each choice of a vanishing cycle in the spectral eigenspace of the monodromy for the eigenvalue \\ $exp(2i\\pi.\\lambda)$ \\ produces a \\ $[\\lambda]-$primitive theme, which is a quotient of the fresco associated to the given relative de Rham class itself. \\\\ We show that for any fresco there exists an {\\em unique} Jordan-H{\\\"o}lder sequence, called the {\\em principal J-H. sequence}, with corresponding quotients giving the opposite of the roots of the Bernstein polynomial in increasing order. We study the semi-simple part of a given fresco and we characterize the semi-simplicity of a fresco by the fact for any given order on the roots of its Bernstein polynomial we may find a J-H. sequence making them appear with this order. Then we construct a numerical invariant, called the \\ $\\beta-$invariant, and we show that it produces numerical criteria in order to give a necessary and sufficient condition on a fresco to be semi-simple. We show that these numerical invariants define a natural algebraic stratification on the set of isomorphism classes of fresco with given fundamental invariants (or equivalently with given roots of the Bernstein polynomial).", "prompt": "In this paper we introduce the word {\\em fresco} to denote a monogenic geometric (a,b)-module. This \"basic object\" (generalized Brieskorn module with one generator) corresponds to the formal germ of the minimal filtered (regular) differential equation. Such an equation is satisfied by a", "orig": "relative de Rham cohomology class at a critical value of a holomorphic function on a smooth complex manifold. In [B.09] the first structure theorems are proved. Then in [B.10] we introduced the notion of {\\em theme} which corresponds in the \\ $[\\lambda]-$primitive", "generated": "In this paper we introduce the word {\\em fresco} to denote a monogenic geometric (a,b)-module. This \"basic object\" (generalized Brieskorn module with one generator) corresponds to the formal germ of the minimal filtered (regular) differential equation. Such an equation is satisfied by a"}
{"full_text": "A so-called smart material is a material that is the seat of one or more multiphysical coupling. One of the key points in the development of the constitutive laws of these materials, either at the local or at the global scale, is to formulate a free energy density (or enthalpy) from vectors, tensors, at a given order and for a class of given symmetry, depending on the symmetry classes of the crystal constituting the material or the symmetry of the representative volume element. This article takes as a support of study the stress and magnetization couple ($\\sigma$, m) involved in the phenomena of magnetoelastic coupling in a cubic symmetry medium. Several studies indeed show a non-monotonic sensitivity of the magnetic susceptibility and magnetostriction of certain soft magnetic materials under stress. Modeling such a phenomenon requires the introduction of a second order stress term in the Gibbs free energy density. A polynomial formulation in the two variables stress and magnetization is preferred over a tensorial formulation. For a given material symmetry class, this allows to express more easily the free energy density at any bi-degree in $\\sigma$ and m (i.e. at any constitutive tensors order for the so-called tensorial formulation). A rigorous and systematic method is essential to obtain the high-degree magneto-mechanical coupling terms and to build a free energy density function at any order which is invariant by the action of the cubic (octahedral) group. For that aim, theoretical and computer tools in Invariant Theory, that allow for the mathematical description of cubic nonlinear magneto-elasticity, are introduced. Minimal integrity bases of the invariant algebra for the pair (m, $\\sigma$), under the proper (orientation-preserving) and the full cubic groups, are then proposed. The minimal integrity basis for the proper cubic group is constituted of 60 invariants, while the minimal integrity basis for the full cubic group (the one of interest for magneto-elasticity) is made up of 30 invariants. These invariants are formulated in a (coordinate free) intrinsic manner, using a generalized cross product to write some of them. The counting of independent invariants of a given multi-degree in (m, $\\sigma$) is performed. It is shown accordingly that it is possible to list without error all the material parameters useful for the description of the coupled magnetoelastic behavior from the integrity basis. The technique is applied to derive general expressions $\\Psi$ $\\star$ ($\\sigma$, m) of the free energy density at the magnetic domains scale exhibiting cubic symmetry. The classic results for an isotropic medium are recovered.", "prompt": "A so-called smart material is a material that is the seat of one or more multiphysical coupling. One of the key points in the development of the constitutive laws of these materials, either at the local or at the global scale, is to formulate a free energy density (or enthalpy) from vectors", "orig": ", tensors, at a given order and for a class of given symmetry, depending on the symmetry classes of the crystal constituting the material or the symmetry of the representative volume element. This article takes as a support of study the stress and magnetization couple ($\\sigma$, m) involved in the phenomena of magnetoelastic", "generated": "A so-called smart material is a material that is the seat of one or more multiphysical coupling. One of the key points in the development of the constitutive laws of these materials, either at the local or at the global scale, is to formulate a free energy density (or enthalpy) from vector"}
{"full_text": "Programming languages and platforms improve over time, sometimes resulting in new language features that offer many benefits. However, despite these benefits, developers may not always be willing to adopt them in their projects for various reasons. In this paper, we describe an empirical study where we assess the adoption of a particular new language feature. Studying how developers use (or do not use) new language features is important in programming language research and engineering because it gives designers insight into the usability of the language to create meaning programs in that language. This knowledge, in turn, can drive future innovations in the area. Here, we explore Java 8 default methods, which allow interfaces to contain (instance) method implementations. Default methods can ease interface evolution, make certain ubiquitous design patterns redundant, and improve both modularity and maintainability. A focus of this work is to discover, through a scientific approach and a novel technique, situations where developers found these constructs useful and where they did not, and the reasons for each. Although several studies center around assessing new language features, to the best of our knowledge, this kind of construct has not been previously considered. Despite their benefits, we found that developers did not adopt default methods in all situations. Our study consisted of submitting pull requests introducing the language feature to 19 real-world, open source Java projects without altering original program semantics. This novel assessment technique is proactive in that the adoption was driven by an automatic refactoring approach rather than waiting for developers to discover and integrate the feature themselves. In this way, we set forth best practices and patterns of using the language feature effectively earlier rather than later and are able to possibly guide (near) future language evolution. We foresee this technique to be useful in assessing other new language features, design patterns, and other programming idioms.", "prompt": "Programming languages and platforms improve over time, sometimes resulting in new language features that offer many benefits. However, despite these benefits, developers may not always be willing to adopt them in their projects for various reasons. In this paper, we describe an empirical study where we assess the adoption of a particular new language feature. Stud", "orig": "ying how developers use (or do not use) new language features is important in programming language research and engineering because it gives designers insight into the usability of the language to create meaning programs in that language. This knowledge, in turn, can drive future innovations in the area. Here, we explore Java 8 default methods,", "generated": "Programming languages and platforms improve over time, sometimes resulting in new language features that offer many benefits. However, despite these benefits, developers may not always be willing to adopt them in their projects for various reasons. In this paper, we describe an empirical study where we assess the adoption of a particular new language feature.\n\nSt"}
{"full_text": "The notion of class is ubiquitous in computer science and is central in many formalisms for the representation of structured knowledge used both in knowledge representation and in databases. In this paper we study the basic issues underlying such representation formalisms and single out both their common characteristics and their distinguishing features. Such investigation leads us to propose a unifying framework in which we are able to capture the fundamental aspects of several representation languages used in different contexts. The proposed formalism is expressed in the style of description logics, which have been introduced in knowledge representation as a means to provide a semantically well-founded basis for the structural aspects of knowledge representation systems. The description logic considered in this paper is a subset of first order logic with nice computational characteristics. It is quite expressive and features a novel combination of constructs that has not been studied before. The distinguishing constructs are number restrictions, which generalize existence and functional dependencies, inverse roles, which allow one to refer to the inverse of a relationship, and possibly cyclic assertions, which are necessary for capturing real world domains. We are able to show that it is precisely such combination of constructs that makes our logic powerful enough to model the essential set of features for defining class structures that are common to frame systems, object-oriented database languages, and semantic data models. As a consequence of the established correspondences, several significant extensions of each of the above formalisms become available. The high expressiveness of the logic we propose and the need for capturing the reasoning in different contexts forces us to distinguish between unrestricted and finite model reasoning. A notable feature of our proposal is that reasoning in both cases is decidable. We argue that, by virtue of the high expressive power and of the associated reasoning capabilities on both unrestricted and finite models, our logic provides a common core for class-based representation formalisms.", "prompt": "The notion of class is ubiquitous in computer science and is central in many formalisms for the representation of structured knowledge used both in knowledge representation and in databases. In this paper we study the basic issues underlying such representation formalisms and single out both their common characteristics and their distinguishing features. Such investigation leads us to propose a", "orig": "unifying framework in which we are able to capture the fundamental aspects of several representation languages used in different contexts. The proposed formalism is expressed in the style of description logics, which have been introduced in knowledge representation as a means to provide a semantically well-founded basis for the structural aspects of knowledge representation systems. The", "generated": "The notion of class is ubiquitous in computer science and is central in many formalisms for the representation of structured knowledge used both in knowledge representation and in databases. In this paper we study the basic issues underlying such representation formalisms and single out both their common characteristics and their distinguishing features. Such investigation leads us to propose a"}
{"full_text": "A compiler consists of a sequence of phases going from lexical analysis to code generation. Ideally, the formal verification of a compiler should include the formal verification of each component of the tool-chain. An example is the CompCert project, a formally verified C compiler, that comes with associated tools and proofs that allow to formally verify most of those components. However, some components, in particular the lexer, remain unverified. In fact, the lexer of Compcert is generated using OCamllex, a lex-like OCaml lexer generator that produces lexers from a set of regular expressions with associated semantic actions. Even though there exist various approaches, like CakeML or Verbatim++, to write verified lexers, they all have only limited practical applicability. In order to contribute to the end-to-end verification of compilers, we implemented a generator of verified lexers whose usage is similar to OCamllex. Our software, called Coqlex, reads a lexer specification and generates a lexer equipped with a Coq proof of its correctness. It provides a formally verified implementation of most features of standard, unverified lexer generators. The conclusions of our work are two-fold: Firstly, verified lexers gain to follow a user experience similar to lex/flex or OCamllex, with a domain-specific syntax to write lexers comfortably. This introduces a small gap between the written artifact and the verified lexer, but our design minimizes this gap and makes it practical to review the generated lexer. The user remains able to prove further properties of their lexer. Secondly, it is possible to combine simplicity and decent performance. Our implementation approach that uses Brzozowski derivatives is noticeably simpler than the previous work in Verbatim++ that tries to generate a deterministic finite automaton (DFA) ahead of time, and it is also noticeably faster thanks to careful design choices. We wrote several example lexers that suggest that the convenience of using Coqlex is close to that of standard verified generators, in particular, OCamllex. We used Coqlex in an industrial project to implement a verified lexer of Ada. This lexer is part of a tool to optimize safety-critical programs, some of which are very large. This experience confirmed that Coqlex is usable in practice, and in particular that its performance is good enough. Finally, we performed detailed performance comparisons between Coqlex, OCamllex, and Verbatim++. Verbatim++ is the state-of-the-art tool for verified lexers in Coq, and the performance of its lexer was carefully optimized in previous work by Egolf and al. (2022). Our results suggest that Coqlex is two orders of magnitude slower than OCamllex, but two orders of magnitude faster than Verbatim++. Verified compilers and other language-processing tools are becoming important tools for safety-critical or security-critical applications. They provide trust and replace more costly approaches to certification, such as manually reading the generated code. Verified lexers are a missing piece in several Coq-based verified compilers today. Coqlex comes with safety guarantees, and thus shows that it is possible to build formally verified front-ends.", "prompt": "A compiler consists of a sequence of phases going from lexical analysis to code generation. Ideally, the formal verification of a compiler should include the formal verification of each component of the tool-chain. An example is the CompCert project, a formally verified C compiler, that comes with associated tools and proofs that allow to formally verify", "orig": "most of those components. However, some components, in particular the lexer, remain unverified. In fact, the lexer of Compcert is generated using OCamllex, a lex-like OCaml lexer generator that produces lexers from a set of regular expressions with associated semantic actions. Even though there exist various approaches,", "generated": "A compiler consists of a sequence of phases going from lexical analysis to code generation. Ideally, the formal verification of a compiler should include the formal verification of each component of the tool-chain. An example is the CompCert project, a formally verified C compiler, that comes with associated tools and proofs that allow to formally verify"}
{"full_text": "The context of this research is testing and building software systems and, specifically, software language repositories (SLRs), i.e., repositories with components for language processing (interpreters, translators, analyzers, transformers, pretty printers, etc.). SLRs are typically set up for developing and using metaprogramming systems, language workbenches, language definition frameworks, executable semantic frameworks, and modeling frameworks. This work is an inquiry into testing and building SLRs in a manner that the repository is seen as a collection of language-typed artifacts being related by the applications of language-typed functions or relations which serve language processing. The notion of language is used in a broad sense to include text-, tree-, graph-based languages as well as representations based on interchange formats and also proprietary formats for serialization. The overall approach underlying this research is one of language design driven by a complex case study, i.e., a specific SLR with a significant number of processed languages and language processors as well as a noteworthy heterogeneity in terms of representation types and implementation languages. The knowledge gained by our research is best understood as a declarative language design for regression testing and build management, we introduce a corresponding language Ueber with an executable semantics which maintains relationships between language-typed artifacts in an SLR. The grounding of the reported research is based on the comprehensive, formal, executable (logic programming-based) definition of the Ueber language and its systematic application to the management of the SLR YAS which consists of hundreds of language definition and processing components (such as interpreters and transformations) for more than thirty languages (not counting different representation types) with Prolog, Haskell, Java, and Python being used as implementation languages. The importance of this work follows from the significant costs implied by regression testing and build management and also from the complexity of SLRs which calls for means to help with understanding.", "prompt": "The context of this research is testing and building software systems and, specifically, software language repositories (SLRs), i.e., repositories with components for language processing (interpreters, translators, analyzers, transformers, pretty printers, etc.). SLRs are typically set up for developing and using metaprogramming systems,", "orig": "language workbenches, language definition frameworks, executable semantic frameworks, and modeling frameworks. This work is an inquiry into testing and building SLRs in a manner that the repository is seen as a collection of language-typed artifacts being related by the applications of language-typed functions or relations which serve language processing. The notion of", "generated": "The context of this research is testing and building software systems and, specifically, software language repositories (SLRs), i.e., repositories with components for language processing (interpreters, translators, analyzers, transformers, pretty printers, etc.). SLRs are typically set up for developing and using metaprogramming systems,"}
{"full_text": "We study operator algebras arising from monomial ideals in the ring of polynomials in noncommuting variables, through the apparatus of subproduct systems and C*-correspondences. We provide a full comparison amongst the related operator algebras. For our analysis we isolate a partially defined dynamical system, to which we refer as the {\\em quantised dynamics} of the monomial ideal. In addition we revisit several previously considered constructions. These include Matsumoto's subshift C*-algebras, as well as the tensor and the Pimsner algebras associated with dynamical systems or graphs. We sort out the various relations by giving concrete conditions and counterexamples that orientate the operator algebras of our context. It appears that the boundary C*-algebras do not arise as the quotient with the compact operators unconditionally. We establish a dichotomy to this effect by examining the resulting tensor algebras. We identify their boundary representations, we analyse their C*-envelopes, and we give criteria for hyperrigidity. Moreover we completely classify them in terms of the data provided by the monomial ideals. For tensor algebras of C*-correspondences and bounded isomorphisms this is achieved up to the level of local conjugacy (in the sense of Davidson and Roydor) for the quantised dynamics. For tensor algebras of subproduct systems and algebraic isomorphisms this is achieved up to the level of equality of monomial ideals modulo permutations of the variables. In the process we accomplish more in different directions. Most notably we show that tensor algebras form a complete invariant for isomorphic (resp. similar) subproduct systems of homogeneous ideals up to isometric (resp. bounded) isomorphisms. The results on local conjugacy are obtained via an alternative proof of the breakthrough result of Davidson and Katsoulis on piecewise conjugate systems. For our purposes we use appropriate compressions of the Fock representation. We then apply this alternative proof locally for the partially defined quantised dynamics. In this way we avoid the topological graphs machinery and pave the way for further applications. These include operator algebras of dynamical systems over commuting contractions or over row commuting contractions.", "prompt": "We study operator algebras arising from monomial ideals in the ring of polynomials in noncommuting variables, through the apparatus of subproduct systems and C*-correspondences. We provide a full comparison amongst the related operator algebras. For our analysis we isolate a partially defined dynamical system, to", "orig": "which we refer as the {\\em quantised dynamics} of the monomial ideal. In addition we revisit several previously considered constructions. These include Matsumoto's subshift C*-algebras, as well as the tensor and the Pimsner algebras associated with dynamical systems or graphs. We sort out", "generated": "We study operator algebras arising from monomial ideals in the ring of polynomials in noncommuting variables, through the apparatus of subproduct systems and C*-correspondences. We provide a full comparison amongst the related operator algebras. For our analysis we isolate a partially defined dynamical system, to"}
{"full_text": "Stylolites are ubiquitous geo-patterns observed in rocks in the upper crust, from geological reservoirs in sedimentary rocks to deformation zones, in folds, faults, and shear zones. These rough surfaces play a major role in the dissolution of rocks around stressed contacts, the transport of dissolved material and the precipitation in surrounding pores. Consequently, they play an active role in the evolution of rock microstructures and rheological properties in the Earth's crust. They are observed individually or in networks, in proximity to fractures and joints, and in numerous geological settings. This review article deals with their geometrical and compositional characteristics and the factors leading to their genesis. The main questions this review focuses on are the following: How do they form? How can they be used to measure strain and formation stress? How do they control fluid flow in the upper crust? Geometrically, stylolites have fractal roughness, with fractal geometrical properties exhibiting typically three scaling regimes: a self-affine scaling with Hurst exponent 1.1+/-0.1 at small scale (up to tens or hundreds of microns), another one with Hurst exponent around 0.5 to 0.6 at intermediate scale (up to millimeters or centimeters), and in the case of sedimentary stylolites, a flat scaling at large scale. More complicated anisotropic scaling (scaling laws depending of the direction of the profile considered) is found in the case of tectonic stylolites. We report models based on first principles from physical chemistry and statistical physics, including a mechanical component for the free-energy associated with stress concentrations, and a precise tracking of the influence of grain-scale heterogeneities and disorder on the resulting (micro)structures. Experimental efforts to reproduce stylolites in the laboratory are also reviewed. We show that although micrometer-size stylolite teeth are obtained in laboratory experiments, teeth deforming numerous grains have not yet been obtained experimentally, which is understandable given the very long formation time of such geometries. Finally, the applications of stylolites as strain and stress markers, to determine paleostress magnitude are reviewed. We show that the scalings in stylolite heights and the crossover scale between these scalings can be used to determine the stress magnitude (its scalar value) perpendicular to the stylolite surface during the stylolite formation, and that the stress anisotropy in the stylolite plane can be determined for the case of tectonic stylolites. We also show that the crossover between medium (millimetric) scales and large (pluricentimetric) scales, in the case of sedimentary stylolites, provides a good marker for the total amount of dissolution, which is still valid even when the largest teeth start to dissolve -- which leads to the loss of information, since the total deformation is not anymore recorded in a single marker structure. We discuss the impact of the stylolites on the evolution of the transport properties of the hosting rock, and show that they promote a permeability increase parallel to the stylolites, whereas their effect on the permeability transverse to the stylolite can be negligible, or may reduce the permeability, depending on the development of the stylolite. Highlights: Stylolite formation depends on rock composition and structure, stress and fluids. Stylolite geometry, fractal and self-affine properties, network structure, are investigated. The experiments and physics-based numerical models for their formation are reviewed. Stylolites can be used as markers of strain, paleostress orientation and magnitude. Stylolites impact transport properties, as function of maturity and flow direction.", "prompt": "Stylolites are ubiquitous geo-patterns observed in rocks in the upper crust, from geological reservoirs in sedimentary rocks to deformation zones, in folds, faults, and shear zones. These rough surfaces play a major role in the dissolution of rocks around stressed contacts, the transport of dissolved material and the precipitation in", "orig": "surrounding pores. Consequently, they play an active role in the evolution of rock microstructures and rheological properties in the Earth's crust. They are observed individually or in networks, in proximity to fractures and joints, and in numerous geological settings. This review article deals with their geometrical and compositional characteristics and the factors leading", "generated": "Stylolites are ubiquitous geo-patterns observed in rocks in the upper crust, from geological reservoirs in sedimentary rocks to deformation zones, in folds, faults, and shear zones. These rough surfaces play a major role in the dissolution of rocks around stressed contacts, the transport of dissolved material and the precipitation is"}
{"full_text": "We consider a parallel version of a classical Bayesian search problem. $k$ agents are looking for a treasure that is placed in one of the boxes indexed by $\\mathbb{N}^+$ according to a known distribution $p$. The aim is to minimize the expected time until the first agent finds it. Searchers run in parallel where at each time step each searcher can \"peek\" into a box. A basic family of algorithms which are inherently robust is \\emph{non-coordinating} algorithms. Such algorithms act independently at each searcher, differing only by their probabilistic choices. We are interested in the price incurred by employing such algorithms when compared with the case of full coordination. We first show that there exists a non-coordination algorithm, that knowing only the relative likelihood of boxes according to $p$, has expected running time of at most $10+4(1+\\frac{1}{k})^2 T$, where $T$ is the expected running time of the best fully coordinated algorithm. This result is obtained by applying a refined version of the main algorithm suggested by Fraigniaud, Korman and Rodeh in STOC'16, which was designed for the context of linear parallel search.We then describe an optimal non-coordinating algorithm for the case where the distribution $p$ is known. The running time of this algorithm is difficult to analyse in general, but we calculate it for several examples. In the case where $p$ is uniform over a finite set of boxes, then the algorithm just checks boxes uniformly at random among all non-checked boxes and is essentially $2$ times worse than the coordinating algorithm.We also show simple algorithms for Pareto distributions over $M$ boxes. That is, in the case where $p(x) \\sim 1/x^b$ for $0< b < 1$, we suggest the following algorithm: at step $t$ choose uniformly from the boxes unchecked in ${1, . . . ,min(M, \\lfloor t/\\sigma\\rfloor)}$, where $\\sigma = b/(b + k - 1)$. It turns out this algorithm is asymptotically optimal, and runs about $2+b$ times worse than the case of full coordination.", "prompt": "We consider a parallel version of a classical Bayesian search problem. $k$ agents are looking for a treasure that is placed in one of the boxes indexed by $\\mathbb{N}^+$ according to a known distribution $p$. The aim is to minimize the expected time until the first agent finds it. Search", "orig": "ers run in parallel where at each time step each searcher can \"peek\" into a box. A basic family of algorithms which are inherently robust is \\emph{non-coordinating} algorithms. Such algorithms act independently at each searcher, differing only by their probabilistic choices. We are interested in the price incurred by", "generated": "We consider a parallel version of a classical Bayesian search problem. $k$ agents are looking for a treasure that is placed in one of the boxes indexed by $\\mathbb{N}^+$ according to a known distribution $p$. The aim is to minimize the expected time until the first agent finds it."}
{"full_text": "Recovery of oil from oil shales and the natural primary migration of hydrocarbons are closely related processes that have received renewed interests in recent years because of the ever tightening supply of conventional hydrocarbons and the growing production of hydrocarbons from low permeability tight rocks. Quantitative models for conversion of kerogen into oil and gas and the timing of hydrocarbon generation have been well documented. However, lack of consensus about the kinetics of hydrocarbon formation in source rocks, expulsion timing and how the resulting hydrocarbons escape from or are retained in the source rocks motivates further investigation. In particular, many mechanisms for the transport of hydrocarbons from the source rocks in which they are generated into adjacent rocks with higher permeabilities and smaller capillary entry pressures have been proposed, and a better understanding of this complex process (primary migration) is needed. To characterize these processes it is imperative to use the latest technological advances. In this study, it is shown how insights into hydrocarbon migration in source rocks can be obtained by using sequential high resolution synchrotron X-ray tomography. Three-dimensional (3D) images of several immature \"shale\" samples were constructed at resolutions close to 5 micrometers. This is sufficient to resolve the source rock structure down to the grain level, but very fine grained silt particles, clay particles and colloids cannot be resolved. Samples used in this investigation came from the R-8 unit in the upper part of the Green River Shale, which is organic rich, varved, lacustrine marl formed in Eocene Lake Uinta, United States of America. One Green River Shale sample was heated in-situ up to 400{\\deg}C as X-ray tomography images were recorded. The other samples were scanned before and after heating at 400{\\deg}C. During the heating phase, the organic matter was decomposed, and gas was released. Gas expulsion from the low permeability shales was coupled with formation of microcracks. The main technical difficulty was numerical extraction of microcracks that have apertures in the 5 to 30 micrometer range (with 5 micrometers being the resolution limit) from a large 3D volume of X-ray attenuation data. The main goal of the work presented here is to develop a methodology to process these 3D data and image the cracks. This methodology is based on several levels of spatial filtering and automatic recognition of connected domains. Supportive petrographic and thermogravimetric data were an important complement to this study. An investigation of the strain field using two-dimensional image correlation analyses was also performed. As one application of the four-dimensional (4D, space + time) microtomography and the developed workflow, we show that fluid generation was accompanied by crack formation. Under different conditions, in the subsurface, this might provide paths for primary migration.", "prompt": "Recovery of oil from oil shales and the natural primary migration of hydrocarbons are closely related processes that have received renewed interests in recent years because of the ever tightening supply of conventional hydrocarbons and the growing production of hydrocarbons from low permeability tight rocks. Quantitative models for conversion of kerogen", "orig": "into oil and gas and the timing of hydrocarbon generation have been well documented. However, lack of consensus about the kinetics of hydrocarbon formation in source rocks, expulsion timing and how the resulting hydrocarbons escape from or are retained in the source rocks motivates further investigation. In particular, many mechanisms for the transport of", "generated": "Recovery of oil from oil shales and the natural primary migration of hydrocarbons are closely related processes that have received renewed interests in recent years because of the ever tightening supply of conventional hydrocarbons and the growing production of hydrocarbons from low permeability tight rocks. Quantitative models for conversion of kerogen"}
{"full_text": "We present a new model for calculating locomotion costs in breath-hold divers. Starting from basic mechanics principles, we calculate the work that the diver must provide through propulsion to counterbalance the action of drag, the buoyant force and weight during immersion. Compared to those in previous studies, the model presented here accurately analyses breath-hold divers which alternate active swimming with prolonged glides during the dive (as is the case in mammals). The energy cost of the dive is strongly dependent on these prolonged gliding phases. Here we investigate the length and impacts on energy cost of these glides with respect to the diver characteristics, and compare them with those observed in different breath-hold diving species. Taking into account the basal metabolic rate and chemical energy to propulsion transformation efficiency, we calculate optimal swim velocity and the corresponding total energy cost (including metabolic rate) and compare them with observations. Energy cost is minimised when the diver passes through neutral buoyancy conditions during the dive. This generally implies the presence of prolonged gliding phases in both ascent and descent, where the buoyancy (varying with depth) is best used against the drag, reducing energy cost. This is in agreement with past results (Miller et al., 2012; Sato et al., 2013) where, when the buoyant force is considered constant during the dive, the energy cost was minimised for neutral buoyancy. In particular, our model confirms the good physical adaption of dolphins for diving, compared to other breath-hold diving species which are mostly positively buoyant (penguins for example). The presence of prolonged glides implies a non-trivial dependency of optimal speed on maximal depth of the dive. This extends previous findings (Sato et al., 2010; Watanabe et al., 2011) which found no dependency of optimal speed on dive depth for particular conditions. The energy cost of the dive can be further diminished by reducing the volume of gas-filled body parts in divers close to neutral buoyancy. This provides a possible additional explanation for the observed exhalation of air before diving in phocid seals to minimise dive energy cost. Until now the only explanation for this phenomenon has been a reduction in the risk of decompression sickness.", "prompt": "We present a new model for calculating locomotion costs in breath-hold divers. Starting from basic mechanics principles, we calculate the work that the diver must provide through propulsion to counterbalance the action of drag, the buoyant force and weight during immersion. Compared to those in previous studies, the model presented here accurately analyses", "orig": "breath-hold divers which alternate active swimming with prolonged glides during the dive (as is the case in mammals). The energy cost of the dive is strongly dependent on these prolonged gliding phases. Here we investigate the length and impacts on energy cost of these glides with respect to the diver characteristics, and compare them with", "generated": "We present a new model for calculating locomotion costs in breath-hold divers. Starting from basic mechanics principles, we calculate the work that the diver must provide through propulsion to counterbalance the action of drag, the buoyant force and weight during immersion. Compared to those in previous studies, the model presented here accurately accounts"}
{"full_text": "In string musical instruments, the sound is radiated by the soundboard, subject to the strings excitation. This vibration of this rather complex structure is described here with models which need only a small number of parameters. Predictions of the models are compared with results of experiments that have been presented in Ege et al. [Vibroacoustics of the piano soundboard: (Non)linearity and modal properties in the low- and mid- frequency ranges, Journal of Sound and Vibration 332 (5) (2013) 1288-1305]. The apparent modal density of the soundboard of an upright piano in playing condition, as seen from various points of the structure, exhibits two well-separated regimes, below and above a frequency flim that is determined by the wood characteristics and by the distance between ribs. Above flim, most modes appear to be localised, presumably due to the irregularity of the spacing and height of the ribs. The low-frequency regime is predicted by a model which consists of coupled sub-structures: the two ribbed areas split by the main bridge and, in most cases, one or two so-called cut-off corners. In order to assess the dynamical properties of each of the subplates (considered here as homogeneous plates), we propose a derivation of the (low-frequency) modal density of an orthotropic homogeneous plate which accounts for the boundary conditions on an arbitrary geometry. Above flim, the soundboard, as seen from a given excitation point, is modelled as a set of three structural wave-guides, namely the three inter-rib spacings surrounding the excitation point. Based on these low- and high-frequency models, computations of the point-mobility and of the apparent modal densities seen at several excitation points match published measurements. The dispersion curve of the wave-guide model displays an acoustical radiation scheme which differs significantly from that of a thin homogeneous plate. It appears that piano dimensioning is such that the subsonic regime of acoustical radiation extends over a much wider frequency range than it would be for a homogeneous plate with the same low-frequency vibration. One problem in piano manufacturing is examined in relationship with the possible radiation schemes induced by the models.", "prompt": "In string musical instruments, the sound is radiated by the soundboard, subject to the strings excitation. This vibration of this rather complex structure is described here with models which need only a small number of parameters. Predictions of the models are compared with results of experiments that have been presented in Ege et al", "orig": ". [Vibroacoustics of the piano soundboard: (Non)linearity and modal properties in the low- and mid- frequency ranges, Journal of Sound and Vibration 332 (5) (2013) 1288-1305]. The apparent modal density of the soundboard of an", "generated": "In string musical instruments, the sound is radiated by the soundboard, subject to the strings excitation. This vibration of this rather complex structure is described here with models which need only a small number of parameters. Predictions of the models are compared with results of experiments that have been presented in Ege et al"}
{"full_text": "Inferring parameters related to the aggregation pattern of parasites and to their dispersal propensity is important for predicting their ecological consequences and evolutionary potential. Nonetheless, it is notoriously difficult to infer these parameters from wildlife parasites given the difficulty in tracking these organisms. Molecular-based inferences constitute a promising approach that has yet rarely been applied in the wild.Here, we combinedseveral population genetic analyses including sibship reconstruction to documentthe genetic structure, patterns of sibship aggregation and the dispersal dynamics of a non-native parasite of fish, the freshwater copepod ectoparasiteTracheliastespolycolpus. We collected parasites according to a hierarchical sampling design,withthe sampling of all parasites from all host individualscapturedineight sites spread along an upstream-downstream river gradient. Individual multilocus genotypes were obtained from 14 microsatellite markers, and used to assign parasites to full-sib families and to investigate the genetic structure of T.polycolpus among both hosts and sampling sites. The distribution of full-sibs obtainedamong the sampling sites was used to estimate individual dispersal distances within families. Our results showed that T. polycolpus sibs tend to be aggregated within sites but not withinhost individuals. We detected important upstream-to-downstream dispersal events of T.polycolpusbetween sites (modal distance: 25.4 km; 95% CI [22.9, 27.7]), becoming scarcer as the geographic distance from their family core location increases. Such a dispersal pattern likely contributes to the strong isolation-by-distance observed at the river scale. We also detected some downstream-to-upstream dispersal events (modal distance: 2.6 km; 95% CI [2.2-23.3]) that likely result from movementsof infected hosts.Within each site, the dispersal of free-living infective larvae among hosts likely contributes to increasing genetic diversity on hosts, possibly fostering the evolutionary potential of T. polycolpus.", "prompt": "Inferring parameters related to the aggregation pattern of parasites and to their dispersal propensity is important for predicting their ecological consequences and evolutionary potential. Nonetheless, it is notoriously difficult to infer these parameters from wildlife parasites given the difficulty in tracking these organisms. Molecular-based inferences constitute a promising approach that has yet rarely been applied in", "orig": "the wild.Here, we combinedseveral population genetic analyses including sibship reconstruction to documentthe genetic structure, patterns of sibship aggregation and the dispersal dynamics of a non-native parasite of fish, the freshwater copepod ectoparasiteTracheliastespolycolpus. We collected parasites according to a", "generated": "Inferring parameters related to the aggregation pattern of parasites and to their dispersal propensity is important for predicting their ecological consequences and evolutionary potential. Nonetheless, it is notoriously difficult to infer these parameters from wildlife parasites given the difficulty in tracking these organisms. Molecular-based inferences constitute a promising approach that has yet rarely been applied in"}
{"full_text": "Since the early days of the Web, web application developers have aspired to develop much of their applications declaratively. However, one aspect of the application, namely its business-logic is constantly left imperative. In this work we present Application Embedding, a novel approach to application development which allows all aspects of an application, including its business-logic, to be programmed declaratively. We develop this approach in a two-step process. First, we draw a mapping between web applications and Domain-Specific Languages (DSLs). Second, we note that out of the two methods for implementing DSLs, namely as either internal or external, most traditional web applications correspond to external DSLs, while the the technique that corresponds to DSL embedding (implementing internal DSLs) is left mostly unexplored. By projecting the well-known technique of DSL embedding onto web applications, we derive a novel technique--Application Embedding. Application embedding offers a separation of code assets that encourages reuse of imperative code, while keeping all application-specific assets, including those specifying its business- logic, declarative. As validation, we implemented a simple, though nontrivial web application using the proposed separation of assets. This implementation includes an application-agnostic imperative host application named FishTank, intended to be applicable for a wide variety of web applications, and a declarative definition of the different aspects of the specific application, intended to be loaded on that host. Our method of separation of code assets facilitates a better separation of work, in comparison to traditional methods. By this separation, host application developers can focus mostly on the extra-functional aspects of a web application, namely on improving performance, scalability, and availability, while developers of an embedded application can focus on the functional aspects of their application, without worrying about extra- functional concerns. The reusability of the host application makes the effort put into a better implementation cost-effective, since it can benefit all applications built on top of it.", "prompt": "Since the early days of the Web, web application developers have aspired to develop much of their applications declaratively. However, one aspect of the application, namely its business-logic is constantly left imperative. In this work we present Application Embedding, a novel approach to application development which allows all aspects of an application", "orig": ", including its business-logic, to be programmed declaratively. We develop this approach in a two-step process. First, we draw a mapping between web applications and Domain-Specific Languages (DSLs). Second, we note that out of the two methods for implementing DSLs, namely as either internal or external, most", "generated": "Since the early days of the Web, web application developers have aspired to develop much of their applications declaratively. However, one aspect of the application, namely its business-logic is constantly left imperative. In this work we present Application Embedding, a novel approach to application development which allows all aspects of an application"}
{"full_text": "We propose an interpretative framework for quantum mechanics corresponding to the specifications of Louis de Broglie's double-solution theory. The principle is to decompose the evolution of a quantum system into two wave functions: an external wave function corresponding to the evolution of its center of mass and an internal wave function corresponding to the evolution of its internal variables in the center-of-mass system. Mathematical decomposition is only possible in certain cases because there are many interactions linking these two parts. In addition, these two wave functions will have different meanings and interpretations. The external wave function \"pilots\" the center of mass of the quantum system: it corresponds to the Broglie pilot wave. When the Planck constant tends to zero, it results mathematically from the convergence of the square of the module and the phase of the external wave function to a density and a classical action verifying the Hamilton-Jacobi statistical equations. This interpretation explains all the measurement results, namely those yielded by interference, spin measurement (Stern and Gerlach) and non-locality (EPR-B) experiments. For the internal wave function, several interpretations are possible : the one of the pilot wave can be applied in cascade to the internal wave function. However, the interpretation proposed by Erwin Schr{\\\"o}dinger at the Solvay Congress in 1927 and restricted to the internal wave function is also possible. For Schr{\\\"o}dinger, the particles are extended and the square of the module of the (internal) wave function of an electron corresponds to the density of its charge in space. We present many arguments in favour of this interpretation, which like the pilot wave interpretation is realistic and deterministic. Finally, we will see that this double interpretation serves as a frame of reference by which to better understand the debates on the interpretation of quantum mechanics and to review the relationships between gravity and quantum mechanics.", "prompt": "We propose an interpretative framework for quantum mechanics corresponding to the specifications of Louis de Broglie's double-solution theory. The principle is to decompose the evolution of a quantum system into two wave functions: an external wave function corresponding to the evolution of its center of mass and an internal wave function corresponding to the", "orig": "evolution of its internal variables in the center-of-mass system. Mathematical decomposition is only possible in certain cases because there are many interactions linking these two parts. In addition, these two wave functions will have different meanings and interpretations. The external wave function \"pilots\" the center of mass of the quantum system: it", "generated": "We propose an interpretative framework for quantum mechanics corresponding to the specifications of Louis de Broglie's double-solution theory. The principle is to decompose the evolution of a quantum system into two wave functions: an external wave function corresponding to the evolution of its center of mass and an internal wave function corresponding to the"}
{"full_text": "A standing challenge in undergraduate Computer Science curricula is the teaching and learning of computer programming. Through this paper which is an essay about programming, we aim to contribute to the plethora of existing pedagogies, approaches and philosophies, by discussing a specific feature of our approach in teaching principled programming to undergraduate students, in their first semester of studies, namely the utilization of pictures, both text-based and raster-based graphics. Although the given course has evolved substantially over the thirty years of its delivery regarding the programming languages (Miranda, C, C++, Java) and paradigms (functional, imperative, object-oriented, combination of procedural and object-oriented) used, the discussed visual feature has been maintained and steadily strengthened. We list abstraction, problem decomposition and synthesis, information hiding, reusability, modularity and extensibility as key principles of problem solving and algorithmic thinking. These principles are closely aligned with the advocated computational thinking techniques of problem decomposition, pattern recognition, pattern generalization and algorithm design. We aim for our students to familiarize themselves with all the above principles through practical problem solving. Our ongoing inquiry has been whether the problem domain of pictures is contributing valuably towards this aim. Moreover, an added-value is that students get a glimpse of computational complexity in a visual, empirical way. The presented work is not related to visual programming, since the students write their programs textually and not graphically; it's the output of their programs which is in visual form. Our approach though is loosely related to the classical paradigm of turtle graphics. However, our focus is Computer Science majors, who should be able to design and build turtles and other objects and not just use them. Indeed, the programming principles course helps them to do both and also to appreciate the multitude of algorithmic ways for producing the same visual output. Currently the given programming principles are approached both from a procedural, process-based and an object-oriented, concept-based perspective and the course uses the Java language. Through the presented example problems, we aim to show the appropriateness of the visual domain of pictures for supporting the learning of principled programming. The problem domain of pictures is abundantly rich with potential examples to draw from. Moreover, as reported in the literature, female students may show higher interest towards visual problem domains in programming classes, in relation to other problem domains. We plan to investigate this conjecture in the context of our broader aim to encourage more females to follow university studies in computer science; in this paper only a cursory finding is presented, that bears some relation to what is reported in the literature.", "prompt": "A standing challenge in undergraduate Computer Science curricula is the teaching and learning of computer programming. Through this paper which is an essay about programming, we aim to contribute to the plethora of existing pedagogies, approaches and philosophies, by discussing a specific feature of our approach in teaching principled programming to undergraduate students,", "orig": "in their first semester of studies, namely the utilization of pictures, both text-based and raster-based graphics. Although the given course has evolved substantially over the thirty years of its delivery regarding the programming languages (Miranda, C, C++, Java) and paradigms (functional, imperative, object-oriented, combination of procedural", "generated": "A standing challenge in undergraduate Computer Science curricula is the teaching and learning of computer programming. Through this paper which is an essay about programming, we aim to contribute to the plethora of existing pedagogies, approaches and philosophies, by discussing a specific feature of our approach in teaching principled programming to undergraduate students,"}
{"full_text": "Context: Linear Temporal Logic (LTL) has been used widely in verification. Its importance and popularity have only grown with the revival of temporal logic synthesis, and with new uses of LTL in robotics and planning activities. All these uses demand that the user have a clear understanding of what an LTL specification means. Inquiry: Despite the growing use of LTL, no studies have investigated the misconceptions users actually have in understanding LTL formulas. This paper addresses the gap with a first study of LTL misconceptions. Approach: We study researchers' and learners' understanding of LTL in four rounds (three written surveys, one talk-aloud) spread across a two-year timeframe. Concretely, we decompose \"understanding LTL\" into three questions. A person reading a spec needs to understand what it is saying, so we study the mapping from LTL to English. A person writing a spec needs to go in the other direction, so we study English to LTL. However, misconceptions could arise from two sources: a misunderstanding of LTL's syntax or of its underlying semantics. Therefore, we also study the relationship between formulas and specific traces. Knowledge: We find several misconceptions that have consequences for learners, tool builders, and designers of new property languages. These findings are already resulting in changes to the Alloy modeling language. We also find that the English to LTL direction was the most common source of errors; unfortunately, this is the critical \"authoring\" direction in which a subtle mistake can lead to a faulty system. We contribute study instruments that are useful for training learners (whether academic or industrial) who are getting acquainted with LTL, and we provide a code book to assist in the analysis of responses to similar-style questions. Grounding: Our findings are grounded in the responses to our survey rounds. Round 1 used Quizius to identify misconceptions among learners in a way that reduces the threat of expert blind spots. Rounds 2 and 3 confirm that both additional learners and researchers (who work in formal methods, robotics, and related fields) make similar errors. Round 4 adds deep support for our misconceptions via talk-aloud surveys. Importance This work provides useful answers to two critical but unexplored questions: in what ways is LTL tricky and what can be done about it? Our survey instruments can serve as a starting point for other studies.", "prompt": "Context: Linear Temporal Logic (LTL) has been used widely in verification. Its importance and popularity have only grown with the revival of temporal logic synthesis, and with new uses of LTL in robotics and planning activities. All these uses demand that the user have a clear understanding of what an LTL specification means", "orig": ". Inquiry: Despite the growing use of LTL, no studies have investigated the misconceptions users actually have in understanding LTL formulas. This paper addresses the gap with a first study of LTL misconceptions. Approach: We study researchers' and learners' understanding of LTL in four rounds (three written surveys, one", "generated": "Linear Temporal Logic (LTL) has been used widely in verification. Its importance and popularity have only grown with the revival of temporal logic synthesis, and with new uses of LTL in robotics and planning activities. All these uses demand that the user have a clear understanding of what an LTL specification means.\n\nL"}
{"full_text": "L'analyse du lien entre l'environnement et la sant\\'e est devenue une pr\\'eoccupation majeure de sant\\'e publique comme en t\\'emoigne l'\\'emergence des deux Plans nationaux sant\\'e environnement. Pour ce faire, les d\\'ecideurs sont confront\\'es au besoin de d\\'eveloppement d'outils n\\'ecessaires \\`a l'identification des zones g\\'eographiques dans lesquelles une surexposition potentielle \\`a des substances toxiques est observ\\'ee. L'objectif du projet Syst\\`eme d'information g\\'eographique (SIG), facteurs de risques environnementaux et d\\'ec\\`es par cancer (SIGFRIED 1) est de construire une plate-forme de mod\\'elisation permettant d'\\'evaluer, par une approche spatiale, l'exposition de la population fran\\c{c}aise aux substances chimiques et d'en identifier ses d\\'eterminants. L'\\'evaluation des expositions est r\\'ealis\\'ee par le biais d'une mod\\'elisation multim\\'edia probabiliste. Les probl\\`emes \\'epist\\'emologiques li\\'es \\`a l'absence de donn\\'ees sont palli\\'es par la mise en {\\oe}uvre d'outils utilisant les techniques d'analyse spatiale. Un exemple est fourni sur la r\\'egion Nord-Pas-de-Calais et Picardie, pour le cadmium, le nickel et le plomb. Le calcul de l'exposition est r\\'ealis\\'e sur une dur\\'ee de 70 ans sur la base des donn\\'ees disponibles autour de l'ann\\'ee 2004 sur une maille de 1 km de c\\^ot\\'e. Par exemple pour le Nord-Pas-de-Calais, les indicateurs permettent de d\\'efinir deux zones pour le cadmium et trois zones pour le plomb. Celles-ci sont li\\'ees \\`a l'historique industriel de la r\\'egion : le bassin minier, les activit\\'es m\\'etallurgiques et l'agglom\\'eration lilloise. La contribution des diff\\'erentes voies d'exposition varie sensiblement d'un polluant \\`a l'autre. Les cartes d'exposition ainsi obtenues permettent d'identifier les zones g\\'eographiques dans lesquelles conduire en priorit\\'e des \\'etudes environnementales de terrains. Le SIG construit constitue la base d'une plate-forme o\\`u les donn\\'ees d'\\'emission \\`a la source, de mesures environnementales, d'exposition, puis sanitaires et socio-\\'economiques pourront \\^etre associ\\'ees. -- Analysis of the association between the environment and health has become a major public health concern, as shown by the development of two national environmental health plans. For such an analysis, policy-makers need tools to identify the geographic areas where overexposure to toxic agents may be observed. The objective of the SIGFRIED 1 project is to build a work station for spatial modeling of the exposure of the French population to chemical substances and for identifying the determinants of this exposure. Probabilistic multimedia modeling is used to assess exposure. The epistemological problems associated with the absence of data are overcome by the implementation of tools that apply spatial analysis techniques. An example is furnished for the region of Nord-Pas-de-Calais and Picardie, for cadmium, nickel and lead exposure. The calculation of exposure is performed for duration of 70 years on the basis of data collected around 2004 fora grid of squares 1 km in length. For example, for Nord-Pas-de-Calais, the indicators allow us to define two areas for cadmium and three for lead. They are linked to the region's industrial history: mining basin, metallurgy activities, and the Lille metropolitan area. The contribution of various exposure pathways varied substantially from one pollutant to another. The exposure maps thus obtained allow us to identify the geographic area where environmental studies must be conducted in priority. The GIS thus constructed is the foundation of a workstation where source emission data, environmental exposure measurements, and finally health and socioeconomic measurements can be combined.", "prompt": "L'analyse du lien entre l'environnement et la sant\\'e est devenue une pr\\'eoccupation majeure de sant\\'e publique comme en t\\'emoigne l'\\'emergence des deux Plans nationaux sant\\'e environnement. Pour ce faire, les d\\'", "orig": "ecideurs sont confront\\'es au besoin de d\\'eveloppement d'outils n\\'ecessaires \\`a l'identification des zones g\\'eographiques dans lesquelles une surexposition potentielle \\`a des substances toxiques est observ\\'ee. L'objectif du projet", "generated": "Je vais r\u00e9p\u00e9ter le texte et le poursuivre avec quelques phrases suppl\u00e9mentaires.\n\nL'analyse du lien entre l'environnement et la sant\u00e9 est devenue une pr\u00e9occupation majeure de sant\u00e9 publique comme en t\u00e9moigne l'\u00e9mergence des deux Plans nationaux sant\u00e9 environ"}
{"full_text": "The evolution of stars and planets is mostly controlled by the properties of their atmosphere. This is particularly true in the case of exoplanets close to their stars, for which one has to account both for an (often intense) irradiation flux, and from an intrinsic flux responsible for the progressive loss of the inner planetary heat. The goals of the present work are to help understanding the coupling between radiative transfer and advection in exoplanetary atmospheres and to provide constraints on the temperatures of the deep atmospheres. This is crucial in assessing whether modifying assumed opacity sources and/or heat transport may explain the inflated sizes of a significant number of giant exoplanets found so far. I use a simple analytical approach inspired by Eddington's approximation for stellar atmospheres to derive a relation between temperature and optical depth valid for plane-parallel static grey atmospheres which are both transporting an intrinsic heat flux and receiving an outer radiation flux. The model is parameterized as a function of mean visible and thermal opacities, respectively. The model is shown to reproduce relatively well temperature profiles obtained from more sophisticated radiative transfer calculations of exoplanetary atmospheres. It naturally explains why a temperature inversion (stratosphere) appears when the opacity in the optical becomes significant compared to that in the infrared. I further show that the mean equivalent flux (proportional to T^4) is conserved in the presence of horizontal advection on constant optical depth levels. This implies with these hypotheses that the deep atmospheric temperature used as outer boundary for the evolution models should be calculated from models pertaining to the entire planetary atmosphere, not from ones that are relevant to the day side or to the substellar point. In these conditions, present-day models yield deep temperatures that are ~1000K too cold to explain the present size of planet HD 209458b. An tenfold increase in the infrared to visible opacity ratio would be required to slow the planetary cooling and contraction sufficiently to explain its size. However, the mean equivalent flux is not conserved anymore in the presence of opacity variations, or in the case of non-radiative vertical transport of energy: The presence of clouds on the night side or a downward transport of kinetic energy and its dissipation at deep levels would help making the deep atmosphere hotter and may explain the inflated sizes of giant exoplanets.", "prompt": "The evolution of stars and planets is mostly controlled by the properties of their atmosphere. This is particularly true in the case of exoplanets close to their stars, for which one has to account both for an (often intense) irradiation flux, and from an intrinsic flux responsible for the progressive loss of the inner planetary", "orig": "heat. The goals of the present work are to help understanding the coupling between radiative transfer and advection in exoplanetary atmospheres and to provide constraints on the temperatures of the deep atmospheres. This is crucial in assessing whether modifying assumed opacity sources and/or heat transport may explain the inflated sizes of a significant number of", "generated": "The evolution of stars and planets is mostly controlled by the properties of their atmosphere. This is particularly true in the case of exoplanets close to their stars, for which one has to account both for an (often intense) irradiation flux, and from an intrinsic flux responsible for the progressive loss of the inner planetary"}
{"full_text": "We pursue here the development of models for complex (viscoelastic) fluids in shallow free-surface gravity flows which was initiated by [Bouchut-Boyaval, M3AS (23) 2013] for 1D (translation invariant) cases. The models we propose are hyperbolic quasilinear systems that generalize Saint-Venant shallow-water equations to incompressible Maxwell fluids. The models are compatible with a formulation of the thermo-dynamics second principle. In comparison with Saint-Venant standard shallow-water model, the momentum balance includes extra-stresses associated with an elastic potential energy in addition to a hydrostatic pressure. The extra-stresses are determined by an additional tensor variable solution to a differential equation with various possible time rates. For the numerical evaluation of solutions to Cauchy problems, we also propose explicit schemes discretizing our generalized Saint-Venant systems with Finite-Volume approximations that are entropy-consistent (under a CFL constraint) in addition to satisfy exact (discrete) mass and momentum conservation laws. In comparison with most standard viscoelastic numerical models, our discrete models can be used for any retardation-time values (i.e. in the vanishing \"solvent-viscosity\" limit). We finally illustrate our hyperbolic viscoelastic flow models numerically using computer simulations in benchmark test cases. On extending to Maxwell fluids some free-shear flow testcases that are standard benchmarks for Newtonian fluids, we first show that our (numerical) models reproduce well the viscoelastic physics, phenomenologically at least, with zero retardation-time. Moreover, with a view to quantitative evaluations, numerical results in the lid-driven cavity testcase show that, in fact, our models can be compared with standard viscoelastic flow models in sheared-flow benchmarks on adequately choosing the physical parameters of our models. Analyzing our models asymptotics should therefore shed new light on the famous High-Weissenberg Number Problem (HWNP), which is a limit for all the existing viscoelastic numerical models.", "prompt": "We pursue here the development of models for complex (viscoelastic) fluids in shallow free-surface gravity flows which was initiated by [Bouchut-Boyaval, M3AS (23) 2013] for 1D (translation invariant) cases. The models we propose are hyperbolic qu", "orig": "asilinear systems that generalize Saint-Venant shallow-water equations to incompressible Maxwell fluids. The models are compatible with a formulation of the thermo-dynamics second principle. In comparison with Saint-Venant standard shallow-water model, the momentum balance includes extra-stresses associated with an elastic potential energy in addition to a hydrostatic pressure", "generated": "We pursue here the development of models for complex (viscoelastic) fluids in shallow free-surface gravity flows which was initiated by [Bouchut-Boyaval, M3AS (23) 2013] for 1D (translation invariant) cases. The models we propose are hyperbolic quasi"}
{"full_text": "Parkinson's disease (PD) is a common neurodegenerative disorder with a poorly understood physiopathology. In clinical practice, challenges are encountered in the diagnosis of early stages and in the prediction of the disease progression due to the absence of established biomarkers. Several biomarkers obtained using neuroimaging techniques such as functional Magnetic Resonance Imaging (fMRI) have been studied recently. However, the reliability and generalizability of neuroimaging-based measurements are susceptible to several different sources of variability, including those introduced by different analysis methods or population sampling. In this context, an evaluation of the robustness of such biomarkers is essential. This study is part of a larger project investigating the replicability of potential neuroimaging biomarkers of PD. Here, we attempt to reproduce (same data, same method) and replicate (different data or method) the models described in Nguyen et al. 2021 to predict individual's PD current state and progression using demographic, clinical and neuroimaging features (fALFF and ReHo extracted from resting-state fMRI). We used the Parkinson's Progression Markers Initiative dataset (PPMI, ppmi-info.org), as in Nguyen et al. 2021 and tried to reproduce the original cohort, imaging features and machine learning models as closely as possible using the information available in the paper and the code. We also investigated methodological variations in cohort selection, feature extraction pipelines and sets of input features. Using the reproduction workflow, we managed to obtain better than chance performance for all our models (R2>0), but this performance remained very different from the ones reported in the original study. The challenges encountered while reproducing and replicating the original work are likely explained by the complexity of neuroimaging studies, in particular in clinical settings. We provide recommendations to facilitate the reproducibility of such studies in the future, for instance with the use of version control tools, standardization of pipelines and publication of analysis code and derived data.", "prompt": "Parkinson's disease (PD) is a common neurodegenerative disorder with a poorly understood physiopathology. In clinical practice, challenges are encountered in the diagnosis of early stages and in the prediction of the disease progression due to the absence of established biomarkers. Several biomarkers obtained using neuroimaging techniques", "orig": "such as functional Magnetic Resonance Imaging (fMRI) have been studied recently. However, the reliability and generalizability of neuroimaging-based measurements are susceptible to several different sources of variability, including those introduced by different analysis methods or population sampling. In this context, an evaluation of the robustness of such biom", "generated": "Parkinson's disease (PD) is a common neurodegenerative disorder with a poorly understood physiopathology. In clinical practice, challenges are encountered in the diagnosis of early stages and in the prediction of the disease progression due to the absence of established biomarkers. Several biomarkers obtained using neuroimaging techniques"}
{"full_text": "Welcome to the proceedings of FOCLASA 2012, the 11th International Workshop on the Foundations of Coordination Languages and Self-Adaptation. FOCLASA 2012 was held in Newcastle upon Tyne, UK, on September 8, 2012 as a satellite event of CONCUR 2012, the 23rd International Conference on Concurrency Theory. The workshop provides a venue where researchers and practitioners could meet, exchange ideas, identify common problems, determine some of the key and fundamental issues related to coordination languages and self adaptation, and explore together and disseminate solutions. Indeed, a number of hot research topics are currently sharing the common problem of combining concurrent, distributed, mobile and heterogeneous components, trying to harness the intrinsic complexity of the resulting systems. Computation nowadays is becoming inherently concurrent, either because of characteristics of the hardware (with multicore processors becoming omnipresent) or due to the ubiquitous presence of distributed systems (incarnated in the Internet). Computational systems are therefore typically distributed, concurrent, mobile, and often involve composition of heterogeneous components. To specify and reason about such systems and go beyond the functional correctness proofs, e.g., by supporting reusability and improving maintainability, approaches such as coordination languages and self adaptation are recognised as fundamental. This year, we received 13 submissions involving 35 authors from 10 different countries. Papers underwent a rigorous review process, and all accepted papers received 3 review reports. After the review process, the international Program Committee of FOCLASA 2012 decided to select 8 papers for presentation during the workshop and inclusion in these proceedings. These papers tackle different issues that are currently central to our community, self-adaptation and coordination, processes and coordination, and type systems. The workshop features an invited talk by Sebastian Uchitel from Imperial College London (UK).", "prompt": "Welcome to the proceedings of FOCLASA 2012, the 11th International Workshop on the Foundations of Coordination Languages and Self-Adaptation. FOCLASA 2012 was held in Newcastle upon Tyne, UK, on September 8, 2012 as a satellite event of CONCUR", "orig": "2012, the 23rd International Conference on Concurrency Theory. The workshop provides a venue where researchers and practitioners could meet, exchange ideas, identify common problems, determine some of the key and fundamental issues related to coordination languages and self adaptation, and explore together and disseminate solutions. Indeed, a number of hot", "generated": "Welcome to the proceedings of FOCLASA 2012, the 11th International Workshop on the Foundations of Coordination Languages and Self-Adaptation. FOCLASA 2012 was held in Newcastle upon Tyne, UK, on September 8, 2012 as a satellite event of CONCUR"}
{"full_text": "Distant clumpy galaxies are thought to be Jeans-unstable disks, and an important channel for the formation of local galaxies, as suggested by recent spatially-resolved kinematic observations of z~2 galaxies. I study the kinematics of clumpy galaxies at z~0.6, and compare their properties with those of counterparts at higher and lower redshifts. I selected a sample of 11 clumpy galaxies at z~0.6 from the representative sample of emission line, intermediate-mass galaxies IMAGES. Selection was based on rest-frame UV morphology from HST/ACS images, mimicking the selection criteria commonly used at higher redshifts. Their spatially-resolved kinematics were derived in the frame of the IMAGES survey, using the VLT/FLAMES-GIRAFFE multi-integral field spectrograph. For those showing large-scale rotation, I derived the Toomre Q parameter, which characterizes the stability of their gaseous and stellar phases. I find that the fraction of UV-selected clumpy galaxies at z~0.6 is 20+/-12%. Roughly half of them (45+/-30%) have complex kinematics inconsistent with Jeans-unstable disks, while those in the remaining half (55+/-30%) show large-scale rotations. The latter reveal a stable gaseous phase, but the contribution of their stellar phase makes them globally unstable to clump formation. Clumpy galaxies appear to be less unstable at z~0.6 than at z~2, which could explain why the UV clumps tend to vanish in rest-frame optical images of z~0.6 clumpy galaxies, conversely to z~2 clumpy galaxies, in which the stellar phase can substantially fragment. This suggests that the former correspond to patchy star-formation regions superimposed on a smoother mass distribution. A possible and widespread scenario for driving clump formation relies on instabilities by cold streams penetrating the dark matter halos where clumpy galaxies inhabit. While such a gas accretion process is predicted to be significant in massive, z~2 haloes, it is also predicted to be strongly suppressed in similar, z~0.6 haloes, which could explain why lowest-z clumpy galaxies appear to be driven by a different mechanism. Instead, I found that interactions are probably the dominant driver leading to the formation of clumpy galaxies at z<1. I argue that the nature of z>1 clumpy galaxies remains more uncertain. While cold flows could be an important driver at z~2, I also argue that the observed and cumulative merger fraction between z=2 and z=3 is large enough so that every z~2 galaxy might be the result of a merger that occurred within their past 1 Gyr. I conclude that it is premature to rule out mergers as a universal driver for galaxy evolution from z~2 down to z=0.", "prompt": "Distant clumpy galaxies are thought to be Jeans-unstable disks, and an important channel for the formation of local galaxies, as suggested by recent spatially-resolved kinematic observations of z~2 galaxies. I study the kinematics of clumpy galaxies at z~0.6, and compare their properties with", "orig": "those of counterparts at higher and lower redshifts. I selected a sample of 11 clumpy galaxies at z~0.6 from the representative sample of emission line, intermediate-mass galaxies IMAGES. Selection was based on rest-frame UV morphology from HST/ACS images, mimicking the selection criteria commonly used at", "generated": "Distant clumpy galaxies are thought to be Jeans-unstable disks, and an important channel for the formation of local galaxies, as suggested by recent spatially-resolved kinematic observations of z~2 galaxies. I study the kinematics of clumpy galaxies at z~0.6, and compare their properties with"}
