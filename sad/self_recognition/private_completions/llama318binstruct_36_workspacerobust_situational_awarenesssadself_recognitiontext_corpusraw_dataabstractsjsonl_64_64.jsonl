{"full_text": "For many decades, ultrahigh energy charged particles of unknown origin that can be observed from the ground have been a puzzle for particle physicists and astrophysicists. As an attempt to discriminate among several possible production scenarios, astrophysicists try to test the statistical isotropy of the directions of arrival of these cosmic rays. At the highest energies, they are supposed to point toward their sources with good accuracy. However, the observations are so rare that testing the distribution of such samples of directional data on the sphere is nontrivial. In this paper, we choose a nonparametric framework that makes weak hypotheses on the alternative distributions and allows in turn to detect various and possibly unexpected forms of anisotropy. We explore two particular procedures. Both are derived from fitting the empirical distribution with wavelet expansions of densities. We use the wavelet frame introduced by [SIAM J. Math. Anal. 38 (2006b) 574-594 (electronic)], the so-called needlets. The expansions are truncated at scale indices no larger than some ${J^{\\star}}$, and the $L^p$ distances between those estimates and the null density are computed. One family of tests (called Multiple) is based on the idea of testing the distance from the null for each choice of $J=1,\\ldots,{J^{\\star}}$, whereas the so-called PlugIn approach is based on the single full ${J^{\\star}}$ expansion, but with thresholded wavelet coefficients. We describe the practical implementation of these two procedures and compare them to other methods in the literature. As alternatives to isotropy, we consider both very simple toy models and more realistic nonisotropic models based on Physics-inspired simulations. The Monte Carlo study shows good performance of the Multiple test, even at moderate sample size, for a wide sample of alternative hypotheses and for different choices of the parameter ${J^{\\star}}$. On the 69 most energetic events published by the Pierre Auger Collaboration, the needlet-based procedures suggest statistical evidence for anisotropy. Using several values for the parameters of the methods, our procedures yield $p$-values below 1%, but with uncontrolled multiplicity issues. The flexibility of this method and the possibility to modify it to take into account a large variety of extensions of the problem make it an interesting option for future investigation of the origin of ultrahigh energy cosmic rays.", "prompt": "For many decades, ultrahigh energy charged particles of unknown origin that can be observed from the ground have been a puzzle for particle physicists and astrophysicists. As an attempt to discriminate among several possible production scenarios, astrophysicists try to test the statistical isotropy of the directions of arrival of", "orig": "these cosmic rays. At the highest energies, they are supposed to point toward their sources with good accuracy. However, the observations are so rare that testing the distribution of such samples of directional data on the sphere is nontrivial. In this paper, we choose a nonparametric framework that makes weak hypotheses on the alternative", "generated": "For many decades, ultrahigh energy charged particles of unknown origin that can be observed from the ground have been a puzzle for particle physicists and astrophysicists. As an attempt to discriminate among several possible production scenarios, astrophysicists try to test the statistical isotropy of the directions of arrival of"}
{"full_text": "We use Small Angle Neutron Scattering (SANS), with an original analysis method, to obtain both the characteristic sizes and the inner composition of lysozyme-pectin complexes depending on the charge density. Lysozyme is a globular protein and pectin a natural anionic semiflexible polysaccharide with a degree of methylation (DM) 0, 43 and 74. For our experimental conditions (buffer ionic strength I = 2.5 10-2 mol/L and pH between 3 and 7), the electrostatic charge of lysozyme is always positive (from 8 to 17 depending on pH). The pectin charge per elementary chain segment is negative and can be varied from almost zero to one through the change of DM and pH. The weight molar ratio of lysozyme on pectin monomers is kept constant. The ratio of negative charge content per volume to positive charge content per volume, -/+, is varied between 10 and 0.007. On a local scale, for all charged pectins, a correlation peak appears at 0.2 {\\AA}-1 due to proteins clustering inside the complexes. On a large scale, the complexes appear as formed of spherical globules with a well defined radius of 10 to 50 nm, containing a few thousands proteins. The volume fraction Phi of organic matter within the globules derived from SANS absolute cross-sections is around 0.1. The protein stacking, which occurs inside the globules, is enhanced when pectin is more charged, due to pH or DM. The linear charge density of the pectin determines the size of the globules for pectin chains of comparable molecular weights whether it is controlled by the pH or the DM. The radius of the globules varies between 10 nm and 50 nm. In conclusion the structure is driven by electrostatic interactions and not by hydrophobic interactions. The molecular weight also has a large influence on the structure of the complexes since long chains tend to form larger globules. This maybe one reason why DM and pH are not completely equivalent in our system since DM 0 has a short mass, but this may not be the only one. For very low pectin charge (-/+ = 0.07), globules do not appear and the scattering signals a gel-like structure. We did not observe any beads-on-a-string structure.", "prompt": "We use Small Angle Neutron Scattering (SANS), with an original analysis method, to obtain both the characteristic sizes and the inner composition of lysozyme-pectin complexes depending on the charge density. Lysozyme is a globular protein and pectin a natural anionic semiflexible polys", "orig": "accharide with a degree of methylation (DM) 0, 43 and 74. For our experimental conditions (buffer ionic strength I = 2.5 10-2 mol/L and pH between 3 and 7), the electrostatic charge of lysozyme is always positive (", "generated": "We use Small Angle Neutron Scattering (SANS), with an original analysis method, to obtain both the characteristic sizes and the inner composition of lysozyme-pectin complexes depending on the charge density. Lysozyme is a globular protein and pectin a natural anionic semiflexible polymer"}
{"full_text": "The COVID-19 pandemic is causing a major outbreak in more than 150 countries around the world, having a severe impact on the health and life of many people globally. One of the crucial step in fighting COVID-19 is the ability to detect the infected patients early enough, and put them under special care. Detecting this disease from radiography and radiology images is perhaps one of the fastest ways to diagnose the patients. Some of the early studies showed specific abnormalities in the chest radiograms of patients infected with COVID-19. Inspired by earlier works, we study the application of deep learning models to detect COVID-19 patients from their chest radiography images. We first prepare a dataset of 5,000 Chest X-rays from the publicly available datasets. Images exhibiting COVID-19 disease presence were identified by board-certified radiologist. Transfer learning on a subset of 2,000 radiograms was used to train four popular convolutional neural networks, including ResNet18, ResNet50, SqueezeNet, and DenseNet-121, to identify COVID-19 disease in the analyzed chest X-ray images. We evaluated these models on the remaining 3,000 images, and most of these networks achieved a sensitivity rate of 98% ($\\pm$ 3%), while having a specificity rate of around 90%. Besides sensitivity and specificity rates, we also present the receiver operating characteristic (ROC) curve, precision-recall curve, average prediction, and confusion matrix of each model. We also used a technique to generate heatmaps of lung regions potentially infected by COVID-19 and show that the generated heatmaps contain most of the infected areas annotated by our board certified radiologist. While the achieved performance is very encouraging, further analysis is required on a larger set of COVID-19 images, to have a more reliable estimation of accuracy rates. The dataset, model implementations (in PyTorch), and evaluations, are all made publicly available for research community at https://github.com/shervinmin/DeepCovid.git", "prompt": "The COVID-19 pandemic is causing a major outbreak in more than 150 countries around the world, having a severe impact on the health and life of many people globally. One of the crucial step in fighting COVID-19 is the ability to detect the infected patients early enough, and put them under special care. Detect", "orig": "ing this disease from radiography and radiology images is perhaps one of the fastest ways to diagnose the patients. Some of the early studies showed specific abnormalities in the chest radiograms of patients infected with COVID-19. Inspired by earlier works, we study the application of deep learning models to detect COVID-19 patients from their", "generated": "The COVID-19 pandemic is causing a major outbreak in more than 150 countries around the world, having a severe impact on the health and life of many people globally. One of the crucial steps in fighting COVID-19 is the ability to detect the infected patients early enough, and put them under special care. Detect"}
{"full_text": "Implementing circular economy (CE) principles is increasingly recommended as a convenient solution to meet the goals of sustainable development. New tools are required to support practitioners, decision-makers and policy-makers towards more CE practices, as well as to monitor the effects of CE adoption. Worldwide, academics, industrialists and politicians all agree on the need to use CE-related measuring instruments to manage this transition at different systemic levels. In this context, a wide range of circularity indicators (C-indicators) has been developed in recent years. Yet, as there is not one single definition of the CE concept, it is of the utmost importance to know what the available indicators measure in order to use them properly. Indeed, through a systematic literature review-considering both academic and grey literature-55 sets of C-indicators, developed by scholars, consulting companies and governmental agencies, have been identified, encompassing different purposes, scopes, and potential usages. Inspired by existing taxonomies of eco-design tools and sustainability indicators, and in line with the CE characteristics, a classification of indicators aiming to assess, improve, monitor and communicate on the CE performance is proposed and discussed. In the developed taxonomy including 10 categories, C-indicators are differentiated regarding criteria such as the levels of CE implementation (e.g. micro, meso, macro), the CE loops (maintain, reuse, remanufacture, recycle), the performance (intrinsic, impacts), the perspective of circularity (actual, potential) they are taking into account, or their degree of transversality (generic, sector-specific). In addition, the database inventorying the 55 sets of C-indicators is linked to an Excel-based query tool to facilitate the selection of appropriate indicators according to the specific user's needs and requirements. This study enriches the literature by giving a first need-driven taxonomy of C-indicators, which is experienced on several use cases. It provides a synthesis and clarification to the emerging and must-needed research theme of C-indicators, and sheds some light on remaining key challenges like their effective uptake by industry. Eventually, limitations, improvement areas, as well as implications of the proposed taxonomy are intently addressed to guide future research on C-indicators and CE implementation.", "prompt": "Implementing circular economy (CE) principles is increasingly recommended as a convenient solution to meet the goals of sustainable development. New tools are required to support practitioners, decision-makers and policy-makers towards more CE practices, as well as to monitor the effects of CE adoption. Worldwide, academics, industrialists and politicians all agree on", "orig": "the need to use CE-related measuring instruments to manage this transition at different systemic levels. In this context, a wide range of circularity indicators (C-indicators) has been developed in recent years. Yet, as there is not one single definition of the CE concept, it is of the utmost importance to know what the", "generated": "Implementing circular economy (CE) principles is increasingly recommended as a convenient solution to meet the goals of sustainable development. New tools are required to support practitioners, decision-makers and policy-makers towards more CE practices, as well as to monitor the effects of CE adoption. Worldwide, academics, industrialists and politicians all agree on"}
{"full_text": "Graphs and various graph-like combinatorial structures, such as preorders and hypergraphs, are ubiquitous in programming. This paper focuses on representing graphs in a purely functional programming language like Haskell. There are several existing approaches; one of the most recently developed ones is the \"algebraic graphs\" approach (2017). It uses an algebraic data type to represent graphs and has attracted users, including from industry, due to its emphasis on equational reasoning and making a common class of bugs impossible by eliminating internal invariants. The previous formulation of algebraic graphs did not support edge labels, which was a serious practical limitation. In this paper, we redesign the main algebraic data type and remove this limitation. We follow a fairly standard approach of parameterising a data structure with a semiring of edge labels. The new formulation is both more general and simpler: the two operations for composing graphs used in the previous work can now be obtained from a single operation by fixing the semiring parameter to zero and one, respectively. By instantiating the new data type with different semirings, and working out laws for interpreting the resulting expression trees, we discover an unusual algebraic structure, which we call \"united monoids\", that is, a pair of monoids whose unit elements coincide. We believe that it is worth studying united monoids in their full generality, going beyond the graphs which prompted their discovery. To that end, we characterise united monoids with a minimal set of axioms, prove a few basic theorems, and discuss several notable examples. We validate the presented approach by implementing it in the open-source *algebraic-graphs* library. Our theoretical contributions are supported by proofs that are included in the paper and have also been machine-checked in Agda. By extending algebraic graphs with support for edge labels, we make them suitable for a much larger class of possible applications. By studying united monoids, we provide a theoretical foundation for further research in this area.", "prompt": "Graphs and various graph-like combinatorial structures, such as preorders and hypergraphs, are ubiquitous in programming. This paper focuses on representing graphs in a purely functional programming language like Haskell. There are several existing approaches; one of the most recently developed ones is the \"algebraic graphs\" approach (201", "orig": "7). It uses an algebraic data type to represent graphs and has attracted users, including from industry, due to its emphasis on equational reasoning and making a common class of bugs impossible by eliminating internal invariants. The previous formulation of algebraic graphs did not support edge labels, which was a serious practical limitation. In", "generated": "Graphs and various graph-like combinatorial structures, such as preorders and hypergraphs, are ubiquitous in programming. This paper focuses on representing graphs in a purely functional programming language like Haskell. There are several existing approaches; one of the most recently developed ones is the \"algebraic graphs\" approach (201"}
{"full_text": "Non-Gaussian outcomes are often modeled using members of the so-called exponential family. Notorious members are the Bernoulli model for binary data, leading to logistic regression, and the Poisson model for count data, leading to Poisson regression. Two of the main reasons for extending this family are (1) the occurrence of overdispersion, meaning that the variability in the data is not adequately described by the models, which often exhibit a prescribed mean--variance link, and (2) the accommodation of hierarchical structure in the data, stemming from clustering in the data which, in turn, may result from repeatedly measuring the outcome, for various members of the same family, etc. The first issue is dealt with through a variety of overdispersion models, such as, for example, the beta-binomial model for grouped binary data and the negative-binomial model for counts. Clustering is often accommodated through the inclusion of random subject-specific effects. Though not always, one conventionally assumes such random effects to be normally distributed. While both of these phenomena may occur simultaneously, models combining them are uncommon. This paper proposes a broad class of generalized linear models accommodating overdispersion and clustering through two separate sets of random effects. We place particular emphasis on so-called conjugate random effects at the level of the mean for the first aspect and normal random effects embedded within the linear predictor for the second aspect, even though our family is more general. The binary, count and time-to-event cases are given particular emphasis. Apart from model formulation, we present an overview of estimation methods, and then settle for maximum likelihood estimation with analytic--numerical integration. Implications for the derivation of marginal correlations functions are discussed. The methodology is applied to data from a study in epileptic seizures, a clinical trial in toenail infection named onychomycosis and survival data in children with asthma.", "prompt": "Non-Gaussian outcomes are often modeled using members of the so-called exponential family. Notorious members are the Bernoulli model for binary data, leading to logistic regression, and the Poisson model for count data, leading to Poisson regression. Two of the main reasons for extending this family are (1) the", "orig": "occurrence of overdispersion, meaning that the variability in the data is not adequately described by the models, which often exhibit a prescribed mean--variance link, and (2) the accommodation of hierarchical structure in the data, stemming from clustering in the data which, in turn, may result from repeatedly measuring the outcome,", "generated": "Non-Gaussian outcomes are often modeled using members of the so-called exponential family. Non-Gaussian outcomes are often modeled using members of the so-called exponential family. Non-Gaussian outcomes are often modeled using members of the so-called exponential family. Non-Gaussian outcomes are often modeled using members of the so-called exponential family"}
{"full_text": "Context: Container data types are ubiquitous in computer programming, enabling developers to efficiently store and process collections of data with an easy-to-use programming interface. Many programming languages offer a variety of container implementations in their standard libraries based on data structures offering different capabilities and performance characteristics. Inquiry: Choosing the *best* container for an application is not always straightforward, as performance characteristics can change drastically in different scenarios, and as real-world performance is not always correlated to theoretical complexity. Approach: We present Primrose, a language-agnostic tool for selecting the best performing valid container implementation from a set of container data types that satisfy *properties* given by application developers. Primrose automatically selects the set of valid container implementations for which the *library specifications*, written by the developers of container libraries, satisfies the specified properties. Finally, Primrose ranks the valid library implementations based on their runtime performance. Knowledge: With Primrose, application developers can specify the expected behaviour of a container as a type refinement with *semantic properties*, e.g., if the container should only contain unique values (such as a `set`) or should satisfy the LIFO property of a `stack`. Semantic properties nicely complement *syntactic properties* (i.e., traits, interfaces, or type classes), together allowing developers to specify a container's programming interface *and* behaviour without committing to a concrete implementation. Grounding: We present our prototype implementation of Primrose that preprocesses annotated Rust code, selects valid container implementations and ranks them on their performance. The design of Primrose is, however, language-agnostic, and is easy to integrate into other programming languages that support container data types and traits, interfaces, or type classes. Our implementation encodes properties and library specifications into verification conditions in Rosette, an interface for SMT solvers, which determines the set of valid container implementations. We evaluate Primrose by specifying several container implementations, and measuring the time taken to select valid implementations for various combinations of properties with the solver. We automatically validate that container implementations conform to their library specifications via property-based testing. Importance: This work provides a novel approach to bring abstract modelling and specification of container types directly into the programmer's workflow. Instead of selecting concrete container implementations, application programmers can now work on the level of specification, merely stating the behaviours they require from their container types, and the best implementation can be selected automatically.", "prompt": "Context: Container data types are ubiquitous in computer programming, enabling developers to efficiently store and process collections of data with an easy-to-use programming interface. Many programming languages offer a variety of container implementations in their standard libraries based on data structures offering different capabilities and performance characteristics. Inquiry: Choosing the *best* container for an", "orig": "application is not always straightforward, as performance characteristics can change drastically in different scenarios, and as real-world performance is not always correlated to theoretical complexity. Approach: We present Primrose, a language-agnostic tool for selecting the best performing valid container implementation from a set of container data types that satisfy *properties* given by application", "generated": "Container data types are ubiquitous in computer programming, enabling developers to efficiently store and process collections of data with an easy-to-use programming interface. Many programming languages offer a variety of container implementations in their standard libraries based on data structures offering different capabilities and performance characteristics.\n\nChoosing the *best* container for a specific task can be"}
{"full_text": "In this study, we investigate porosity evolution through the sedimentary input section of the North Sumatra Subduction zone by quantifying interstitial porosity, bound water content and fracture porosity based on IODP Expedition 362 data and post-cruise chemical analyses. During IODP Expedition 362, total porosity of the sedimentary section entering the North Sumatra subduction zone was measured. This total porosity is derived from the total water content of core samples thus including pore water and water bound to hydrous minerals like smectite. Clay mineral composition varies over the sedimentary section and is mainly kaolinite/illite in the Nicobar Fan units and smectite/illite in the prefan pelagic unit below. The prefan pelagic unit shows anomalously high total porosity values and is stratigraphically correlated to a high amplitude negative polarity (HANP) seismic reflector located landward. This HANP reflector has been previously interpreted as a porous fluid-rich layer where the d{\\'e}collement may develop along parts of the margin as a consequence of pore pressure buildup. We estimate clay bound water content from Cation Exchange Capacity (CEC) which gives information about the smectite/illite composition and soluble chloride content data. Interstitial porosity corresponds to onboard total porosity corrected from clay bound water and is more relevant in terms of sediment compaction state and fluid flow properties. Interstitial porosity versus vertical effective stress curve shows no evidence of undercompaction and suggests that the input section 2 has been experiencing normal consolidation due to high sediment accumulation rate. The porosity anomaly observed in the prefan pelagic unit results from the local occurrence of water-bearing minerals like smectite rather than excess pore pressure, which might, however, buildup more landward in the basin. We also estimate fracture porosity using a resistivity model for shales used in previous works based on wireline resistivity log and show that fracture porosity yields 4-6% in damaged parts of the sedimentary section investigated.", "prompt": "In this study, we investigate porosity evolution through the sedimentary input section of the North Sumatra Subduction zone by quantifying interstitial porosity, bound water content and fracture porosity based on IODP Expedition 362 data and post-cruise chemical analyses. During IODP Expedition 362, total", "orig": "porosity of the sedimentary section entering the North Sumatra subduction zone was measured. This total porosity is derived from the total water content of core samples thus including pore water and water bound to hydrous minerals like smectite. Clay mineral composition varies over the sedimentary section and is mainly kaolinite", "generated": "In this study, we investigate porosity evolution through the sedimentary input section of the North Sumatra Subduction zone by quantifying interstitial porosity, bound water content and fracture porosity based on IODP Expedition 362 data and post-cruise chemical analyses. During IODP Expedition 362, total"}
{"full_text": "CANVAS caused by RFC1 biallelic expansions is a major cause of inherited sensory neuronopathy. Detection of RFC1 expansion is challenging and CANVAS can be associated with atypical features. We clinically and genetically characterized 50 patients, selected based on the presence of sensory neuronopathy confirmed by EMG. We screened RFC1 expansion by PCR, repeat-primed PCR, and Southern blotting of long-range PCR products, a newly developed method. Neuropathological characterization was performed on the brain and spinal cord of one patient. Most patients (88%) carried a biallelic (AAGGG)n expansion in RFC1. In addition to the core CANVAS phenotype (sensory neuronopathy, cerebellar syndrome, and vestibular impairment), we observed chronic cough (97%), oculomotor signs (85%), motor neuron involvement (55%), dysautonomia (50%), and parkinsonism (10%). Motor neuron involvement was found for 24 of 38 patients (63.1%). First motor neuron signs, such as brisk reflexes, extensor plantar responses, and/or spasticity, were present in 29% of patients, second motor neuron signs, such as fasciculations, wasting, weakness, or a neurogenic pattern on EMG in 18%, and both in 16%. Mixed motor and sensory neuronopathy was observed in 19% of patients. Among six non-RFC1 patients, one carried a heterozygous AAGGG expansion and a pathogenic variant in GRM1. Neuropathological examination of one RFC1 patient with an enriched phenotype, including parkinsonism, dysautonomia, and cognitive decline, showed posterior column and lumbar posterior root atrophy. Degeneration of the vestibulospinal and spinocerebellar tracts was mild. We observed marked astrocytic gliosis and axonal swelling of the synapse between first and second motor neurons in the anterior horn at the lumbar level. The cerebellum showed mild depletion of Purkinje cells, with empty baskets, torpedoes, and astrogliosis characterized by a disorganization of the Bergmann's radial glia. We found neuronal loss in the vagal nucleus. The pars compacta of the substantia nigra was depleted, with widespread Lewy bodies in the locus coeruleus, substantia nigra, hippocampus, entorhinal cortex, and amygdala. We propose new guidelines for the screening of RFC1 expansion, considering different expansion motifs. Here, we developed a new method to more easily detect pathogenic RFC1 expansions. We report frequent motor neuron involvement and different neuronopathy subtypes. Parkinsonism was more prevalent in this cohort than in the general population, 10% versus the expected 1% (p < .001). We describe, for the first time, the spinal cord pathology in CANVAS, showing the alteration of posterior columns and roots, astrocytic gliosis and axonal swelling, suggesting motor neuron synaptic dysfunction.", "prompt": "CANVAS caused by RFC1 biallelic expansions is a major cause of inherited sensory neuronopathy. Detection of RFC1 expansion is challenging and CANVAS can be associated with atypical features. We clinically and genetically characterized 50 patients, selected based on the presence of sensory neuronopathy confirmed by EMG. We", "orig": "screened RFC1 expansion by PCR, repeat-primed PCR, and Southern blotting of long-range PCR products, a newly developed method. Neuropathological characterization was performed on the brain and spinal cord of one patient. Most patients (88%) carried a biallelic (AAGGG)n expansion in RFC", "generated": "CANVAS caused by RFC1 biallelic expansions is a major cause of inherited sensory neuronopathy. Detection of RFC1 expansion is challenging and CANVAS can be associated with atypical features. We clinically and genetically characterized 50 patients, selected based on the presence of sensory neuronopathy confirmed by EMG. We"}
{"full_text": "The finance industry is producing an increasing amount of datasets that investment professionals can consider to be influential on the price of financial assets. These datasets were initially mainly limited to exchange data, namely price, capitalization and volume. Their coverage has now considerably expanded to include, for example, macroeconomic data, supply and demand of commodities, balance sheet data and more recently extra-financial data such as ESG scores. This broadening of the factors retained as influential constitutes a serious challenge for statistical modeling. Indeed, the instability of the correlations between these factors makes it practically impossible to identify the joint laws needed to construct scenarios. Fortunately, spectacular advances in Deep Learning field in recent years have given rise to GANs. GANs are a type of generative machine learning models that produce new data samples with the same characteristics as a training data distribution in an unsupervised way, avoiding data assumptions and human induced biases. In this work, we are exploring the use of GANs for synthetic financial scenarios generation. This pilot study is the result of a collaboration between Fujitsu and Advestis and it will be followed by a thorough exploration of the use cases that can benefit from the proposed solution. We propose a GANs-based algorithm that allows the replication of multivariate data representing several properties (including, but not limited to, price, market capitalization, ESG score, controversy score,. . .) of a set of stocks. This approach differs from examples in the financial literature, which are mainly focused on the reproduction of temporal asset price scenarios. We also propose several metrics to evaluate the quality of the data generated by the GANs. This approach is well fit for the generation of scenarios, the time direction simply arising as a subsequent (eventually conditioned) generation of data points drawn from the learned distribution. Our method will allow to simulate high dimensional scenarios (compared to $\\lesssim10$ features currently employed in most recent use cases) where network complexity is reduced thanks to a wisely performed feature engineering and selection. Complete results will be presented in a forthcoming study.", "prompt": "The finance industry is producing an increasing amount of datasets that investment professionals can consider to be influential on the price of financial assets. These datasets were initially mainly limited to exchange data, namely price, capitalization and volume. Their coverage has now considerably expanded to include, for example, macroeconomic data, supply and demand of", "orig": "commodities, balance sheet data and more recently extra-financial data such as ESG scores. This broadening of the factors retained as influential constitutes a serious challenge for statistical modeling. Indeed, the instability of the correlations between these factors makes it practically impossible to identify the joint laws needed to construct scenarios. Fortunately, spectacular advances in", "generated": "The finance industry is producing an increasing amount of datasets that investment professionals can consider to be influential on the price of financial assets. These datasets were initially mainly limited to exchange data, namely price, capitalization and volume. Their coverage has now considerably expanded to include, for example, macroeconomic data, supply and demand of"}
{"full_text": "Context: The identification of long-gamma-ray-bursts (LGRBs) is still uncertain, although the collapsar engine of fast-rotating massive stars is gaining a strong consensus. Aims: We propose that low-metallicity Be and Oe stars, which are massive fast rotators, as potential LGRBs progenitors. Methods: We checked this hypothesis by 1) testing the global specific angular momentum of Oe/Be stars in the ZAMS with the SMC metallicity, 2) comparing the ZAMS ($\\Omega/\\Omega_{\\rm c},M/M_{\\odot}$) parameters of these stars with the area predicted theoretically for progenitors with metallicity $Z=0.002$, and 3) calculating the expected rate of LGRBs/year/galaxy and comparing them with the observed ones. To this end, we determined the ZAMS linear and angular rotational velocities for SMC Be and Oe stars using the observed vsini parameters, corrected from the underestimation induced by the gravitational darkening effect. Results: The angular velocities of SMC Oe/Be stars are on average $<\\Omega/\\Omega_{\\rm c}>=0.95$ in the ZAMS. These velocities are in the area theoretically predicted for the LGRBs progenitors. We estimated the yearly rate per galaxy of LGRBs and the number of LGRBs produced in the local Universe up to z=0.2. We have considered that the mass range of LGRB progenitors corresponds to stars hotter than spectral types B0-B1 and used individual beaming angles from 5 to 15\\degr. We thus obtain $R^{\\rm pred}_{\\rm LGRB}\\sim10^{-7}$ to $\\sim10^{-6}$ LGRBs/year/galaxy, which represents on average 2 to 14 LGRB predicted events in the local Universe during the past 11 years. The predicted rates could widely surpass the observed ones [(0.2-3)$\\times10^{-7}$ LGRBs/year/galaxy; 8 LGRBs observed in the local Universe during the last 11 years] if the stellar counts were made from the spectral type B1-B2, in accordance with the expected apparent spectral types of the appropriate massive fast rotators. Conclusion: We conclude that the massive Be/Oe stars with SMC metallicity could be LGRBs progenitors. Nevertheless, other SMC O/B stars without emission lines, which have high enough specific angular momentum, can enhance the predicted $R_{\\rm LGRB}$ rate.", "prompt": "Context: The identification of long-gamma-ray-bursts (LGRBs) is still uncertain, although the collapsar engine of fast-rotating massive stars is gaining a strong consensus. Aims: We propose that low-metallicity Be and Oe stars, which are massive fast rotators, as potential", "orig": "LGRBs progenitors. Methods: We checked this hypothesis by 1) testing the global specific angular momentum of Oe/Be stars in the ZAMS with the SMC metallicity, 2) comparing the ZAMS ($\\Omega/\\Omega_{\\rm c},M/M_{\\odot}$)", "generated": "The identification of long-gamma-ray-bursts (LGRBs) is still uncertain, although the collapsar engine of fast-rotating massive stars is gaining a strong consensus. Aims: We propose that low-metallicity Be and Oe stars, which are massive fast rotators, as potential candidates to"}
{"full_text": "This study is about the properties of the sets of objects associated in a structure resulting from multiple-processes involving chance as are materials whose texture is unordered and random. Being a paper scientist the author refers to the sheet of paper which is a stochastic fibrous set whose porous texture can be considered as an archetype for many natural or artificial human structures. The paper properties are correlated with its texture by taking account the effect of chance occurring during its manufacturing process. The theoretical developments, the formalism and the application methods presented in this study have a general significance beyond the only paper material.A specific property of sets of objects randomly unorderly distributed in space is their interfaces orientation distribution. This distribution is usually obtained by the analysis of images sampled in the object sets. The density of orientation probability of the fibers or of the texture interfaces, weighted by their length or by their area, can be interpreted as the radius of curvature of an outline or of a warped surface which characterizes, from a global and statistical point of view, the texture geometry in two or three dimensions. This figure named by the author the \"equivalent pore\", is with its elliptical shape similar to the one of the mean pore defined by the mean directional chord between the interfaces in the texture. Different methods of \"equivalent pore\" establishing are analyzed : by conformal map of the fiber network or of the texture interfaces, by images stereometric analysis of texture tomographical cuts, by scattering and diffraction of a laser light beam impacting the fibrous texture or the material surface replica, by hard X-ray absorption and phase contrast at the European Synchrotron Radiation Facilities(ESRF), in Grenoble. The \"equivalent pore\" concept allows us to study random unordered sets behavior in strength fields while simplifying this analysis. Thus a phenomenon occurring in a plane set, in two dimensions, can be analyzed on its \"equivalent pore\" linear outline, and a phenomenon which occurs in volume in a three dimensional set can be analyzed on its \"equivalent pore\" warped surface. This concept has been applied for physical, mechanical, optical and ionic conduction properties of materials like papers, boards, felts, nonwoven textiles, polymer foams, metallic alloys with grain joints, geological grounds, and for the surface mapping of natural relief and of materials with different gloss, worn or roughness levels.The ellipse and the ellipso\\\"id, as well as multi-modes compositions of it, are the most appropriate figures to represent the \"equivalent pore\" of materials with a random unordered texture. The fact that a law, which defines the curvature of an elliptic deterministic geometrical configuration, is essential to represent interface orientation allocation of elements whose spatial distribution is probabilistic is a noteworthy fact that makes us wonder. This assertion is corroborated by fluid flow analysis through porous media. The global dissipated energy for fluid flow is distributed along the motion (translation and rotation) and fluid deformation components on the \"equivalent pore\" whose surface is conformal to the texture interfaces tangential space. The porous media being homogenous and the fluid particles indistinguishable each ones from the others, due to permanent stochastic exchanges from one fluid volume element into another, we conclude that their motion quantification is invariant on each point of the \"equivalent pore\" surface. This quantification can be represented by a group of elliptical cylinders.The \"equivalent pore\" and cylinders group surface intersections define the fluid particle probabilistic paths in the porous media. One singular geometrical configuration of the elliptic cylinders group union with the ellipsoidal \"equivalent pore\" fulfills the minimal energy dissipation requirements in the stress field applied to the fluid. The resulting paths are ellipso\\\"ido-cylindrical curves carpeting the ellipso\\\"id by a beam of isoperim\\'etric lacets, in close or open loops depending on the possible pairing off the curves in their nodals and isoclinal tangency points. The flow laminarity and unrotationality are globally established in the porous media at the macroscopic scale, for Reynold's number small values, in agreement with the results found moreover. The so defined ellipso\\\"ido-cylindrical curve is a stationary curve at the average least action meaning, for a punctual mobile or a deformable volume element moving on the surface of the ellipsoid from a nodal spot to the other in an antipodal position. This curve establishes a gauge which characterizes, from a global point of view, the physical space embraced by the fluid during its flow, in dynamical balance conditions compatible with the stress field. The ellipso\\\"ido-cylindrical curve allows to carpet the ellipso\\\"id as a function of one cyclic angular variable, which is a new construction for the ellipso\\\"id. When the set of the objects is isotropic the curve is sphero-cylindrical whose plan mapping permit to decipher the tai-chi figure of the Tao\\\"ist Buddhism philosophy.This study of random unordered object sets, and more specially of fibrous porous media, allows to establish a structural link between their small scale texture stochastic disorder and the harmonious order that emerges from these sets at a larger scale. The largest chance possible, compatible with the stress field which put a strain on these sets, is the necessary and sufficient variable which allows to best globally adjust their behaviors according to the probabilistic and deterministic laws governing their evolution.The vocabulary used in this study is issued from the common language, most of the time adapted to the material class, it is transposable in other fields of interest. The text is completed by notes and a bibliography which refer to the works done following the presented concepts or in relation to this field of studies.", "prompt": "This study is about the properties of the sets of objects associated in a structure resulting from multiple-processes involving chance as are materials whose texture is unordered and random. Being a paper scientist the author refers to the sheet of paper which is a stochastic fibrous set whose porous texture can be considered as an archetype for many natural", "orig": "or artificial human structures. The paper properties are correlated with its texture by taking account the effect of chance occurring during its manufacturing process. The theoretical developments, the formalism and the application methods presented in this study have a general significance beyond the only paper material.A specific property of sets of objects randomly unorderly distributed in space", "generated": "This study is about the properties of the sets of objects associated in a structure resulting from multiple-processes involving chance as are materials whose texture is unordered and random. Being a paper scientist the author refers to the sheet of paper which is a stochastic fibrous set whose porous texture can be considered as an archetype for many natural"}
{"full_text": "Numerical schemes for the solution of the Euler equations have recently been developed, which involve the discretisation of the internal energy equation, with corrective terms to ensure the correct capture of shocks, and, more generally, the consistency in the Lax-Wendroff sense. These schemes may be staggered or colocated, using either struc-tured meshes or general simplicial or tetrahedral/hexahedral meshes. The time discretization is performed by fractional-step algorithms; these may be either based on semi-implicit pressure correction techniques or segregated in such a way that only explicit steps are involved (referred to hereafter as \"explicit\" variants). In order to ensure the positivity of the density, the internal energy and the pressure, the discrete convection operators for the mass and internal energy balance equations are carefully designed; they use an upwind technique with respect to the material velocity only. The construction of the fluxes thus does not need any Rie-mann or approximate Riemann solver, and yields easily implementable algorithms. The stability is obtained without restriction on the time step for the pressure correction scheme and under a CFL-like condition for explicit variants: preservation of the integral of the total energy over the computational domain, and positivity of the density and the internal energy. The semi-implicit first-order upwind scheme satisfies a local discrete entropy inequality. If a MUSCL-like scheme is used in order to limit the scheme diffusion, then a weaker property holds: the entropy inequality is satisfied up to a remainder term which is shown to tend to zero with the space and time steps, if the discrete solution is controlled in L $\\infty$ and BV norms. The explicit upwind variant also satisfies such a weaker property, at the price of an estimate for the velocity which could be derived from the introduction of a new stabilization term in the momentum balance. Still for the explicit scheme, with the above-mentioned MUSCL-like scheme, the same result only holds if the ratio of the time to the space step tends to zero.", "prompt": "Numerical schemes for the solution of the Euler equations have recently been developed, which involve the discretisation of the internal energy equation, with corrective terms to ensure the correct capture of shocks, and, more generally, the consistency in the Lax-Wendroff sense. These schemes may be staggered or colocated", "orig": ", using either struc-tured meshes or general simplicial or tetrahedral/hexahedral meshes. The time discretization is performed by fractional-step algorithms; these may be either based on semi-implicit pressure correction techniques or segregated in such a way that only explicit steps are involved (referred to hereafter", "generated": "Numerical schemes for the solution of the Euler equations have recently been developed, which involve the discretisation of the internal energy equation, with corrective terms to ensure the correct capture of shocks, and, more generally, the consistency in the Lax-Wendroff sense. These schemes may be staggered or colocated"}
{"full_text": "How to efficiently and reliably spread information in a system is one of the most fundamental problems in distributed computing. Recently, inspired by biological scenarios, several works focused on identifying the minimal communication resources necessary to spread information under faulty conditions. Here we study the self-stabilizing bit-dissemination problem, introduced by Boczkowski, Korman, and Natale in [SODA 2017]. The problem considers a fully-connected network of n agents, with a binary world of opinions, one of which is called correct. At any given time, each agent holds an opinion bit as its public output. The population contains a source agent which knows which opinion is correct. This agent adopts the correct opinion and remains with it throughout the execution. We consider the basic PULL model of communication, in which each agent observes relatively few randomly chosen agents in each round. The goal of the non-source agents is to quickly converge on the correct opinion, despite having an arbitrary initial configuration, i.e., in a self-stabilizing manner. Once the population converges on the correct opinion, it should remain with it forever. Motivated by biological scenarios in which animals observe and react to the behavior of others, we focus on the extremely constrained model of passive communication, which assumes that when observing another agent the only information that can be extracted is the opinion bit of that agent. We prove that this problem can be solved in a poly-logarithmic in n number of rounds with high probability, while sampling a logarithmic number of agents at each round. Previous works solved this problem faster and using fewer samples, but they did that by decoupling the messages sent by agents from their output opinion, and hence do not fit the framework of passive communication. Moreover, these works use complex recursive algorithms with refined clocks that are unlikely to be used by biological entities. In contrast, our proposed algorithm has a natural appeal as it is based on letting agents estimate the current tendency direction of the dynamics, and then adapt to the emerging trend.", "prompt": "How to efficiently and reliably spread information in a system is one of the most fundamental problems in distributed computing. Recently, inspired by biological scenarios, several works focused on identifying the minimal communication resources necessary to spread information under faulty conditions. Here we study the self-stabilizing bit-dissemination problem, introduced by Boc", "orig": "zkowski, Korman, and Natale in [SODA 2017]. The problem considers a fully-connected network of n agents, with a binary world of opinions, one of which is called correct. At any given time, each agent holds an opinion bit as its public output. The population contains a", "generated": "How to efficiently and reliably spread information in a system is one of the most fundamental problems in distributed computing. Recently, inspired by biological scenarios, several works focused on identifying the minimal communication resources necessary to spread information under faulty conditions. Here we study the self-stabilizing bit-dissemination problem, introduced by Boc"}
{"full_text": "Structure discovery in graphical models is the determination of the topology of a graph that encodes conditional independence properties of the joint distribution of all variables in the model. For some class of probability distributions, an edge between two variables is present if and only if the corresponding entry in the precision matrix is non-zero. For a finite sample estimate of the precision matrix, entries close to zero may be due to low sample effects, or due to an actual association between variables; these two cases are not readily distinguishable. %Fisher provided a hypothesis test based on a parametric approximation to the distribution of an entry in the precision matrix of a Gaussian distribution, but this may not provide valid upper bounds on $p$-values for non-Gaussian distributions. Many related works on this topic consider potentially restrictive distributional or sparsity assumptions that may not apply to a data sample of interest, and direct estimation of the uncertainty of an estimate of the precision matrix for general distributions remains challenging. Consequently, we make use of results for $U$-statistics and apply them to the covariance matrix. By probabilistically bounding the distortion of the covariance matrix, we can apply Weyl's theorem to bound the distortion of the precision matrix, yielding a conservative, but sound test threshold for a much wider class of distributions than considered in previous works. The resulting test enables one to answer with statistical significance whether an edge is present in the graph, and convergence results are known for a wide range of distributions. The computational complexities is linear in the sample size enabling the application of the test to large data samples for which computation time becomes a limiting factor. We experimentally validate the correctness and scalability of the test on multivariate distributions for which the distributional assumptions of competing tests result in underestimates of the false positive ratio. By contrast, the proposed test remains sound, promising to be a useful tool for hypothesis testing for diverse real-world problems.", "prompt": "Structure discovery in graphical models is the determination of the topology of a graph that encodes conditional independence properties of the joint distribution of all variables in the model. For some class of probability distributions, an edge between two variables is present if and only if the corresponding entry in the precision matrix is non-zero. For a finite", "orig": "sample estimate of the precision matrix, entries close to zero may be due to low sample effects, or due to an actual association between variables; these two cases are not readily distinguishable. %Fisher provided a hypothesis test based on a parametric approximation to the distribution of an entry in the precision matrix of a Gaussian distribution", "generated": "Structure discovery in graphical models is the determination of the topology of a graph that encodes conditional independence properties of the joint distribution of all variables in the model. For some class of probability distributions, an edge between two variables is present if and only if the corresponding entry in the precision matrix is non-zero. For a finite"}
{"full_text": "Device-to-Device (D2D) communication is expected to be a key feature supported by 5G networks, especially due to the proliferation of Mobile Edge Computing (MEC), which has a prominent role in reducing network stress by shifting computational tasks from the Internet to the mobile edge. Apart from being part of MEC, D2D can extend cellular coverage allowing users to communicate directly when telecommunication infrastructure is highly congested or absent. This significant departure from the typical cellular paradigm imposes the need for decentralised network routing protocols. Moreover, enhanced capabilities of mobile devices and D2D networking will likely result in proliferation of new malware types and epidemics. Although the literature is rich in terms of D2D routing protocols that enhance quality-of-service and energy consumption, they provide only basic security support, e.g., in the form of encryption. Routing decisions can, however, contribute to collaborative detection of mobile malware by leveraging different kinds of anti-malware software installed on mobile devices. Benefiting from the cooperative nature of D2D communications, devices can rely on each other's contributions to detect malware. The impact of our work is geared towards having more malware-free D2D networks. To achieve this, we designed and implemented a novel routing protocol for D2D communications that optimises routing decisions for explicitly improving malware detection. The protocol identifies optimal network paths, in terms of malware mitigation and energy spent for malware detection, based on a game theoretic model. Diverse capabilities of network devices running different types of anti-malware software and their potential for inspecting messages relayed towards an intended destination device are leveraged using game theoretic tools. An optimality analysis of both Nash and Stackelberg security games is undertaken, including both zero and non-zero sum variants, and the Defender's equilibrium strategies. By undertaking network simulations, theoretical results obtained are illustrated through randomly generated network scenarios showing how our protocol outperforms conventional routing protocols, in terms of expected payoff, which consists of: security damage in inflicted by malware and malware detection cost.", "prompt": "Device-to-Device (D2D) communication is expected to be a key feature supported by 5G networks, especially due to the proliferation of Mobile Edge Computing (MEC), which has a prominent role in reducing network stress by shifting computational tasks from the Internet to the mobile edge. Apart from being part of", "orig": "MEC, D2D can extend cellular coverage allowing users to communicate directly when telecommunication infrastructure is highly congested or absent. This significant departure from the typical cellular paradigm imposes the need for decentralised network routing protocols. Moreover, enhanced capabilities of mobile devices and D2D networking will likely result in proliferation of new malware", "generated": "Device-to-Device (D2D) communication is expected to be a key feature supported by 5G networks, especially due to the proliferation of Mobile Edge Computing (MEC), which has a prominent role in reducing network stress by shifting computational tasks from the Internet to the mobile edge. Apart from being part of"}
{"full_text": "In planetary fluid cores, the density depends on temperature and chemical composition, which diffuse at very different rates. This leads to various instabilities, bearing the name of double-diffusive convection. We investigate rotating double-diffusive convection (RDDC) in fluid spheres. We use the Boussinesq approximation with homogeneous internal thermal and compositional source terms. We focus on the finger regime, in which the thermal gradient is stabilising whereas the compositional one is destabilising. First, we perform a global linear stability analysis in spheres. The critical Rayleigh numbers drastically drop for stably stratified fluids, yielding large-scale convective motions where local analyses predict stability. We evidence the inviscid nature of this large-scale double-diffusive instability, enabling the determination of the marginal stability curve at realistic planetary regimes. In particular, we show that in stably stratified spheres, the Rayleigh numbers $Ra$ at the onset evolve like $Ra \\sim Ek^{-1}$, where $Ek$ is the Ekman number. This differs from rotating convection in unstably stratified spheres, for which $Ra \\sim Ek^{-4/3}$. The domain of existence of inviscid convection thus increases as $Ek^{-1/3}$. Second, we perform nonlinear simulations. We find a transition between two regimes of RDDC, controlled by the strength of the stratification. Furthermore, far from the RDDC onset, we find a dominating equatorially anti-symmetric, large-scale zonal flow slightly above the associated linear onset. Unexpectedly, a purely linear mechanism can explain this phenomenon, even far from the instability onset, yielding a symmetry breaking of the nonlinear flow at saturation. For even stronger stable stratification, the flow becomes mainly equatorially-symmetric and intense zonal jets develop. Finally, we apply our results to the early Earth core. Double diffusion can reduce the critical Rayleigh number by four decades for realistic core conditions. We suggest that the early Earth core was prone to turbulent RDDC, with large-scale zonal flows.", "prompt": "In planetary fluid cores, the density depends on temperature and chemical composition, which diffuse at very different rates. This leads to various instabilities, bearing the name of double-diffusive convection. We investigate rotating double-diffusive convection (RDDC) in fluid spheres. We use the Boussinesq", "orig": "approximation with homogeneous internal thermal and compositional source terms. We focus on the finger regime, in which the thermal gradient is stabilising whereas the compositional one is destabilising. First, we perform a global linear stability analysis in spheres. The critical Rayleigh numbers drastically drop for stably stratified fluids, yielding large-scale", "generated": "In planetary fluid cores, the density depends on temperature and chemical composition, which diffuse at very different rates. This leads to various instabilities, bearing the name of double-diffusive convection. We investigate rotating double-diffusive convection (RDDC) in fluid spheres. We use the Boussinesq"}
{"full_text": "Emerging computer architectures will feature drastically decreased flops/byte (ratio of peak processing rate to memory bandwidth) as highlighted by recent studies on Exascale architectural trends. Further, flops are getting cheaper while the energy cost of data movement is increasingly dominant. The understanding and characterization of data locality properties of computations is critical in order to guide efforts to enhance data locality. Reuse distance analysis of memory address traces is a valuable tool to perform data locality characterization of programs. A single reuse distance analysis can be used to estimate the number of cache misses in a fully associative LRU cache of any size, thereby providing estimates on the minimum bandwidth requirements at different levels of the memory hierarchy to avoid being bandwidth bound. However, such an analysis only holds for the particular execution order that produced the trace. It cannot estimate potential improvement in data locality through dependence preserving transformations that change the execution schedule of the operations in the computation. In this article, we develop a novel dynamic analysis approach to characterize the inherent locality properties of a computation and thereby assess the potential for data locality enhancement via dependence preserving transformations. The execution trace of a code is analyzed to extract a computational directed acyclic graph (CDAG) of the data dependences. The CDAG is then partitioned into convex subsets, and the convex partitioning is used to reorder the operations in the execution trace to enhance data locality. The approach enables us to go beyond reuse distance analysis of a single specific order of execution of the operations of a computation in characterization of its data locality properties. It can serve a valuable role in identifying promising code regions for manual transformation, as well as assessing the effectiveness of compiler transformations for data locality enhancement. We demonstrate the effectiveness of the approach using a number of benchmarks, including case studies where the potential shown by the analysis is exploited to achieve lower data movement costs and better performance.", "prompt": "Emerging computer architectures will feature drastically decreased flops/byte (ratio of peak processing rate to memory bandwidth) as highlighted by recent studies on Exascale architectural trends. Further, flops are getting cheaper while the energy cost of data movement is increasingly dominant. The understanding and characterization of data locality properties of computations is", "orig": "critical in order to guide efforts to enhance data locality. Reuse distance analysis of memory address traces is a valuable tool to perform data locality characterization of programs. A single reuse distance analysis can be used to estimate the number of cache misses in a fully associative LRU cache of any size, thereby providing estimates on the minimum bandwidth", "generated": "Emerging computer architectures will feature drastically decreased flops/byte (ratio of peak processing rate to memory bandwidth) as highlighted by recent studies on Exascale architectural trends. Further, flops are getting cheaper while the energy cost of data movement is increasingly dominant. The understanding and characterization of data locality properties of computations is"}
{"full_text": "The practice of unit testing enables programmers to obtain automated feedback on whether a currently edited program is consistent with the expectations specified in test cases. Feedback is most valuable when it happens immediately, as defects can be corrected instantly before they become harder to fix. With growing and longer running test suites, however, feedback is obtained less frequently and lags behind program changes. The objective of test prioritization is to rank tests so that defects, if present, are found as early as possible or with the least costs. While there are numerous static approaches that output a ranking of tests solely based on the current version of a program, we focus on change-based test prioritization, which recommends tests that likely fail in response to the most recent program change. The canonical approach relies on coverage data and prioritizes tests that cover the changed region, but obtaining and updating coverage data is costly. More recently, information retrieval techniques that exploit overlapping vocabulary between change and tests have proven to be powerful, yet lightweight. In this work, we demonstrate the capabilities of information retrieval for prioritizing tests in dynamic programming languages using Python as example. We discuss and measure previously understudied variation points, including how contextual information around a program change can be used, and design alternatives to the widespread \\emph{TF-IDF} retrieval model tailored to retrieving failing tests. To obtain program changes with associated test failures, we designed a tool that generates a large set of faulty changes from version history along with their test results. Using this data set, we compared existing and new lexical prioritization strategies using four open-source Python projects, showing large improvements over untreated and random test orders and results consistent with related work in statically typed languages. We conclude that lightweight IR-based prioritization strategies are effective tools to predict failing tests in the absence of coverage data or when static analysis is intractable like in dynamic languages. This knowledge can benefit both individual programmers that rely on fast feedback, as well as operators of continuous integration infrastructure, where resources can be freed sooner by detecting defects earlier in the build cycle.", "prompt": "The practice of unit testing enables programmers to obtain automated feedback on whether a currently edited program is consistent with the expectations specified in test cases. Feedback is most valuable when it happens immediately, as defects can be corrected instantly before they become harder to fix. With growing and longer running test suites, however, feedback is obtained less", "orig": "frequently and lags behind program changes. The objective of test prioritization is to rank tests so that defects, if present, are found as early as possible or with the least costs. While there are numerous static approaches that output a ranking of tests solely based on the current version of a program, we focus on change-based", "generated": "The practice of unit testing enables programmers to obtain automated feedback on whether a currently edited program is consistent with the expectations specified in test cases. Feedback is most valuable when it happens immediately, as defects can be corrected instantly before they become harder to fix. With growing and longer running test suites, however, feedback is obtained less"}
{"full_text": "Introduction: Nottingham grading system is a major prognostic factor for invasive breast carcinoma (IBC). Its determination requires the evaluation of the mitotic score (MS) which is subject to low intra- and inter-observer reproducibility. The MS shall be performed in the most proliferative area of the tumor, which determination is hard but critical. Artificial intelligence based tools could help pathologists to detect mitosis on whole slide images (WSI). Objective: The aim of this study was to evaluate the contribution of a mitosis detection algorithms pecifically developed to assist the pathologist during the evaluation of the MS on WSI. Methods: Algorithmic mitosis detection is a two-step process: first the algorithm detects candidate objects resembling mitosis, then the selection is refined by a classifier. The densest mitoticregions are shown to the pathologist, then he can establish the MS with algorithm results. For this study, three expert pathologists have determined a consensual ground truth for MS on fifty WSI of IBC. Those slides were also submitted to two readers pathologists who evaluated the MS of each slide twice, with and without the assistance of the algorithm, with a four week wash-out period. Interobserver reproducibility was measured by evaluating the scores obtained with, and without assistance between two readers pathologists and was also measured between each reader pathologist and the expert ground truth to determine the accuracy of the established score. Results:Baseline linearly weighted Cohen's Kappa for interobserver agreement of MS between two readers pathologists was 0.482. Using the algorithm generated mitotic detection in WSI, the agreement score increased to 0.672. Baseline linearly weighted Cohen's Kappa for interobserver agreement of MS between each reader pathologist and expert consensus was 0.378 and 0.457 for pathologist 1 and 2 respectively. Using the algorithm generated mitoticdetection in WSI, the agreement score increased respectively to 0.629 and 0.726. Conclusion:The use of the developed algorithm constitutes a viable approach to assist the pathologist for the evaluation of the MS of IBC on WSI. Its use makes it possible to improve interobserver reproducibility between pathologists and the accuracy of the score established by expert consensus. The use of such a tool constitutes a new approach in the evaluation of the mitoticscore which could lead to an evolution of practices.", "prompt": "Introduction: Nottingham grading system is a major prognostic factor for invasive breast carcinoma (IBC). Its determination requires the evaluation of the mitotic score (MS) which is subject to low intra- and inter-observer reproducibility. The MS shall be performed in the most proliferative area of the tumor, which", "orig": "determination is hard but critical. Artificial intelligence based tools could help pathologists to detect mitosis on whole slide images (WSI). Objective: The aim of this study was to evaluate the contribution of a mitosis detection algorithms pecifically developed to assist the pathologist during the evaluation of the MS on WSI. Methods", "generated": "Introduction: Nottingham grading system is a major prognostic factor for invasive breast carcinoma (IBC). Its determination requires the evaluation of the mitotic score (MS) which is subject to low intra- and inter-observer reproducibility. The MS shall be performed in the most proliferative area of the tumor, which"}
{"full_text": "Shock modelling predicts an electron density enhancement within the magnetic precursor of C-shocks. Previous observations of SiO, H13CO+, HN13C and H13CN toward the young L1448-mm outflow showed an over-excitation of the ion fluid that was attributed to an electron density enhancement in the precursor. We re-visit this interpretation and test if it still holds when we consider different source morphologies and kinetic temperatures for the observed molecules, and also give some insight on the spatial extent of the electron density enhancement around L1448-mm. We estimate the opacities of H13CO+ and HN13C by observing the J=3\\to2 lines of rarer isotopologues to confirm that the emission is optically thin. To model the excitation of the molecules, we use the large velocity gradient (LVG) approximation with updated collisional coefficients to i) re- analyse the observations toward the positions where the over-excitation of H13CO+ has previously been observed [i.e. toward L1448- mm at offsets (0,0) and (0,-10)], and ii) to investigate if the electron density enhancement is still required for the cases of extended and compact emission, and for kinetic temperatures of up to 400 K. We also report several lines of SiO, HN13C and H13CO+ toward new positions around this outflow, to investigate the spatial extent of the over-excitation of the ions in L1448-mm. From the isotopologue observations, we find that the emission of H13CO+ and HN13C from the precursor is optically thin if this emission is extended. Using the new collisional coefficients, an electron density enhancement is still needed to explain the excitation of H13CO+ for extended emission and for gas temperatures of\\le 400 K toward L1448-mm (0,-10), and possibly also toward L1448-mm (0,0). For compact emission the data cannot be fitted. We do not find any evidence for the over-excitation of the ion fluid toward the newly observed positions around L1448-mm. The observed line emission of SiO, H13CO+ and HN13C toward L1448-mm (0,0) and (0,-10) is consistent with an electron density enhancement in the precursor component, if this emission is spatially extended. This is also true for the case of high gas temperatures (\\le400 K) toward the (0,-10) offset. The electron density enhancement seems to be restricted to the southern, redshifted lobe of the L1448-mm outflow. Interferometric images of the line emission of these molecules are needed to confirm the spatial extent of the over-excitation of the ions and thus, of the electron density enhancement in the magnetic precursor of L1448-mm.", "prompt": "Shock modelling predicts an electron density enhancement within the magnetic precursor of C-shocks. Previous observations of SiO, H13CO+, HN13C and H13CN toward the young L1448-mm outflow showed an over-excitation of the ion fluid that was attributed to an electron density enhancement in the precursor", "orig": ". We re-visit this interpretation and test if it still holds when we consider different source morphologies and kinetic temperatures for the observed molecules, and also give some insight on the spatial extent of the electron density enhancement around L1448-mm. We estimate the opacities of H13CO+ and HN13C", "generated": "Shock modelling predicts an electron density enhancement within the magnetic precursor of C-shocks. Previous observations of SiO, H13CO+, HN13C, and H13CN toward the young L1448-mm outflow showed an over-excitation of the ion fluid that was attributed to an electron density enhancement in the"}
{"full_text": "The impact of outliers and anomalies on model estimation and data processing is of paramount importance, as evidenced by the extensive body of research spanning various fields over several decades: thousands of research papers have been published on the subject. As a consequence, numerous reviews, surveys, and textbooks have sought to summarize the existing literature, encompassing a wide range of methods from both the statistical and data mining communities. While these endeavors to organize and summarize the research are invaluable, they face inherent challenges due to the pervasive nature of outliers and anomalies in all data-intensive applications, irrespective of the specific application field or scientific discipline. As a result, the resulting collection of papers remains voluminous and somewhat heterogeneous. To address the need for knowledge organization in this domain, this paper implements the first systematic meta-survey of general surveys and reviews on outlier and anomaly detection. Employing a classical systematic survey approach, the study collects nearly 500 papers using two specialized scientific search engines. From this comprehensive collection, a subset of 56 papers that claim to be general surveys on outlier detection is selected using a snowball search technique to enhance field coverage. A meticulous quality assessment phase further refines the selection to a subset of 25 high-quality general surveys. Using this curated collection, the paper investigates the evolution of the outlier detection field over a 20-year period, revealing emerging themes and methods. Furthermore, an analysis of the surveys sheds light on the survey writing practices adopted by scholars from different communities who have contributed to this field. Finally, the paper delves into several topics where consensus has emerged from the literature. These include taxonomies of outlier types, challenges posed by high-dimensional data, the importance of anomaly scores, the impact of learning conditions, difficulties in benchmarking, and the significance of neural networks. Non-consensual aspects are also discussed, particularly the distinction between local and global outliers and the challenges in organizing detection methods into meaningful taxonomies.", "prompt": "The impact of outliers and anomalies on model estimation and data processing is of paramount importance, as evidenced by the extensive body of research spanning various fields over several decades: thousands of research papers have been published on the subject. As a consequence, numerous reviews, surveys, and textbooks have sought to summarize the existing literature, encompass", "orig": "ing a wide range of methods from both the statistical and data mining communities. While these endeavors to organize and summarize the research are invaluable, they face inherent challenges due to the pervasive nature of outliers and anomalies in all data-intensive applications, irrespective of the specific application field or scientific discipline. As a result, the resulting collection of", "generated": "The impact of outliers and anomalies on model estimation and data processing is of paramount importance, as evidenced by the extensive body of research spanning various fields over several decades: thousands of research papers have been published on the subject. As a consequence, numerous reviews, surveys, and textbooks have sought to summarize the existing literature, encompass"}
{"full_text": "Several multiagent reinforcement learning (MARL) algorithms have been proposed to optimize agents decisions. Due to the complexity of the problem, the majority of the previously developed MARL algorithms assumed agents either had some knowledge of the underlying game (such as Nash equilibria) and/or observed other agents actions and the rewards they received. We introduce a new MARL algorithm called the Weighted Policy Learner (WPL), which allows agents to reach a Nash Equilibrium (NE) in benchmark 2-player-2-action games with minimum knowledge. Using WPL, the only feedback an agent needs is its own local reward (the agent does not observe other agents actions or rewards). Furthermore, WPL does not assume that agents know the underlying game or the corresponding Nash Equilibrium a priori. We experimentally show that our algorithm converges in benchmark two-player-two-action games. We also show that our algorithm converges in the challenging Shapleys game where previous MARL algorithms failed to converge without knowing the underlying game or the NE. Furthermore, we show that WPL outperforms the state-of-the-art algorithms in a more realistic setting of 100 agents interacting and learning concurrently. An important aspect of understanding the behavior of a MARL algorithm is analyzing the dynamics of the algorithm: how the policies of multiple learning agents evolve over time as agents interact with one another. Such an analysis not only verifies whether agents using a given MARL algorithm will eventually converge, but also reveals the behavior of the MARL algorithm prior to convergence. We analyze our algorithm in two-player-two-action games and show that symbolically proving WPLs convergence is difficult, because of the non-linear nature of WPLs dynamics, unlike previous MARL algorithms that had either linear or piece-wise-linear dynamics. Instead, we numerically solve WPLs dynamics differential equations and compare the solution to the dynamics of previous MARL algorithms.", "prompt": "Several multiagent reinforcement learning (MARL) algorithms have been proposed to optimize agents decisions. Due to the complexity of the problem, the majority of the previously developed MARL algorithms assumed agents either had some knowledge of the underlying game (such as Nash equilibria) and/or observed other agents actions and the rewards they", "orig": "received. We introduce a new MARL algorithm called the Weighted Policy Learner (WPL), which allows agents to reach a Nash Equilibrium (NE) in benchmark 2-player-2-action games with minimum knowledge. Using WPL, the only feedback an agent needs is its own local reward (the agent does", "generated": "Several multiagent reinforcement learning (MARL) algorithms have been proposed to optimize agents' decisions. Due to the complexity of the problem, the majority of the previously developed MARL algorithms assumed agents either had some knowledge of the underlying game (such as Nash equilibria) and/or observed other agents' actions and the"}
{"full_text": "Lean processes focus on doing only necessery things in an efficient way. Artificial intelligence and Machine Learning offer new opportunities to optimizing processes. The presented approach demonstrates an improvement of the test process by using Machine Learning as a support tool for test management. The scope is the semi-automation of the selection of regression tests. The proposed lean testing process uses Machine Learning as a supporting machine, while keeping the human test manager in charge of the adequate test case selection. 1 Introduction Many established long running projects and programs are execute regression tests during the release tests. The regression tests are the part of the release test to ensure that functionality from past releases still works fine in the new release. In many projects, a significant part of these regression tests are not automated and therefore executed manually. Manual tests are expensive and time intensive [1], which is why often only a relevant subset of all possible regression tests are executed in order to safe time and money. Depending on the software process, different approaches can be used to identify the right set of regression tests. The source code file level is a frequent entry point for this identification [2]. Advanced approaches combine different file level methods [3]. To handle black-box tests, methods like [4] or [5] can be used for test case prioritiza-tion. To decide which tests can be skipped, a relevance ranking of the tests in a regression test suite is needed. Based on the relevance a test is in or out of the regression test set for a specific release. This decision is a task of the test manager supported by experts. The task can be time-consuming in case of big (often a 4-to 5-digit number) regression test suites because the selection is specific to each release. Trends are going to continuous prioritization [6], which this work wants to support with the presented ML based approach for black box regression test case prioritization. Any regression test selection is made upon release specific changes. Changes can be new or deleted code based on refactoring or implementation of new features. But also changes on externals systems which are connected by interfaces have to be considered", "prompt": "Lean processes focus on doing only necessery things in an efficient way. Artificial intelligence and Machine Learning offer new opportunities to optimizing processes. The presented approach demonstrates an improvement of the test process by using Machine Learning as a support tool for test management. The scope is the semi-automation of the selection of regression tests. The", "orig": "proposed lean testing process uses Machine Learning as a supporting machine, while keeping the human test manager in charge of the adequate test case selection. 1 Introduction Many established long running projects and programs are execute regression tests during the release tests. The regression tests are the part of the release test to ensure that functionality from past releases still", "generated": "Lean processes focus on doing only necessary things in an efficient way. Artificial intelligence and Machine Learning offer new opportunities to optimizing processes. The presented approach demonstrates an improvement of the test process by using Machine Learning as a support tool for test management. The scope is the semi-automation of the selection of regression tests. The semi"}
{"full_text": "The symmetry algebras of certain families of quantum spin chains are considered in detail. The simplest examples possess m states per site (m\\geq2), with nearest-neighbor interactions with U(m) symmetry, under which the sites transform alternately along the chain in the fundamental m and its conjugate representation \\bar{m}. We find that these spin chains, even with {\\em arbitrary} coefficients of these interactions, have a symmetry algebra A_m much larger than U(m), which implies that the energy eigenstates fall into sectors that for open chains (i.e., free boundary conditions) can be labeled by j=0, 1, >..., L, for the 2L-site chain, such that the degeneracies of all eigenvalues in the jth sector are generically the same and increase rapidly with j. For large j, these degeneracies are much larger than those that would be expected from the U(m) symmetry alone. The enlarged symmetry algebra A_m(2L) consists of operators that commute in this space of states with the Temperley-Lieb algebra that is generated by the set of nearest-neighbor interaction terms; A_m(2L) is not a Yangian. There are similar results for supersymmetric chains with gl(m+n|n) symmetry of nearest-neighbor interactions, and a richer representation structure for closed chains (i.e., periodic boundary conditions). The symmetries also apply to the loop models that can be obtained from the spin chains in a spacetime or transfer matrix picture. In the loop language, the symmetries arise because the loops cannot cross. We further define tensor products of representations (for the open chains) by joining chains end to end. The fusion rules for decomposing the tensor product of representations labeled j_1 and j_2 take the same form as the Clebsch-Gordan series for SU(2). This and other structures turn the symmetry algebra \\cA_m into a ribbon Hopf algebra, and we show that this is ``Morita equivalent'' to the quantum group U_q(sl_2) for m=q+q^{-1}. The open-chain results are extended to the cases |m|< 2 for which the algebras are no longer semisimple; these possess continuum limits that are critical (conformal) field theories, or massive perturbations thereof. Such models, for open and closed boundary conditions, arise in connection with disordered fermions, percolation, and polymers (self-avoiding walks), and certain non-linear sigma models, all in two dimensions. A product operation is defined in a related way for the Temperley-Lieb representations also, and the fusion rules for this are related to those for A_m or U_q(sl_2) representations; this is useful for the continuum limits also, as we discuss in a companion paper.", "prompt": "The symmetry algebras of certain families of quantum spin chains are considered in detail. The simplest examples possess m states per site (m\\geq2), with nearest-neighbor interactions with U(m) symmetry, under which the sites transform alternately along the chain in the fundamental m and its conjugate representation \\", "orig": "bar{m}. We find that these spin chains, even with {\\em arbitrary} coefficients of these interactions, have a symmetry algebra A_m much larger than U(m), which implies that the energy eigenstates fall into sectors that for open chains (i.e., free boundary conditions) can be labeled by j=0,", "generated": "The symmetry algebras of certain families of quantum spin chains are considered in detail. The simplest examples possess m states per site (m\\geq2), with nearest-neighbor interactions with U(m) symmetry, under which the sites transform alternately along the chain in the fundamental m and its conjugate representation."}
{"full_text": "During the summer of 1989, the Forecast Systems Laboratory of the National Oceanic and Atmospheric Administration sponsored an evaluation of artificial intelligence-based systems that forecast severe convective storms. The evaluation experiment, called Shootout-89, took place in Boulder, and focussed on storms over the northeastern Colorado foothills and plains (Moninger, et al., 1990). Six systems participated in Shootout-89. These included traditional expert systems, an analogy-based system, and a system developed using methods from the cognitive science/judgment analysis tradition. Each day of the exercise, the systems generated 2 to 9 hour forecasts of the probabilities of occurrence of: non significant weather, significant weather, and severe weather, in each of four regions in northeastern Colorado. A verification coordinator working at the Denver Weather Service Forecast Office gathered ground-truth data from a network of observers. Systems were evaluated on the basis of several measures of forecast skill, and on other metrics such as timeliness, ease of learning, and ease of use. Systems were generally easy to operate, however the various systems required substantially different levels of meteorological expertise on the part of their users--reflecting the various operational environments for which the systems had been designed. Systems varied in their statistical behavior, but on this difficult forecast problem, the systems generally showed a skill approximately equal to that of persistence forecasts and climatological (historical frequency) forecasts. The two systems that appeared best able to discriminate significant from non significant weather events were traditional expert systems. Both of these systems required the operator to make relatively sophisticated meteorological judgments. We are unable, based on only one summer's worth of data, to determine the extent to which the greater skill of the two systems was due to the content of their knowledge bases, or to the subjective judgments of the operator. A follow-on experiment, Shootout-91, is currently being planned. Interested potential participants are encouraged to contact the author at the address above.", "prompt": "During the summer of 1989, the Forecast Systems Laboratory of the National Oceanic and Atmospheric Administration sponsored an evaluation of artificial intelligence-based systems that forecast severe convective storms. The evaluation experiment, called Shootout-89, took place in Boulder, and focussed on storms over the northeastern Colorado foothills and plains", "orig": "(Moninger, et al., 1990). Six systems participated in Shootout-89. These included traditional expert systems, an analogy-based system, and a system developed using methods from the cognitive science/judgment analysis tradition. Each day of the exercise, the systems generated 2 to 9 hour forecasts of", "generated": "During the summer of 1989, the Forecast Systems Laboratory of the National Oceanic and Atmospheric Administration sponsored an evaluation of artificial intelligence-based systems that forecast severe convective storms. The evaluation experiment, called Shootout-89, took place in Boulder, and focused on storms over the northeastern Colorado foothills and plains."}
{"full_text": "Many current large-scale multiagent team implementations can be characterized as following the belief-desire-intention (BDI) paradigm, with explicit representation of team plans. Despite their promise, current BDI team approaches lack tools for quantitative performance analysis under uncertainty. Distributed partially observable Markov decision problems (POMDPs) are well suited for such analysis, but the complexity of finding optimal policies in such models is highly intractable. The key contribution of this article is a hybrid BDI-POMDP approach, where BDI team plans are exploited to improve POMDP tractability and POMDP analysis improves BDI team plan performance. Concretely, we focus on role allocation, a fundamental problem in BDI teams: which agents to allocate to the different roles in the team. The article provides three key contributions. First, we describe a role allocation technique that takes into account future uncertainties in the domain; prior work in multiagent role allocation has failed to address such uncertainties. To that end, we introduce RMTDP (Role-based Markov Team Decision Problem), a new distributed POMDP model for analysis of role allocations. Our technique gains in tractability by significantly curtailing RMTDP policy search; in particular, BDI team plans provide incomplete RMTDP policies, and the RMTDP policy search fills the gaps in such incomplete policies by searching for the best role allocation. Our second key contribution is a novel decomposition technique to further improve RMTDP policy search efficiency. Even though limited to searching role allocations, there are still combinatorially many role allocations, and evaluating each in RMTDP to identify the best is extremely difficult. Our decomposition technique exploits the structure in the BDI team plans to significantly prune the search space of role allocations. Our third key contribution is a significantly faster policy evaluation algorithm suited for our BDI-POMDP hybrid approach. Finally, we also present experimental results from two domains: mission rehearsal simulation and RoboCupRescue disaster rescue simulation.", "prompt": "Many current large-scale multiagent team implementations can be characterized as following the belief-desire-intention (BDI) paradigm, with explicit representation of team plans. Despite their promise, current BDI team approaches lack tools for quantitative performance analysis under uncertainty. Distributed partially observable Markov decision problems (POMDPs)", "orig": "are well suited for such analysis, but the complexity of finding optimal policies in such models is highly intractable. The key contribution of this article is a hybrid BDI-POMDP approach, where BDI team plans are exploited to improve POMDP tractability and POMDP analysis improves BDI team plan performance", "generated": "Many current large-scale multiagent team implementations can be characterized as following the belief-desire-intention (BDI) paradigm, with explicit representation of team plans. Despite their promise, current BDI team approaches lack tools for quantitative performance analysis under uncertainty. Distributed partially observable Markov decision problems (POMDPs)"}
{"full_text": "Core helium burning is the dominant source of energy of extreme horizontal branch stars, as the hydrogen envelope is too small to contribute to the nuclear energy output. The evolution of each mass in the HR diagram occurs along vertical tracks that, when the core helium is consumed, evolve to higher Teff and then to the white dwarf stage. The larger is the mass, the smaller is the Teff of the models, so that the zero age horizontal branch (ZAHB) is \"horizontal\". In this paper we show that, if the helium mass fraction (Y) of the envelope is larger than Y~0.5, the shape of the tracks changes completely: the hydrogen burning becomes efficient again also for very small envelope masses, thanks to the higher molecular weight and to the higher temperatures of the hydrogen shell. The larger is Y, the smaller is the envelope mass that provides strong H-shell burning. These tracks have a curled shape, are located at a Teff following the approximate relation Teff=8090+ 32900xY, and become more luminous for larger envelope masses. Consequently, the ZAHB of the very high helium models is \"vertical\" in the HR diagram. Synthetic models based on these tracks nicely reproduce the location and shape of the \"blue hook\" in the globular cluster wCen, best fit by a very high Teff (bluer) sequence with Y=0.80 and a cooler (redder) one with Y=0.65. Although these precise values of Y may depend on the color-Teff conversions, we know that the helium content of the progenitors of the blue hook stars can not be larger than Y~0.38-0.40, if they are descendants of the cluster blue main sequence. Consequently, this interpretation implies that all these objects must in fact be progeny of the blue main sequence, but they have all suffered further deep mixing, that has largely and uniformly increased their surface helium abundance, during the red giant branch evolution. A late helium flash can not be the cause of this deep mixing, as the models we propose have hydrogen rich envelopes much more massive than those required for a late flash. We discuss different models of deep mixing proposed in the literature, and conclude that our interpretation of the blue hook can not be ruled out, but requires a much deeper investigation before it can be accepted.", "prompt": "Core helium burning is the dominant source of energy of extreme horizontal branch stars, as the hydrogen envelope is too small to contribute to the nuclear energy output. The evolution of each mass in the HR diagram occurs along vertical tracks that, when the core helium is consumed, evolve to higher Teff and then to the white dwarf", "orig": "stage. The larger is the mass, the smaller is the Teff of the models, so that the zero age horizontal branch (ZAHB) is \"horizontal\". In this paper we show that, if the helium mass fraction (Y) of the envelope is larger than Y~0.5, the shape of", "generated": "Core helium burning is the dominant source of energy of extreme horizontal branch stars, as the hydrogen envelope is too small to contribute to the nuclear energy output. The evolution of each mass in the HR diagram occurs along vertical tracks that, when the core helium is consumed, evolve to higher Teff and then to the white dwarf"}
{"full_text": "Ranking schemes drive many real-world decisions, like, where to study, whom to hire, what to buy, etc. Many of these decisions often come with high consequences. For example, a university can be deemed less prestigious if not featured in a top-k list, and consumers might not even explore products that do not get recommended to buyers. At the heart of most of these decisions are opaque ranking schemes, which dictate the ordering of data entities, but their internal logic is inaccessible or proprietary. Drawing inferences about the ranking differences is like a guessing game to the stakeholders, like, the rankees (i.e., the entities who are ranked, like product companies) and the decision-makers (i.e., who use the rankings, like buyers). In this paper, we aim to enable transparency in ranking interpretation by using algorithmic rankers that learn from available data and by enabling human reasoning about the learned ranking differences using explainable AI (XAI) methods. To realize this aim, we leverage the exploration-explanation paradigm of human-data interaction to let human stakeholders explore subsets and groupings of complex multi-attribute ranking data using visual explanations of model fit and attribute influence on rankings. We realize this explanation paradigm for transparent ranking interpretation in TRIVEA, a visual analytic system that is fueled by: i) visualizations of model fit derived from algorithmic rankers that learn the associations between attributes and rankings from available data and ii) visual explanations derived from XAI methods that help abstract important patterns, like, the relative influence of attributes in different ranking ranges. Using TRIVEA, end users not trained in data science have the agency to transparently reason about the global and local behavior of the rankings without the need to open black-box ranking models and develop confidence in the resulting attribute-based inferences. We demonstrate the efficacy of TRIVEA using multiple usage scenarios and subjective feedback from researchers with diverse domain expertise. Keywords: Visual Analytics, Learning-to-Rank, Explainable ML, Ranking", "prompt": "Ranking schemes drive many real-world decisions, like, where to study, whom to hire, what to buy, etc. Many of these decisions often come with high consequences. For example, a university can be deemed less prestigious if not featured in a top-k list, and consumers might not even explore products that do", "orig": "not get recommended to buyers. At the heart of most of these decisions are opaque ranking schemes, which dictate the ordering of data entities, but their internal logic is inaccessible or proprietary. Drawing inferences about the ranking differences is like a guessing game to the stakeholders, like, the rankees (i.e., the entities who", "generated": "Ranking schemes drive many real-world decisions, like, where to study, whom to hire, what to buy, etc. Many of these decisions often come with high consequences. For example, a university can be deemed less prestigious if not featured in a top-k list, and consumers might not even explore products that do"}
{"full_text": "Synthesis is a particularly challenging problem for concurrent programs. At the same time it is a very promising approach, since concurrent programs are difficult to get right, or to analyze with traditional verification techniques. This paper gives an introduction to distributed synthesis in the setting of Mazurkiewicz traces, and its applications to decentralized runtime monitoring. 1 Context Modern computing systems are increasingly distributed and heterogeneous. Software needs to be able to exploit these advances, providing means for applications to be more performant. Traditional concurrent programming paradigms, as in Java, are based on threads, shared-memory, and locking mechanisms that guard access to common data. More recent paradigms like the reactive programming model of Erlang [4] and Scala [35,36] replace shared memory by asynchronous message passing, where sending a message is non-blocking. In all these concurrent frameworks, writing reliable software is a serious challenge. Programmers tend to think about code mostly in a sequential way, and it is hard to grasp all possible schedulings of events in a concurrent execution. For similar reasons, verification and analysis of concurrent programs is a difficult task. Testing, which is still the main method for error detection in software, has low coverage for concurrent programs. The reason is that bugs in such programs are difficult to reproduce: they may happen under very specific thread schedules and the likelihood of taking such corner-case schedules is very low. Automated verification, such as model-checking and other traditional exploration techniques, can handle very limited instances of concurrent programs, mostly because of the very large number of possible states and of possible interleavings of executions. Formal analysis of programs requires as a prerequisite a clean mathematical model for programs. Verification of sequential programs starts usually with an abstraction step -- reducing the value domains of variables to finite domains, viewing conditional branching as non-determinism, etc. Another major simplification consists in disallowing recursion. This leads to a very robust computational model, namely finite-state automata and regular languages. Regular languages of words (and trees) are particularly well understood notions. The deep connections between logic and automata revealed by the foundational work of B\\\"uchi, Rabin, and others, are the main ingredients in automata-based verification .", "prompt": "Synthesis is a particularly challenging problem for concurrent programs. At the same time it is a very promising approach, since concurrent programs are difficult to get right, or to analyze with traditional verification techniques. This paper gives an introduction to distributed synthesis in the setting of Mazurkiewicz traces, and its applications to decentralized", "orig": "runtime monitoring. 1 Context Modern computing systems are increasingly distributed and heterogeneous. Software needs to be able to exploit these advances, providing means for applications to be more performant. Traditional concurrent programming paradigms, as in Java, are based on threads, shared-memory, and locking mechanisms that guard access to common data.", "generated": "Synthesis is a particularly challenging problem for concurrent programs. At the same time it is a very promising approach, since concurrent programs are difficult to get right, or to analyze with traditional verification techniques. This paper gives an introduction to distributed synthesis in the setting of Mazurkiewicz traces, and its applications to decentralized"}
{"full_text": "When we consider a proper holomorphic map \\ $\\tilde{f}: X \\to C$ \\ of a complex manifold \\ $X$ \\ on a smooth complex curve \\ $C$ \\ with a critical value at a point \\ $0$ \\ in \\ $C$, the choice of a local coordinate near this point allows to dispose of an holomorphic function \\ $f$. Then we may construct, using this function, an (a,b)-modules structure on the cohomology sheaves of the formal completion (in \\ $f$) \\ of the complex of sheaves \\ $(Ker\\, df^{\\bullet},d^{\\bullet})$. These (a,b)-modules represent a filtered version of the Gauss-Manin connection of \\ $f$. The most simple example of this construction is the Brieskorn module (see [Br.70]) of a function with an isolated singular point. See [B.08] for the case of a 1-dimensional critical locus. But it is clear that this construction depends seriously on the choice of the function \\ $f$ \\ that is to say on the choice of the local coordinate near the critical point \\ $0$ \\ in the complex curve \\ $C$. The aim of the present paper is to study the behaviour of such constructions when we make a change of local coordinate near the origin. We consider the case of \\ $[\\lambda]-$primitive frescos, which are monogenic geometric (a,b)-modules corresponding to a minimal filtered differential equation associated to a relative de Rham cohomology class on \\ $X$ \\ (see [B.09-a] and [B.09-b]). An holomorphic parameter is a function on the set of isomorphism classes of frescos which behave holomorphically in an holomorphic family of frescos. In general, an holomorphic parameter is not invariant by a change of variable, but we prove a theorem of stability of holomorphic families of frescos by a change of variable and it implies that an holomorphic parameter gives again an holomorphic parameter by a change of variable. We construct here two different kinds of holomorphic parameters which are (quasi-)invariant by change of variable. The first kind is associated to Jordan blocks of the monodromy with size at least two. The second kind is associated to the semi-simple part of the monodromy and look like some \"cross ratio\" of eigenvectors. They allow, in the situation describe above, to associate to a given (vanishing) relative de Rham cohomology class some numbers, which will depend holomorphically of our data, and are independant of the choice of the local coordinate near \\ $0$ \\ to study the Gauss-Manin connection of this degeneration of compact complex manifolds.", "prompt": "When we consider a proper holomorphic map \\ $\\tilde{f}: X \\to C$ \\ of a complex manifold \\ $X$ \\ on a smooth complex curve \\ $C$ \\ with a critical value at a point \\ $0$ \\ in \\ $C$, the choice of a local coordinate", "orig": "near this point allows to dispose of an holomorphic function \\ $f$. Then we may construct, using this function, an (a,b)-modules structure on the cohomology sheaves of the formal completion (in \\ $f$) \\ of the complex of sheaves \\ $(Ker\\, df^", "generated": "When we consider a proper holomorphic map \\ $\\tilde{f}: X \\to C$ \\ of a complex manifold \\ $X$ \\ on a smooth complex curve \\ $C$ \\ with a critical value at a point \\ $0$ \\ in \\ $C$, the choice of a local coordinate"}
{"full_text": "This PhD thesis is devoted to deterministic study of the turbulence in the Navier- Stokes equations. The thesis is divided in four independent chapters.The first chapter involves a rigorous discussion about the energy's dissipation law, proposed by theory of the turbulence K41, in the deterministic setting of the homogeneous and incompressible Navier-Stokes equations, with a stationary external force (the force only depends of the spatial variable) and on the whole space R3. The energy's dissipation law, also called the Kolmogorov's dissipation law, characterizes the energy's dissipation rate (in the form of heat) of a turbulent fluid and this law was developed by A.N. Kolmogorov in 1941. However, its deduction (which uses mainly tools of statistics) is not fully understood until our days and then an active research area consists in studying this law in the rigorous framework of the Navier-Stokes equations which describe in a mathematical way the fluids motion and in particular the movement of turbulent fluids. In this setting, the purpose of this chapter is to highlight the fact that if we consider the Navier-Stokes equations on R3 then certain physical quantities, necessary for the study of the Kolmogorov's dissipation law, have no a rigorous definition and then to give a sense to these quantities we suggest to consider the Navier-Stokes equations with an additional damping term. In the framework of these damped equations, we obtain some estimates for the energy's dissipation rate according to the Kolmogorov's dissipation law.In the second chapter we are interested in study the stationary solutions of the damped Navier- Stokes introduced in the previous chapter. These stationary solutions are a particular type of solutions which do not depend of the temporal variable and their study is motivated by the fact that we always consider the Navier-Stokes equations with a stationary external force. In this chapter we study two properties of the stationary solutions : the first property concerns the stability of these solutions where we prove that if we have a control on the external force then all non stationary solution (with depends of both spatial and temporal variables) converges toward a stationary solution. The second property concerns the decay in spatial variable of the stationary solutions. These properties of stationary solutions are a consequence of the damping term introduced in the Navier-Stokes equations.In the third chapter we still study the stationary solutions of Navier-Stokes equations but now we consider the classical equations (without any additional damping term). The purpose of this chapter is to study an other problem related to the deterministic description of the turbulence : the frequency decay of the stationary solutions. Indeed, according to the K41 theory, if the fluid is in a laminar setting then the stationary solutions of the Navier-Stokes equations must exhibit a exponential frequency decay which starts at lows frequencies. But, if the fluid is in a turbulent setting then this exponential frequency decay must be observed only at highs frequencies. In this chapter, using some Fourier analysis tools, we give a precise description of this exponential frequency decay in the laminar and in the turbulent setting.In the fourth and last chapter we return to the stationary solutions of the classical Navier-Stokes equations and we study the uniqueness of these solutions in the particular case without any external force. Following some ideas of G. Seregin, we study the uniqueness of these solutions first in the framework of Lebesgue spaces of and then in the a general framework of Morrey spaces.", "prompt": "This PhD thesis is devoted to deterministic study of the turbulence in the Navier- Stokes equations. The thesis is divided in four independent chapters.The first chapter involves a rigorous discussion about the energy's dissipation law, proposed by theory of the turbulence K41, in the deterministic setting of the homogeneous and incompressible Nav", "orig": "ier-Stokes equations, with a stationary external force (the force only depends of the spatial variable) and on the whole space R3. The energy's dissipation law, also called the Kolmogorov's dissipation law, characterizes the energy's dissipation rate (in the form of heat) of", "generated": "This PhD thesis is devoted to deterministic study of the turbulence in the Navier- Stokes equations. The thesis is divided in four independent chapters. The first chapter involves a rigorous discussion about the energy's dissipation law, proposed by theory of the turbulence K41, in the deterministic setting of the homogeneous and incompressible"}
{"full_text": "The ability to compose parts to form a more complex whole, and to analyze a whole as a combination of elements, is desirable across disciplines. This workshop bring together researchers applying compositional approaches to physics, NLP, cognitive science, and game theory. Within NLP, a long-standing aim is to represent how words can combine to form phrases and sentences. Within the framework of distributional semantics, words are represented as vectors in vector spaces. The categorical model of Coecke et al. [2010], inspired by quantum protocols, has provided a convincing account of compositionality in vector space models of NLP. There is furthermore a history of vector space models in cognitive science. Theories of categorization such as those developed by Nosofsky [1986] and Smith et al. [1988] utilise notions of distance between feature vectors. More recently G\\\"ardenfors [2004, 2014] has developed a model of concepts in which conceptual spaces provide geometric structures, and information is represented by points, vectors and regions in vector spaces. The same compositional approach has been applied to this formalism, giving conceptual spaces theory a richer model of compositionality than previously [Bolt et al., 2018]. Compositional approaches have also been applied in the study of strategic games and Nash equilibria. In contrast to classical game theory, where games are studied monolithically as one global object, compositional game theory works bottom-up by building large and complex games from smaller components. Such an approach is inherently difficult since the interaction between games has to be considered. Research into categorical compositional methods for this field have recently begun [Ghani et al., 2018]. Moreover, the interaction between the three disciplines of cognitive science, linguistics and game theory is a fertile ground for research. Game theory in cognitive science is a well-established area [Camerer, 2011]. Similarly game theoretic approaches have been applied in linguistics [J\\\"ager, 2008]. Lastly, the study of linguistics and cognitive science is intimately intertwined [Smolensky and Legendre, 2006, Jackendoff, 2007]. Physics supplies compositional approaches via vector spaces and categorical quantum theory, allowing the interplay between the three disciplines to be examined.", "prompt": "The ability to compose parts to form a more complex whole, and to analyze a whole as a combination of elements, is desirable across disciplines. This workshop bring together researchers applying compositional approaches to physics, NLP, cognitive science, and game theory. Within NLP, a long-standing aim is to represent how words", "orig": "can combine to form phrases and sentences. Within the framework of distributional semantics, words are represented as vectors in vector spaces. The categorical model of Coecke et al. [2010], inspired by quantum protocols, has provided a convincing account of compositionality in vector space models of NLP. There is furthermore a", "generated": "The ability to compose parts to form a more complex whole, and to analyze a whole as a combination of elements, is desirable across disciplines. This workshop brings together researchers applying compositional approaches to physics, NLP, cognitive science, and game theory. Within NLP, a long-standing aim is to represent how words"}
{"full_text": "Homozygous mutations in the progranulin gene (GRN) are associated with neuronal ceroid lipofuscinosis 11 (CLN11), a rare lysosomal-storage disorder characterized by cerebellar ataxia, seizures, retinitis pigmentosa, and cognitive disorders, usually beginning between 13 and 25 years of age. This is a rare condition, previously reported in only four families. In contrast, heterozygous GRN mutations are a major cause of frontotemporal dementia associated with neuronal cytoplasmic TDP-43 inclusions. We identified homozygous GRN mutations in six new patients. The phenotypic spectrum is much broader than previously reported, with two remarkably distinct presentations, depending on the age of onset. A childhood/juvenile form is characterized by classical CLN11 symptoms at an early age at onset. Unexpectedly, other homozygous patients presented a distinct delayed phenotype of frontotemporal dementia and parkinsonism after 50 years; none had epilepsy or cerebellar ataxia. Another major finding of this study is that all GRN mutations may not have the same impact on progranulin protein synthesis. A hypomorphic effect of some mutations is supported by the presence of residual levels of plasma progranulin and low levels of normal transcript detected in one case with a homozygous splice-site mutation and late onset frontotemporal dementia. This is a new critical finding that must be considered in therapeutic trials based on replacement strategies. The first neuropathological study in a homozygous carrier provides new insights into the pathological mechanisms of the disease. Hallmarks of neuronal ceroid lipofuscinosis were present. The absence of TDP-43 cytoplasmic inclusions markedly differs from observations of heterozygous mutations, suggesting a pathological shift between lysosomal and TDP-43 pathologies depending on the mono or bi-allelic status. An intriguing observation was the loss of normal TDP-43 staining in the nucleus of some neurons, which could be the first stage of the TDP-43 pathological process preceding the formation of typical cytoplasmic inclusions. Finally, this study has important implications for genetic counselling and molecular diagnosis. Semi-dominant inheritance of GRN mutations implies that specific genetic counseling should be delivered to children and parents of CLN11 patients, as they are heterozygous carriers with a high risk of developing dementia. More broadly, this study illustrates the fact that genetic variants can lead to different phenotypes according to their mono- or bi-allelic state, which is a challenge for genetic diagnosis.", "prompt": "Homozygous mutations in the progranulin gene (GRN) are associated with neuronal ceroid lipofuscinosis 11 (CLN11), a rare lysosomal-storage disorder characterized by cerebellar ataxia, seizures, retinitis pigmentosa, and cognitive disorders, usually beginning", "orig": "between 13 and 25 years of age. This is a rare condition, previously reported in only four families. In contrast, heterozygous GRN mutations are a major cause of frontotemporal dementia associated with neuronal cytoplasmic TDP-43 inclusions. We identified homozygous GR", "generated": "Homozygous mutations in the progranulin gene (GRN) are associated with neuronal ceroid lipofuscinosis 11 (CLN11), a rare lysosomal-storage disorder characterized by cerebellar ataxia, seizures, retinitis pigmentosa, and cognitive disorders, usually beginning"}
{"full_text": "The type problem is the problem of deciding, for a simply connected Riemann surface, whether it is conformally equivalent to the complex plane or to the unit dic in the complex plane. We report on Teichm{\\\"u}ller's results on the type problem from his two papers Eine Anwendung quasikonformer Abbildungen auf das Typenproblem (An application of quasiconformal map-pings to the type problem) (1937) and Untersuchungen{\\\"u}ber kon-forme und quasikonforme Abbildungen (Investigations on con-formal and quasiconformal mappings) (1938). They concern simply connected Riemann surfaces defined as branched covers of the sphere. At the same time, we review the theory of line complexes, a combinatorial device used by Teichm{\\\"u}ller and others to encode branched coverings of the sphere. In the first paper, Teichm{\\\"u}ller proves that any two simply connected Riemann surfaces which are branched coverings of the Riemann sphere with finitely many branch values and which have the same line complex are quasiconformally equivalent. For this purpose, he introduces a technique for piecing together quasi-conformal mappings. He also obtains a result on the extension of smooth orientation-preserving diffeomorphisms of the circle to quasiconformal mappings of the disc which are conformal at the boundary. In the second paper, using line complexes, Teichm{\\\"u}ller gives a type criterion for a simply-connected surface which is a branched covering of the sphere, in terms of an adequately defined measure of ramification, defined by a limiting process. The result says that if the surface is ''sufficiently ramified'' (in a sense to be made precise), then it is hyperbolic. In the same paper, Te-ichm{\\\"u}ller answers by the negative a conjecture made by Nevan-linna which states a criterion for parabolicity in terms of the value of a (different) measure of ramification, defined by a limiting process. Teichm{\\\"u}ller's results in his first paper are used in the proof of the results of the second one.The final version of this paper will appear in Vol. VII of the Handbook of Teichm{\\\"u}ller theory (European Mathematical Society Publishing House, 2020).", "prompt": "The type problem is the problem of deciding, for a simply connected Riemann surface, whether it is conformally equivalent to the complex plane or to the unit dic in the complex plane. We report on Teichm{\\\"u}ller's results on the type problem from his two papers Eine Anwendung quas", "orig": "ikonformer Abbildungen auf das Typenproblem (An application of quasiconformal map-pings to the type problem) (1937) and Untersuchungen{\\\"u}ber kon-forme und quasikonforme Abbildungen (Investigations on con-formal and quasiconformal", "generated": "The type problem is the problem of deciding, for a simply connected Riemann surface, whether it is conformally equivalent to the complex plane or to the unit disk in the complex plane. We report on Teichm{\\\"u}ller's results on the type problem from his two papers \"Eine Anwendung quer"}
{"full_text": "Context. Recent studies show that developers spend most of their programming time testing, verifying and debugging software. As applications become more and more complex, developers demand more advanced debugging support to ease the software development process. Inquiry. Since the 70's many debugging solutions were introduced. Amongst them, online debuggers provide a good insight on the conditions that led to a bug, allowing inspection and interaction with the variables of the program. However, most of the online debugging solutions introduce \\textit{debugging interference} to the execution of the program, i.e. pauses, latency, and evaluation of code containing side-effects. Approach. This paper investigates a novel debugging technique called \\outofplace debugging. The goal is to minimize the debugging interference characteristic of online debugging while allowing online remote capabilities. An \\outofplace debugger transfers the program execution and application state from the debugged application to the debugger application, both running in different processes. Knowledge. On the one hand, \\outofplace debugging allows developers to debug applications remotely, overcoming the need of physical access to the machine where the debugged application is running. On the other hand, debugging happens locally on the remote machine avoiding latency. That makes it suitable to be deployed on a distributed system and handle the debugging of several processes running in parallel. Grounding. We implemented a concrete out-of-place debugger for the Pharo Smalltalk programming language. We show that our approach is practical by performing several benchmarks, comparing our approach with a classic remote online debugger. We show that our prototype debugger outperforms by a 1000 times a traditional remote debugger in several scenarios. Moreover, we show that the presence of our debugger does not impact the overall performance of an application. Importance. This work combines remote debugging with the debugging experience of a local online debugger. Out-of-place debugging is the first online debugging technique that can minimize debugging interference while debugging a remote application. Yet, it still keeps the benefits of online debugging ( e.g. step-by-step execution). This makes the technique suitable for modern applications which are increasingly parallel, distributed and reactive to streams of data from various sources like sensors, UI, network, etc.", "prompt": "Context. Recent studies show that developers spend most of their programming time testing, verifying and debugging software. As applications become more and more complex, developers demand more advanced debugging support to ease the software development process. Inquiry. Since the 70's many debugging solutions were introduced. Amongst them, online debuggers provide a", "orig": "good insight on the conditions that led to a bug, allowing inspection and interaction with the variables of the program. However, most of the online debugging solutions introduce \\textit{debugging interference} to the execution of the program, i.e. pauses, latency, and evaluation of code containing side-effects. Approach. This", "generated": "Context. Recent studies show that developers spend most of their programming time testing, verifying and debugging software. As applications become more and more complex, developers demand more advanced debugging support to ease the software development process. Inquiry. Since the 70's many debugging solutions were introduced. Amongst them, online debuggers provide a"}
