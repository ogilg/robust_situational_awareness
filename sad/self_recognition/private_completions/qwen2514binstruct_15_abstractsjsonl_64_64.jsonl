{"full_text": "Numerical schemes for the solution of the Euler equations have recently been developed, which involve the discretisation of the internal energy equation, with corrective terms to ensure the correct capture of shocks, and, more generally, the consistency in the Lax-Wendroff sense. These schemes may be staggered or colocated, using either struc-tured meshes or general simplicial or tetrahedral/hexahedral meshes. The time discretization is performed by fractional-step algorithms; these may be either based on semi-implicit pressure correction techniques or segregated in such a way that only explicit steps are involved (referred to hereafter as \"explicit\" variants). In order to ensure the positivity of the density, the internal energy and the pressure, the discrete convection operators for the mass and internal energy balance equations are carefully designed; they use an upwind technique with respect to the material velocity only. The construction of the fluxes thus does not need any Rie-mann or approximate Riemann solver, and yields easily implementable algorithms. The stability is obtained without restriction on the time step for the pressure correction scheme and under a CFL-like condition for explicit variants: preservation of the integral of the total energy over the computational domain, and positivity of the density and the internal energy. The semi-implicit first-order upwind scheme satisfies a local discrete entropy inequality. If a MUSCL-like scheme is used in order to limit the scheme diffusion, then a weaker property holds: the entropy inequality is satisfied up to a remainder term which is shown to tend to zero with the space and time steps, if the discrete solution is controlled in L $\\infty$ and BV norms. The explicit upwind variant also satisfies such a weaker property, at the price of an estimate for the velocity which could be derived from the introduction of a new stabilization term in the momentum balance. Still for the explicit scheme, with the above-mentioned MUSCL-like scheme, the same result only holds if the ratio of the time to the space step tends to zero.", "prompt": "Numerical schemes for the solution of the Euler equations have recently been developed, which involve the discretisation of the internal energy equation, with corrective terms to ensure the correct capture of shocks, and, more generally, the consistency in the Lax-Wendroff sense. These schemes may be staggered or colocated,", "orig": "using either struc-tured meshes or general simplicial or tetrahedral/hexahedral meshes. The time discretization is performed by fractional-step algorithms; these may be either based on semi-implicit pressure correction techniques or segregated in such a way that only explicit steps are involved (referred to hereafter as", "generated": "Numerical schemes for the solution of the Euler equations have recently been developed, which involve the discretisation of the internal energy equation, with corrective terms to ensure the correct capture of shocks, and, more generally, the consistency in the Lax-Wendroff sense. These schemes may be staggered or colocated,"}
{"full_text": "Introduction: Nottingham grading system is a major prognostic factor for invasive breast carcinoma (IBC). Its determination requires the evaluation of the mitotic score (MS) which is subject to low intra- and inter-observer reproducibility. The MS shall be performed in the most proliferative area of the tumor, which determination is hard but critical. Artificial intelligence based tools could help pathologists to detect mitosis on whole slide images (WSI). Objective: The aim of this study was to evaluate the contribution of a mitosis detection algorithms pecifically developed to assist the pathologist during the evaluation of the MS on WSI. Methods: Algorithmic mitosis detection is a two-step process: first the algorithm detects candidate objects resembling mitosis, then the selection is refined by a classifier. The densest mitoticregions are shown to the pathologist, then he can establish the MS with algorithm results. For this study, three expert pathologists have determined a consensual ground truth for MS on fifty WSI of IBC. Those slides were also submitted to two readers pathologists who evaluated the MS of each slide twice, with and without the assistance of the algorithm, with a four week wash-out period. Interobserver reproducibility was measured by evaluating the scores obtained with, and without assistance between two readers pathologists and was also measured between each reader pathologist and the expert ground truth to determine the accuracy of the established score. Results:Baseline linearly weighted Cohen's Kappa for interobserver agreement of MS between two readers pathologists was 0.482. Using the algorithm generated mitotic detection in WSI, the agreement score increased to 0.672. Baseline linearly weighted Cohen's Kappa for interobserver agreement of MS between each reader pathologist and expert consensus was 0.378 and 0.457 for pathologist 1 and 2 respectively. Using the algorithm generated mitoticdetection in WSI, the agreement score increased respectively to 0.629 and 0.726. Conclusion:The use of the developed algorithm constitutes a viable approach to assist the pathologist for the evaluation of the MS of IBC on WSI. Its use makes it possible to improve interobserver reproducibility between pathologists and the accuracy of the score established by expert consensus. The use of such a tool constitutes a new approach in the evaluation of the mitoticscore which could lead to an evolution of practices.", "prompt": "Introduction: Nottingham grading system is a major prognostic factor for invasive breast carcinoma (IBC). Its determination requires the evaluation of the mitotic score (MS) which is subject to low intra- and inter-observer reproducibility. The MS shall be performed in the most proliferative area of the tumor, which determination", "orig": "is hard but critical. Artificial intelligence based tools could help pathologists to detect mitosis on whole slide images (WSI). Objective: The aim of this study was to evaluate the contribution of a mitosis detection algorithms pecifically developed to assist the pathologist during the evaluation of the MS on WSI. Methods:", "generated": "n can also introduce variability among different observers. To address these challenges, recent studies have explored the use of artificial intelligence (AI) and machine learning algorithms to improve the accuracy and consistency of the Nottingham grading system.\n\nOne promising approach involves training deep learning models on large datasets of histopathological images to automatically identify and quantify mit"}
{"full_text": "For many decades, ultrahigh energy charged particles of unknown origin that can be observed from the ground have been a puzzle for particle physicists and astrophysicists. As an attempt to discriminate among several possible production scenarios, astrophysicists try to test the statistical isotropy of the directions of arrival of these cosmic rays. At the highest energies, they are supposed to point toward their sources with good accuracy. However, the observations are so rare that testing the distribution of such samples of directional data on the sphere is nontrivial. In this paper, we choose a nonparametric framework that makes weak hypotheses on the alternative distributions and allows in turn to detect various and possibly unexpected forms of anisotropy. We explore two particular procedures. Both are derived from fitting the empirical distribution with wavelet expansions of densities. We use the wavelet frame introduced by [SIAM J. Math. Anal. 38 (2006b) 574-594 (electronic)], the so-called needlets. The expansions are truncated at scale indices no larger than some ${J^{\\star}}$, and the $L^p$ distances between those estimates and the null density are computed. One family of tests (called Multiple) is based on the idea of testing the distance from the null for each choice of $J=1,\\ldots,{J^{\\star}}$, whereas the so-called PlugIn approach is based on the single full ${J^{\\star}}$ expansion, but with thresholded wavelet coefficients. We describe the practical implementation of these two procedures and compare them to other methods in the literature. As alternatives to isotropy, we consider both very simple toy models and more realistic nonisotropic models based on Physics-inspired simulations. The Monte Carlo study shows good performance of the Multiple test, even at moderate sample size, for a wide sample of alternative hypotheses and for different choices of the parameter ${J^{\\star}}$. On the 69 most energetic events published by the Pierre Auger Collaboration, the needlet-based procedures suggest statistical evidence for anisotropy. Using several values for the parameters of the methods, our procedures yield $p$-values below 1%, but with uncontrolled multiplicity issues. The flexibility of this method and the possibility to modify it to take into account a large variety of extensions of the problem make it an interesting option for future investigation of the origin of ultrahigh energy cosmic rays.", "prompt": "For many decades, ultrahigh energy charged particles of unknown origin that can be observed from the ground have been a puzzle for particle physicists and astrophysicists. As an attempt to discriminate among several possible production scenarios, astrophysicists try to test the statistical isotropy of the directions of arrival of these", "orig": "cosmic rays. At the highest energies, they are supposed to point toward their sources with good accuracy. However, the observations are so rare that testing the distribution of such samples of directional data on the sphere is nontrivial. In this paper, we choose a nonparametric framework that makes weak hypotheses on the alternative distributions", "generated": "ese particles. Statistical isotropy implies that these particles come from all directions in the sky with equal probability, which would support models where they originate from distant cosmic sources like supermassive black holes or gamma-ray bursts.\n\nOne of the key experiments studying this phenomenon is the Pierre Auger Observatory in Argentina. This observatory uses"}
{"full_text": "In planetary fluid cores, the density depends on temperature and chemical composition, which diffuse at very different rates. This leads to various instabilities, bearing the name of double-diffusive convection. We investigate rotating double-diffusive convection (RDDC) in fluid spheres. We use the Boussinesq approximation with homogeneous internal thermal and compositional source terms. We focus on the finger regime, in which the thermal gradient is stabilising whereas the compositional one is destabilising. First, we perform a global linear stability analysis in spheres. The critical Rayleigh numbers drastically drop for stably stratified fluids, yielding large-scale convective motions where local analyses predict stability. We evidence the inviscid nature of this large-scale double-diffusive instability, enabling the determination of the marginal stability curve at realistic planetary regimes. In particular, we show that in stably stratified spheres, the Rayleigh numbers $Ra$ at the onset evolve like $Ra \\sim Ek^{-1}$, where $Ek$ is the Ekman number. This differs from rotating convection in unstably stratified spheres, for which $Ra \\sim Ek^{-4/3}$. The domain of existence of inviscid convection thus increases as $Ek^{-1/3}$. Second, we perform nonlinear simulations. We find a transition between two regimes of RDDC, controlled by the strength of the stratification. Furthermore, far from the RDDC onset, we find a dominating equatorially anti-symmetric, large-scale zonal flow slightly above the associated linear onset. Unexpectedly, a purely linear mechanism can explain this phenomenon, even far from the instability onset, yielding a symmetry breaking of the nonlinear flow at saturation. For even stronger stable stratification, the flow becomes mainly equatorially-symmetric and intense zonal jets develop. Finally, we apply our results to the early Earth core. Double diffusion can reduce the critical Rayleigh number by four decades for realistic core conditions. We suggest that the early Earth core was prone to turbulent RDDC, with large-scale zonal flows.", "prompt": "In planetary fluid cores, the density depends on temperature and chemical composition, which diffuse at very different rates. This leads to various instabilities, bearing the name of double-diffusive convection. We investigate rotating double-diffusive convection (RDDC) in fluid spheres. We use the Boussinesq approximation", "orig": "with homogeneous internal thermal and compositional source terms. We focus on the finger regime, in which the thermal gradient is stabilising whereas the compositional one is destabilising. First, we perform a global linear stability analysis in spheres. The critical Rayleigh numbers drastically drop for stably stratified fluids, yielding large-scale conv", "generated": "n, a common approach in fluid dynamics that simplifies the equations by assuming the fluid density varies significantly only in the buoyancy term. Our investigation focuses on understanding how rotation affects the stability and dynamics of RDDC in spherical geometries, which are more representative of planetary interiors than traditional Cartesian models.\n\nTo conduct our study,"}
{"full_text": "Context. Recent studies show that developers spend most of their programming time testing, verifying and debugging software. As applications become more and more complex, developers demand more advanced debugging support to ease the software development process. Inquiry. Since the 70's many debugging solutions were introduced. Amongst them, online debuggers provide a good insight on the conditions that led to a bug, allowing inspection and interaction with the variables of the program. However, most of the online debugging solutions introduce \\textit{debugging interference} to the execution of the program, i.e. pauses, latency, and evaluation of code containing side-effects. Approach. This paper investigates a novel debugging technique called \\outofplace debugging. The goal is to minimize the debugging interference characteristic of online debugging while allowing online remote capabilities. An \\outofplace debugger transfers the program execution and application state from the debugged application to the debugger application, both running in different processes. Knowledge. On the one hand, \\outofplace debugging allows developers to debug applications remotely, overcoming the need of physical access to the machine where the debugged application is running. On the other hand, debugging happens locally on the remote machine avoiding latency. That makes it suitable to be deployed on a distributed system and handle the debugging of several processes running in parallel. Grounding. We implemented a concrete out-of-place debugger for the Pharo Smalltalk programming language. We show that our approach is practical by performing several benchmarks, comparing our approach with a classic remote online debugger. We show that our prototype debugger outperforms by a 1000 times a traditional remote debugger in several scenarios. Moreover, we show that the presence of our debugger does not impact the overall performance of an application. Importance. This work combines remote debugging with the debugging experience of a local online debugger. Out-of-place debugging is the first online debugging technique that can minimize debugging interference while debugging a remote application. Yet, it still keeps the benefits of online debugging ( e.g. step-by-step execution). This makes the technique suitable for modern applications which are increasingly parallel, distributed and reactive to streams of data from various sources like sensors, UI, network, etc.", "prompt": "Context. Recent studies show that developers spend most of their programming time testing, verifying and debugging software. As applications become more and more complex, developers demand more advanced debugging support to ease the software development process. Inquiry. Since the 70's many debugging solutions were introduced. Amongst them, online debuggers provide a", "orig": "good insight on the conditions that led to a bug, allowing inspection and interaction with the variables of the program. However, most of the online debugging solutions introduce \\textit{debugging interference} to the execution of the program, i.e. pauses, latency, and evaluation of code containing side-effects. Approach. This", "generated": "unique way to interact with running programs in real-time, allowing developers to inspect program states and modify execution paths dynamically. However, these tools often come with their own set of challenges, such as performance overhead and complexity in setup and usage.\n\nIn recent years, there has been a surge in the development of sophisticated debugging frameworks and"}
{"full_text": "The impact of outliers and anomalies on model estimation and data processing is of paramount importance, as evidenced by the extensive body of research spanning various fields over several decades: thousands of research papers have been published on the subject. As a consequence, numerous reviews, surveys, and textbooks have sought to summarize the existing literature, encompassing a wide range of methods from both the statistical and data mining communities. While these endeavors to organize and summarize the research are invaluable, they face inherent challenges due to the pervasive nature of outliers and anomalies in all data-intensive applications, irrespective of the specific application field or scientific discipline. As a result, the resulting collection of papers remains voluminous and somewhat heterogeneous. To address the need for knowledge organization in this domain, this paper implements the first systematic meta-survey of general surveys and reviews on outlier and anomaly detection. Employing a classical systematic survey approach, the study collects nearly 500 papers using two specialized scientific search engines. From this comprehensive collection, a subset of 56 papers that claim to be general surveys on outlier detection is selected using a snowball search technique to enhance field coverage. A meticulous quality assessment phase further refines the selection to a subset of 25 high-quality general surveys. Using this curated collection, the paper investigates the evolution of the outlier detection field over a 20-year period, revealing emerging themes and methods. Furthermore, an analysis of the surveys sheds light on the survey writing practices adopted by scholars from different communities who have contributed to this field. Finally, the paper delves into several topics where consensus has emerged from the literature. These include taxonomies of outlier types, challenges posed by high-dimensional data, the importance of anomaly scores, the impact of learning conditions, difficulties in benchmarking, and the significance of neural networks. Non-consensual aspects are also discussed, particularly the distinction between local and global outliers and the challenges in organizing detection methods into meaningful taxonomies.", "prompt": "The impact of outliers and anomalies on model estimation and data processing is of paramount importance, as evidenced by the extensive body of research spanning various fields over several decades: thousands of research papers have been published on the subject. As a consequence, numerous reviews, surveys, and textbooks have sought to summarize the existing literature, encompassing", "orig": "a wide range of methods from both the statistical and data mining communities. While these endeavors to organize and summarize the research are invaluable, they face inherent challenges due to the pervasive nature of outliers and anomalies in all data-intensive applications, irrespective of the specific application field or scientific discipline. As a result, the resulting collection of papers", "generated": "a wide array of methodologies and techniques for detecting, treating, and preventing these anomalies in datasets. These resources not only highlight the challenges posed by outliers but also provide a comprehensive overview of the tools and strategies available to mitigate their effects.\n\nOutliers can significantly skew statistical analyses, leading to misleading conclusions and poor model performance. In"}
{"full_text": "Core helium burning is the dominant source of energy of extreme horizontal branch stars, as the hydrogen envelope is too small to contribute to the nuclear energy output. The evolution of each mass in the HR diagram occurs along vertical tracks that, when the core helium is consumed, evolve to higher Teff and then to the white dwarf stage. The larger is the mass, the smaller is the Teff of the models, so that the zero age horizontal branch (ZAHB) is \"horizontal\". In this paper we show that, if the helium mass fraction (Y) of the envelope is larger than Y~0.5, the shape of the tracks changes completely: the hydrogen burning becomes efficient again also for very small envelope masses, thanks to the higher molecular weight and to the higher temperatures of the hydrogen shell. The larger is Y, the smaller is the envelope mass that provides strong H-shell burning. These tracks have a curled shape, are located at a Teff following the approximate relation Teff=8090+ 32900xY, and become more luminous for larger envelope masses. Consequently, the ZAHB of the very high helium models is \"vertical\" in the HR diagram. Synthetic models based on these tracks nicely reproduce the location and shape of the \"blue hook\" in the globular cluster wCen, best fit by a very high Teff (bluer) sequence with Y=0.80 and a cooler (redder) one with Y=0.65. Although these precise values of Y may depend on the color-Teff conversions, we know that the helium content of the progenitors of the blue hook stars can not be larger than Y~0.38-0.40, if they are descendants of the cluster blue main sequence. Consequently, this interpretation implies that all these objects must in fact be progeny of the blue main sequence, but they have all suffered further deep mixing, that has largely and uniformly increased their surface helium abundance, during the red giant branch evolution. A late helium flash can not be the cause of this deep mixing, as the models we propose have hydrogen rich envelopes much more massive than those required for a late flash. We discuss different models of deep mixing proposed in the literature, and conclude that our interpretation of the blue hook can not be ruled out, but requires a much deeper investigation before it can be accepted.", "prompt": "Core helium burning is the dominant source of energy of extreme horizontal branch stars, as the hydrogen envelope is too small to contribute to the nuclear energy output. The evolution of each mass in the HR diagram occurs along vertical tracks that, when the core helium is consumed, evolve to higher Teff and then to the white dwarf stage", "orig": ". The larger is the mass, the smaller is the Teff of the models, so that the zero age horizontal branch (ZAHB) is \"horizontal\". In this paper we show that, if the helium mass fraction (Y) of the envelope is larger than Y~0.5, the shape of the", "generated": "e. This process is characterized by a significant increase in temperature and a decrease in radius, leading these stars to become some of the hottest stars in our galaxy.\n\nAs the helium burns in the core, it produces carbon and oxygen. Once the helium is exhausted, the star can no longer sustain nuclear reactions in its core,"}
{"full_text": "Shock modelling predicts an electron density enhancement within the magnetic precursor of C-shocks. Previous observations of SiO, H13CO+, HN13C and H13CN toward the young L1448-mm outflow showed an over-excitation of the ion fluid that was attributed to an electron density enhancement in the precursor. We re-visit this interpretation and test if it still holds when we consider different source morphologies and kinetic temperatures for the observed molecules, and also give some insight on the spatial extent of the electron density enhancement around L1448-mm. We estimate the opacities of H13CO+ and HN13C by observing the J=3\\to2 lines of rarer isotopologues to confirm that the emission is optically thin. To model the excitation of the molecules, we use the large velocity gradient (LVG) approximation with updated collisional coefficients to i) re- analyse the observations toward the positions where the over-excitation of H13CO+ has previously been observed [i.e. toward L1448- mm at offsets (0,0) and (0,-10)], and ii) to investigate if the electron density enhancement is still required for the cases of extended and compact emission, and for kinetic temperatures of up to 400 K. We also report several lines of SiO, HN13C and H13CO+ toward new positions around this outflow, to investigate the spatial extent of the over-excitation of the ions in L1448-mm. From the isotopologue observations, we find that the emission of H13CO+ and HN13C from the precursor is optically thin if this emission is extended. Using the new collisional coefficients, an electron density enhancement is still needed to explain the excitation of H13CO+ for extended emission and for gas temperatures of\\le 400 K toward L1448-mm (0,-10), and possibly also toward L1448-mm (0,0). For compact emission the data cannot be fitted. We do not find any evidence for the over-excitation of the ion fluid toward the newly observed positions around L1448-mm. The observed line emission of SiO, H13CO+ and HN13C toward L1448-mm (0,0) and (0,-10) is consistent with an electron density enhancement in the precursor component, if this emission is spatially extended. This is also true for the case of high gas temperatures (\\le400 K) toward the (0,-10) offset. The electron density enhancement seems to be restricted to the southern, redshifted lobe of the L1448-mm outflow. Interferometric images of the line emission of these molecules are needed to confirm the spatial extent of the over-excitation of the ions and thus, of the electron density enhancement in the magnetic precursor of L1448-mm.", "prompt": "Shock modelling predicts an electron density enhancement within the magnetic precursor of C-shocks. Previous observations of SiO, H13CO+, HN13C and H13CN toward the young L1448-mm outflow showed an over-excitation of the ion fluid that was attributed to an electron density", "orig": "enhancement in the precursor. We re-visit this interpretation and test if it still holds when we consider different source morphologies and kinetic temperatures for the observed molecules, and also give some insight on the spatial extent of the electron density enhancement around L1448-mm. We estimate the opacities of H13", "generated": "enhancement due to a C-shock. These results suggest that the magnetic field plays a significant role in the excitation processes within such shocks. The enhanced electron density, in turn, can lead to higher rates of collisional deexcitation and ionization reactions, which could be responsible for the observed over-excitation."}
{"full_text": "The finance industry is producing an increasing amount of datasets that investment professionals can consider to be influential on the price of financial assets. These datasets were initially mainly limited to exchange data, namely price, capitalization and volume. Their coverage has now considerably expanded to include, for example, macroeconomic data, supply and demand of commodities, balance sheet data and more recently extra-financial data such as ESG scores. This broadening of the factors retained as influential constitutes a serious challenge for statistical modeling. Indeed, the instability of the correlations between these factors makes it practically impossible to identify the joint laws needed to construct scenarios. Fortunately, spectacular advances in Deep Learning field in recent years have given rise to GANs. GANs are a type of generative machine learning models that produce new data samples with the same characteristics as a training data distribution in an unsupervised way, avoiding data assumptions and human induced biases. In this work, we are exploring the use of GANs for synthetic financial scenarios generation. This pilot study is the result of a collaboration between Fujitsu and Advestis and it will be followed by a thorough exploration of the use cases that can benefit from the proposed solution. We propose a GANs-based algorithm that allows the replication of multivariate data representing several properties (including, but not limited to, price, market capitalization, ESG score, controversy score,. . .) of a set of stocks. This approach differs from examples in the financial literature, which are mainly focused on the reproduction of temporal asset price scenarios. We also propose several metrics to evaluate the quality of the data generated by the GANs. This approach is well fit for the generation of scenarios, the time direction simply arising as a subsequent (eventually conditioned) generation of data points drawn from the learned distribution. Our method will allow to simulate high dimensional scenarios (compared to $\\lesssim10$ features currently employed in most recent use cases) where network complexity is reduced thanks to a wisely performed feature engineering and selection. Complete results will be presented in a forthcoming study.", "prompt": "The finance industry is producing an increasing amount of datasets that investment professionals can consider to be influential on the price of financial assets. These datasets were initially mainly limited to exchange data, namely price, capitalization and volume. Their coverage has now considerably expanded to include, for example, macroeconomic data, supply and demand of commodities", "orig": ", balance sheet data and more recently extra-financial data such as ESG scores. This broadening of the factors retained as influential constitutes a serious challenge for statistical modeling. Indeed, the instability of the correlations between these factors makes it practically impossible to identify the joint laws needed to construct scenarios. Fortunately, spectacular advances in Deep", "generated": "es, news sentiment analysis, and social media trends. This expansion has enabled a more nuanced understanding of market dynamics, allowing investors to make more informed decisions.\n\nFor instance, macroeconomic indicators such as GDP growth rates, inflation rates, and unemployment figures provide insights into the overall health of economies and can predict shifts in investor confidence"}
{"full_text": "When we consider a proper holomorphic map \\ $\\tilde{f}: X \\to C$ \\ of a complex manifold \\ $X$ \\ on a smooth complex curve \\ $C$ \\ with a critical value at a point \\ $0$ \\ in \\ $C$, the choice of a local coordinate near this point allows to dispose of an holomorphic function \\ $f$. Then we may construct, using this function, an (a,b)-modules structure on the cohomology sheaves of the formal completion (in \\ $f$) \\ of the complex of sheaves \\ $(Ker\\, df^{\\bullet},d^{\\bullet})$. These (a,b)-modules represent a filtered version of the Gauss-Manin connection of \\ $f$. The most simple example of this construction is the Brieskorn module (see [Br.70]) of a function with an isolated singular point. See [B.08] for the case of a 1-dimensional critical locus. But it is clear that this construction depends seriously on the choice of the function \\ $f$ \\ that is to say on the choice of the local coordinate near the critical point \\ $0$ \\ in the complex curve \\ $C$. The aim of the present paper is to study the behaviour of such constructions when we make a change of local coordinate near the origin. We consider the case of \\ $[\\lambda]-$primitive frescos, which are monogenic geometric (a,b)-modules corresponding to a minimal filtered differential equation associated to a relative de Rham cohomology class on \\ $X$ \\ (see [B.09-a] and [B.09-b]). An holomorphic parameter is a function on the set of isomorphism classes of frescos which behave holomorphically in an holomorphic family of frescos. In general, an holomorphic parameter is not invariant by a change of variable, but we prove a theorem of stability of holomorphic families of frescos by a change of variable and it implies that an holomorphic parameter gives again an holomorphic parameter by a change of variable. We construct here two different kinds of holomorphic parameters which are (quasi-)invariant by change of variable. The first kind is associated to Jordan blocks of the monodromy with size at least two. The second kind is associated to the semi-simple part of the monodromy and look like some \"cross ratio\" of eigenvectors. They allow, in the situation describe above, to associate to a given (vanishing) relative de Rham cohomology class some numbers, which will depend holomorphically of our data, and are independant of the choice of the local coordinate near \\ $0$ \\ to study the Gauss-Manin connection of this degeneration of compact complex manifolds.", "prompt": "When we consider a proper holomorphic map \\ $\\tilde{f}: X \\to C$ \\ of a complex manifold \\ $X$ \\ on a smooth complex curve \\ $C$ \\ with a critical value at a point \\ $0$ \\ in \\ $C$, the choice of a local coordinate near", "orig": "this point allows to dispose of an holomorphic function \\ $f$. Then we may construct, using this function, an (a,b)-modules structure on the cohomology sheaves of the formal completion (in \\ $f$) \\ of the complex of sheaves \\ $(Ker\\, df^{\\", "generated": "r the origin on the base curve \\( C \\) is significant. Let's denote this local coordinate by \\( z \\), such that \\( z(0) = 0 \\). The choice of \\( z \\) can affect the detailed form of the map near the critical point, but the essential topological and geometric"}
{"full_text": "CANVAS caused by RFC1 biallelic expansions is a major cause of inherited sensory neuronopathy. Detection of RFC1 expansion is challenging and CANVAS can be associated with atypical features. We clinically and genetically characterized 50 patients, selected based on the presence of sensory neuronopathy confirmed by EMG. We screened RFC1 expansion by PCR, repeat-primed PCR, and Southern blotting of long-range PCR products, a newly developed method. Neuropathological characterization was performed on the brain and spinal cord of one patient. Most patients (88%) carried a biallelic (AAGGG)n expansion in RFC1. In addition to the core CANVAS phenotype (sensory neuronopathy, cerebellar syndrome, and vestibular impairment), we observed chronic cough (97%), oculomotor signs (85%), motor neuron involvement (55%), dysautonomia (50%), and parkinsonism (10%). Motor neuron involvement was found for 24 of 38 patients (63.1%). First motor neuron signs, such as brisk reflexes, extensor plantar responses, and/or spasticity, were present in 29% of patients, second motor neuron signs, such as fasciculations, wasting, weakness, or a neurogenic pattern on EMG in 18%, and both in 16%. Mixed motor and sensory neuronopathy was observed in 19% of patients. Among six non-RFC1 patients, one carried a heterozygous AAGGG expansion and a pathogenic variant in GRM1. Neuropathological examination of one RFC1 patient with an enriched phenotype, including parkinsonism, dysautonomia, and cognitive decline, showed posterior column and lumbar posterior root atrophy. Degeneration of the vestibulospinal and spinocerebellar tracts was mild. We observed marked astrocytic gliosis and axonal swelling of the synapse between first and second motor neurons in the anterior horn at the lumbar level. The cerebellum showed mild depletion of Purkinje cells, with empty baskets, torpedoes, and astrogliosis characterized by a disorganization of the Bergmann's radial glia. We found neuronal loss in the vagal nucleus. The pars compacta of the substantia nigra was depleted, with widespread Lewy bodies in the locus coeruleus, substantia nigra, hippocampus, entorhinal cortex, and amygdala. We propose new guidelines for the screening of RFC1 expansion, considering different expansion motifs. Here, we developed a new method to more easily detect pathogenic RFC1 expansions. We report frequent motor neuron involvement and different neuronopathy subtypes. Parkinsonism was more prevalent in this cohort than in the general population, 10% versus the expected 1% (p < .001). We describe, for the first time, the spinal cord pathology in CANVAS, showing the alteration of posterior columns and roots, astrocytic gliosis and axonal swelling, suggesting motor neuron synaptic dysfunction.", "prompt": "CANVAS caused by RFC1 biallelic expansions is a major cause of inherited sensory neuronopathy. Detection of RFC1 expansion is challenging and CANVAS can be associated with atypical features. We clinically and genetically characterized 50 patients, selected based on the presence of sensory neuronopathy confirmed by EMG. We", "orig": "screened RFC1 expansion by PCR, repeat-primed PCR, and Southern blotting of long-range PCR products, a newly developed method. Neuropathological characterization was performed on the brain and spinal cord of one patient. Most patients (88%) carried a biallelic (AAGGG)n expansion in", "generated": "e identified 17 unique mutations in the RFC1 gene among these patients, confirming the diagnosis of CANVAS (Ciliary Axonemal Neuronal And Sensory Atrophy). The clinical presentation varied widely, from early-onset severe peripheral neuropathy to milder cases presenting later in life with less pronounced symptoms"}
{"full_text": "In this study, we investigate porosity evolution through the sedimentary input section of the North Sumatra Subduction zone by quantifying interstitial porosity, bound water content and fracture porosity based on IODP Expedition 362 data and post-cruise chemical analyses. During IODP Expedition 362, total porosity of the sedimentary section entering the North Sumatra subduction zone was measured. This total porosity is derived from the total water content of core samples thus including pore water and water bound to hydrous minerals like smectite. Clay mineral composition varies over the sedimentary section and is mainly kaolinite/illite in the Nicobar Fan units and smectite/illite in the prefan pelagic unit below. The prefan pelagic unit shows anomalously high total porosity values and is stratigraphically correlated to a high amplitude negative polarity (HANP) seismic reflector located landward. This HANP reflector has been previously interpreted as a porous fluid-rich layer where the d{\\'e}collement may develop along parts of the margin as a consequence of pore pressure buildup. We estimate clay bound water content from Cation Exchange Capacity (CEC) which gives information about the smectite/illite composition and soluble chloride content data. Interstitial porosity corresponds to onboard total porosity corrected from clay bound water and is more relevant in terms of sediment compaction state and fluid flow properties. Interstitial porosity versus vertical effective stress curve shows no evidence of undercompaction and suggests that the input section 2 has been experiencing normal consolidation due to high sediment accumulation rate. The porosity anomaly observed in the prefan pelagic unit results from the local occurrence of water-bearing minerals like smectite rather than excess pore pressure, which might, however, buildup more landward in the basin. We also estimate fracture porosity using a resistivity model for shales used in previous works based on wireline resistivity log and show that fracture porosity yields 4-6% in damaged parts of the sedimentary section investigated.", "prompt": "In this study, we investigate porosity evolution through the sedimentary input section of the North Sumatra Subduction zone by quantifying interstitial porosity, bound water content and fracture porosity based on IODP Expedition 362 data and post-cruise chemical analyses. During IODP Expedition 36", "orig": "2, total porosity of the sedimentary section entering the North Sumatra subduction zone was measured. This total porosity is derived from the total water content of core samples thus including pore water and water bound to hydrous minerals like smectite. Clay mineral composition varies over the sedimentary section and is mainly", "generated": "362, scientists collected core samples from various depths within the subduction zone, allowing for a detailed analysis of the sedimentary layers. The data collected during this expedition has been crucial in understanding the physical and chemical properties of these sediments as they transition into deeper parts of the Earth's crust.\n\nThe analysis of inter"}
{"full_text": "Ranking schemes drive many real-world decisions, like, where to study, whom to hire, what to buy, etc. Many of these decisions often come with high consequences. For example, a university can be deemed less prestigious if not featured in a top-k list, and consumers might not even explore products that do not get recommended to buyers. At the heart of most of these decisions are opaque ranking schemes, which dictate the ordering of data entities, but their internal logic is inaccessible or proprietary. Drawing inferences about the ranking differences is like a guessing game to the stakeholders, like, the rankees (i.e., the entities who are ranked, like product companies) and the decision-makers (i.e., who use the rankings, like buyers). In this paper, we aim to enable transparency in ranking interpretation by using algorithmic rankers that learn from available data and by enabling human reasoning about the learned ranking differences using explainable AI (XAI) methods. To realize this aim, we leverage the exploration-explanation paradigm of human-data interaction to let human stakeholders explore subsets and groupings of complex multi-attribute ranking data using visual explanations of model fit and attribute influence on rankings. We realize this explanation paradigm for transparent ranking interpretation in TRIVEA, a visual analytic system that is fueled by: i) visualizations of model fit derived from algorithmic rankers that learn the associations between attributes and rankings from available data and ii) visual explanations derived from XAI methods that help abstract important patterns, like, the relative influence of attributes in different ranking ranges. Using TRIVEA, end users not trained in data science have the agency to transparently reason about the global and local behavior of the rankings without the need to open black-box ranking models and develop confidence in the resulting attribute-based inferences. We demonstrate the efficacy of TRIVEA using multiple usage scenarios and subjective feedback from researchers with diverse domain expertise. Keywords: Visual Analytics, Learning-to-Rank, Explainable ML, Ranking", "prompt": "Ranking schemes drive many real-world decisions, like, where to study, whom to hire, what to buy, etc. Many of these decisions often come with high consequences. For example, a university can be deemed less prestigious if not featured in a top-k list, and consumers might not even explore products that do not", "orig": "get recommended to buyers. At the heart of most of these decisions are opaque ranking schemes, which dictate the ordering of data entities, but their internal logic is inaccessible or proprietary. Drawing inferences about the ranking differences is like a guessing game to the stakeholders, like, the rankees (i.e., the entities who are", "generated": "appear on the first page of search results. Such ranking systems are pivotal because they shape perceptions and influence outcomes in various sectors.\n\nOne critical aspect of these ranking schemes is their transparency and fairness. If a ranking system is perceived as biased or unfair, it can lead to significant dissatisfaction among stakeholders. For instance, if job candidates"}
{"full_text": "Context: Container data types are ubiquitous in computer programming, enabling developers to efficiently store and process collections of data with an easy-to-use programming interface. Many programming languages offer a variety of container implementations in their standard libraries based on data structures offering different capabilities and performance characteristics. Inquiry: Choosing the *best* container for an application is not always straightforward, as performance characteristics can change drastically in different scenarios, and as real-world performance is not always correlated to theoretical complexity. Approach: We present Primrose, a language-agnostic tool for selecting the best performing valid container implementation from a set of container data types that satisfy *properties* given by application developers. Primrose automatically selects the set of valid container implementations for which the *library specifications*, written by the developers of container libraries, satisfies the specified properties. Finally, Primrose ranks the valid library implementations based on their runtime performance. Knowledge: With Primrose, application developers can specify the expected behaviour of a container as a type refinement with *semantic properties*, e.g., if the container should only contain unique values (such as a `set`) or should satisfy the LIFO property of a `stack`. Semantic properties nicely complement *syntactic properties* (i.e., traits, interfaces, or type classes), together allowing developers to specify a container's programming interface *and* behaviour without committing to a concrete implementation. Grounding: We present our prototype implementation of Primrose that preprocesses annotated Rust code, selects valid container implementations and ranks them on their performance. The design of Primrose is, however, language-agnostic, and is easy to integrate into other programming languages that support container data types and traits, interfaces, or type classes. Our implementation encodes properties and library specifications into verification conditions in Rosette, an interface for SMT solvers, which determines the set of valid container implementations. We evaluate Primrose by specifying several container implementations, and measuring the time taken to select valid implementations for various combinations of properties with the solver. We automatically validate that container implementations conform to their library specifications via property-based testing. Importance: This work provides a novel approach to bring abstract modelling and specification of container types directly into the programmer's workflow. Instead of selecting concrete container implementations, application programmers can now work on the level of specification, merely stating the behaviours they require from their container types, and the best implementation can be selected automatically.", "prompt": "Context: Container data types are ubiquitous in computer programming, enabling developers to efficiently store and process collections of data with an easy-to-use programming interface. Many programming languages offer a variety of container implementations in their standard libraries based on data structures offering different capabilities and performance characteristics. Inquiry: Choosing the *best* container for an application", "orig": "is not always straightforward, as performance characteristics can change drastically in different scenarios, and as real-world performance is not always correlated to theoretical complexity. Approach: We present Primrose, a language-agnostic tool for selecting the best performing valid container implementation from a set of container data types that satisfy *properties* given by application developers", "generated": "n involves understanding the specific requirements of the application. The \"best\" container type can vary significantly depending on factors such as whether the application requires fast access times, efficient insertion or deletion operations, memory usage, and whether the data needs to be sorted or not.\n\nFor example, if an application requires constant-time access to elements"}
{"full_text": "The practice of unit testing enables programmers to obtain automated feedback on whether a currently edited program is consistent with the expectations specified in test cases. Feedback is most valuable when it happens immediately, as defects can be corrected instantly before they become harder to fix. With growing and longer running test suites, however, feedback is obtained less frequently and lags behind program changes. The objective of test prioritization is to rank tests so that defects, if present, are found as early as possible or with the least costs. While there are numerous static approaches that output a ranking of tests solely based on the current version of a program, we focus on change-based test prioritization, which recommends tests that likely fail in response to the most recent program change. The canonical approach relies on coverage data and prioritizes tests that cover the changed region, but obtaining and updating coverage data is costly. More recently, information retrieval techniques that exploit overlapping vocabulary between change and tests have proven to be powerful, yet lightweight. In this work, we demonstrate the capabilities of information retrieval for prioritizing tests in dynamic programming languages using Python as example. We discuss and measure previously understudied variation points, including how contextual information around a program change can be used, and design alternatives to the widespread \\emph{TF-IDF} retrieval model tailored to retrieving failing tests. To obtain program changes with associated test failures, we designed a tool that generates a large set of faulty changes from version history along with their test results. Using this data set, we compared existing and new lexical prioritization strategies using four open-source Python projects, showing large improvements over untreated and random test orders and results consistent with related work in statically typed languages. We conclude that lightweight IR-based prioritization strategies are effective tools to predict failing tests in the absence of coverage data or when static analysis is intractable like in dynamic languages. This knowledge can benefit both individual programmers that rely on fast feedback, as well as operators of continuous integration infrastructure, where resources can be freed sooner by detecting defects earlier in the build cycle.", "prompt": "The practice of unit testing enables programmers to obtain automated feedback on whether a currently edited program is consistent with the expectations specified in test cases. Feedback is most valuable when it happens immediately, as defects can be corrected instantly before they become harder to fix. With growing and longer running test suites, however, feedback is obtained less frequently", "orig": "and lags behind program changes. The objective of test prioritization is to rank tests so that defects, if present, are found as early as possible or with the least costs. While there are numerous static approaches that output a ranking of tests solely based on the current version of a program, we focus on change-based test", "generated": "and the time between making changes and seeing if those changes pass all tests increases. This delay can lead to a situation where developers must wait for long periods to confirm that their modifications are working as intended, which can reduce productivity and diminish the effectiveness of the immediate feedback loop that unit testing is designed to provide.\n\nTo mitigate this"}
