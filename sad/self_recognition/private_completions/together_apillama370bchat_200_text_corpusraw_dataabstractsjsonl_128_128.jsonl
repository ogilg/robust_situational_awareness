{"full_text": "Shock modelling predicts an electron density enhancement within the magnetic precursor of C-shocks. Previous observations of SiO, H13CO+, HN13C and H13CN toward the young L1448-mm outflow showed an over-excitation of the ion fluid that was attributed to an electron density enhancement in the precursor. We re-visit this interpretation and test if it still holds when we consider different source morphologies and kinetic temperatures for the observed molecules, and also give some insight on the spatial extent of the electron density enhancement around L1448-mm. We estimate the opacities of H13CO+ and HN13C by observing the J=3\\to2 lines of rarer isotopologues to confirm that the emission is optically thin. To model the excitation of the molecules, we use the large velocity gradient (LVG) approximation with updated collisional coefficients to i) re- analyse the observations toward the positions where the over-excitation of H13CO+ has previously been observed [i.e. toward L1448- mm at offsets (0,0) and (0,-10)], and ii) to investigate if the electron density enhancement is still required for the cases of extended and compact emission, and for kinetic temperatures of up to 400 K. We also report several lines of SiO, HN13C and H13CO+ toward new positions around this outflow, to investigate the spatial extent of the over-excitation of the ions in L1448-mm. From the isotopologue observations, we find that the emission of H13CO+ and HN13C from the precursor is optically thin if this emission is extended. Using the new collisional coefficients, an electron density enhancement is still needed to explain the excitation of H13CO+ for extended emission and for gas temperatures of\\le 400 K toward L1448-mm (0,-10), and possibly also toward L1448-mm (0,0). For compact emission the data cannot be fitted. We do not find any evidence for the over-excitation of the ion fluid toward the newly observed positions around L1448-mm. The observed line emission of SiO, H13CO+ and HN13C toward L1448-mm (0,0) and (0,-10) is consistent with an electron density enhancement in the precursor component, if this emission is spatially extended. This is also true for the case of high gas temperatures (\\le400 K) toward the (0,-10) offset. The electron density enhancement seems to be restricted to the southern, redshifted lobe of the L1448-mm outflow. Interferometric images of the line emission of these molecules are needed to confirm the spatial extent of the over-excitation of the ions and thus, of the electron density enhancement in the magnetic precursor of L1448-mm.", "prompt": "Shock modelling predicts an electron density enhancement within the magnetic precursor of C-shocks. Previous observations of SiO, H13CO+, HN13C and H13CN toward the young L1448-mm outflow showed an over-excitation of the ion fluid that was attributed to an electron density enhancement in the precursor. We re-visit this interpretation and test if it still holds when we consider different source morphologies and kinetic temperatures for the observed molecules, and also give some insight on the spatial extent of the electron density enhancement around L1448-mm. We estimate the opacities of H13CO+ and HN13C by", "orig": "observing the J=3\\to2 lines of rarer isotopologues to confirm that the emission is optically thin. To model the excitation of the molecules, we use the large velocity gradient (LVG) approximation with updated collisional coefficients to i) re- analyse the observations toward the positions where the over-excitation of H13CO+ has previously been observed [i.e. toward L1448- mm at offsets (0,0) and (0,-10)], and ii) to investigate if the electron density enhancement is still required for the cases of extended and compact emission, and for kinetic temperatures of up to", "generated": "using a non-LTE radiative transfer model, which allows us to account for the complex interactions between the molecular gas and the radiation field.\n\nOur results show that the electron density enhancement is not limited to the precursor region, but rather extends to a larger area around L1448-mm. We find that the opacities of H13CO+ and HN13C are significantly affected by the electron density enhancement, which leads to an over-excitation of the ion fluid. This is consistent with previous observations, but our results provide a more detailed picture of the spatial extent of the electron density enhancement. We also investigate the effect of different kinetic"}
{"full_text": "Context. Recent studies show that developers spend most of their programming time testing, verifying and debugging software. As applications become more and more complex, developers demand more advanced debugging support to ease the software development process. Inquiry. Since the 70's many debugging solutions were introduced. Amongst them, online debuggers provide a good insight on the conditions that led to a bug, allowing inspection and interaction with the variables of the program. However, most of the online debugging solutions introduce \\textit{debugging interference} to the execution of the program, i.e. pauses, latency, and evaluation of code containing side-effects. Approach. This paper investigates a novel debugging technique called \\outofplace debugging. The goal is to minimize the debugging interference characteristic of online debugging while allowing online remote capabilities. An \\outofplace debugger transfers the program execution and application state from the debugged application to the debugger application, both running in different processes. Knowledge. On the one hand, \\outofplace debugging allows developers to debug applications remotely, overcoming the need of physical access to the machine where the debugged application is running. On the other hand, debugging happens locally on the remote machine avoiding latency. That makes it suitable to be deployed on a distributed system and handle the debugging of several processes running in parallel. Grounding. We implemented a concrete out-of-place debugger for the Pharo Smalltalk programming language. We show that our approach is practical by performing several benchmarks, comparing our approach with a classic remote online debugger. We show that our prototype debugger outperforms by a 1000 times a traditional remote debugger in several scenarios. Moreover, we show that the presence of our debugger does not impact the overall performance of an application. Importance. This work combines remote debugging with the debugging experience of a local online debugger. Out-of-place debugging is the first online debugging technique that can minimize debugging interference while debugging a remote application. Yet, it still keeps the benefits of online debugging ( e.g. step-by-step execution). This makes the technique suitable for modern applications which are increasingly parallel, distributed and reactive to streams of data from various sources like sensors, UI, network, etc.", "prompt": "Context. Recent studies show that developers spend most of their programming time testing, verifying and debugging software. As applications become more and more complex, developers demand more advanced debugging support to ease the software development process. Inquiry. Since the 70's many debugging solutions were introduced. Amongst them, online debuggers provide a good insight on the conditions that led to a bug, allowing inspection and interaction with the variables of the program. However, most of the online debugging solutions introduce \\textit{debugging interference} to the execution of the program, i.e. pauses, latency, and evaluation of code containing side-effects. Approach. This paper", "orig": "investigates a novel debugging technique called \\outofplace debugging. The goal is to minimize the debugging interference characteristic of online debugging while allowing online remote capabilities. An \\outofplace debugger transfers the program execution and application state from the debugged application to the debugger application, both running in different processes. Knowledge. On the one hand, \\outofplace debugging allows developers to debug applications remotely, overcoming the need of physical access to the machine where the debugged application is running. On the other hand, debugging happens locally on the remote machine avoiding latency. That makes it suitable to be deployed on a distributed system and handle the debugging of several processes", "generated": "presents a novel approach to online debugging that minimizes debugging interference while providing advanced debugging capabilities. Our solution, called \" DebugGenie\", leverages the concept of temporal debugging, where the program execution is temporarily reversed to a previous state, allowing the developer to inspect and interact with the variables without introducing significant interference. DebugGenie uses a combination of static and dynamic analysis to identify the root cause of the bug and provides a step-by-step replay of the execution leading up to the error. This approach enables developers to quickly identify and fix errors, reducing the time spent on debugging and increasing overall productivity. \n\nIn addition, DebugGenie"}
{"full_text": "The impact of outliers and anomalies on model estimation and data processing is of paramount importance, as evidenced by the extensive body of research spanning various fields over several decades: thousands of research papers have been published on the subject. As a consequence, numerous reviews, surveys, and textbooks have sought to summarize the existing literature, encompassing a wide range of methods from both the statistical and data mining communities. While these endeavors to organize and summarize the research are invaluable, they face inherent challenges due to the pervasive nature of outliers and anomalies in all data-intensive applications, irrespective of the specific application field or scientific discipline. As a result, the resulting collection of papers remains voluminous and somewhat heterogeneous. To address the need for knowledge organization in this domain, this paper implements the first systematic meta-survey of general surveys and reviews on outlier and anomaly detection. Employing a classical systematic survey approach, the study collects nearly 500 papers using two specialized scientific search engines. From this comprehensive collection, a subset of 56 papers that claim to be general surveys on outlier detection is selected using a snowball search technique to enhance field coverage. A meticulous quality assessment phase further refines the selection to a subset of 25 high-quality general surveys. Using this curated collection, the paper investigates the evolution of the outlier detection field over a 20-year period, revealing emerging themes and methods. Furthermore, an analysis of the surveys sheds light on the survey writing practices adopted by scholars from different communities who have contributed to this field. Finally, the paper delves into several topics where consensus has emerged from the literature. These include taxonomies of outlier types, challenges posed by high-dimensional data, the importance of anomaly scores, the impact of learning conditions, difficulties in benchmarking, and the significance of neural networks. Non-consensual aspects are also discussed, particularly the distinction between local and global outliers and the challenges in organizing detection methods into meaningful taxonomies.", "prompt": "The impact of outliers and anomalies on model estimation and data processing is of paramount importance, as evidenced by the extensive body of research spanning various fields over several decades: thousands of research papers have been published on the subject. As a consequence, numerous reviews, surveys, and textbooks have sought to summarize the existing literature, encompassing a wide range of methods from both the statistical and data mining communities. While these endeavors to organize and summarize the research are invaluable, they face inherent challenges due to the pervasive nature of outliers and anomalies in all data-intensive applications, irrespective of the specific application field or scientific discipline. As a result, the resulting collection of papers", "orig": "remains voluminous and somewhat heterogeneous. To address the need for knowledge organization in this domain, this paper implements the first systematic meta-survey of general surveys and reviews on outlier and anomaly detection. Employing a classical systematic survey approach, the study collects nearly 500 papers using two specialized scientific search engines. From this comprehensive collection, a subset of 56 papers that claim to be general surveys on outlier detection is selected using a snowball search technique to enhance field coverage. A meticulous quality assessment phase further refines the selection to a subset of 25 high-quality general surveys. Using this curated collection, the paper investigates the evolution of the outlier detection", "generated": "often reflects the fragmented nature of the research, with different subfields and communities developing their own approaches and methodologies. This has led to a landscape where the same concepts are often rediscovered and rebranded, and where the connections between methods developed in different fields are not always apparent. Furthermore, the complexity and diversity of outlier and anomaly detection methods make it difficult to evaluate and compare their performance across different datasets and scenarios. Moreover, the vast majority of these methods are designed to tackle specific types of outliers or anomalies, neglecting the fact that real-world data is often characterized by multiple types of anomalies coexisting. Therefore, there is an urgent"}
{"full_text": "CANVAS caused by RFC1 biallelic expansions is a major cause of inherited sensory neuronopathy. Detection of RFC1 expansion is challenging and CANVAS can be associated with atypical features. We clinically and genetically characterized 50 patients, selected based on the presence of sensory neuronopathy confirmed by EMG. We screened RFC1 expansion by PCR, repeat-primed PCR, and Southern blotting of long-range PCR products, a newly developed method. Neuropathological characterization was performed on the brain and spinal cord of one patient. Most patients (88%) carried a biallelic (AAGGG)n expansion in RFC1. In addition to the core CANVAS phenotype (sensory neuronopathy, cerebellar syndrome, and vestibular impairment), we observed chronic cough (97%), oculomotor signs (85%), motor neuron involvement (55%), dysautonomia (50%), and parkinsonism (10%). Motor neuron involvement was found for 24 of 38 patients (63.1%). First motor neuron signs, such as brisk reflexes, extensor plantar responses, and/or spasticity, were present in 29% of patients, second motor neuron signs, such as fasciculations, wasting, weakness, or a neurogenic pattern on EMG in 18%, and both in 16%. Mixed motor and sensory neuronopathy was observed in 19% of patients. Among six non-RFC1 patients, one carried a heterozygous AAGGG expansion and a pathogenic variant in GRM1. Neuropathological examination of one RFC1 patient with an enriched phenotype, including parkinsonism, dysautonomia, and cognitive decline, showed posterior column and lumbar posterior root atrophy. Degeneration of the vestibulospinal and spinocerebellar tracts was mild. We observed marked astrocytic gliosis and axonal swelling of the synapse between first and second motor neurons in the anterior horn at the lumbar level. The cerebellum showed mild depletion of Purkinje cells, with empty baskets, torpedoes, and astrogliosis characterized by a disorganization of the Bergmann's radial glia. We found neuronal loss in the vagal nucleus. The pars compacta of the substantia nigra was depleted, with widespread Lewy bodies in the locus coeruleus, substantia nigra, hippocampus, entorhinal cortex, and amygdala. We propose new guidelines for the screening of RFC1 expansion, considering different expansion motifs. Here, we developed a new method to more easily detect pathogenic RFC1 expansions. We report frequent motor neuron involvement and different neuronopathy subtypes. Parkinsonism was more prevalent in this cohort than in the general population, 10% versus the expected 1% (p < .001). We describe, for the first time, the spinal cord pathology in CANVAS, showing the alteration of posterior columns and roots, astrocytic gliosis and axonal swelling, suggesting motor neuron synaptic dysfunction.", "prompt": "CANVAS caused by RFC1 biallelic expansions is a major cause of inherited sensory neuronopathy. Detection of RFC1 expansion is challenging and CANVAS can be associated with atypical features. We clinically and genetically characterized 50 patients, selected based on the presence of sensory neuronopathy confirmed by EMG. We screened RFC1 expansion by PCR, repeat-primed PCR, and Southern blotting of long-range PCR products, a newly developed method. Neuropathological characterization was performed on the brain and spinal cord of one patient. Most patients (88%) carried a biallelic (AAGGG)n expansion in RFC1", "orig": ". In addition to the core CANVAS phenotype (sensory neuronopathy, cerebellar syndrome, and vestibular impairment), we observed chronic cough (97%), oculomotor signs (85%), motor neuron involvement (55%), dysautonomia (50%), and parkinsonism (10%). Motor neuron involvement was found for 24 of 38 patients (63.1%). First motor neuron signs, such as brisk reflexes, extensor plantar responses, and/or spasticity, were present in 29% of patients, second motor neuron signs, such as fasciculations, wasting, weakness, or a neuro", "generated": ".\n\nOur comprehensive analysis revealed that the majority of patients (70%) exhibited a classic presentation of CANVAS, characterized by symmetric and progressive sensorimotor polyneuropathy, accompanied by sensory ataxia and deafness. However, a significant proportion (30%) presented with atypical features, including asymmetric or focal neuropathy, suggesting that RFC1 expansions may be underrecognized in patients with non-classic presentations. Notably, we found that the presence of RFC1 expansions correlated with a more severe disease course, as measured by the overall disability sum score (ODSS).\n\nThe neuropathological examination of the brain and spinal cord of one"}
{"full_text": "For many decades, ultrahigh energy charged particles of unknown origin that can be observed from the ground have been a puzzle for particle physicists and astrophysicists. As an attempt to discriminate among several possible production scenarios, astrophysicists try to test the statistical isotropy of the directions of arrival of these cosmic rays. At the highest energies, they are supposed to point toward their sources with good accuracy. However, the observations are so rare that testing the distribution of such samples of directional data on the sphere is nontrivial. In this paper, we choose a nonparametric framework that makes weak hypotheses on the alternative distributions and allows in turn to detect various and possibly unexpected forms of anisotropy. We explore two particular procedures. Both are derived from fitting the empirical distribution with wavelet expansions of densities. We use the wavelet frame introduced by [SIAM J. Math. Anal. 38 (2006b) 574-594 (electronic)], the so-called needlets. The expansions are truncated at scale indices no larger than some ${J^{\\star}}$, and the $L^p$ distances between those estimates and the null density are computed. One family of tests (called Multiple) is based on the idea of testing the distance from the null for each choice of $J=1,\\ldots,{J^{\\star}}$, whereas the so-called PlugIn approach is based on the single full ${J^{\\star}}$ expansion, but with thresholded wavelet coefficients. We describe the practical implementation of these two procedures and compare them to other methods in the literature. As alternatives to isotropy, we consider both very simple toy models and more realistic nonisotropic models based on Physics-inspired simulations. The Monte Carlo study shows good performance of the Multiple test, even at moderate sample size, for a wide sample of alternative hypotheses and for different choices of the parameter ${J^{\\star}}$. On the 69 most energetic events published by the Pierre Auger Collaboration, the needlet-based procedures suggest statistical evidence for anisotropy. Using several values for the parameters of the methods, our procedures yield $p$-values below 1%, but with uncontrolled multiplicity issues. The flexibility of this method and the possibility to modify it to take into account a large variety of extensions of the problem make it an interesting option for future investigation of the origin of ultrahigh energy cosmic rays.", "prompt": "For many decades, ultrahigh energy charged particles of unknown origin that can be observed from the ground have been a puzzle for particle physicists and astrophysicists. As an attempt to discriminate among several possible production scenarios, astrophysicists try to test the statistical isotropy of the directions of arrival of these cosmic rays. At the highest energies, they are supposed to point toward their sources with good accuracy. However, the observations are so rare that testing the distribution of such samples of directional data on the sphere is nontrivial. In this paper, we choose a nonparametric framework that makes weak hypotheses on the alternative distributions", "orig": "and allows in turn to detect various and possibly unexpected forms of anisotropy. We explore two particular procedures. Both are derived from fitting the empirical distribution with wavelet expansions of densities. We use the wavelet frame introduced by [SIAM J. Math. Anal. 38 (2006b) 574-594 (electronic)], the so-called needlets. The expansions are truncated at scale indices no larger than some ${J^{\\star}}$, and the $L^p$ distances between those estimates and the null density are computed. One family of tests (called Multiple) is based on the idea of testing the", "generated": ".\n\nContinuing this line of research, we aim to develop a novel approach that can effectively handle the scarcity of data while maintaining the flexibility to accommodate complex patterns in the arrival directions. To achieve this, we employ a combination of statistical techniques, including bootstrap resampling and Monte Carlo simulations, to create a robust testing framework. This allows us to assess the significance of any observed anisotropies and distinguish between chance fluctuations and genuine deviations from isotropy. Furthermore, we explore the use of machine learning algorithms to identify potential sources of these high-energy particles, leveraging the power of neural networks to recognize patterns in high-dimensional data.\n\nOur methodology is"}
{"full_text": "Context: Container data types are ubiquitous in computer programming, enabling developers to efficiently store and process collections of data with an easy-to-use programming interface. Many programming languages offer a variety of container implementations in their standard libraries based on data structures offering different capabilities and performance characteristics. Inquiry: Choosing the *best* container for an application is not always straightforward, as performance characteristics can change drastically in different scenarios, and as real-world performance is not always correlated to theoretical complexity. Approach: We present Primrose, a language-agnostic tool for selecting the best performing valid container implementation from a set of container data types that satisfy *properties* given by application developers. Primrose automatically selects the set of valid container implementations for which the *library specifications*, written by the developers of container libraries, satisfies the specified properties. Finally, Primrose ranks the valid library implementations based on their runtime performance. Knowledge: With Primrose, application developers can specify the expected behaviour of a container as a type refinement with *semantic properties*, e.g., if the container should only contain unique values (such as a `set`) or should satisfy the LIFO property of a `stack`. Semantic properties nicely complement *syntactic properties* (i.e., traits, interfaces, or type classes), together allowing developers to specify a container's programming interface *and* behaviour without committing to a concrete implementation. Grounding: We present our prototype implementation of Primrose that preprocesses annotated Rust code, selects valid container implementations and ranks them on their performance. The design of Primrose is, however, language-agnostic, and is easy to integrate into other programming languages that support container data types and traits, interfaces, or type classes. Our implementation encodes properties and library specifications into verification conditions in Rosette, an interface for SMT solvers, which determines the set of valid container implementations. We evaluate Primrose by specifying several container implementations, and measuring the time taken to select valid implementations for various combinations of properties with the solver. We automatically validate that container implementations conform to their library specifications via property-based testing. Importance: This work provides a novel approach to bring abstract modelling and specification of container types directly into the programmer's workflow. Instead of selecting concrete container implementations, application programmers can now work on the level of specification, merely stating the behaviours they require from their container types, and the best implementation can be selected automatically.", "prompt": "Context: Container data types are ubiquitous in computer programming, enabling developers to efficiently store and process collections of data with an easy-to-use programming interface. Many programming languages offer a variety of container implementations in their standard libraries based on data structures offering different capabilities and performance characteristics. Inquiry: Choosing the *best* container for an application is not always straightforward, as performance characteristics can change drastically in different scenarios, and as real-world performance is not always correlated to theoretical complexity. Approach: We present Primrose, a language-agnostic tool for selecting the best performing valid container implementation from a set of container data types that satisfy *properties* given by application developers", "orig": ". Primrose automatically selects the set of valid container implementations for which the *library specifications*, written by the developers of container libraries, satisfies the specified properties. Finally, Primrose ranks the valid library implementations based on their runtime performance. Knowledge: With Primrose, application developers can specify the expected behaviour of a container as a type refinement with *semantic properties*, e.g., if the container should only contain unique values (such as a `set`) or should satisfy the LIFO property of a `stack`. Semantic properties nicely complement *syntactic properties* (i.e., traits, interfaces, or type classes), together allowing developers to specify", "generated": ".\n\nPrimrose is designed to bridge the gap between theoretical complexity and real-world performance, allowing developers to make informed decisions about container selection for their specific application needs. By providing a language-agnostic framework, Primrose can be seamlessly integrated into various development workflows, regardless of the programming language or framework being used. Our approach involves a multi-step process, where developers first specify the properties required by their application, such as data structure constraints, performance metrics, and resource limitations. Primrose then utilizes a comprehensive repository of container implementations, which are extensively benchmarked and profiled across diverse scenarios, to identify the most suitable container for the given application requirements."}
{"full_text": "In planetary fluid cores, the density depends on temperature and chemical composition, which diffuse at very different rates. This leads to various instabilities, bearing the name of double-diffusive convection. We investigate rotating double-diffusive convection (RDDC) in fluid spheres. We use the Boussinesq approximation with homogeneous internal thermal and compositional source terms. We focus on the finger regime, in which the thermal gradient is stabilising whereas the compositional one is destabilising. First, we perform a global linear stability analysis in spheres. The critical Rayleigh numbers drastically drop for stably stratified fluids, yielding large-scale convective motions where local analyses predict stability. We evidence the inviscid nature of this large-scale double-diffusive instability, enabling the determination of the marginal stability curve at realistic planetary regimes. In particular, we show that in stably stratified spheres, the Rayleigh numbers $Ra$ at the onset evolve like $Ra \\sim Ek^{-1}$, where $Ek$ is the Ekman number. This differs from rotating convection in unstably stratified spheres, for which $Ra \\sim Ek^{-4/3}$. The domain of existence of inviscid convection thus increases as $Ek^{-1/3}$. Second, we perform nonlinear simulations. We find a transition between two regimes of RDDC, controlled by the strength of the stratification. Furthermore, far from the RDDC onset, we find a dominating equatorially anti-symmetric, large-scale zonal flow slightly above the associated linear onset. Unexpectedly, a purely linear mechanism can explain this phenomenon, even far from the instability onset, yielding a symmetry breaking of the nonlinear flow at saturation. For even stronger stable stratification, the flow becomes mainly equatorially-symmetric and intense zonal jets develop. Finally, we apply our results to the early Earth core. Double diffusion can reduce the critical Rayleigh number by four decades for realistic core conditions. We suggest that the early Earth core was prone to turbulent RDDC, with large-scale zonal flows.", "prompt": "In planetary fluid cores, the density depends on temperature and chemical composition, which diffuse at very different rates. This leads to various instabilities, bearing the name of double-diffusive convection. We investigate rotating double-diffusive convection (RDDC) in fluid spheres. We use the Boussinesq approximation with homogeneous internal thermal and compositional source terms. We focus on the finger regime, in which the thermal gradient is stabilising whereas the compositional one is destabilising. First, we perform a global linear stability analysis in spheres. The critical Rayleigh numbers drastically drop for stably stratified fluids, yielding large-scale conv", "orig": "ective motions where local analyses predict stability. We evidence the inviscid nature of this large-scale double-diffusive instability, enabling the determination of the marginal stability curve at realistic planetary regimes. In particular, we show that in stably stratified spheres, the Rayleigh numbers $Ra$ at the onset evolve like $Ra \\sim Ek^{-1}$, where $Ek$ is the Ekman number. This differs from rotating convection in unstably stratified spheres, for which $Ra \\sim Ek^{-4/3}$. The domain of existence of inviscid convection thus increases as $Ek^{-1/3}", "generated": "ective patterns.\n\nOur results show that the onset of RDDC is significantly influenced by the ratio of the thermal to compositional diffusivities. We observe a complex interplay between the stabilising effect of the thermal gradient and the destabilising effect of the compositional gradient, leading to a rich array of convective patterns. In particular, we find that the finger regime is characterised by the formation of(radial, spherical) rolls, which are sustained by the balancing effect of the two gradients.\n\nTo further investigate the nonlinear dynamics of RDDC, we perform direct numerical simulations (DNS) using a pseudospectral method. Our simulations reveal a"}
{"full_text": "In this study, we investigate porosity evolution through the sedimentary input section of the North Sumatra Subduction zone by quantifying interstitial porosity, bound water content and fracture porosity based on IODP Expedition 362 data and post-cruise chemical analyses. During IODP Expedition 362, total porosity of the sedimentary section entering the North Sumatra subduction zone was measured. This total porosity is derived from the total water content of core samples thus including pore water and water bound to hydrous minerals like smectite. Clay mineral composition varies over the sedimentary section and is mainly kaolinite/illite in the Nicobar Fan units and smectite/illite in the prefan pelagic unit below. The prefan pelagic unit shows anomalously high total porosity values and is stratigraphically correlated to a high amplitude negative polarity (HANP) seismic reflector located landward. This HANP reflector has been previously interpreted as a porous fluid-rich layer where the d{\\'e}collement may develop along parts of the margin as a consequence of pore pressure buildup. We estimate clay bound water content from Cation Exchange Capacity (CEC) which gives information about the smectite/illite composition and soluble chloride content data. Interstitial porosity corresponds to onboard total porosity corrected from clay bound water and is more relevant in terms of sediment compaction state and fluid flow properties. Interstitial porosity versus vertical effective stress curve shows no evidence of undercompaction and suggests that the input section 2 has been experiencing normal consolidation due to high sediment accumulation rate. The porosity anomaly observed in the prefan pelagic unit results from the local occurrence of water-bearing minerals like smectite rather than excess pore pressure, which might, however, buildup more landward in the basin. We also estimate fracture porosity using a resistivity model for shales used in previous works based on wireline resistivity log and show that fracture porosity yields 4-6% in damaged parts of the sedimentary section investigated.", "prompt": "In this study, we investigate porosity evolution through the sedimentary input section of the North Sumatra Subduction zone by quantifying interstitial porosity, bound water content and fracture porosity based on IODP Expedition 362 data and post-cruise chemical analyses. During IODP Expedition 362, total porosity of the sedimentary section entering the North Sumatra subduction zone was measured. This total porosity is derived from the total water content of core samples thus including pore water and water bound to hydrous minerals like smectite. Clay mineral composition varies over the sedimentary section and is mainly kaolinite/", "orig": "illite in the Nicobar Fan units and smectite/illite in the prefan pelagic unit below. The prefan pelagic unit shows anomalously high total porosity values and is stratigraphically correlated to a high amplitude negative polarity (HANP) seismic reflector located landward. This HANP reflector has been previously interpreted as a porous fluid-rich layer where the d{\\'e}collement may develop along parts of the margin as a consequence of pore pressure buildup. We estimate clay bound water content from Cation Exchange Capacity (CEC) which gives information about the smectite/illite", "generated": "illite-dominated in the shallowest part of the section, gradually transitioning to smectite-rich assemblages at greater depths.\n\nOur investigation focuses on the sedimentary input section, which spans approximately 3 km in thickness, and comprises a mix of terrigenous and marine sediments. The variation in clay mineral composition is thought to be controlled by changes in sediment provenance, with the shallowest part of the section receiving more terrestrially-derived sediment and the deeper part receiving more marine sediments. The smectite-rich assemblages in the deeper part of the section are likely derived from the alteration of volcanic rocks and sediments"}
{"full_text": "Numerical schemes for the solution of the Euler equations have recently been developed, which involve the discretisation of the internal energy equation, with corrective terms to ensure the correct capture of shocks, and, more generally, the consistency in the Lax-Wendroff sense. These schemes may be staggered or colocated, using either struc-tured meshes or general simplicial or tetrahedral/hexahedral meshes. The time discretization is performed by fractional-step algorithms; these may be either based on semi-implicit pressure correction techniques or segregated in such a way that only explicit steps are involved (referred to hereafter as \"explicit\" variants). In order to ensure the positivity of the density, the internal energy and the pressure, the discrete convection operators for the mass and internal energy balance equations are carefully designed; they use an upwind technique with respect to the material velocity only. The construction of the fluxes thus does not need any Rie-mann or approximate Riemann solver, and yields easily implementable algorithms. The stability is obtained without restriction on the time step for the pressure correction scheme and under a CFL-like condition for explicit variants: preservation of the integral of the total energy over the computational domain, and positivity of the density and the internal energy. The semi-implicit first-order upwind scheme satisfies a local discrete entropy inequality. If a MUSCL-like scheme is used in order to limit the scheme diffusion, then a weaker property holds: the entropy inequality is satisfied up to a remainder term which is shown to tend to zero with the space and time steps, if the discrete solution is controlled in L $\\infty$ and BV norms. The explicit upwind variant also satisfies such a weaker property, at the price of an estimate for the velocity which could be derived from the introduction of a new stabilization term in the momentum balance. Still for the explicit scheme, with the above-mentioned MUSCL-like scheme, the same result only holds if the ratio of the time to the space step tends to zero.", "prompt": "Numerical schemes for the solution of the Euler equations have recently been developed, which involve the discretisation of the internal energy equation, with corrective terms to ensure the correct capture of shocks, and, more generally, the consistency in the Lax-Wendroff sense. These schemes may be staggered or colocated, using either struc-tured meshes or general simplicial or tetrahedral/hexahedral meshes. The time discretization is performed by fractional-step algorithms; these may be either based on semi-implicit pressure correction techniques or segregated in such a way that only explicit steps are involved (referred to hereafter as", "orig": "\"explicit\" variants). In order to ensure the positivity of the density, the internal energy and the pressure, the discrete convection operators for the mass and internal energy balance equations are carefully designed; they use an upwind technique with respect to the material velocity only. The construction of the fluxes thus does not need any Rie-mann or approximate Riemann solver, and yields easily implementable algorithms. The stability is obtained without restriction on the time step for the pressure correction scheme and under a CFL-like condition for explicit variants: preservation of the integral of the total energy over the computational domain, and positivity of the density and the internal", "generated": "fully explicit algorithms). The latter approach allows for a more flexible and efficient solution process, as it eliminates the need for solving linear systems or implicit equations, which can be computationally expensive. However, this comes at the cost of possibly reduced accuracy and stability. On the other hand, semi-implicit methods can provide improved robustness and accuracy, particularly in regions with strong shock waves or complex flow features. Nevertheless, both approaches have their merits and can be successfully applied to a wide range of fluid dynamics problems, from simple academic test cases to more complex industrial applications.\n\nOne of the key challenges in developing these numerical schemes is ensuring that they accurately"}
{"full_text": "When we consider a proper holomorphic map \\ $\\tilde{f}: X \\to C$ \\ of a complex manifold \\ $X$ \\ on a smooth complex curve \\ $C$ \\ with a critical value at a point \\ $0$ \\ in \\ $C$, the choice of a local coordinate near this point allows to dispose of an holomorphic function \\ $f$. Then we may construct, using this function, an (a,b)-modules structure on the cohomology sheaves of the formal completion (in \\ $f$) \\ of the complex of sheaves \\ $(Ker\\, df^{\\bullet},d^{\\bullet})$. These (a,b)-modules represent a filtered version of the Gauss-Manin connection of \\ $f$. The most simple example of this construction is the Brieskorn module (see [Br.70]) of a function with an isolated singular point. See [B.08] for the case of a 1-dimensional critical locus. But it is clear that this construction depends seriously on the choice of the function \\ $f$ \\ that is to say on the choice of the local coordinate near the critical point \\ $0$ \\ in the complex curve \\ $C$. The aim of the present paper is to study the behaviour of such constructions when we make a change of local coordinate near the origin. We consider the case of \\ $[\\lambda]-$primitive frescos, which are monogenic geometric (a,b)-modules corresponding to a minimal filtered differential equation associated to a relative de Rham cohomology class on \\ $X$ \\ (see [B.09-a] and [B.09-b]). An holomorphic parameter is a function on the set of isomorphism classes of frescos which behave holomorphically in an holomorphic family of frescos. In general, an holomorphic parameter is not invariant by a change of variable, but we prove a theorem of stability of holomorphic families of frescos by a change of variable and it implies that an holomorphic parameter gives again an holomorphic parameter by a change of variable. We construct here two different kinds of holomorphic parameters which are (quasi-)invariant by change of variable. The first kind is associated to Jordan blocks of the monodromy with size at least two. The second kind is associated to the semi-simple part of the monodromy and look like some \"cross ratio\" of eigenvectors. They allow, in the situation describe above, to associate to a given (vanishing) relative de Rham cohomology class some numbers, which will depend holomorphically of our data, and are independant of the choice of the local coordinate near \\ $0$ \\ to study the Gauss-Manin connection of this degeneration of compact complex manifolds.", "prompt": "When we consider a proper holomorphic map \\ $\\tilde{f}: X \\to C$ \\ of a complex manifold \\ $X$ \\ on a smooth complex curve \\ $C$ \\ with a critical value at a point \\ $0$ \\ in \\ $C$, the choice of a local coordinate near this point allows to dispose of an holomorphic function \\ $f$. Then we may construct, using this function, an (a,b)-modules structure on the cohomology sheaves of the formal completion (in \\ $f$) \\ of the complex of sheaves \\ $(Ker\\, df^{\\", "orig": "bullet},d^{\\bullet})$. These (a,b)-modules represent a filtered version of the Gauss-Manin connection of \\ $f$. The most simple example of this construction is the Brieskorn module (see [Br.70]) of a function with an isolated singular point. See [B.08] for the case of a 1-dimensional critical locus. But it is clear that this construction depends seriously on the choice of the function \\ $f$ \\ that is to say on the choice of the local coordinate near the critical point \\ $0$ \\ in the complex curve \\ $C$. The aim of the", "generated": "vee} \\to \\Omega^{1}_{X/C} \\to 0)$, where $df^{\\vee}$ denotes the dual of the Jacobian matrix of $f$.\n\nThis construction is particularly useful when studying the degeneration of complex structures on $X$ as the critical value $0$ is approached. In this context, the (a,b)-modules structure can be used to analyze the asymptotic behavior of the cohomology sheaves, providing valuable insights into the geometry of the degenerating family. Moreover, the choice of local coordinate near the critical point $0$ allows for a more explicit description of"}
{"full_text": "The practice of unit testing enables programmers to obtain automated feedback on whether a currently edited program is consistent with the expectations specified in test cases. Feedback is most valuable when it happens immediately, as defects can be corrected instantly before they become harder to fix. With growing and longer running test suites, however, feedback is obtained less frequently and lags behind program changes. The objective of test prioritization is to rank tests so that defects, if present, are found as early as possible or with the least costs. While there are numerous static approaches that output a ranking of tests solely based on the current version of a program, we focus on change-based test prioritization, which recommends tests that likely fail in response to the most recent program change. The canonical approach relies on coverage data and prioritizes tests that cover the changed region, but obtaining and updating coverage data is costly. More recently, information retrieval techniques that exploit overlapping vocabulary between change and tests have proven to be powerful, yet lightweight. In this work, we demonstrate the capabilities of information retrieval for prioritizing tests in dynamic programming languages using Python as example. We discuss and measure previously understudied variation points, including how contextual information around a program change can be used, and design alternatives to the widespread \\emph{TF-IDF} retrieval model tailored to retrieving failing tests. To obtain program changes with associated test failures, we designed a tool that generates a large set of faulty changes from version history along with their test results. Using this data set, we compared existing and new lexical prioritization strategies using four open-source Python projects, showing large improvements over untreated and random test orders and results consistent with related work in statically typed languages. We conclude that lightweight IR-based prioritization strategies are effective tools to predict failing tests in the absence of coverage data or when static analysis is intractable like in dynamic languages. This knowledge can benefit both individual programmers that rely on fast feedback, as well as operators of continuous integration infrastructure, where resources can be freed sooner by detecting defects earlier in the build cycle.", "prompt": "The practice of unit testing enables programmers to obtain automated feedback on whether a currently edited program is consistent with the expectations specified in test cases. Feedback is most valuable when it happens immediately, as defects can be corrected instantly before they become harder to fix. With growing and longer running test suites, however, feedback is obtained less frequently and lags behind program changes. The objective of test prioritization is to rank tests so that defects, if present, are found as early as possible or with the least costs. While there are numerous static approaches that output a ranking of tests solely based on the current version of a program, we focus on change-based test", "orig": "prioritization, which recommends tests that likely fail in response to the most recent program change. The canonical approach relies on coverage data and prioritizes tests that cover the changed region, but obtaining and updating coverage data is costly. More recently, information retrieval techniques that exploit overlapping vocabulary between change and tests have proven to be powerful, yet lightweight. In this work, we demonstrate the capabilities of information retrieval for prioritizing tests in dynamic programming languages using Python as example. We discuss and measure previously understudied variation points, including how contextual information around a program change can be used, and design alternatives to the widespread \\emph{TF-IDF}", "generated": "prioritization techniques that take into account the changes made to the program.\n\nThese change-based approaches prioritize tests that are most likely to fail based on the recent modifications made to the code. This is achieved by analyzing the differences between the previous and current versions of the program, identifying the parts of the code that have changed, and then selecting the tests that are most relevant to those changes. This approach ensures that the tests that are most likely to fail are executed first, reducing the time and effort required to identify and fix defects. Furthermore, change-based test prioritization can also be used to optimize the testing process by selecting a subset of tests that provide"}
{"full_text": "Core helium burning is the dominant source of energy of extreme horizontal branch stars, as the hydrogen envelope is too small to contribute to the nuclear energy output. The evolution of each mass in the HR diagram occurs along vertical tracks that, when the core helium is consumed, evolve to higher Teff and then to the white dwarf stage. The larger is the mass, the smaller is the Teff of the models, so that the zero age horizontal branch (ZAHB) is \"horizontal\". In this paper we show that, if the helium mass fraction (Y) of the envelope is larger than Y~0.5, the shape of the tracks changes completely: the hydrogen burning becomes efficient again also for very small envelope masses, thanks to the higher molecular weight and to the higher temperatures of the hydrogen shell. The larger is Y, the smaller is the envelope mass that provides strong H-shell burning. These tracks have a curled shape, are located at a Teff following the approximate relation Teff=8090+ 32900xY, and become more luminous for larger envelope masses. Consequently, the ZAHB of the very high helium models is \"vertical\" in the HR diagram. Synthetic models based on these tracks nicely reproduce the location and shape of the \"blue hook\" in the globular cluster wCen, best fit by a very high Teff (bluer) sequence with Y=0.80 and a cooler (redder) one with Y=0.65. Although these precise values of Y may depend on the color-Teff conversions, we know that the helium content of the progenitors of the blue hook stars can not be larger than Y~0.38-0.40, if they are descendants of the cluster blue main sequence. Consequently, this interpretation implies that all these objects must in fact be progeny of the blue main sequence, but they have all suffered further deep mixing, that has largely and uniformly increased their surface helium abundance, during the red giant branch evolution. A late helium flash can not be the cause of this deep mixing, as the models we propose have hydrogen rich envelopes much more massive than those required for a late flash. We discuss different models of deep mixing proposed in the literature, and conclude that our interpretation of the blue hook can not be ruled out, but requires a much deeper investigation before it can be accepted.", "prompt": "Core helium burning is the dominant source of energy of extreme horizontal branch stars, as the hydrogen envelope is too small to contribute to the nuclear energy output. The evolution of each mass in the HR diagram occurs along vertical tracks that, when the core helium is consumed, evolve to higher Teff and then to the white dwarf stage. The larger is the mass, the smaller is the Teff of the models, so that the zero age horizontal branch (ZAHB) is \"horizontal\". In this paper we show that, if the helium mass fraction (Y) of the envelope is larger than Y~0.5, the shape of the", "orig": "tracks changes completely: the hydrogen burning becomes efficient again also for very small envelope masses, thanks to the higher molecular weight and to the higher temperatures of the hydrogen shell. The larger is Y, the smaller is the envelope mass that provides strong H-shell burning. These tracks have a curled shape, are located at a Teff following the approximate relation Teff=8090+ 32900xY, and become more luminous for larger envelope masses. Consequently, the ZAHB of the very high helium models is \"vertical\" in the HR diagram. Synthetic models based on these tracks nicely reproduce the location and shape of the \"blue hook", "generated": "horizontal branch is significantly altered, leading to a deviation from the classical horizontal branch morphology.\n\nThis deviation is characterized by a pronounced bend in the horizontal branch, which becomes more pronounced as Y increases. This bend is a result of the increased helium abundance in the envelope, which leads to a higher opacity and a subsequent increase in the star's surface temperature. As a consequence, the star's luminosity also increases, causing it to move towards the blue part of the HR diagram. This trend is more pronounced for more massive stars, which have a larger helium core and a more pronounced helium enrichment in the envelope.\n\nThe implications of this finding are significant,"}
{"full_text": "Ranking schemes drive many real-world decisions, like, where to study, whom to hire, what to buy, etc. Many of these decisions often come with high consequences. For example, a university can be deemed less prestigious if not featured in a top-k list, and consumers might not even explore products that do not get recommended to buyers. At the heart of most of these decisions are opaque ranking schemes, which dictate the ordering of data entities, but their internal logic is inaccessible or proprietary. Drawing inferences about the ranking differences is like a guessing game to the stakeholders, like, the rankees (i.e., the entities who are ranked, like product companies) and the decision-makers (i.e., who use the rankings, like buyers). In this paper, we aim to enable transparency in ranking interpretation by using algorithmic rankers that learn from available data and by enabling human reasoning about the learned ranking differences using explainable AI (XAI) methods. To realize this aim, we leverage the exploration-explanation paradigm of human-data interaction to let human stakeholders explore subsets and groupings of complex multi-attribute ranking data using visual explanations of model fit and attribute influence on rankings. We realize this explanation paradigm for transparent ranking interpretation in TRIVEA, a visual analytic system that is fueled by: i) visualizations of model fit derived from algorithmic rankers that learn the associations between attributes and rankings from available data and ii) visual explanations derived from XAI methods that help abstract important patterns, like, the relative influence of attributes in different ranking ranges. Using TRIVEA, end users not trained in data science have the agency to transparently reason about the global and local behavior of the rankings without the need to open black-box ranking models and develop confidence in the resulting attribute-based inferences. We demonstrate the efficacy of TRIVEA using multiple usage scenarios and subjective feedback from researchers with diverse domain expertise. Keywords: Visual Analytics, Learning-to-Rank, Explainable ML, Ranking", "prompt": "Ranking schemes drive many real-world decisions, like, where to study, whom to hire, what to buy, etc. Many of these decisions often come with high consequences. For example, a university can be deemed less prestigious if not featured in a top-k list, and consumers might not even explore products that do not get recommended to buyers. At the heart of most of these decisions are opaque ranking schemes, which dictate the ordering of data entities, but their internal logic is inaccessible or proprietary. Drawing inferences about the ranking differences is like a guessing game to the stakeholders, like, the rankees (i.e., the entities who are", "orig": "ranked, like product companies) and the decision-makers (i.e., who use the rankings, like buyers). In this paper, we aim to enable transparency in ranking interpretation by using algorithmic rankers that learn from available data and by enabling human reasoning about the learned ranking differences using explainable AI (XAI) methods. To realize this aim, we leverage the exploration-explanation paradigm of human-data interaction to let human stakeholders explore subsets and groupings of complex multi-attribute ranking data using visual explanations of model fit and attribute influence on rankings. We realize this explanation paradigm for transparent ranking interpretation in TRIVEA, a visual analytic system that", "generated": "ranked).\n\nAs a result, stakeholders often feel disillusioned and powerless in the face of these mysterious ranking systems. They are left wondering why certain entities are favored over others, and what criteria are used to make these decisions. This lack of transparency can lead to feelings of mistrust and frustration, particularly when the ranking schemes are perceived as biased or unfair. For instance, a job applicant may feel that they were unfairly rejected from a position due to a flawed ranking algorithm, or a small business owner may feel that they are being unfairly penalized by a search engine's ranking scheme.\n\nThe opaqueness of ranking schemes can also have broader societal implications"}
{"full_text": "Several multiagent reinforcement learning (MARL) algorithms have been proposed to optimize agents decisions. Due to the complexity of the problem, the majority of the previously developed MARL algorithms assumed agents either had some knowledge of the underlying game (such as Nash equilibria) and/or observed other agents actions and the rewards they received. We introduce a new MARL algorithm called the Weighted Policy Learner (WPL), which allows agents to reach a Nash Equilibrium (NE) in benchmark 2-player-2-action games with minimum knowledge. Using WPL, the only feedback an agent needs is its own local reward (the agent does not observe other agents actions or rewards). Furthermore, WPL does not assume that agents know the underlying game or the corresponding Nash Equilibrium a priori. We experimentally show that our algorithm converges in benchmark two-player-two-action games. We also show that our algorithm converges in the challenging Shapleys game where previous MARL algorithms failed to converge without knowing the underlying game or the NE. Furthermore, we show that WPL outperforms the state-of-the-art algorithms in a more realistic setting of 100 agents interacting and learning concurrently. An important aspect of understanding the behavior of a MARL algorithm is analyzing the dynamics of the algorithm: how the policies of multiple learning agents evolve over time as agents interact with one another. Such an analysis not only verifies whether agents using a given MARL algorithm will eventually converge, but also reveals the behavior of the MARL algorithm prior to convergence. We analyze our algorithm in two-player-two-action games and show that symbolically proving WPLs convergence is difficult, because of the non-linear nature of WPLs dynamics, unlike previous MARL algorithms that had either linear or piece-wise-linear dynamics. Instead, we numerically solve WPLs dynamics differential equations and compare the solution to the dynamics of previous MARL algorithms.", "prompt": "Several multiagent reinforcement learning (MARL) algorithms have been proposed to optimize agents decisions. Due to the complexity of the problem, the majority of the previously developed MARL algorithms assumed agents either had some knowledge of the underlying game (such as Nash equilibria) and/or observed other agents actions and the rewards they received. We introduce a new MARL algorithm called the Weighted Policy Learner (WPL), which allows agents to reach a Nash Equilibrium (NE) in benchmark 2-player-2-action games with minimum knowledge. Using WPL, the only feedback an agent needs is its own local reward (the agent does not", "orig": "observe other agents actions or rewards). Furthermore, WPL does not assume that agents know the underlying game or the corresponding Nash Equilibrium a priori. We experimentally show that our algorithm converges in benchmark two-player-two-action games. We also show that our algorithm converges in the challenging Shapleys game where previous MARL algorithms failed to converge without knowing the underlying game or the NE. Furthermore, we show that WPL outperforms the state-of-the-art algorithms in a more realistic setting of 100 agents interacting and learning concurrently. An important aspect of understanding the behavior of a MARL algorithm is analyzing the dynamics of the algorithm", "generated": "need to observe other agents' actions or rewards).\n\nThe WPL algorithm is based on a decentralized approach, where each agent learns its own policy independently, without explicit communication with other agents. This is achieved by maintaining a weighted policy that is updated based on the agent's local reward. The weights assigned to each policy are adjusted based on the agent's perceived performance, which is measured by its cumulative reward. The WPL algorithm is able to converge to a NE in benchmark 2-player-2-action games, where the agents' policies are optimal given the other agent's policy.\n\nOne of the key advantages of the WPL algorithm is its ability"}
{"full_text": "Introduction: Nottingham grading system is a major prognostic factor for invasive breast carcinoma (IBC). Its determination requires the evaluation of the mitotic score (MS) which is subject to low intra- and inter-observer reproducibility. The MS shall be performed in the most proliferative area of the tumor, which determination is hard but critical. Artificial intelligence based tools could help pathologists to detect mitosis on whole slide images (WSI). Objective: The aim of this study was to evaluate the contribution of a mitosis detection algorithms pecifically developed to assist the pathologist during the evaluation of the MS on WSI. Methods: Algorithmic mitosis detection is a two-step process: first the algorithm detects candidate objects resembling mitosis, then the selection is refined by a classifier. The densest mitoticregions are shown to the pathologist, then he can establish the MS with algorithm results. For this study, three expert pathologists have determined a consensual ground truth for MS on fifty WSI of IBC. Those slides were also submitted to two readers pathologists who evaluated the MS of each slide twice, with and without the assistance of the algorithm, with a four week wash-out period. Interobserver reproducibility was measured by evaluating the scores obtained with, and without assistance between two readers pathologists and was also measured between each reader pathologist and the expert ground truth to determine the accuracy of the established score. Results:Baseline linearly weighted Cohen's Kappa for interobserver agreement of MS between two readers pathologists was 0.482. Using the algorithm generated mitotic detection in WSI, the agreement score increased to 0.672. Baseline linearly weighted Cohen's Kappa for interobserver agreement of MS between each reader pathologist and expert consensus was 0.378 and 0.457 for pathologist 1 and 2 respectively. Using the algorithm generated mitoticdetection in WSI, the agreement score increased respectively to 0.629 and 0.726. Conclusion:The use of the developed algorithm constitutes a viable approach to assist the pathologist for the evaluation of the MS of IBC on WSI. Its use makes it possible to improve interobserver reproducibility between pathologists and the accuracy of the score established by expert consensus. The use of such a tool constitutes a new approach in the evaluation of the mitoticscore which could lead to an evolution of practices.", "prompt": "Introduction: Nottingham grading system is a major prognostic factor for invasive breast carcinoma (IBC). Its determination requires the evaluation of the mitotic score (MS) which is subject to low intra- and inter-observer reproducibility. The MS shall be performed in the most proliferative area of the tumor, which determination is hard but critical. Artificial intelligence based tools could help pathologists to detect mitosis on whole slide images (WSI). Objective: The aim of this study was to evaluate the contribution of a mitosis detection algorithms pecifically developed to assist the pathologist during the evaluation of the MS on WSI. Methods:", "orig": "Algorithmic mitosis detection is a two-step process: first the algorithm detects candidate objects resembling mitosis, then the selection is refined by a classifier. The densest mitoticregions are shown to the pathologist, then he can establish the MS with algorithm results. For this study, three expert pathologists have determined a consensual ground truth for MS on fifty WSI of IBC. Those slides were also submitted to two readers pathologists who evaluated the MS of each slide twice, with and without the assistance of the algorithm, with a four week wash-out period. Interobserver reproducibility was measured by evaluating the scores obtained with,", "generated": "A total of 500 breast cancer cases were randomly selected from a hospital database and WSI were generated using a high-resolution digital scanner. The mitosis detection algorithm was trained on a separate dataset of 200 cases, annotated by experienced pathologists. The algorithm was then applied to the 500 study cases, and the performance was evaluated using receiver operating characteristic (ROC) curve analysis. Additionally, 20 pathologists from different institutions were asked to evaluate the same 500 cases using traditional microscopy and their results were compared to those obtained by the algorithm. Inter-observer variability was assessed using the Fleiss' kappa statistic. \n\nThe results showed"}
{"full_text": "The finance industry is producing an increasing amount of datasets that investment professionals can consider to be influential on the price of financial assets. These datasets were initially mainly limited to exchange data, namely price, capitalization and volume. Their coverage has now considerably expanded to include, for example, macroeconomic data, supply and demand of commodities, balance sheet data and more recently extra-financial data such as ESG scores. This broadening of the factors retained as influential constitutes a serious challenge for statistical modeling. Indeed, the instability of the correlations between these factors makes it practically impossible to identify the joint laws needed to construct scenarios. Fortunately, spectacular advances in Deep Learning field in recent years have given rise to GANs. GANs are a type of generative machine learning models that produce new data samples with the same characteristics as a training data distribution in an unsupervised way, avoiding data assumptions and human induced biases. In this work, we are exploring the use of GANs for synthetic financial scenarios generation. This pilot study is the result of a collaboration between Fujitsu and Advestis and it will be followed by a thorough exploration of the use cases that can benefit from the proposed solution. We propose a GANs-based algorithm that allows the replication of multivariate data representing several properties (including, but not limited to, price, market capitalization, ESG score, controversy score,. . .) of a set of stocks. This approach differs from examples in the financial literature, which are mainly focused on the reproduction of temporal asset price scenarios. We also propose several metrics to evaluate the quality of the data generated by the GANs. This approach is well fit for the generation of scenarios, the time direction simply arising as a subsequent (eventually conditioned) generation of data points drawn from the learned distribution. Our method will allow to simulate high dimensional scenarios (compared to $\\lesssim10$ features currently employed in most recent use cases) where network complexity is reduced thanks to a wisely performed feature engineering and selection. Complete results will be presented in a forthcoming study.", "prompt": "The finance industry is producing an increasing amount of datasets that investment professionals can consider to be influential on the price of financial assets. These datasets were initially mainly limited to exchange data, namely price, capitalization and volume. Their coverage has now considerably expanded to include, for example, macroeconomic data, supply and demand of commodities, balance sheet data and more recently extra-financial data such as ESG scores. This broadening of the factors retained as influential constitutes a serious challenge for statistical modeling. Indeed, the instability of the correlations between these factors makes it practically impossible to identify the joint laws needed to construct scenarios. Fortunately, spectacular advances in Deep", "orig": "Learning field in recent years have given rise to GANs. GANs are a type of generative machine learning models that produce new data samples with the same characteristics as a training data distribution in an unsupervised way, avoiding data assumptions and human induced biases. In this work, we are exploring the use of GANs for synthetic financial scenarios generation. This pilot study is the result of a collaboration between Fujitsu and Advestis and it will be followed by a thorough exploration of the use cases that can benefit from the proposed solution. We propose a GANs-based algorithm that allows the replication of multivariate data representing several", "generated": "Learning and Artificial Intelligence have provided new tools and methods that can handle the complexity and high dimensionality of these datasets.\n\nOne of the most promising approaches is the use of neural networks, which have been shown to be effective in modeling non-linear relationships and identifying complex patterns in large datasets. For instance, convolutional neural networks can be used to analyze and extract relevant features from unstructured data such as text and images, while recurrent neural networks can model temporal dependencies and relationships in time-series data. Moreover, the development of visualization tools and techniques has enabled researchers and practitioners to better understand and interpret the results of these models, facilitating the identification of key drivers"}
{"full_text": "Lean processes focus on doing only necessery things in an efficient way. Artificial intelligence and Machine Learning offer new opportunities to optimizing processes. The presented approach demonstrates an improvement of the test process by using Machine Learning as a support tool for test management. The scope is the semi-automation of the selection of regression tests. The proposed lean testing process uses Machine Learning as a supporting machine, while keeping the human test manager in charge of the adequate test case selection. 1 Introduction Many established long running projects and programs are execute regression tests during the release tests. The regression tests are the part of the release test to ensure that functionality from past releases still works fine in the new release. In many projects, a significant part of these regression tests are not automated and therefore executed manually. Manual tests are expensive and time intensive [1], which is why often only a relevant subset of all possible regression tests are executed in order to safe time and money. Depending on the software process, different approaches can be used to identify the right set of regression tests. The source code file level is a frequent entry point for this identification [2]. Advanced approaches combine different file level methods [3]. To handle black-box tests, methods like [4] or [5] can be used for test case prioritiza-tion. To decide which tests can be skipped, a relevance ranking of the tests in a regression test suite is needed. Based on the relevance a test is in or out of the regression test set for a specific release. This decision is a task of the test manager supported by experts. The task can be time-consuming in case of big (often a 4-to 5-digit number) regression test suites because the selection is specific to each release. Trends are going to continuous prioritization [6], which this work wants to support with the presented ML based approach for black box regression test case prioritization. Any regression test selection is made upon release specific changes. Changes can be new or deleted code based on refactoring or implementation of new features. But also changes on externals systems which are connected by interfaces have to be considered", "prompt": "Lean processes focus on doing only necessery things in an efficient way. Artificial intelligence and Machine Learning offer new opportunities to optimizing processes. The presented approach demonstrates an improvement of the test process by using Machine Learning as a support tool for test management. The scope is the semi-automation of the selection of regression tests. The proposed lean testing process uses Machine Learning as a supporting machine, while keeping the human test manager in charge of the adequate test case selection. 1 Introduction Many established long running projects and programs are execute regression tests during the release tests. The regression tests are the part of the release test to ensure that functionality from past releases still works", "orig": "fine in the new release. In many projects, a significant part of these regression tests are not automated and therefore executed manually. Manual tests are expensive and time intensive [1], which is why often only a relevant subset of all possible regression tests are executed in order to safe time and money. Depending on the software process, different approaches can be used to identify the right set of regression tests. The source code file level is a frequent entry point for this identification [2]. Advanced approaches combine different file level methods [3]. To handle black-box tests, methods like [4] or [5] can be used for test case prioritiza-t", "generated": ".\n\nHowever, the regression test process can be time-consuming and labor-intensive, especially in large-scale projects with extensive codebases. The traditional approach involves manual selection of test cases, which can lead to human error, bias, and inefficiency. This is where Machine Learning comes into play, offering a more efficient and effective way to identify the most critical test cases required for regression testing. By leveraging Machine Learning algorithms, the test process can be optimized to reduce the number of test cases while maintaining high test coverage. \n\nThe proposed approach utilizes historical data from previous releases to train a Machine Learning model that can predict the likelihood of defects or errors in specific"}
{"full_text": "The symmetry algebras of certain families of quantum spin chains are considered in detail. The simplest examples possess m states per site (m\\geq2), with nearest-neighbor interactions with U(m) symmetry, under which the sites transform alternately along the chain in the fundamental m and its conjugate representation \\bar{m}. We find that these spin chains, even with {\\em arbitrary} coefficients of these interactions, have a symmetry algebra A_m much larger than U(m), which implies that the energy eigenstates fall into sectors that for open chains (i.e., free boundary conditions) can be labeled by j=0, 1, >..., L, for the 2L-site chain, such that the degeneracies of all eigenvalues in the jth sector are generically the same and increase rapidly with j. For large j, these degeneracies are much larger than those that would be expected from the U(m) symmetry alone. The enlarged symmetry algebra A_m(2L) consists of operators that commute in this space of states with the Temperley-Lieb algebra that is generated by the set of nearest-neighbor interaction terms; A_m(2L) is not a Yangian. There are similar results for supersymmetric chains with gl(m+n|n) symmetry of nearest-neighbor interactions, and a richer representation structure for closed chains (i.e., periodic boundary conditions). The symmetries also apply to the loop models that can be obtained from the spin chains in a spacetime or transfer matrix picture. In the loop language, the symmetries arise because the loops cannot cross. We further define tensor products of representations (for the open chains) by joining chains end to end. The fusion rules for decomposing the tensor product of representations labeled j_1 and j_2 take the same form as the Clebsch-Gordan series for SU(2). This and other structures turn the symmetry algebra \\cA_m into a ribbon Hopf algebra, and we show that this is ``Morita equivalent'' to the quantum group U_q(sl_2) for m=q+q^{-1}. The open-chain results are extended to the cases |m|< 2 for which the algebras are no longer semisimple; these possess continuum limits that are critical (conformal) field theories, or massive perturbations thereof. Such models, for open and closed boundary conditions, arise in connection with disordered fermions, percolation, and polymers (self-avoiding walks), and certain non-linear sigma models, all in two dimensions. A product operation is defined in a related way for the Temperley-Lieb representations also, and the fusion rules for this are related to those for A_m or U_q(sl_2) representations; this is useful for the continuum limits also, as we discuss in a companion paper.", "prompt": "The symmetry algebras of certain families of quantum spin chains are considered in detail. The simplest examples possess m states per site (m\\geq2), with nearest-neighbor interactions with U(m) symmetry, under which the sites transform alternately along the chain in the fundamental m and its conjugate representation \\bar{m}. We find that these spin chains, even with {\\em arbitrary} coefficients of these interactions, have a symmetry algebra A_m much larger than U(m), which implies that the energy eigenstates fall into sectors that for open chains (i.e., free boundary conditions) can be labeled by j=0,", "orig": "1, >..., L, for the 2L-site chain, such that the degeneracies of all eigenvalues in the jth sector are generically the same and increase rapidly with j. For large j, these degeneracies are much larger than those that would be expected from the U(m) symmetry alone. The enlarged symmetry algebra A_m(2L) consists of operators that commute in this space of states with the Temperley-Lieb algebra that is generated by the set of nearest-neighbor interaction terms; A_m(2L) is not a Yangian. There are similar results for supersymmetric chains with gl", "generated": "...\n\nThis enlarged symmetry algebra A_m has far-reaching consequences for the structure and behavior of the spin chains. In particular, it enables the construction of conserved quantities that commuting with the Hamiltonian, providing a powerful tool for analyzing the dynamics of the system. The existence of these conserved quantities also implies the presence of degeneracies in the energy spectrum, which can be lifted by the application of external fields or the introduction of impurities into the chain.\n\nFurthermore, the symmetry algebra A_m also sheds light on the nature of the phase transitions that occur in these spin chains as the interaction coefficients are varied. By exploiting the algebraic structure of"}
{"full_text": "We use Small Angle Neutron Scattering (SANS), with an original analysis method, to obtain both the characteristic sizes and the inner composition of lysozyme-pectin complexes depending on the charge density. Lysozyme is a globular protein and pectin a natural anionic semiflexible polysaccharide with a degree of methylation (DM) 0, 43 and 74. For our experimental conditions (buffer ionic strength I = 2.5 10-2 mol/L and pH between 3 and 7), the electrostatic charge of lysozyme is always positive (from 8 to 17 depending on pH). The pectin charge per elementary chain segment is negative and can be varied from almost zero to one through the change of DM and pH. The weight molar ratio of lysozyme on pectin monomers is kept constant. The ratio of negative charge content per volume to positive charge content per volume, -/+, is varied between 10 and 0.007. On a local scale, for all charged pectins, a correlation peak appears at 0.2 {\\AA}-1 due to proteins clustering inside the complexes. On a large scale, the complexes appear as formed of spherical globules with a well defined radius of 10 to 50 nm, containing a few thousands proteins. The volume fraction Phi of organic matter within the globules derived from SANS absolute cross-sections is around 0.1. The protein stacking, which occurs inside the globules, is enhanced when pectin is more charged, due to pH or DM. The linear charge density of the pectin determines the size of the globules for pectin chains of comparable molecular weights whether it is controlled by the pH or the DM. The radius of the globules varies between 10 nm and 50 nm. In conclusion the structure is driven by electrostatic interactions and not by hydrophobic interactions. The molecular weight also has a large influence on the structure of the complexes since long chains tend to form larger globules. This maybe one reason why DM and pH are not completely equivalent in our system since DM 0 has a short mass, but this may not be the only one. For very low pectin charge (-/+ = 0.07), globules do not appear and the scattering signals a gel-like structure. We did not observe any beads-on-a-string structure.", "prompt": "We use Small Angle Neutron Scattering (SANS), with an original analysis method, to obtain both the characteristic sizes and the inner composition of lysozyme-pectin complexes depending on the charge density. Lysozyme is a globular protein and pectin a natural anionic semiflexible polysaccharide with a degree of methylation (DM) 0, 43 and 74. For our experimental conditions (buffer ionic strength I = 2.5 10-2 mol/L and pH between 3 and 7), the electrostatic charge of lysozyme is always positive (from", "orig": "8 to 17 depending on pH). The pectin charge per elementary chain segment is negative and can be varied from almost zero to one through the change of DM and pH. The weight molar ratio of lysozyme on pectin monomers is kept constant. The ratio of negative charge content per volume to positive charge content per volume, -/+, is varied between 10 and 0.007. On a local scale, for all charged pectins, a correlation peak appears at 0.2 {\\AA}-1 due to proteins clustering inside the complexes. On a large scale, the complexes appear as", "generated": "+5 to +10).\n\nThe SANS data analysis reveals that the size of the lysozyme-pectin complexes increases with the charge density of the pectin, which is attributed to the enhanced electrostatic attraction between the positively charged lysozyme and the negatively charged pectin. Interestingly, we found that the inner composition of the complexes is strongly influenced by the degree of methylation of pectin. For instance, at low charge density (DM 0), the complexes have a core-shell structure with a lysozyme-rich core and a pectin-rich shell, while at high charge density (DM"}
{"full_text": "Synthesis is a particularly challenging problem for concurrent programs. At the same time it is a very promising approach, since concurrent programs are difficult to get right, or to analyze with traditional verification techniques. This paper gives an introduction to distributed synthesis in the setting of Mazurkiewicz traces, and its applications to decentralized runtime monitoring. 1 Context Modern computing systems are increasingly distributed and heterogeneous. Software needs to be able to exploit these advances, providing means for applications to be more performant. Traditional concurrent programming paradigms, as in Java, are based on threads, shared-memory, and locking mechanisms that guard access to common data. More recent paradigms like the reactive programming model of Erlang [4] and Scala [35,36] replace shared memory by asynchronous message passing, where sending a message is non-blocking. In all these concurrent frameworks, writing reliable software is a serious challenge. Programmers tend to think about code mostly in a sequential way, and it is hard to grasp all possible schedulings of events in a concurrent execution. For similar reasons, verification and analysis of concurrent programs is a difficult task. Testing, which is still the main method for error detection in software, has low coverage for concurrent programs. The reason is that bugs in such programs are difficult to reproduce: they may happen under very specific thread schedules and the likelihood of taking such corner-case schedules is very low. Automated verification, such as model-checking and other traditional exploration techniques, can handle very limited instances of concurrent programs, mostly because of the very large number of possible states and of possible interleavings of executions. Formal analysis of programs requires as a prerequisite a clean mathematical model for programs. Verification of sequential programs starts usually with an abstraction step -- reducing the value domains of variables to finite domains, viewing conditional branching as non-determinism, etc. Another major simplification consists in disallowing recursion. This leads to a very robust computational model, namely finite-state automata and regular languages. Regular languages of words (and trees) are particularly well understood notions. The deep connections between logic and automata revealed by the foundational work of B\\\"uchi, Rabin, and others, are the main ingredients in automata-based verification .", "prompt": "Synthesis is a particularly challenging problem for concurrent programs. At the same time it is a very promising approach, since concurrent programs are difficult to get right, or to analyze with traditional verification techniques. This paper gives an introduction to distributed synthesis in the setting of Mazurkiewicz traces, and its applications to decentralized runtime monitoring. 1 Context Modern computing systems are increasingly distributed and heterogeneous. Software needs to be able to exploit these advances, providing means for applications to be more performant. Traditional concurrent programming paradigms, as in Java, are based on threads, shared-memory, and locking mechanisms that guard access to common data. More", "orig": "recent paradigms like the reactive programming model of Erlang [4] and Scala [35,36] replace shared memory by asynchronous message passing, where sending a message is non-blocking. In all these concurrent frameworks, writing reliable software is a serious challenge. Programmers tend to think about code mostly in a sequential way, and it is hard to grasp all possible schedulings of events in a concurrent execution. For similar reasons, verification and analysis of concurrent programs is a difficult task. Testing, which is still the main method for error detection in software, has low coverage for concurrent programs. The reason is that bugs in such programs are difficult", "generated": "importantly, the rise of multi-core processors, cloud computing, and the Internet of Things (IoT) has further emphasized the need for efficient and reliable concurrency control mechanisms.\n\nIn recent years, there has been a growing interest in alternative concurrency models, such as actor-based systems, data-flow concurrency, and declarative concurrency. These models aim to simplify the development of concurrent systems by providing higher-level abstractions and restricting the use of low-level synchronization primitives. However, these approaches still rely on manual programming and verification, which can be error-prone and time-consuming. Distributed synthesis, on the other hand, offers a promising solution by automatically generating correct"}
{"full_text": "How to efficiently and reliably spread information in a system is one of the most fundamental problems in distributed computing. Recently, inspired by biological scenarios, several works focused on identifying the minimal communication resources necessary to spread information under faulty conditions. Here we study the self-stabilizing bit-dissemination problem, introduced by Boczkowski, Korman, and Natale in [SODA 2017]. The problem considers a fully-connected network of n agents, with a binary world of opinions, one of which is called correct. At any given time, each agent holds an opinion bit as its public output. The population contains a source agent which knows which opinion is correct. This agent adopts the correct opinion and remains with it throughout the execution. We consider the basic PULL model of communication, in which each agent observes relatively few randomly chosen agents in each round. The goal of the non-source agents is to quickly converge on the correct opinion, despite having an arbitrary initial configuration, i.e., in a self-stabilizing manner. Once the population converges on the correct opinion, it should remain with it forever. Motivated by biological scenarios in which animals observe and react to the behavior of others, we focus on the extremely constrained model of passive communication, which assumes that when observing another agent the only information that can be extracted is the opinion bit of that agent. We prove that this problem can be solved in a poly-logarithmic in n number of rounds with high probability, while sampling a logarithmic number of agents at each round. Previous works solved this problem faster and using fewer samples, but they did that by decoupling the messages sent by agents from their output opinion, and hence do not fit the framework of passive communication. Moreover, these works use complex recursive algorithms with refined clocks that are unlikely to be used by biological entities. In contrast, our proposed algorithm has a natural appeal as it is based on letting agents estimate the current tendency direction of the dynamics, and then adapt to the emerging trend.", "prompt": "How to efficiently and reliably spread information in a system is one of the most fundamental problems in distributed computing. Recently, inspired by biological scenarios, several works focused on identifying the minimal communication resources necessary to spread information under faulty conditions. Here we study the self-stabilizing bit-dissemination problem, introduced by Boczkowski, Korman, and Natale in [SODA 2017]. The problem considers a fully-connected network of n agents, with a binary world of opinions, one of which is called correct. At any given time, each agent holds an opinion bit as its public output. The population contains a source", "orig": "agent which knows which opinion is correct. This agent adopts the correct opinion and remains with it throughout the execution. We consider the basic PULL model of communication, in which each agent observes relatively few randomly chosen agents in each round. The goal of the non-source agents is to quickly converge on the correct opinion, despite having an arbitrary initial configuration, i.e., in a self-stabilizing manner. Once the population converges on the correct opinion, it should remain with it forever. Motivated by biological scenarios in which animals observe and react to the behavior of others, we focus on the extremely constrained model of passive communication, which assumes that", "generated": "whose initial opinion is the correct one, and the goal is to design a protocol that ensures all agents eventually converge to the correct opinion despite the presence of transient failures.\n\nIn this context, it is crucial to minimize the communication overhead and ensure robustness against failures. Our approach leverages the concept of epidemic protocols, which have been successfully applied to various distributed computing problems. We propose a novel gossip-based protocol that allows agents to exchange their opinions with their neighbors and adapt their own opinions based on the received information. To handle transient failures, our protocol incorporates a mechanism for detecting and recovering from inconsistencies in the opinions held by agents. We prove that our"}
{"full_text": "Structure discovery in graphical models is the determination of the topology of a graph that encodes conditional independence properties of the joint distribution of all variables in the model. For some class of probability distributions, an edge between two variables is present if and only if the corresponding entry in the precision matrix is non-zero. For a finite sample estimate of the precision matrix, entries close to zero may be due to low sample effects, or due to an actual association between variables; these two cases are not readily distinguishable. %Fisher provided a hypothesis test based on a parametric approximation to the distribution of an entry in the precision matrix of a Gaussian distribution, but this may not provide valid upper bounds on $p$-values for non-Gaussian distributions. Many related works on this topic consider potentially restrictive distributional or sparsity assumptions that may not apply to a data sample of interest, and direct estimation of the uncertainty of an estimate of the precision matrix for general distributions remains challenging. Consequently, we make use of results for $U$-statistics and apply them to the covariance matrix. By probabilistically bounding the distortion of the covariance matrix, we can apply Weyl's theorem to bound the distortion of the precision matrix, yielding a conservative, but sound test threshold for a much wider class of distributions than considered in previous works. The resulting test enables one to answer with statistical significance whether an edge is present in the graph, and convergence results are known for a wide range of distributions. The computational complexities is linear in the sample size enabling the application of the test to large data samples for which computation time becomes a limiting factor. We experimentally validate the correctness and scalability of the test on multivariate distributions for which the distributional assumptions of competing tests result in underestimates of the false positive ratio. By contrast, the proposed test remains sound, promising to be a useful tool for hypothesis testing for diverse real-world problems.", "prompt": "Structure discovery in graphical models is the determination of the topology of a graph that encodes conditional independence properties of the joint distribution of all variables in the model. For some class of probability distributions, an edge between two variables is present if and only if the corresponding entry in the precision matrix is non-zero. For a finite sample estimate of the precision matrix, entries close to zero may be due to low sample effects, or due to an actual association between variables; these two cases are not readily distinguishable. %Fisher provided a hypothesis test based on a parametric approximation to the distribution of an entry in the precision matrix of a Gaussian distribution,", "orig": "but this may not provide valid upper bounds on $p$-values for non-Gaussian distributions. Many related works on this topic consider potentially restrictive distributional or sparsity assumptions that may not apply to a data sample of interest, and direct estimation of the uncertainty of an estimate of the precision matrix for general distributions remains challenging. Consequently, we make use of results for $U$-statistics and apply them to the covariance matrix. By probabilistically bounding the distortion of the covariance matrix, we can apply Weyl's theorem to bound the distortion of the precision matrix, yielding a conservative, but sound test threshold for a much wider class of", "generated": "The Fisher test is a widely used method for structure discovery, but it has some limitations. One major issue is that it assumes the underlying distribution is Gaussian, which may not always be the case in real-world data. Additionally, the test is sensitive to the choice of threshold value for determining significance, and the results can be influenced by the sample size and the presence of outliers. Furthermore, the test is based on a parametric approximation, which may not be accurate for complex distributions or small samples. \n\nRecent advancements in structure discovery have led to the development of more robust and flexible methods, such as the graphical lasso and the sparse inverse"}
{"full_text": "Non-Gaussian outcomes are often modeled using members of the so-called exponential family. Notorious members are the Bernoulli model for binary data, leading to logistic regression, and the Poisson model for count data, leading to Poisson regression. Two of the main reasons for extending this family are (1) the occurrence of overdispersion, meaning that the variability in the data is not adequately described by the models, which often exhibit a prescribed mean--variance link, and (2) the accommodation of hierarchical structure in the data, stemming from clustering in the data which, in turn, may result from repeatedly measuring the outcome, for various members of the same family, etc. The first issue is dealt with through a variety of overdispersion models, such as, for example, the beta-binomial model for grouped binary data and the negative-binomial model for counts. Clustering is often accommodated through the inclusion of random subject-specific effects. Though not always, one conventionally assumes such random effects to be normally distributed. While both of these phenomena may occur simultaneously, models combining them are uncommon. This paper proposes a broad class of generalized linear models accommodating overdispersion and clustering through two separate sets of random effects. We place particular emphasis on so-called conjugate random effects at the level of the mean for the first aspect and normal random effects embedded within the linear predictor for the second aspect, even though our family is more general. The binary, count and time-to-event cases are given particular emphasis. Apart from model formulation, we present an overview of estimation methods, and then settle for maximum likelihood estimation with analytic--numerical integration. Implications for the derivation of marginal correlations functions are discussed. The methodology is applied to data from a study in epileptic seizures, a clinical trial in toenail infection named onychomycosis and survival data in children with asthma.", "prompt": "Non-Gaussian outcomes are often modeled using members of the so-called exponential family. Notorious members are the Bernoulli model for binary data, leading to logistic regression, and the Poisson model for count data, leading to Poisson regression. Two of the main reasons for extending this family are (1) the occurrence of overdispersion, meaning that the variability in the data is not adequately described by the models, which often exhibit a prescribed mean--variance link, and (2) the accommodation of hierarchical structure in the data, stemming from clustering in the data which, in turn, may result from repeatedly measuring the outcome, for", "orig": "various members of the same family, etc. The first issue is dealt with through a variety of overdispersion models, such as, for example, the beta-binomial model for grouped binary data and the negative-binomial model for counts. Clustering is often accommodated through the inclusion of random subject-specific effects. Though not always, one conventionally assumes such random effects to be normally distributed. While both of these phenomena may occur simultaneously, models combining them are uncommon. This paper proposes a broad class of generalized linear models accommodating overdispersion and clustering through two separate sets of random effects. We place particular emphasis on so-called conjugate random", "generated": "instance, in longitudinal studies where the same individual is observed multiple times.\n\nOne way to extend the exponential family is by using generalized linear mixed models (GLMMs), which incorporate random effects to account for the hierarchical structure in the data. This approach allows for modeling the variation between clusters, such as individuals or groups, while also estimating the relationships between the outcome and covariates. Another approach is to use finite mixture models, which assume that the data arise from a mixture of underlying distributions. This can be particularly useful when dealing with overdispersion, as it allows for modeling the excess variation in the data.\n\nIn addition, Bayesian non-param"}
{"full_text": "Device-to-Device (D2D) communication is expected to be a key feature supported by 5G networks, especially due to the proliferation of Mobile Edge Computing (MEC), which has a prominent role in reducing network stress by shifting computational tasks from the Internet to the mobile edge. Apart from being part of MEC, D2D can extend cellular coverage allowing users to communicate directly when telecommunication infrastructure is highly congested or absent. This significant departure from the typical cellular paradigm imposes the need for decentralised network routing protocols. Moreover, enhanced capabilities of mobile devices and D2D networking will likely result in proliferation of new malware types and epidemics. Although the literature is rich in terms of D2D routing protocols that enhance quality-of-service and energy consumption, they provide only basic security support, e.g., in the form of encryption. Routing decisions can, however, contribute to collaborative detection of mobile malware by leveraging different kinds of anti-malware software installed on mobile devices. Benefiting from the cooperative nature of D2D communications, devices can rely on each other's contributions to detect malware. The impact of our work is geared towards having more malware-free D2D networks. To achieve this, we designed and implemented a novel routing protocol for D2D communications that optimises routing decisions for explicitly improving malware detection. The protocol identifies optimal network paths, in terms of malware mitigation and energy spent for malware detection, based on a game theoretic model. Diverse capabilities of network devices running different types of anti-malware software and their potential for inspecting messages relayed towards an intended destination device are leveraged using game theoretic tools. An optimality analysis of both Nash and Stackelberg security games is undertaken, including both zero and non-zero sum variants, and the Defender's equilibrium strategies. By undertaking network simulations, theoretical results obtained are illustrated through randomly generated network scenarios showing how our protocol outperforms conventional routing protocols, in terms of expected payoff, which consists of: security damage in inflicted by malware and malware detection cost.", "prompt": "Device-to-Device (D2D) communication is expected to be a key feature supported by 5G networks, especially due to the proliferation of Mobile Edge Computing (MEC), which has a prominent role in reducing network stress by shifting computational tasks from the Internet to the mobile edge. Apart from being part of MEC, D2D can extend cellular coverage allowing users to communicate directly when telecommunication infrastructure is highly congested or absent. This significant departure from the typical cellular paradigm imposes the need for decentralised network routing protocols. Moreover, enhanced capabilities of mobile devices and D2D networking will likely result in proliferation of new malware types", "orig": "and epidemics. Although the literature is rich in terms of D2D routing protocols that enhance quality-of-service and energy consumption, they provide only basic security support, e.g., in the form of encryption. Routing decisions can, however, contribute to collaborative detection of mobile malware by leveraging different kinds of anti-malware software installed on mobile devices. Benefiting from the cooperative nature of D2D communications, devices can rely on each other's contributions to detect malware. The impact of our work is geared towards having more malware-free D2D networks. To achieve this, we designed and implemented a novel routing protocol for D2D communications", "generated": ".\n\nThe growth of D2D communication will also raise concerns about data security and privacy. With devices communicating directly with each other, there is a higher risk of data breaches and unauthorized access. Therefore, it is essential to develop robust security protocols that can ensure the integrity and confidentiality of data transmitted over D2D networks. Moreover, the decentralized nature of D2D communication will require novel approaches to authentication, authorization, and accounting (AAA) mechanisms to ensure that only authorized devices can participate in D2D communication.\n\nIn addition, the proliferation of D2D communication will also have significant implications on the development of new applications and services. For"}
{"full_text": "Emerging computer architectures will feature drastically decreased flops/byte (ratio of peak processing rate to memory bandwidth) as highlighted by recent studies on Exascale architectural trends. Further, flops are getting cheaper while the energy cost of data movement is increasingly dominant. The understanding and characterization of data locality properties of computations is critical in order to guide efforts to enhance data locality. Reuse distance analysis of memory address traces is a valuable tool to perform data locality characterization of programs. A single reuse distance analysis can be used to estimate the number of cache misses in a fully associative LRU cache of any size, thereby providing estimates on the minimum bandwidth requirements at different levels of the memory hierarchy to avoid being bandwidth bound. However, such an analysis only holds for the particular execution order that produced the trace. It cannot estimate potential improvement in data locality through dependence preserving transformations that change the execution schedule of the operations in the computation. In this article, we develop a novel dynamic analysis approach to characterize the inherent locality properties of a computation and thereby assess the potential for data locality enhancement via dependence preserving transformations. The execution trace of a code is analyzed to extract a computational directed acyclic graph (CDAG) of the data dependences. The CDAG is then partitioned into convex subsets, and the convex partitioning is used to reorder the operations in the execution trace to enhance data locality. The approach enables us to go beyond reuse distance analysis of a single specific order of execution of the operations of a computation in characterization of its data locality properties. It can serve a valuable role in identifying promising code regions for manual transformation, as well as assessing the effectiveness of compiler transformations for data locality enhancement. We demonstrate the effectiveness of the approach using a number of benchmarks, including case studies where the potential shown by the analysis is exploited to achieve lower data movement costs and better performance.", "prompt": "Emerging computer architectures will feature drastically decreased flops/byte (ratio of peak processing rate to memory bandwidth) as highlighted by recent studies on Exascale architectural trends. Further, flops are getting cheaper while the energy cost of data movement is increasingly dominant. The understanding and characterization of data locality properties of computations is critical in order to guide efforts to enhance data locality. Reuse distance analysis of memory address traces is a valuable tool to perform data locality characterization of programs. A single reuse distance analysis can be used to estimate the number of cache misses in a fully associative LRU cache of any size, thereby providing estimates on the minimum bandwidth requirements", "orig": "at different levels of the memory hierarchy to avoid being bandwidth bound. However, such an analysis only holds for the particular execution order that produced the trace. It cannot estimate potential improvement in data locality through dependence preserving transformations that change the execution schedule of the operations in the computation. In this article, we develop a novel dynamic analysis approach to characterize the inherent locality properties of a computation and thereby assess the potential for data locality enhancement via dependence preserving transformations. The execution trace of a code is analyzed to extract a computational directed acyclic graph (CDAG) of the data dependences. The CDAG is then partitioned into convex subsets, and the convex", "generated": ".\n\nAs the industry continues to push the boundaries of computing performance, the importance of data locality will only continue to grow. By understanding the data locality properties of computations, researchers and developers can begin to design and optimize algorithms that minimize data movement and maximize reuse. This is particularly critical in the era of Exascale computing, where even small improvements in data locality can result in significant reductions in energy consumption and increases in overall system performance.\n\nOne key area of research in this space is the development of new and innovative techniques for performing reuse distance analysis. While traditional methods have relied on statistical models and simulation-based approaches, new advances in machine learning and artificial"}
{"full_text": "Context: The identification of long-gamma-ray-bursts (LGRBs) is still uncertain, although the collapsar engine of fast-rotating massive stars is gaining a strong consensus. Aims: We propose that low-metallicity Be and Oe stars, which are massive fast rotators, as potential LGRBs progenitors. Methods: We checked this hypothesis by 1) testing the global specific angular momentum of Oe/Be stars in the ZAMS with the SMC metallicity, 2) comparing the ZAMS ($\\Omega/\\Omega_{\\rm c},M/M_{\\odot}$) parameters of these stars with the area predicted theoretically for progenitors with metallicity $Z=0.002$, and 3) calculating the expected rate of LGRBs/year/galaxy and comparing them with the observed ones. To this end, we determined the ZAMS linear and angular rotational velocities for SMC Be and Oe stars using the observed vsini parameters, corrected from the underestimation induced by the gravitational darkening effect. Results: The angular velocities of SMC Oe/Be stars are on average $<\\Omega/\\Omega_{\\rm c}>=0.95$ in the ZAMS. These velocities are in the area theoretically predicted for the LGRBs progenitors. We estimated the yearly rate per galaxy of LGRBs and the number of LGRBs produced in the local Universe up to z=0.2. We have considered that the mass range of LGRB progenitors corresponds to stars hotter than spectral types B0-B1 and used individual beaming angles from 5 to 15\\degr. We thus obtain $R^{\\rm pred}_{\\rm LGRB}\\sim10^{-7}$ to $\\sim10^{-6}$ LGRBs/year/galaxy, which represents on average 2 to 14 LGRB predicted events in the local Universe during the past 11 years. The predicted rates could widely surpass the observed ones [(0.2-3)$\\times10^{-7}$ LGRBs/year/galaxy; 8 LGRBs observed in the local Universe during the last 11 years] if the stellar counts were made from the spectral type B1-B2, in accordance with the expected apparent spectral types of the appropriate massive fast rotators. Conclusion: We conclude that the massive Be/Oe stars with SMC metallicity could be LGRBs progenitors. Nevertheless, other SMC O/B stars without emission lines, which have high enough specific angular momentum, can enhance the predicted $R_{\\rm LGRB}$ rate.", "prompt": "Context: The identification of long-gamma-ray-bursts (LGRBs) is still uncertain, although the collapsar engine of fast-rotating massive stars is gaining a strong consensus. Aims: We propose that low-metallicity Be and Oe stars, which are massive fast rotators, as potential LGRBs progenitors. Methods: We checked this hypothesis by 1) testing the global specific angular momentum of Oe/Be stars in the ZAMS with the SMC metallicity, 2) comparing the ZAMS ($\\Omega/\\Omega_{\\rm c},M/M_{\\odot}$) parameters", "orig": "of these stars with the area predicted theoretically for progenitors with metallicity $Z=0.002$, and 3) calculating the expected rate of LGRBs/year/galaxy and comparing them with the observed ones. To this end, we determined the ZAMS linear and angular rotational velocities for SMC Be and Oe stars using the observed vsini parameters, corrected from the underestimation induced by the gravitational darkening effect. Results: The angular velocities of SMC Oe/Be stars are on average $<\\Omega/\\Omega_{\\rm c}>=0.95$ in the ZAMS. These velocities are in", "generated": "of these stars with those of the typical LGRB progenitors, and 3) analyzing the evolutionary tracks of these stars to determine their final states. Our results show that low-metallicity Oe/Be stars can retain a significant fraction of their initial angular momentum, allowing them to reach critical velocities and undergo a chemically homogeneous evolution. This, in turn, would lead to the formation of a large, rapidly rotating iron core, which could collapse to form a black hole, accompanied by a massive accretion disk and a relativistic jet, thus fulfilling the necessary conditions for an LGRB event. \n\nFurthermore, our analysis"}
{"full_text": "Implementing circular economy (CE) principles is increasingly recommended as a convenient solution to meet the goals of sustainable development. New tools are required to support practitioners, decision-makers and policy-makers towards more CE practices, as well as to monitor the effects of CE adoption. Worldwide, academics, industrialists and politicians all agree on the need to use CE-related measuring instruments to manage this transition at different systemic levels. In this context, a wide range of circularity indicators (C-indicators) has been developed in recent years. Yet, as there is not one single definition of the CE concept, it is of the utmost importance to know what the available indicators measure in order to use them properly. Indeed, through a systematic literature review-considering both academic and grey literature-55 sets of C-indicators, developed by scholars, consulting companies and governmental agencies, have been identified, encompassing different purposes, scopes, and potential usages. Inspired by existing taxonomies of eco-design tools and sustainability indicators, and in line with the CE characteristics, a classification of indicators aiming to assess, improve, monitor and communicate on the CE performance is proposed and discussed. In the developed taxonomy including 10 categories, C-indicators are differentiated regarding criteria such as the levels of CE implementation (e.g. micro, meso, macro), the CE loops (maintain, reuse, remanufacture, recycle), the performance (intrinsic, impacts), the perspective of circularity (actual, potential) they are taking into account, or their degree of transversality (generic, sector-specific). In addition, the database inventorying the 55 sets of C-indicators is linked to an Excel-based query tool to facilitate the selection of appropriate indicators according to the specific user's needs and requirements. This study enriches the literature by giving a first need-driven taxonomy of C-indicators, which is experienced on several use cases. It provides a synthesis and clarification to the emerging and must-needed research theme of C-indicators, and sheds some light on remaining key challenges like their effective uptake by industry. Eventually, limitations, improvement areas, as well as implications of the proposed taxonomy are intently addressed to guide future research on C-indicators and CE implementation.", "prompt": "Implementing circular economy (CE) principles is increasingly recommended as a convenient solution to meet the goals of sustainable development. New tools are required to support practitioners, decision-makers and policy-makers towards more CE practices, as well as to monitor the effects of CE adoption. Worldwide, academics, industrialists and politicians all agree on the need to use CE-related measuring instruments to manage this transition at different systemic levels. In this context, a wide range of circularity indicators (C-indicators) has been developed in recent years. Yet, as there is not one single definition of the CE concept, it is of the utmost importance to know what the available", "orig": "indicators measure in order to use them properly. Indeed, through a systematic literature review-considering both academic and grey literature-55 sets of C-indicators, developed by scholars, consulting companies and governmental agencies, have been identified, encompassing different purposes, scopes, and potential usages. Inspired by existing taxonomies of eco-design tools and sustainability indicators, and in line with the CE characteristics, a classification of indicators aiming to assess, improve, monitor and communicate on the CE performance is proposed and discussed. In the developed taxonomy including 10 categories, C-indicators are differentiated regarding criteria such as the levels of CE implementation (e.g.", "generated": "C-indicators measure, how they are calculated, and what their limitations are.\n\nOne of the main challenges in implementing CE principles is the lack of a standardized framework for measuring circularity. The development of C-indicators has been carried out by various organizations, research institutions, and companies, resulting in a plethora of indicators that often overlap or contradict each other. This makes it difficult for practitioners to choose the most suitable indicators for their specific context and goals. Moreover, the complexity and variability of CE systems make it challenging to develop indicators that can accurately capture the full range of CE benefits and impacts.\n\nTo address these challenges, there is a growing need"}
{"full_text": "The type problem is the problem of deciding, for a simply connected Riemann surface, whether it is conformally equivalent to the complex plane or to the unit dic in the complex plane. We report on Teichm{\\\"u}ller's results on the type problem from his two papers Eine Anwendung quasikonformer Abbildungen auf das Typenproblem (An application of quasiconformal map-pings to the type problem) (1937) and Untersuchungen{\\\"u}ber kon-forme und quasikonforme Abbildungen (Investigations on con-formal and quasiconformal mappings) (1938). They concern simply connected Riemann surfaces defined as branched covers of the sphere. At the same time, we review the theory of line complexes, a combinatorial device used by Teichm{\\\"u}ller and others to encode branched coverings of the sphere. In the first paper, Teichm{\\\"u}ller proves that any two simply connected Riemann surfaces which are branched coverings of the Riemann sphere with finitely many branch values and which have the same line complex are quasiconformally equivalent. For this purpose, he introduces a technique for piecing together quasi-conformal mappings. He also obtains a result on the extension of smooth orientation-preserving diffeomorphisms of the circle to quasiconformal mappings of the disc which are conformal at the boundary. In the second paper, using line complexes, Teichm{\\\"u}ller gives a type criterion for a simply-connected surface which is a branched covering of the sphere, in terms of an adequately defined measure of ramification, defined by a limiting process. The result says that if the surface is ''sufficiently ramified'' (in a sense to be made precise), then it is hyperbolic. In the same paper, Te-ichm{\\\"u}ller answers by the negative a conjecture made by Nevan-linna which states a criterion for parabolicity in terms of the value of a (different) measure of ramification, defined by a limiting process. Teichm{\\\"u}ller's results in his first paper are used in the proof of the results of the second one.The final version of this paper will appear in Vol. VII of the Handbook of Teichm{\\\"u}ller theory (European Mathematical Society Publishing House, 2020).", "prompt": "The type problem is the problem of deciding, for a simply connected Riemann surface, whether it is conformally equivalent to the complex plane or to the unit dic in the complex plane. We report on Teichm{\\\"u}ller's results on the type problem from his two papers Eine Anwendung quasikonformer Abbildungen auf das Typenproblem (An application of quasiconformal map-pings to the type problem) (1937) and Untersuchungen{\\\"u}ber kon-forme und quasikonforme Abbildungen (Investigations on con-formal and quasiconformal mappings", "orig": ") (1938). They concern simply connected Riemann surfaces defined as branched covers of the sphere. At the same time, we review the theory of line complexes, a combinatorial device used by Teichm{\\\"u}ller and others to encode branched coverings of the sphere. In the first paper, Teichm{\\\"u}ller proves that any two simply connected Riemann surfaces which are branched coverings of the Riemann sphere with finitely many branch values and which have the same line complex are quasiconformally equivalent. For this purpose, he introduces a technique for piecing", "generated": ".\n\nTeichm{\\\"u}ller's work on the type problem marked a significant milestone in the development of complex analysis and geometry. In his 1937 paper, he introduced the concept of quasiconformal mappings, which are functions between Riemann surfaces that are smooth and have bounded distortion. He showed that quasiconformal mappings can be used to solve the type problem, by establishing a criterion for determining whether a simply connected Riemann surface is conformally equivalent to the complex plane or the unit disc.\n\nTeichm{\\\"u}ller's results have had a profound impact on the study of Riemann"}
{"full_text": "Graphs and various graph-like combinatorial structures, such as preorders and hypergraphs, are ubiquitous in programming. This paper focuses on representing graphs in a purely functional programming language like Haskell. There are several existing approaches; one of the most recently developed ones is the \"algebraic graphs\" approach (2017). It uses an algebraic data type to represent graphs and has attracted users, including from industry, due to its emphasis on equational reasoning and making a common class of bugs impossible by eliminating internal invariants. The previous formulation of algebraic graphs did not support edge labels, which was a serious practical limitation. In this paper, we redesign the main algebraic data type and remove this limitation. We follow a fairly standard approach of parameterising a data structure with a semiring of edge labels. The new formulation is both more general and simpler: the two operations for composing graphs used in the previous work can now be obtained from a single operation by fixing the semiring parameter to zero and one, respectively. By instantiating the new data type with different semirings, and working out laws for interpreting the resulting expression trees, we discover an unusual algebraic structure, which we call \"united monoids\", that is, a pair of monoids whose unit elements coincide. We believe that it is worth studying united monoids in their full generality, going beyond the graphs which prompted their discovery. To that end, we characterise united monoids with a minimal set of axioms, prove a few basic theorems, and discuss several notable examples. We validate the presented approach by implementing it in the open-source *algebraic-graphs* library. Our theoretical contributions are supported by proofs that are included in the paper and have also been machine-checked in Agda. By extending algebraic graphs with support for edge labels, we make them suitable for a much larger class of possible applications. By studying united monoids, we provide a theoretical foundation for further research in this area.", "prompt": "Graphs and various graph-like combinatorial structures, such as preorders and hypergraphs, are ubiquitous in programming. This paper focuses on representing graphs in a purely functional programming language like Haskell. There are several existing approaches; one of the most recently developed ones is the \"algebraic graphs\" approach (2017). It uses an algebraic data type to represent graphs and has attracted users, including from industry, due to its emphasis on equational reasoning and making a common class of bugs impossible by eliminating internal invariants. The previous formulation of algebraic graphs did not support edge labels, which was a serious practical limitation. In this", "orig": "paper, we redesign the main algebraic data type and remove this limitation. We follow a fairly standard approach of parameterising a data structure with a semiring of edge labels. The new formulation is both more general and simpler: the two operations for composing graphs used in the previous work can now be obtained from a single operation by fixing the semiring parameter to zero and one, respectively. By instantiating the new data type with different semirings, and working out laws for interpreting the resulting expression trees, we discover an unusual algebraic structure, which we call \"united monoids\", that is, a pair of monoids whose unit", "generated": "paper, we extend the algebraic graphs approach to support edge labels, allowing for a more expressive and flexible representation of graphs.\n\nOur extension is based on a novel encoding of edge labels as a free monoid, which enables us to leverage the existing algebraic graphs framework while adding support for labeled edges. We show that our approach preserves the desirable properties of algebraic graphs, including the ability to reason equationally about graphs and the elimination of internal invariants. We also demonstrate the practical applicability of our approach through a series of case studies, including graph algorithms and data analytics applications.\n\nA key benefit of our approach is that it allows developers to"}
{"full_text": "Homozygous mutations in the progranulin gene (GRN) are associated with neuronal ceroid lipofuscinosis 11 (CLN11), a rare lysosomal-storage disorder characterized by cerebellar ataxia, seizures, retinitis pigmentosa, and cognitive disorders, usually beginning between 13 and 25 years of age. This is a rare condition, previously reported in only four families. In contrast, heterozygous GRN mutations are a major cause of frontotemporal dementia associated with neuronal cytoplasmic TDP-43 inclusions. We identified homozygous GRN mutations in six new patients. The phenotypic spectrum is much broader than previously reported, with two remarkably distinct presentations, depending on the age of onset. A childhood/juvenile form is characterized by classical CLN11 symptoms at an early age at onset. Unexpectedly, other homozygous patients presented a distinct delayed phenotype of frontotemporal dementia and parkinsonism after 50 years; none had epilepsy or cerebellar ataxia. Another major finding of this study is that all GRN mutations may not have the same impact on progranulin protein synthesis. A hypomorphic effect of some mutations is supported by the presence of residual levels of plasma progranulin and low levels of normal transcript detected in one case with a homozygous splice-site mutation and late onset frontotemporal dementia. This is a new critical finding that must be considered in therapeutic trials based on replacement strategies. The first neuropathological study in a homozygous carrier provides new insights into the pathological mechanisms of the disease. Hallmarks of neuronal ceroid lipofuscinosis were present. The absence of TDP-43 cytoplasmic inclusions markedly differs from observations of heterozygous mutations, suggesting a pathological shift between lysosomal and TDP-43 pathologies depending on the mono or bi-allelic status. An intriguing observation was the loss of normal TDP-43 staining in the nucleus of some neurons, which could be the first stage of the TDP-43 pathological process preceding the formation of typical cytoplasmic inclusions. Finally, this study has important implications for genetic counselling and molecular diagnosis. Semi-dominant inheritance of GRN mutations implies that specific genetic counseling should be delivered to children and parents of CLN11 patients, as they are heterozygous carriers with a high risk of developing dementia. More broadly, this study illustrates the fact that genetic variants can lead to different phenotypes according to their mono- or bi-allelic state, which is a challenge for genetic diagnosis.", "prompt": "Homozygous mutations in the progranulin gene (GRN) are associated with neuronal ceroid lipofuscinosis 11 (CLN11), a rare lysosomal-storage disorder characterized by cerebellar ataxia, seizures, retinitis pigmentosa, and cognitive disorders, usually beginning between 13 and 25 years of age. This is a rare condition, previously reported in only four families. In contrast, heterozygous GRN mutations are a major cause of frontotemporal dementia associated with neuronal cytoplasmic TDP-43 inclusions. We identified homozygous GRN", "orig": "mutations in six new patients. The phenotypic spectrum is much broader than previously reported, with two remarkably distinct presentations, depending on the age of onset. A childhood/juvenile form is characterized by classical CLN11 symptoms at an early age at onset. Unexpectedly, other homozygous patients presented a distinct delayed phenotype of frontotemporal dementia and parkinsonism after 50 years; none had epilepsy or cerebellar ataxia. Another major finding of this study is that all GRN mutations may not have the same impact on progranulin protein synthesis. A hypomorphic effect of some mutations is supported by the", "generated": "mutations in a large consanguineous family with multiple affected individuals, presenting with a unique combination of symptoms. The proband, a 20-year-old woman, exhibited severe cerebellar ataxia, myoclonic seizures, and significant cognitive decline, including memory impairment and language difficulties. Her younger brother, aged 17, showed similar symptoms, although less severe, while their 35-year-old uncle had a more slowly progressive course with prominent retinitis pigmentosa.\n\nFurther genetic analysis revealed a novel homozygous mutation in the GRN gene, consisting of a frameshift insertion in exon 12, leading to a"}
{"full_text": "The ability to compose parts to form a more complex whole, and to analyze a whole as a combination of elements, is desirable across disciplines. This workshop bring together researchers applying compositional approaches to physics, NLP, cognitive science, and game theory. Within NLP, a long-standing aim is to represent how words can combine to form phrases and sentences. Within the framework of distributional semantics, words are represented as vectors in vector spaces. The categorical model of Coecke et al. [2010], inspired by quantum protocols, has provided a convincing account of compositionality in vector space models of NLP. There is furthermore a history of vector space models in cognitive science. Theories of categorization such as those developed by Nosofsky [1986] and Smith et al. [1988] utilise notions of distance between feature vectors. More recently G\\\"ardenfors [2004, 2014] has developed a model of concepts in which conceptual spaces provide geometric structures, and information is represented by points, vectors and regions in vector spaces. The same compositional approach has been applied to this formalism, giving conceptual spaces theory a richer model of compositionality than previously [Bolt et al., 2018]. Compositional approaches have also been applied in the study of strategic games and Nash equilibria. In contrast to classical game theory, where games are studied monolithically as one global object, compositional game theory works bottom-up by building large and complex games from smaller components. Such an approach is inherently difficult since the interaction between games has to be considered. Research into categorical compositional methods for this field have recently begun [Ghani et al., 2018]. Moreover, the interaction between the three disciplines of cognitive science, linguistics and game theory is a fertile ground for research. Game theory in cognitive science is a well-established area [Camerer, 2011]. Similarly game theoretic approaches have been applied in linguistics [J\\\"ager, 2008]. Lastly, the study of linguistics and cognitive science is intimately intertwined [Smolensky and Legendre, 2006, Jackendoff, 2007]. Physics supplies compositional approaches via vector spaces and categorical quantum theory, allowing the interplay between the three disciplines to be examined.", "prompt": "The ability to compose parts to form a more complex whole, and to analyze a whole as a combination of elements, is desirable across disciplines. This workshop bring together researchers applying compositional approaches to physics, NLP, cognitive science, and game theory. Within NLP, a long-standing aim is to represent how words can combine to form phrases and sentences. Within the framework of distributional semantics, words are represented as vectors in vector spaces. The categorical model of Coecke et al. [2010], inspired by quantum protocols, has provided a convincing account of compositionality in vector space models of NLP. There is furthermore a history", "orig": "of vector space models in cognitive science. Theories of categorization such as those developed by Nosofsky [1986] and Smith et al. [1988] utilise notions of distance between feature vectors. More recently G\\\"ardenfors [2004, 2014] has developed a model of concepts in which conceptual spaces provide geometric structures, and information is represented by points, vectors and regions in vector spaces. The same compositional approach has been applied to this formalism, giving conceptual spaces theory a richer model of compositionality than previously [Bolt et al., 2018]. Compositional approaches have also been applied", "generated": "of exploring compositional approaches in cognitive science, where researchers have sought to understand how children acquire language and how adults process linguistic information.\n\nHowever, despite these advances, there remain significant challenges in developing compositional models that can scale to more complex linguistic structures and still capture the nuances of human language. For instance, existing models often struggle to capture the context-dependent meanings of words, or to account for the role of pragmatics and world knowledge in shaping linguistic interpretation. Moreover, the integration of compositional models with other areas of NLP, such as machine learning and deep learning, remains an open research question.\n\nOne potential solution to these challenges is"}
{"full_text": "During the summer of 1989, the Forecast Systems Laboratory of the National Oceanic and Atmospheric Administration sponsored an evaluation of artificial intelligence-based systems that forecast severe convective storms. The evaluation experiment, called Shootout-89, took place in Boulder, and focussed on storms over the northeastern Colorado foothills and plains (Moninger, et al., 1990). Six systems participated in Shootout-89. These included traditional expert systems, an analogy-based system, and a system developed using methods from the cognitive science/judgment analysis tradition. Each day of the exercise, the systems generated 2 to 9 hour forecasts of the probabilities of occurrence of: non significant weather, significant weather, and severe weather, in each of four regions in northeastern Colorado. A verification coordinator working at the Denver Weather Service Forecast Office gathered ground-truth data from a network of observers. Systems were evaluated on the basis of several measures of forecast skill, and on other metrics such as timeliness, ease of learning, and ease of use. Systems were generally easy to operate, however the various systems required substantially different levels of meteorological expertise on the part of their users--reflecting the various operational environments for which the systems had been designed. Systems varied in their statistical behavior, but on this difficult forecast problem, the systems generally showed a skill approximately equal to that of persistence forecasts and climatological (historical frequency) forecasts. The two systems that appeared best able to discriminate significant from non significant weather events were traditional expert systems. Both of these systems required the operator to make relatively sophisticated meteorological judgments. We are unable, based on only one summer's worth of data, to determine the extent to which the greater skill of the two systems was due to the content of their knowledge bases, or to the subjective judgments of the operator. A follow-on experiment, Shootout-91, is currently being planned. Interested potential participants are encouraged to contact the author at the address above.", "prompt": "During the summer of 1989, the Forecast Systems Laboratory of the National Oceanic and Atmospheric Administration sponsored an evaluation of artificial intelligence-based systems that forecast severe convective storms. The evaluation experiment, called Shootout-89, took place in Boulder, and focussed on storms over the northeastern Colorado foothills and plains (Moninger, et al., 1990). Six systems participated in Shootout-89. These included traditional expert systems, an analogy-based system, and a system developed using methods from the cognitive science/judgment analysis tradition. Each day of the exercise, the systems generated 2 to 9 hour forecasts of the", "orig": "probabilities of occurrence of: non significant weather, significant weather, and severe weather, in each of four regions in northeastern Colorado. A verification coordinator working at the Denver Weather Service Forecast Office gathered ground-truth data from a network of observers. Systems were evaluated on the basis of several measures of forecast skill, and on other metrics such as timeliness, ease of learning, and ease of use. Systems were generally easy to operate, however the various systems required substantially different levels of meteorological expertise on the part of their users--reflecting the various operational environments for which the systems had been designed. Systems varied in their statistical behavior, but on this", "generated": "location, intensity, and duration of severe convective storms. The forecasts were evaluated using a variety of metrics, including the threat score, the bias score, and the probability of detection. The results of Shootout-89 provided valuable insights into the strengths and weaknesses of each system and highlighted the potential of artificial intelligence-based approaches for improving severe storm forecasting. For instance, the analogy-based system demonstrated a high probability of detection for severe thunderstorms, while the expert systems performed well in terms of threat score. On the other hand, the system developed using methods from cognitive science/judgment analysis showed promise in predicting the duration of severe storms."}
{"full_text": "Many current large-scale multiagent team implementations can be characterized as following the belief-desire-intention (BDI) paradigm, with explicit representation of team plans. Despite their promise, current BDI team approaches lack tools for quantitative performance analysis under uncertainty. Distributed partially observable Markov decision problems (POMDPs) are well suited for such analysis, but the complexity of finding optimal policies in such models is highly intractable. The key contribution of this article is a hybrid BDI-POMDP approach, where BDI team plans are exploited to improve POMDP tractability and POMDP analysis improves BDI team plan performance. Concretely, we focus on role allocation, a fundamental problem in BDI teams: which agents to allocate to the different roles in the team. The article provides three key contributions. First, we describe a role allocation technique that takes into account future uncertainties in the domain; prior work in multiagent role allocation has failed to address such uncertainties. To that end, we introduce RMTDP (Role-based Markov Team Decision Problem), a new distributed POMDP model for analysis of role allocations. Our technique gains in tractability by significantly curtailing RMTDP policy search; in particular, BDI team plans provide incomplete RMTDP policies, and the RMTDP policy search fills the gaps in such incomplete policies by searching for the best role allocation. Our second key contribution is a novel decomposition technique to further improve RMTDP policy search efficiency. Even though limited to searching role allocations, there are still combinatorially many role allocations, and evaluating each in RMTDP to identify the best is extremely difficult. Our decomposition technique exploits the structure in the BDI team plans to significantly prune the search space of role allocations. Our third key contribution is a significantly faster policy evaluation algorithm suited for our BDI-POMDP hybrid approach. Finally, we also present experimental results from two domains: mission rehearsal simulation and RoboCupRescue disaster rescue simulation.", "prompt": "Many current large-scale multiagent team implementations can be characterized as following the belief-desire-intention (BDI) paradigm, with explicit representation of team plans. Despite their promise, current BDI team approaches lack tools for quantitative performance analysis under uncertainty. Distributed partially observable Markov decision problems (POMDPs) are well suited for such analysis, but the complexity of finding optimal policies in such models is highly intractable. The key contribution of this article is a hybrid BDI-POMDP approach, where BDI team plans are exploited to improve POMDP tractability and POMDP analysis improves BDI team plan performance.", "orig": "Concretely, we focus on role allocation, a fundamental problem in BDI teams: which agents to allocate to the different roles in the team. The article provides three key contributions. First, we describe a role allocation technique that takes into account future uncertainties in the domain; prior work in multiagent role allocation has failed to address such uncertainties. To that end, we introduce RMTDP (Role-based Markov Team Decision Problem), a new distributed POMDP model for analysis of role allocations. Our technique gains in tractability by significantly curtailing RMTDP policy search; in particular, BDI team plans provide incomplete RMTDP", "generated": "This hybrid approach leverages the strengths of both BDI and POMDP models to provide a more comprehensive and efficient framework for multiagent team decision-making. By incorporating BDI team plans into the POMDP framework, the complexity of finding optimal policies is significantly reduced, making it more feasible to perform quantitative performance analysis under uncertainty. Conversely, the POMDP analysis provides a more nuanced understanding of the team's decision-making process, enabling the refinement of BDI team plans to achieve better performance. This synergy between BDI and POMDP models enables the development of more robust and adaptive multiagent team implementations that can operate effectively in complex"}
{"full_text": "This PhD thesis is devoted to deterministic study of the turbulence in the Navier- Stokes equations. The thesis is divided in four independent chapters.The first chapter involves a rigorous discussion about the energy's dissipation law, proposed by theory of the turbulence K41, in the deterministic setting of the homogeneous and incompressible Navier-Stokes equations, with a stationary external force (the force only depends of the spatial variable) and on the whole space R3. The energy's dissipation law, also called the Kolmogorov's dissipation law, characterizes the energy's dissipation rate (in the form of heat) of a turbulent fluid and this law was developed by A.N. Kolmogorov in 1941. However, its deduction (which uses mainly tools of statistics) is not fully understood until our days and then an active research area consists in studying this law in the rigorous framework of the Navier-Stokes equations which describe in a mathematical way the fluids motion and in particular the movement of turbulent fluids. In this setting, the purpose of this chapter is to highlight the fact that if we consider the Navier-Stokes equations on R3 then certain physical quantities, necessary for the study of the Kolmogorov's dissipation law, have no a rigorous definition and then to give a sense to these quantities we suggest to consider the Navier-Stokes equations with an additional damping term. In the framework of these damped equations, we obtain some estimates for the energy's dissipation rate according to the Kolmogorov's dissipation law.In the second chapter we are interested in study the stationary solutions of the damped Navier- Stokes introduced in the previous chapter. These stationary solutions are a particular type of solutions which do not depend of the temporal variable and their study is motivated by the fact that we always consider the Navier-Stokes equations with a stationary external force. In this chapter we study two properties of the stationary solutions : the first property concerns the stability of these solutions where we prove that if we have a control on the external force then all non stationary solution (with depends of both spatial and temporal variables) converges toward a stationary solution. The second property concerns the decay in spatial variable of the stationary solutions. These properties of stationary solutions are a consequence of the damping term introduced in the Navier-Stokes equations.In the third chapter we still study the stationary solutions of Navier-Stokes equations but now we consider the classical equations (without any additional damping term). The purpose of this chapter is to study an other problem related to the deterministic description of the turbulence : the frequency decay of the stationary solutions. Indeed, according to the K41 theory, if the fluid is in a laminar setting then the stationary solutions of the Navier-Stokes equations must exhibit a exponential frequency decay which starts at lows frequencies. But, if the fluid is in a turbulent setting then this exponential frequency decay must be observed only at highs frequencies. In this chapter, using some Fourier analysis tools, we give a precise description of this exponential frequency decay in the laminar and in the turbulent setting.In the fourth and last chapter we return to the stationary solutions of the classical Navier-Stokes equations and we study the uniqueness of these solutions in the particular case without any external force. Following some ideas of G. Seregin, we study the uniqueness of these solutions first in the framework of Lebesgue spaces of and then in the a general framework of Morrey spaces.", "prompt": "This PhD thesis is devoted to deterministic study of the turbulence in the Navier- Stokes equations. The thesis is divided in four independent chapters.The first chapter involves a rigorous discussion about the energy's dissipation law, proposed by theory of the turbulence K41, in the deterministic setting of the homogeneous and incompressible Navier-Stokes equations, with a stationary external force (the force only depends of the spatial variable) and on the whole space R3. The energy's dissipation law, also called the Kolmogorov's dissipation law, characterizes the energy's dissipation rate (in the form of heat) of a", "orig": "turbulent fluid and this law was developed by A.N. Kolmogorov in 1941. However, its deduction (which uses mainly tools of statistics) is not fully understood until our days and then an active research area consists in studying this law in the rigorous framework of the Navier-Stokes equations which describe in a mathematical way the fluids motion and in particular the movement of turbulent fluids. In this setting, the purpose of this chapter is to highlight the fact that if we consider the Navier-Stokes equations on R3 then certain physical quantities, necessary for the study of the Kolmogorov's dissipation law,", "generated": "turbulent flow.\n\nThe significance of studying the energy's dissipation law lies in its ability to provide insights into the fundamental mechanisms governing the behavior of turbulent flows. In particular, understanding the energy's dissipation rate is crucial for predicting the computational costs associated with direct numerical simulations of turbulent flows. The K41 theory, proposed by Andrey Kolmogorov, is a phenomenological theory that describes the statistical properties of turbulent flows. Despite its simplicity, the K41 theory has been remarkably successful in capturing the essential features of turbulent flows, including the energy's dissipation law.\n\nIn the context of the homogeneous and incompressible Navier-St"}
{"full_text": "We present Spitzer IRS mid-infrared (5-35 micron) spectra of a complete flux-limited sample (> 3 mJy at 8 micron) of young stellar object (YSO) candidates selected on the basis of their infrared colors in the Serpens Molecular Cloud. Spectra of 147 sources are presented and classified. Background stars (with slope consistent with a reddened stellar spectrum and silicate features in absorption), galaxies (with redshifted PAH features) and a planetary nebula (with high ionization lines) amount to 22% of contamination in this sample, leaving 115 true YSOs. Sources with rising spectra and ice absorption features, classified as embedded Stage I protostars, amount to 18% of the sample. The remaining 82% (94) of the disk sources are analyzed in terms of spectral energy distribution shapes, PAHs and silicate features. The presence, strength and shape of these silicate features are used to infer disk properties for these systems. About 8% of the disks have 30/13 micron flux ratios consistent with cold disks with inner holes or gaps, and 3% of the disks show PAH emission. Comparison with models indicates that dust grains in the surface of these disks have sizes of at least a few \\mu\\m. The 20 micron silicate feature is sometimes seen in absence of the 10 micron feature, which may be indicative of very small holes in these disks. No significant difference is found in the distribution of silicate feature shapes and strengths between sources in clusters and in the field. Moreover, the results in Serpens are compared with other well-studied samples: the c2d IRS sample distributed over 5 clouds and a large sample of disks in the Taurus star-forming region. The remarkably similar distributions of silicate feature characteristics in samples with different environment and median ages - if significant - imply that the dust population in the disk surface results from an equilibrium between dust growth and destructive collision processes that are maintained over a few million years for any YSO population irrespective of environment.", "prompt": "We present Spitzer IRS mid-infrared (5-35 micron) spectra of a complete flux-limited sample (> 3 mJy at 8 micron) of young stellar object (YSO) candidates selected on the basis of their infrared colors in the Serpens Molecular Cloud. Spectra of 147 sources are presented and classified. Background stars (with slope consistent with a reddened stellar spectrum and silicate features in absorption), galaxies (with redshifted PAH features) and a planetary nebula (with high ionization lines) amount to 22% of contamination in this sample, leaving 115 true YSOs", "orig": ". Sources with rising spectra and ice absorption features, classified as embedded Stage I protostars, amount to 18% of the sample. The remaining 82% (94) of the disk sources are analyzed in terms of spectral energy distribution shapes, PAHs and silicate features. The presence, strength and shape of these silicate features are used to infer disk properties for these systems. About 8% of the disks have 30/13 micron flux ratios consistent with cold disks with inner holes or gaps, and 3% of the disks show PAH emission. Comparison with models indicates that dust grains in the surface of these", "generated": ".\n\nAmong the 115 classified YSOs, we find that 70% exhibit spectral features characteristic of disk emission, with approximately 30% displaying evidence of accretion activity. The remaining 20% of sources show flat or rising spectra, suggesting the presence of dense envelopes or multiple components. Furthermore, our analysis reveals a correlation between the strength of the 10 micron silicate feature and the slope of the spectral energy distribution (SED), indicating that the silicate feature can be used as a diagnostic tool for assessing the level of dust settling in the disk.\n\nWe also investigate the spatial distribution of the YSOs within the Serp"}
{"full_text": "The aim of this paper is to present the results of the land use dynamic simulations in the municipality of Paragominas-PA. The simulation is based on models built from past land use data and spatial variables of the natural environment and infrastructure. Two spatial units were analyzed: the central area of commercial agricultural production and the area of settlements and smallholdings the east. The results show distinct spatial dynamics between the analyzed areas, among which we highlight the role of soil characteristics and, associated with the topography and the occupation history, are part of the context in which is defined rationality producers. Considering the transition from forest to pasture in the commercial farming area most often associated soils are sandy. This raises the following hypothesis: the deforestation that occurred in the period are related to livestock activities. Livestock favors access to water and low fertility sands does not affect production. On the other hand, the soybean expansion occurred preferentially on existing pastures and on clay soils (Belterra clay), reducing the availability of pastures on these soils. The relative importance of types of soil increases with time. In the area of settlements, the transition from forest to pasture and family crops occurred preferentially on the variegated clay. However, it is possible that the prevalence of this transition on this texture has been given due more to the history of occupation of this area. Historically the sandy valleys were the first to be occupied, and the continuity of the deforestation occurred toward the slopes dominated by variegated clay and plateaus with Belterra clay. These associations observed indicate that, within a wider context of social, economic and political factors, natural variable factors in space are important for the choice of managements in the properties, but they are done differently in the territory, and the best knowledge of these relationships are useful for territorial planning.", "prompt": "The aim of this paper is to present the results of the land use dynamic simulations in the municipality of Paragominas-PA. The simulation is based on models built from past land use data and spatial variables of the natural environment and infrastructure. Two spatial units were analyzed: the central area of commercial agricultural production and the area of settlements and smallholdings the east. The results show distinct spatial dynamics between the analyzed areas, among which we highlight the role of soil characteristics and, associated with the topography and the occupation history, are part of the context in which is defined rationality producers. Considering the transition from forest to pasture in the", "orig": "commercial farming area most often associated soils are sandy. This raises the following hypothesis: the deforestation that occurred in the period are related to livestock activities. Livestock favors access to water and low fertility sands does not affect production. On the other hand, the soybean expansion occurred preferentially on existing pastures and on clay soils (Belterra clay), reducing the availability of pastures on these soils. The relative importance of types of soil increases with time. In the area of settlements, the transition from forest to pasture and family crops occurred preferentially on the variegated clay. However, it is possible that the prevalence of this", "generated": "central area, we observed a significant increase in the area of pastureland, driven mainly by the expansion of cattle ranching activities. This process was facilitated by the fertile soil and flat topography of the region, which made it easier to clear and convert forestland into pasture. Moreover, the proximity to the highway and the presence of settlements and infrastructure in the area also contributed to the attractiveness of this region for agricultural expansion. In contrast, the area of settlements and smallholdings in the east presented a more complex dynamic, with a greater diversity of land uses and a more pronounced fragmentation of land holding. This can be attributed to the historical"}
{"full_text": "The COVID-19 pandemic is causing a major outbreak in more than 150 countries around the world, having a severe impact on the health and life of many people globally. One of the crucial step in fighting COVID-19 is the ability to detect the infected patients early enough, and put them under special care. Detecting this disease from radiography and radiology images is perhaps one of the fastest ways to diagnose the patients. Some of the early studies showed specific abnormalities in the chest radiograms of patients infected with COVID-19. Inspired by earlier works, we study the application of deep learning models to detect COVID-19 patients from their chest radiography images. We first prepare a dataset of 5,000 Chest X-rays from the publicly available datasets. Images exhibiting COVID-19 disease presence were identified by board-certified radiologist. Transfer learning on a subset of 2,000 radiograms was used to train four popular convolutional neural networks, including ResNet18, ResNet50, SqueezeNet, and DenseNet-121, to identify COVID-19 disease in the analyzed chest X-ray images. We evaluated these models on the remaining 3,000 images, and most of these networks achieved a sensitivity rate of 98% ($\\pm$ 3%), while having a specificity rate of around 90%. Besides sensitivity and specificity rates, we also present the receiver operating characteristic (ROC) curve, precision-recall curve, average prediction, and confusion matrix of each model. We also used a technique to generate heatmaps of lung regions potentially infected by COVID-19 and show that the generated heatmaps contain most of the infected areas annotated by our board certified radiologist. While the achieved performance is very encouraging, further analysis is required on a larger set of COVID-19 images, to have a more reliable estimation of accuracy rates. The dataset, model implementations (in PyTorch), and evaluations, are all made publicly available for research community at https://github.com/shervinmin/DeepCovid.git", "prompt": "The COVID-19 pandemic is causing a major outbreak in more than 150 countries around the world, having a severe impact on the health and life of many people globally. One of the crucial step in fighting COVID-19 is the ability to detect the infected patients early enough, and put them under special care. Detecting this disease from radiography and radiology images is perhaps one of the fastest ways to diagnose the patients. Some of the early studies showed specific abnormalities in the chest radiograms of patients infected with COVID-19. Inspired by earlier works, we study the application of deep learning models to detect COVID-19 patients from their chest", "orig": "radiography images. We first prepare a dataset of 5,000 Chest X-rays from the publicly available datasets. Images exhibiting COVID-19 disease presence were identified by board-certified radiologist. Transfer learning on a subset of 2,000 radiograms was used to train four popular convolutional neural networks, including ResNet18, ResNet50, SqueezeNet, and DenseNet-121, to identify COVID-19 disease in the analyzed chest X-ray images. We evaluated these models on the remaining 3,000 images, and most of these networks achieved a sensitivity rate of 98% ($\\pm$ 3%),", "generated": "radiographs.\n\nAs we delve deeper into this research, our primary objective is to develop an accurate and efficient deep learning-based system that can assist radiologists and healthcare professionals in identifying COVID-19 infected patients rapidly and reliably. We focus on leveraging advanced convolutional neural networks (CNNs) to extract relevant features from chest radiographs, which can then be used to classify patients as either COVID-19 positive or negative. Our proposed system involves a multi-step approach, where we first preprocess the radiographs to enhance image quality and remove noise, followed by training and fine-tuning our deep learning model using a large dataset of labeled images.\n\nOur preliminary"}
{"full_text": "Born in Sydney, Australia, on April 20, 1939, Chris Heyde shifted his interest from sport to mathematics thanks to inspiration from a schoolteacher. After earning an M.Sc. degree from the University of Sydney and a Ph.D. from the Australian National University (ANU), he began his academic career in the United States at Michigan State University, and then in the United Kingdom at the University of Sheffield and the University of Manchester. In 1968, Chris moved back to Australia to teach at ANU until 1975, when he joined CSIRO, where he was Acting Chief of the Division of Mathematics and Statistics. From 1983 to 1986, he was a Professor and Chairman of the Department of Statistics at the University of Melbourne. Chris then returned to ANU to become the Head of the Statistics Department, and later the Foundation Dean of the School of Mathematical Sciences (now the Mathematical Sciences Institute). Since 1993, he has also spent one semester each year teaching at the Department of Statistics, Columbia University, and has been the director of the Center for Applied Probability at Columbia University since its creation in 1993. Chris has been honored worldwide for his contributions in probability, statistics and the history of statistics. He is a Fellow of the International Statistical Institute and the Institute of Mathematical Statistics, and he is one of three people to be a member of both the Australian Academy of Science and the Australian Academy of Social Sciences. In 2003, he received the Order of Australia from the Australian government. He has been awarded the Pitman Medal and the Hannan Medal. Chris was conferred a D.Sc. honoris causa by University of Sydney in 1998. Chris has been very active in serving the statistical community, including as the Vice President of the International Statistical Institute, President of the Bernoulli Society and Vice President of the Australian Mathematical Society. He has served on numerous editorial boards, most notably as Editor of Stochastic Processes and Their Applications from 1983 to 1989, and as Editor-in-Chief of Journal of Applied Probability and Advances in Applied Probability since 1990.", "prompt": "Born in Sydney, Australia, on April 20, 1939, Chris Heyde shifted his interest from sport to mathematics thanks to inspiration from a schoolteacher. After earning an M.Sc. degree from the University of Sydney and a Ph.D. from the Australian National University (ANU), he began his academic career in the United States at Michigan State University, and then in the United Kingdom at the University of Sheffield and the University of Manchester. In 1968, Chris moved back to Australia to teach at ANU until 1975, when he joined CSIRO, where he was Acting Chief of the Division of Mathematics and Statistics", "orig": ". From 1983 to 1986, he was a Professor and Chairman of the Department of Statistics at the University of Melbourne. Chris then returned to ANU to become the Head of the Statistics Department, and later the Foundation Dean of the School of Mathematical Sciences (now the Mathematical Sciences Institute). Since 1993, he has also spent one semester each year teaching at the Department of Statistics, Columbia University, and has been the director of the Center for Applied Probability at Columbia University since its creation in 1993. Chris has been honored worldwide for his contributions in probability, statistics and the history of statistics. He is a Fellow of", "generated": ".\n\nChris's work at CSIRO marked a significant turning point in his career, as he began to focus on applying mathematical techniques to real-world problems. He became particularly interested in statistical analysis and its applications in fields such as economics, biology, and engineering. His research in this area led to the publication of numerous papers and books, including his seminal work on stochastic processes and time series analysis.\n\nDuring his time at CSIRO, Chris also played an active role in promoting mathematics and statistics education in Australia. He was involved in the development of curricula for schools and universities, and worked closely with teachers and educators to ensure that mathematics was taught in"}
{"full_text": "This study is about the properties of the sets of objects associated in a structure resulting from multiple-processes involving chance as are materials whose texture is unordered and random. Being a paper scientist the author refers to the sheet of paper which is a stochastic fibrous set whose porous texture can be considered as an archetype for many natural or artificial human structures. The paper properties are correlated with its texture by taking account the effect of chance occurring during its manufacturing process. The theoretical developments, the formalism and the application methods presented in this study have a general significance beyond the only paper material.A specific property of sets of objects randomly unorderly distributed in space is their interfaces orientation distribution. This distribution is usually obtained by the analysis of images sampled in the object sets. The density of orientation probability of the fibers or of the texture interfaces, weighted by their length or by their area, can be interpreted as the radius of curvature of an outline or of a warped surface which characterizes, from a global and statistical point of view, the texture geometry in two or three dimensions. This figure named by the author the \"equivalent pore\", is with its elliptical shape similar to the one of the mean pore defined by the mean directional chord between the interfaces in the texture. Different methods of \"equivalent pore\" establishing are analyzed : by conformal map of the fiber network or of the texture interfaces, by images stereometric analysis of texture tomographical cuts, by scattering and diffraction of a laser light beam impacting the fibrous texture or the material surface replica, by hard X-ray absorption and phase contrast at the European Synchrotron Radiation Facilities(ESRF), in Grenoble. The \"equivalent pore\" concept allows us to study random unordered sets behavior in strength fields while simplifying this analysis. Thus a phenomenon occurring in a plane set, in two dimensions, can be analyzed on its \"equivalent pore\" linear outline, and a phenomenon which occurs in volume in a three dimensional set can be analyzed on its \"equivalent pore\" warped surface. This concept has been applied for physical, mechanical, optical and ionic conduction properties of materials like papers, boards, felts, nonwoven textiles, polymer foams, metallic alloys with grain joints, geological grounds, and for the surface mapping of natural relief and of materials with different gloss, worn or roughness levels.The ellipse and the ellipso\\\"id, as well as multi-modes compositions of it, are the most appropriate figures to represent the \"equivalent pore\" of materials with a random unordered texture. The fact that a law, which defines the curvature of an elliptic deterministic geometrical configuration, is essential to represent interface orientation allocation of elements whose spatial distribution is probabilistic is a noteworthy fact that makes us wonder. This assertion is corroborated by fluid flow analysis through porous media. The global dissipated energy for fluid flow is distributed along the motion (translation and rotation) and fluid deformation components on the \"equivalent pore\" whose surface is conformal to the texture interfaces tangential space. The porous media being homogenous and the fluid particles indistinguishable each ones from the others, due to permanent stochastic exchanges from one fluid volume element into another, we conclude that their motion quantification is invariant on each point of the \"equivalent pore\" surface. This quantification can be represented by a group of elliptical cylinders.The \"equivalent pore\" and cylinders group surface intersections define the fluid particle probabilistic paths in the porous media. One singular geometrical configuration of the elliptic cylinders group union with the ellipsoidal \"equivalent pore\" fulfills the minimal energy dissipation requirements in the stress field applied to the fluid. The resulting paths are ellipso\\\"ido-cylindrical curves carpeting the ellipso\\\"id by a beam of isoperim\\'etric lacets, in close or open loops depending on the possible pairing off the curves in their nodals and isoclinal tangency points. The flow laminarity and unrotationality are globally established in the porous media at the macroscopic scale, for Reynold's number small values, in agreement with the results found moreover. The so defined ellipso\\\"ido-cylindrical curve is a stationary curve at the average least action meaning, for a punctual mobile or a deformable volume element moving on the surface of the ellipsoid from a nodal spot to the other in an antipodal position. This curve establishes a gauge which characterizes, from a global point of view, the physical space embraced by the fluid during its flow, in dynamical balance conditions compatible with the stress field. The ellipso\\\"ido-cylindrical curve allows to carpet the ellipso\\\"id as a function of one cyclic angular variable, which is a new construction for the ellipso\\\"id. When the set of the objects is isotropic the curve is sphero-cylindrical whose plan mapping permit to decipher the tai-chi figure of the Tao\\\"ist Buddhism philosophy.This study of random unordered object sets, and more specially of fibrous porous media, allows to establish a structural link between their small scale texture stochastic disorder and the harmonious order that emerges from these sets at a larger scale. The largest chance possible, compatible with the stress field which put a strain on these sets, is the necessary and sufficient variable which allows to best globally adjust their behaviors according to the probabilistic and deterministic laws governing their evolution.The vocabulary used in this study is issued from the common language, most of the time adapted to the material class, it is transposable in other fields of interest. The text is completed by notes and a bibliography which refer to the works done following the presented concepts or in relation to this field of studies.", "prompt": "This study is about the properties of the sets of objects associated in a structure resulting from multiple-processes involving chance as are materials whose texture is unordered and random. Being a paper scientist the author refers to the sheet of paper which is a stochastic fibrous set whose porous texture can be considered as an archetype for many natural or artificial human structures. The paper properties are correlated with its texture by taking account the effect of chance occurring during its manufacturing process. The theoretical developments, the formalism and the application methods presented in this study have a general significance beyond the only paper material.A specific property of sets of objects randomly unorderly distributed in space is", "orig": "their interfaces orientation distribution. This distribution is usually obtained by the analysis of images sampled in the object sets. The density of orientation probability of the fibers or of the texture interfaces, weighted by their length or by their area, can be interpreted as the radius of curvature of an outline or of a warped surface which characterizes, from a global and statistical point of view, the texture geometry in two or three dimensions. This figure named by the author the \"equivalent pore\", is with its elliptical shape similar to the one of the mean pore defined by the mean directional chord between the interfaces in the texture. Different methods of \"equivalent pore", "generated": "that they exhibit unique characteristics that can be utilized to understand and describe various natural phenomena. For instance, the random distribution of fibers in paper can be paralleled to the arrangement of trees in a forest, the structure of a bird's nest, or even the clustering of galaxies in the universe. The study of these stochastic systems can provide valuable insights into the underlying mechanisms that govern their behavior, and can lead to the development of new models and theories that can be applied to a wide range of fields.\n\nThe concept of chance and randomness plays a crucial role in the formation of these structures, and understanding how these factors influence the final product is essential"}
{"full_text": "Due to increasing numbers of product piracy worldwide, a cost-effective method for verifying the origin of a product is to be developed. For this purpose, a certificate of authenticity can be created using precisely measurable, unique properties of special physical objects that are difficult to reconstruct. In the context of the present work, this is a counterfeit-proof label composed of randomly distributed gold nanospheres or rods in a semi-transparent material. The characteristic positioning of the label's elements can be precisely measured using a smartphone's camera and additional technologies. This can create an offline usable verification method for the general public without the need for an existing network connection. The present work provides a first part of the proof of concept that such a system and especially the associated algorithmic computation method can be implemented and efficiently used in a mobile application. In addition, a method suitable in practice for transmitting and securing the required information is determined in each case. Furthermore, the results of the validation of counterfeit-proof product labels are analyzed in detail and existing weaknesses are pointed out. -- Auf Grund weltweit steigender Zahlen der Produktpiraterie soll ein kosteng\\\"unstiges Verfahren zur Verifizierung der Herkunft eines Produktes entwickelt werden. Daf\\\"ur l\\\"asst sich durch exakt messbare, einzigartige, jedoch schwer rekonstruierbare Eigenschaften spezieller physischer Objekte ein Echtheitszertifikat kreieren. Dieses ist im Kontext der vorliegenden Arbeit ein f\\\"alschungssicheres Label, das sich in einem semi-transparenten Material aus zuf\\\"allig verteilten Goldnanok\\\"ugelchen oder -st\\\"abchen zusammensetzt. Die charakteristischen Positionierungen der Elemente des Labels lassen sich mit der Kamera eines Smartphones und zus\\\"atzlichen Technologien pr\\\"azise messen. Dadurch kann f\\\"ur die breite Bev\\\"olkerung ohne die Notwendigkeit einer bestehenden Netzwerkverbindung ein offline verwendbares Verifikationsverfahren erschaffen werden. Die vorliegende Arbeit liefert einen ersten Teil des Machbarkeitsnachweises, dass ein derartiges System und insbesondere das damit einhergehende algorithmische Berechnungsverfahren in einer mobilen Applikation implementier -- und effizient einsetzbar ist. Zudem wird je eine in der Praxis geeignete Methode zur \\\"Ubermittlung und Sicherung der ben\\\"otigten Informationen eruiert. Des Weiteren werden die Resultate der Validierung von f\\\"alschungssicheren Produktlabeln ausf\\\"uhrlich analysiert und vorhandene Schw\\\"achen aufgezeigt.", "prompt": "Due to increasing numbers of product piracy worldwide, a cost-effective method for verifying the origin of a product is to be developed. For this purpose, a certificate of authenticity can be created using precisely measurable, unique properties of special physical objects that are difficult to reconstruct. In the context of the present work, this is a counterfeit-proof label composed of randomly distributed gold nanospheres or rods in a semi-transparent material. The characteristic positioning of the label's elements can be precisely measured using a smartphone's camera and additional technologies. This can create an offline usable verification method for the general public without the need for an existing network connection. The present work provides", "orig": "a first part of the proof of concept that such a system and especially the associated algorithmic computation method can be implemented and efficiently used in a mobile application. In addition, a method suitable in practice for transmitting and securing the required information is determined in each case. Furthermore, the results of the validation of counterfeit-proof product labels are analyzed in detail and existing weaknesses are pointed out. -- Auf Grund weltweit steigender Zahlen der Produktpiraterie soll ein kosteng\\\"unstiges Verfahren zur Verifizierung der Herkunft eines Produktes entwickelt werden. Daf\\\"ur l\\\"asst sich durch ex", "generated": "a comprehensive study on the development and feasibility of such a label, examining its technical capabilities, security features, and potential applications in various industries.\n\nThe project's primary objective is to design a label that can be easily integrated into existing packaging and production lines, while maintaining a high level of security and authenticity. The randomly distributed gold nanospheres or rods create a unique fingerprint that can be read by a smartphone's camera, using a specially designed app that utilizes machine learning algorithms to analyze the pattern and verify its authenticity. This approach eliminates the need for expensive equipment, specialized skills, or a stable internet connection, making it accessible to a wide range of"}
{"full_text": "Voting is a general method for aggregating the preferences of multiple agents. Each agent ranks all the possible alternatives, and based on this, an aggregate ranking of the alternatives (or at least a winning alternative) is produced. However, when there are many alternatives, it is impractical to simply ask agents to report their complete preferences. Rather, the agents preferences, or at least the relevant parts thereof, need to be elicited. This is done by asking the agents a (hopefully small) number of simple queries about their preferences, such as comparison queries, which ask an agent to compare two of the alternatives. Prior work on preference elicitation in voting has focused on the case of unrestricted preferences. It has been shown that in this setting, it is sometimes necessary to ask each agent (almost) as many queries as would be required to determine an arbitrary ranking of the alternatives. In contrast, in this paper, we focus on single-peaked preferences. We show that such preferences can be elicited using only a linear number of comparison queries, if either the order with respect to which preferences are single-peaked is known, or at least one other agents complete preferences are known. We show that using a sublinear number of queries does not suffice. We also consider the case of cardinally single-peaked preferences. For this case, we show that if the alternatives cardinal positions are known, then an agents preferences can be elicited using only a logarithmic number of queries; however, we also show that if the cardinal positions are not known, then a sublinear number of queries does not suffice. We present experimental results for all elicitation algorithms. We also consider the problem of only eliciting enough information to determine the aggregate ranking, and show that even for this more modest objective, a sublinear number of queries per agent does not suffice for known ordinal or unknown cardinal positions. Finally, we discuss whether and how these techniques can be applied when preferences are almost single-peaked.", "prompt": "Voting is a general method for aggregating the preferences of multiple agents. Each agent ranks all the possible alternatives, and based on this, an aggregate ranking of the alternatives (or at least a winning alternative) is produced. However, when there are many alternatives, it is impractical to simply ask agents to report their complete preferences. Rather, the agents preferences, or at least the relevant parts thereof, need to be elicited. This is done by asking the agents a (hopefully small) number of simple queries about their preferences, such as comparison queries, which ask an agent to compare two of the alternatives. Prior work on preference elic", "orig": "itation in voting has focused on the case of unrestricted preferences. It has been shown that in this setting, it is sometimes necessary to ask each agent (almost) as many queries as would be required to determine an arbitrary ranking of the alternatives. In contrast, in this paper, we focus on single-peaked preferences. We show that such preferences can be elicited using only a linear number of comparison queries, if either the order with respect to which preferences are single-peaked is known, or at least one other agents complete preferences are known. We show that using a sublinear number of queries does not suffice. We also consider the case of", "generated": "itation has focused on minimizing the number of queries needed to determine the winner, or to reach a certain level of confidence in the outcome.\n\nOne approach to preference elicitation is to use a pairwise comparison framework, where agents are asked to compare pairs of alternatives. This approach can be particularly effective when there are a large number of alternatives, as it allows the system to focus on the most informative comparisons first. For example, if there are 10 alternatives, the system might first ask an agent to compare the top two alternatives, and then use the result of that comparison to determine which of the remaining alternatives to present next. This process can be repeated"}
{"full_text": "Bottleneck problems are an important class of optimization problems that have recently gained increasing attention in the domain of machine learning and information theory. They are widely used in generative models, fair machine learning algorithms, design of privacy-assuring mechanisms, and appear as information-theoretic performance bounds in various multi-user communication problems. In this work, we propose a general family of optimization problems, termed as complexity-leakage-utility bottleneck (CLUB) model, which (i) provides a unified theoretical framework that generalizes most of the state-of-the-art literature for the information-theoretic privacy models, (ii) establishes a new interpretation of the popular generative and discriminative models, (iii) constructs new insights to the generative compression models, and (iv) can be used in the fair generative models. We first formulate the CLUB model as a complexity-constrained privacy-utility optimization problem. We then connect it with the closely related bottleneck problems, namely information bottleneck (IB), privacy funnel (PF), deterministic IB (DIB), conditional entropy bottleneck (CEB), and conditional PF (CPF). We show that the CLUB model generalizes all these problems as well as most other information-theoretic privacy models. Then, we construct the deep variational CLUB (DVCLUB) models by employing neural networks to parameterize variational approximations of the associated information quantities. Building upon these information quantities, we present unified objectives of the supervised and unsupervised DVCLUB models. Leveraging the DVCLUB model in an unsupervised setup, we then connect it with state-of-the-art generative models, such as variational auto-encoders (VAEs), generative adversarial networks (GANs), as well as the Wasserstein GAN (WGAN), Wasserstein auto-encoder (WAE), and adversarial auto-encoder (AAE) models through the optimal transport (OT) problem. We then show that the DVCLUB model can also be used in fair representation learning problems, where the goal is to mitigate the undesired bias during the training phase of a machine learning model. We conduct extensive quantitative experiments on colored-MNIST and CelebA datasets, with a public implementation available, to evaluate and analyze the CLUB model.", "prompt": "Bottleneck problems are an important class of optimization problems that have recently gained increasing attention in the domain of machine learning and information theory. They are widely used in generative models, fair machine learning algorithms, design of privacy-assuring mechanisms, and appear as information-theoretic performance bounds in various multi-user communication problems. In this work, we propose a general family of optimization problems, termed as complexity-leakage-utility bottleneck (CLUB) model, which (i) provides a unified theoretical framework that generalizes most of the state-of-the-art literature for the information-theoretic privacy models, (ii) establishes a new interpretation", "orig": "of the popular generative and discriminative models, (iii) constructs new insights to the generative compression models, and (iv) can be used in the fair generative models. We first formulate the CLUB model as a complexity-constrained privacy-utility optimization problem. We then connect it with the closely related bottleneck problems, namely information bottleneck (IB), privacy funnel (PF), deterministic IB (DIB), conditional entropy bottleneck (CEB), and conditional PF (CPF). We show that the CLUB model generalizes all these problems as well as most other information-theoretic privacy models. Then, we construct the deep variational CLUB (", "generated": "of the privacyutility tradeoff, and (iii) offers a flexible and modular approach to tackle complex optimization problems in machine learning and information theory.\n\nThe CLUB model is constructed by incorporating three fundamental components: complexity, leakage, and utility. The complexity component captures the intrinsic difficulty of the optimization problem, the leakage component represents the amount of private information that is revealed, and the utility component measures the performance of the optimization algorithm. By varying the parameters of these components, the CLUB model can be adapted to tackle a wide range of optimization problems, from fair clustering and private data release to secure multi-party computation and privacy-preserving machine learning. The proposed"}
{"full_text": "Objective: Realistic tissue-mimicking phantoms are essential for the development, the investigation and the calibration of medical imaging techniques and protocols. Because it requires taking both mechanical and imaging properties into account, the development of robust, calibrated phantoms is a major challenge in elastography. Soft polyvinyl chloride gels in a liquid plasticizer (plastisol or PVCP) have been proposed as soft tissue-mimicking phantoms (TMP) for elasticity imaging. PVCP phantoms are relatively low-cost and can be easily stored over long time periods without any specific requirements. In this work, the preparation of a PVCP gel phantom for both MR and ultrasoundelastography is proposed and its acoustic, NMR and mechanical properties are studied.Material and methods: The acoustic and magnetic resonance imaging properties of PVCP are measured for different mass ratios between ultrasound speckle particles and PVCP solution, and between resin and plasticizer. The linear mechanical properties of plastisol samples are then investigated over time using not only indentation tests, but also MR and ultrasound-elastography clinical protocols. These properties are compared to typical values reported for biological soft tissues and to the values found in the literature for PVCP gels.Results and conclusions: After a period of two weeks, the mechanical properties of the plastisol samples measured with indentation testing are stable for at least the following 4 weeks (end of follow-up period 43 days after gelation-fusion). Neither the mechanical nor the NMR properties of plastisol gels were found to be affected by the addition of cellulose as acoustic speckle. Mechanical properties of the proposed gels were successfully characterized by clinical, commercially-available MR Elastography and sonoelastography protocols. PVCP with a mass ratio of ultrasound speckle particles of 0.6% to 0.8% and a mass ratio between resin and plasticizer between 50 and 70% appears as a good TMP candidate that can be used with both MR and ultrasound-based elastography methods.", "prompt": "Objective: Realistic tissue-mimicking phantoms are essential for the development, the investigation and the calibration of medical imaging techniques and protocols. Because it requires taking both mechanical and imaging properties into account, the development of robust, calibrated phantoms is a major challenge in elastography. Soft polyvinyl chloride gels in a liquid plasticizer (plastisol or PVCP) have been proposed as soft tissue-mimicking phantoms (TMP) for elasticity imaging. PVCP phantoms are relatively low-cost and can be easily stored over long time periods without any specific requirements. In this work, the preparation of a", "orig": "PVCP gel phantom for both MR and ultrasoundelastography is proposed and its acoustic, NMR and mechanical properties are studied.Material and methods: The acoustic and magnetic resonance imaging properties of PVCP are measured for different mass ratios between ultrasound speckle particles and PVCP solution, and between resin and plasticizer. The linear mechanical properties of plastisol samples are then investigated over time using not only indentation tests, but also MR and ultrasound-elastography clinical protocols. These properties are compared to typical values reported for biological soft tissues and to the values found in the literature for PVCP gels.Results and conclusions: After a period of two weeks", "generated": "novel PVCP phantom with improved mechanical and imaging properties is presented.\n\nThe novel PVCP phantom is designed to mimic the mechanical properties of soft tissues, such as liver or breast tissue, with high accuracy. The preparation of the phantom involves the mixing of polyvinyl chloride resin with a liquid plasticizer, followed by the addition of specific additives to enhance the phantom's mechanical and imaging properties. The resulting phantom exhibits a Young's modulus in the range of 1-10 kPa, which is comparable to that of soft tissues. The phantom's elasticity is also tunable by adjusting the concentration of the plasticizer, allowing for the creation of ph"}
{"full_text": "The absorption of free linear chains in a polymer brush was studied with respect to chain size $L$ and compatibility $\\chi$ with the brush by means of Monte Carlo (MC) simulations and Density Functional Theory (DFT) / Self-Consistent Field Theory (SCFT) at both moderate, $\\sigma_g = 0.25$, and high, $\\sigma_g = 1.00$, grafting densities using a bead-spring model. Different concentrations of the free chains $0.0625 \\le \\phi_o \\le 0.375$ are examined. Contrary to the case of $\\chi = 0$ when all species are almost completely ejected by the polymer brush irrespective of their length $L$, for $\\chi < 0$ we find that the degree of absorption (absorbed amount) $\\Gamma(L)$ undergoes a sharp crossover from weak to strong ($\\approx 100%$) absorption, discriminating between oligomers, $1\\le L\\le 8$, and longer chains. For a moderately dense brush, $\\sigma_g = 0.25$, the longer species, $L > 8$, populate predominantly the deep inner part of the brush whereas in a dense brush $\\sigma_g = 1.00$ they penetrate into the \"fluffy\" tail of the dense brush only. Gyration radius $R_g$ and end-to-end distance $R_e$ of absorbed chains thereby scale with length $L$ as free polymers in the bulk. Using both MC and DFT/SCFT methods for brushes of different chain length $32 \\le N \\le 256$, we demonstrate the existence of unique {\\em critical} value of compatibility $\\chi = \\chi^{c}<0$. For $\\chi^{c}(\\phi_o)$ the energy of free chains attains the {\\em same} value, irrespective of length $L$ whereas the entropy of free chain displays a pronounced minimum. At $\\chi^{c}$ all density profiles of absorbing chains with different $L$ intersect at the same distance from the grafting plane. The penetration/expulsion kinetics of free chains into the polymer brush after an instantaneous change in their compatibility $\\chi$ displays a rather rich behavior. We find three distinct regimes of penetration kinetics of free chains regarding the length $L$: I ($1\\le L\\le 8$), II ($8 \\le L \\le N$), and III ($L > N$), in which the time of absorption $\\tau$ grows with $L$ at a different rate. During the initial stages of penetration into the brush one observes a power-law increase of $\\Gamma \\propto t^\\alpha$ with power $\\alpha \\propto -\\ln \\phi_o$ whereby penetration of the free chains into the brush gets {\\em slower} as their concentration rises.", "prompt": "The absorption of free linear chains in a polymer brush was studied with respect to chain size $L$ and compatibility $\\chi$ with the brush by means of Monte Carlo (MC) simulations and Density Functional Theory (DFT) / Self-Consistent Field Theory (SCFT) at both moderate, $\\sigma_g = 0.25$, and high, $\\sigma_g = 1.00$, grafting densities using a bead-spring model. Different concentrations of the free chains $0.0625 \\le \\phi_o \\le 0.375$ are examined. Contrary to the case of $\\chi = 0", "orig": "$ when all species are almost completely ejected by the polymer brush irrespective of their length $L$, for $\\chi < 0$ we find that the degree of absorption (absorbed amount) $\\Gamma(L)$ undergoes a sharp crossover from weak to strong ($\\approx 100%$) absorption, discriminating between oligomers, $1\\le L\\le 8$, and longer chains. For a moderately dense brush, $\\sigma_g = 0.25$, the longer species, $L > 8$, populate predominantly the deep inner part of the brush whereas in a dense brush $\\sigma_g = 1.", "generated": "$, where the absorption is governed by the steric repulsion between the free chains and the brush, the presence of nonzero $\\chi$ values led to a more complex behavior.\n\nOur results show that at low grafting densities, the absorption of free chains is favored for smaller chain sizes and lower concentrations. This can be attributed to the increased mobility of the shorter chains, which allows them to penetrate deeper into the brush. In contrast, at high grafting densities, the absorption is suppressed for all chain sizes and concentrations, indicating a strong steric repulsion between the dense brush and the free chains. The addition of a nonzero compatibility parameter $\\chi"}
{"full_text": "An expression of the piano soundboard mechanical mobility (in the direction normal to the soundboard) depending on a small number of parameters and valid up to several kHz is given in this communication. Up to 1.1 kHz, our experimental and numerical investigations confirm previous results showing that the soundboard behaves like a homogeneous plate with isotropic properties and clamped boundary conditions. Therefore, according to the Skudrzyk mean-value theorem (Skudrzyk 1980), only the mass of the structure M, the modal density n(f), and the mean loss factor eta(f), are needed to express the average driving point mobility. Moreover, the expression of the envelope - resonances and antiresonances - of the mobility can be derived, according to (Langley 1994). We measured the modal loss factor and the modal density of the soundboard of an upright piano in playing condition, in an anechoic environment. The measurements could be done up to 2.5 kHz, with a novel high-resolution modal analysis technique (see the ICA companion-paper, Ege and Boutillon (2010)). Above 1.1 kHz, the change in the observed modal density together with numerical simulations confirm Berthaut's finding that the waves in the soundboard are confined between adjacent ribs (Berthaut et al. 2003). Extending the Skudrzyk and Langley approaches, we synthesize the mechanical mobility at the bridge up to 2.5 kHz. The validity of the computation for an extended spectral domain is discussed. It is also shown that the evolution of the modal density with frequency is consistent with the rise of mobility (fall of impedance) in this frequency range and that both are due to the inter-rib effect appearing when the half-wavelength becomes equal to the rib spacing. Results match previous observations by Wogram (1980), Conklin (1996), Giordano (1998), Nakamura (1983) and could be used for numerical simulations for example. This approach avoids the detailed description of the soundboard, based on a very high number of parameters. However, it can be used to predict the changes of the driving point mobility, and possibly of the sound radiation in the treble range, resulting from structural modifications.", "prompt": "An expression of the piano soundboard mechanical mobility (in the direction normal to the soundboard) depending on a small number of parameters and valid up to several kHz is given in this communication. Up to 1.1 kHz, our experimental and numerical investigations confirm previous results showing that the soundboard behaves like a homogeneous plate with isotropic properties and clamped boundary conditions. Therefore, according to the Skudrzyk mean-value theorem (Skudrzyk 1980), only the mass of the structure M, the modal density n(f), and the mean loss factor eta(f), are needed to express the average driving point mobility. Moreover", "orig": ", the expression of the envelope - resonances and antiresonances - of the mobility can be derived, according to (Langley 1994). We measured the modal loss factor and the modal density of the soundboard of an upright piano in playing condition, in an anechoic environment. The measurements could be done up to 2.5 kHz, with a novel high-resolution modal analysis technique (see the ICA companion-paper, Ege and Boutillon (2010)). Above 1.1 kHz, the change in the observed modal density together with numerical simulations confirm Berthaut's finding that the waves in the sound", "generated": ",\n\nthe simplicity of this approach enables a straightforward interpretation of the soundboard's vibration behavior in terms of its physical properties. Notably, the modal density, which represents the number of resonant modes per frequency unit, plays a crucial role in shaping the soundboard's frequency response. By examining the modal density curves, we can infer the presence of distinct resonant regions, which are characteristic of the soundboard's acoustic properties. For instance, the prominent peak around 800 Hz corresponds to the soundboard's primary resonant frequency, where the structure exhibits maximum vibrational amplitude. Furthermore, the mean loss factor, which accounts for the dissipative"}
{"full_text": "Most seismic risk assessment methods focus on estimating the damages to the built environment and the consequent socioeconomic losses without fully taking into account the social aspect of risk. Yet, human behaviour is a key element in predicting the human impact of an earthquake, therefore, it is important to include it in quantitative risk assessment studies. In this study, an interdisciplinary approach simulating pedestrians' evacuation during earthquakes at the city scale is developed using an agent-based model. The model integrates the seismic hazard, the physical vulnerability as well as individuals' behaviours and mobility. The simulator is applied to the case of Beirut, Lebanon. Lebanon is at the heart of the Levant fault system that has generated several Mw>7 earthquakes, the latest being in 1759. It is one of the countries with the highest seismic risk in the Mediterranean region. This is due to the high seismic vulnerability of the buildings due to the absence of mandatory seismic regulation until 2012, the high level of urbanization, and the lack of adequate spatial planning and risk prevention policies. Beirut as the main residential, economic and institutional hub of Lebanon is densely populated. To accommodate the growing need for urban development, constructions have almost taken over all of the green areas of the city; squares and gardens are disappearing to give place to skyscrapers. However, open spaces are safe places to shelter, away from debris, and therefore play an essential role in earthquake evacuation. Despite the massive urbanization, there are a few open spaces but locked gates and other types of anthropogenic barriers often limit their access. To simulate this complex context, pedestrians' evacuation simulations are run in a highly realistic spatial environment implemented in GAMA [1]. Previous data concerning soil and buildings in Beirut [2, 3] are complemented by new geographic data extracted from high-resolution Pleiades satellite images. The seismic loading is defined as a peak ground acceleration of 0.3g, as stated in Lebanese seismic regulations. Building damages are estimated using an artificial neural network trained to predict the mean damage [4] based on the seismic loading as well as the soil and building vibrational properties [5]. Moreover, the quantity and the footprint of the generated debris around each building are also estimated and included in the model. We simulate how topography, buildings, debris, and access to open spaces, affect individuals' mobility. Two city configurations are implemented: 1. Open spaces are accessible without any barriers; 2. Access to some open spaces is blocked. The first simulation results show that while 52% of the population is able to arrive to an open space within 5 minutes after an earthquake, this number is reduced to 39% when one of the open spaces is locked. These results show that the presence of accessible open spaces in a city and their proximity to the residential buildings is a crucial factor for ensuring people's safety when an earthquake occurs.", "prompt": "Most seismic risk assessment methods focus on estimating the damages to the built environment and the consequent socioeconomic losses without fully taking into account the social aspect of risk. Yet, human behaviour is a key element in predicting the human impact of an earthquake, therefore, it is important to include it in quantitative risk assessment studies. In this study, an interdisciplinary approach simulating pedestrians' evacuation during earthquakes at the city scale is developed using an agent-based model. The model integrates the seismic hazard, the physical vulnerability as well as individuals' behaviours and mobility. The simulator is applied to the case of Beirut, Lebanon. Lebanon is at the heart of the Levant", "orig": "fault system that has generated several Mw>7 earthquakes, the latest being in 1759. It is one of the countries with the highest seismic risk in the Mediterranean region. This is due to the high seismic vulnerability of the buildings due to the absence of mandatory seismic regulation until 2012, the high level of urbanization, and the lack of adequate spatial planning and risk prevention policies. Beirut as the main residential, economic and institutional hub of Lebanon is densely populated. To accommodate the growing need for urban development, constructions have almost taken over all of the green areas of the city; squares and gardens are disappearing to give place to skys", "generated": ", a region prone to devastating earthquakes, making it an ideal location to test the efficacy of the proposed model.\n\nThe agent-based model used in this study is innovative in that it considers the complex interactions between pedestrians, buildings, and the road network during an earthquake. By simulating the evacuation process, researchers can identify areas of high risk and prioritize interventions aimed at reducing the human impact of earthquakes. For instance, the model can pinpoint building locations that are prone to collapse, and areas where pedestrian congestion is likely to occur, allowing emergency responders to develop targeted evacuation plans.\n\nIn the context of Beirut, the model reveals that the city's dense urban fabric"}
{"full_text": "The Lasso has attracted the attention of many authors these last years. While many efforts have been made to prove that the Lasso behaves like a variable selection procedure at the price of strong (though unavoidable) assumptions on the geometric structure of these variables, much less attention has been paid to the analysis of the performance of the Lasso as a regularization algorithm. Our first purpose here is to provide a conceptually very simple result in this direction. We shall prove that, provided that the regularization parameter is properly chosen, the Lasso works almost as well as the deterministic Lasso. This result does not require any assumption at all, neither on the structure of the variables nor on the regression function. Our second purpose is to introduce a new estimator particularly adapted to deal with infinite countable dictionaries. This estimator is constructed as an l0-penalized estimator among a sequence of Lasso estimators associated to a dyadic sequence of growing truncated dictionaries. The selection procedure automatically chooses the best level of truncation of the dictionary so as to make the best tradeoff between approximation, l1-regularization and sparsity. From a theoretical point of view, we shall provide an oracle inequality satisfied by this selected Lasso estimator. The oracle inequalities established for the Lasso and the selected Lasso estimators shall enable us to derive rates of convergence on a wide class of functions, showing that these estimators perform at least as well as greedy algorithms. Besides, we shall prove that the rates of convergence achieved by the selected Lasso estimator are optimal in the orthonormal case by bounding from below the minimax risk on some Besov bodies. Finally, some theoretical results about the performance of the Lasso for infinite uncountable dictionaries will be studied in the specific framework of neural networks. All the oracle inequalities presented in this paper are obtained via the application of a single general theorem of model selection among a collection of nonlinear models which is a direct consequence of the Gaussian concentration inequality. The key idea that enables us to apply this general theorem is to see l1-regularization as a model selection procedure among l1-balls.", "prompt": "The Lasso has attracted the attention of many authors these last years. While many efforts have been made to prove that the Lasso behaves like a variable selection procedure at the price of strong (though unavoidable) assumptions on the geometric structure of these variables, much less attention has been paid to the analysis of the performance of the Lasso as a regularization algorithm. Our first purpose here is to provide a conceptually very simple result in this direction. We shall prove that, provided that the regularization parameter is properly chosen, the Lasso works almost as well as the deterministic Lasso. This result does not require any assumption at all, neither on the", "orig": "structure of the variables nor on the regression function. Our second purpose is to introduce a new estimator particularly adapted to deal with infinite countable dictionaries. This estimator is constructed as an l0-penalized estimator among a sequence of Lasso estimators associated to a dyadic sequence of growing truncated dictionaries. The selection procedure automatically chooses the best level of truncation of the dictionary so as to make the best tradeoff between approximation, l1-regularization and sparsity. From a theoretical point of view, we shall provide an oracle inequality satisfied by this selected Lasso estimator. The oracle inequalities established for the Lasso and the selected Lasso", "generated": "geometric structure of the variables nor on the distribution of the noise.\n\nIn particular, our result shows that the Lasso is able to adapt to the unknown sparsity pattern of the regression vector, without requiring any prior knowledge of the number of nonzero coefficients. This is in contrast to many existing results, which assume that the number of nonzero coefficients is known or can be accurately estimated. Our result also implies that the Lasso is a robust algorithm, in the sense that it is able to handle noisy data and still produce accurate estimates of the regression vector.\n\nMoreover, our result has important implications for the practical use of the Lasso. It suggests that"}
{"full_text": "Ion accelerators have been used by material scientists for decades to investigate radiation damage formation in nuclear materials and thus to emulate neutron-induced changes. The versatility of conditions in terms of particle energy, dose rate, fluence, etc., is a key asset of ion beams allowing for fully instrumented analytical studies. In addition, very short irradiation times and handling of non-radioactive samples dramatically curtail the global cost and duration as compared to in-reactor testing. Coupling of two or more beams, use of heated/cooled sample holders, and implementation of in situ characterization and microscopy pave the way to real time observation of microstructural and property evolution in various extreme radiation conditions more closely mimicking the nuclear environments. For these reasons, multiple ion beam facilities have been commissioned worldwide. In France, under the auspices of the Universit{\\'e} Paris-Saclay, the JANNuS platform for 'Joint Accelerators for Nanosciences and Nuclear Simulation' comprises five ion implanter and electrostatic accelerators with complementary performances. At CSNSM (CNRS \\& Univ Paris-Sud, Orsay), a 200 kV Transmission Electron Microscope is coupled to an accelerator and an implanter for in situ observation of microstructure modifications induced by ion beams in a material, making important contribution to the understanding of physical phenomena at the nanoscale. At CEA Paris-Saclay, the unique triple beam facility in Europe allows the simultaneous irradiation with heavy ions (like Fe, W) for nuclear recoil damage and implantation of a large array of ions including gasses for well-controlled modelling-oriented experiments. Several classes of materials are of interest for the nuclear industry ranging from metals and alloys, to oxides or glasses and carbides. This paper gives selected examples that illustrate the use of JANNuS ion beams in investigating the radiation resistance of structural materials for today's and tomorrow's nuclear reactors.", "prompt": "Ion accelerators have been used by material scientists for decades to investigate radiation damage formation in nuclear materials and thus to emulate neutron-induced changes. The versatility of conditions in terms of particle energy, dose rate, fluence, etc., is a key asset of ion beams allowing for fully instrumented analytical studies. In addition, very short irradiation times and handling of non-radioactive samples dramatically curtail the global cost and duration as compared to in-reactor testing. Coupling of two or more beams, use of heated/cooled sample holders, and implementation of in situ characterization and microscopy pave the way to real time observation of microstructural and property evolution", "orig": "in various extreme radiation conditions more closely mimicking the nuclear environments. For these reasons, multiple ion beam facilities have been commissioned worldwide. In France, under the auspices of the Universit{\\'e} Paris-Saclay, the JANNuS platform for 'Joint Accelerators for Nanosciences and Nuclear Simulation' comprises five ion implanter and electrostatic accelerators with complementary performances. At CSNSM (CNRS \\& Univ Paris-Sud, Orsay), a 200 kV Transmission Electron Microscope is coupled to an accelerator and an implanter for in situ observation of microstructure modifications induced by ion beams", "generated": ".\n\nThis advanced approach has enabled researchers to gain valuable insights into the fundamental mechanisms governing radiation damage and has facilitated the development of novel materials with improved radiation tolerance. For instance, the use of dual-beam irradiation has allowed for the simultaneous simulation of both neutron and ion-induced damage, enabling a more comprehensive understanding of the complex interactions between radiation and materials. Furthermore, the ability to perform in situ transmission electron microscopy (TEM) and X-ray diffraction (XRD) analysis has provided unparalleled spatial and temporal resolution, revealing the dynamic evolution of microstructure and property changes during irradiation.\n\nThe applications of ion accelerator-based research extend beyond nuclear materials,"}
{"full_text": "In this paper, we introduce a new learning strategy based on a seminal idea of Mojirsheibani (1999, 2000, 2002a, 2002b), who proposed a smart method for combining several classifiers, relying on a consensus notion. In many aggregation methods, the prediction for a new observation x is computed by building a linear or convex combination over a collection of basic estimators r1(x),. .. , rm(x) previously calibrated using a training data set. Mojirsheibani proposes to compute the prediction associated to a new observation by combining selected outputs of the training examples. The output of a training example is selected if some kind of consensus is observed: the predictions computed for the training example with the different machines have to be \"similar\" to the prediction for the new observation. This approach has been recently extended to the context of regression in Biau et al. (2016). In the original scheme, the agreement condition is actually required to hold for all individual estimators, which appears inadequate if there is one bad initial estimator. In practice, a few disagreements are allowed ; for establishing the theoretical results, the proportion of estimators satisfying the condition is required to tend to 1. In this paper, we propose an alternative procedure, mixing the previous consensus ideas on the predictions with the Euclidean distance computed between entries. This may be seen as an alternative approach allowing to reduce the effect of a possibly bad estimator in the initial list, using a constraint on the inputs. We prove the consistency of our strategy in classification and in regression. We also provide some numerical experiments on simulated and real data to illustrate the benefits of this new aggregation method. On the whole, our practical study shows that our method may perform much better than the original combination technique, and, in particular, exhibit far less variance. We also show on simulated examples that this procedure mixing inputs and outputs is still robust to high dimensional inputs.", "prompt": "In this paper, we introduce a new learning strategy based on a seminal idea of Mojirsheibani (1999, 2000, 2002a, 2002b), who proposed a smart method for combining several classifiers, relying on a consensus notion. In many aggregation methods, the prediction for a new observation x is computed by building a linear or convex combination over a collection of basic estimators r1(x),..., rm(x) previously calibrated using a training data set. Mojirsheibani proposes to compute the prediction associated to a new observation by combining selected outputs of the training examples. The output of", "orig": "a training example is selected if some kind of consensus is observed: the predictions computed for the training example with the different machines have to be \"similar\" to the prediction for the new observation. This approach has been recently extended to the context of regression in Biau et al. (2016). In the original scheme, the agreement condition is actually required to hold for all individual estimators, which appears inadequate if there is one bad initial estimator. In practice, a few disagreements are allowed ; for establishing the theoretical results, the proportion of estimators satisfying the condition is required to tend to 1. In this paper, we propose an", "generated": "each training example is considered as a vote, and the predicted output is the one that receives the majority of votes.\n\nOur proposed learning strategy, which we call \"Consensus-based Ensemble Learning\" (CEL), builds upon Mojirsheibani's idea and extends it by incorporating a novel weighting scheme. In CEL, each training example is assigned a weight based on its similarity to the new observation x. The weights are computed using a kernel function, which measures the similarity between the feature vectors of the training examples and the new observation. The weighted votes are then combined to produce the final prediction.\n\nThe key advantage of CEL is that it can effectively"}
{"full_text": "SUMMARY Geophysical imaging using the inversion procedure is a powerful tool for the exploration of the Earth's subsurface. However, the interpretation of inverted images can sometimes be difficult, due to the inherent limitations of existing inversion algorithms, which produce smoothed sections. In order to improve and automate the processing and interpretation of inverted geophysical models, we propose an approach inspired from data mining. We selected an algorithm known as DBSCAN (Density-Based Spatial Clustering of Applications with Noise) to perform clustering of inverted geophysical sections. The methodology relies on the automatic sorting and clustering of data. DBSCAN detects clusters in the inverted electrical resistivity values, with no prior knowledge of the number of clusters. This algorithm has the advantage of being defined by only two parameters: the neighbourhood of a point in the data space, and the minimum number of data points in this neighbourhood. We propose an objective procedure for the determination of these two parameters. The proof of concept described here is applied to simulated ERT (electrical resistivity tomography) sections, for the following three cases: two layers with a step, two layers with a rebound, and two layers with an anomaly embedded in the upper layer. To validate this approach, sensitivity studies were carried out on both of the above parameters, as well as to assess the influence of noise on the algorithm's performance. Finally, this methodology was tested on real field data. DBSCAN detects clusters in the inverted electrical resistivity models, and the former are then associated with various types of earth materials, thus allowing the structure of the prospected area to be determined. The proposed data-mining algorithm is shown to be effective, and to improve the interpretation of the inverted ERT sections. This new approach has considerable potential, as it can be applied to any geophysical data represented in the form of sections or maps.", "prompt": "SUMMARY Geophysical imaging using the inversion procedure is a powerful tool for the exploration of the Earth's subsurface. However, the interpretation of inverted images can sometimes be difficult, due to the inherent limitations of existing inversion algorithms, which produce smoothed sections. In order to improve and automate the processing and interpretation of inverted geophysical models, we propose an approach inspired from data mining. We selected an algorithm known as DBSCAN (Density-Based Spatial Clustering of Applications with Noise) to perform clustering of inverted geophysical sections. The methodology relies on the automatic sorting and clustering of data. DBSCAN detects clusters in the inverted electrical resistivity values, with", "orig": "no prior knowledge of the number of clusters. This algorithm has the advantage of being defined by only two parameters: the neighbourhood of a point in the data space, and the minimum number of data points in this neighbourhood. We propose an objective procedure for the determination of these two parameters. The proof of concept described here is applied to simulated ERT (electrical resistivity tomography) sections, for the following three cases: two layers with a step, two layers with a rebound, and two layers with an anomaly embedded in the upper layer. To validate this approach, sensitivity studies were carried out on both of the above parameters, as well as", "generated": "Continuation: \n...with varying densities and spatial distributions, allowing for the identification of distinct geological structures and features. Through this approach, we aim to overcome the limitations of traditional inversion methods and provide a more detailed and accurate representation of the subsurface. The proposed methodology involves several stages, including data preparation, clustering analysis, and result interpretation. Initially, the inverted geophysical sections are preprocessed to remove noise and artifacts, and then the DBSCAN algorithm is applied to group similar electrical resistivity values into clusters. The resulting clusters are subsequently analyzed to identify meaningful patterns and relationships, which can be used to infer the presence of specific geological structures"}
{"full_text": "We introduce the dependent doors problem as an abstraction for situations in which one must perform a sequence of possibly dependent decisions, without receiving feedback information on the effectiveness of previously made actions. Informally, the problem considers a set of $d$ doors that are initially closed, and the aim is to open all of them as fast as possible. To open a door, the algorithm knocks on it and it might open or not according to some probability distribution. This distribution may depend on which other doors are currently open, as well as on which other doors were open during each of the previous knocks on that door. The algorithm aims to minimize the expected time until all doors open. Crucially, it must act at any time without knowing whether or which other doors have already opened. In this work, we focus on scenarios where dependencies between doors are both positively correlated and acyclic.The fundamental distribution of a door describes the probability it opens in the best of conditions (with respect to other doors being open or closed). We show that if in two configurations of $d$ doors corresponding doors share the same fundamental distribution, then these configurations have the same optimal running time up to a universal constant, no matter what are the dependencies between doors and what are the distributions. We also identify algorithms that are optimal up to a universal constant factor. For the case in which all doors share the same fundamental distribution we additionally provide a simpler algorithm, and a formula to calculate its running time. We furthermore analyse the price of lacking feedback for several configurations governed by standard fundamental distributions. In particular, we show that the price is logarithmic in $d$ for memoryless doors, but can potentially grow to be linear in $d$ for other distributions.We then turn our attention to investigate precise bounds. Even for the case of two doors, identifying the optimal sequence is an intriguing combinatorial question. Here, we study the case of two cascading memoryless doors. That is, the first door opens on each knock independently with probability $p\\_1$. The second door can only open if the first door is open, in which case it will open on each knock independently with probability $p\\_2$. We solve this problem almost completely by identifying algorithms that are optimal up to an additive term of 1.", "prompt": "We introduce the dependent doors problem as an abstraction for situations in which one must perform a sequence of possibly dependent decisions, without receiving feedback information on the effectiveness of previously made actions. Informally, the problem considers a set of $d$ doors that are initially closed, and the aim is to open all of them as fast as possible. To open a door, the algorithm knocks on it and it might open or not according to some probability distribution. This distribution may depend on which other doors are currently open, as well as on which other doors were open during each of the previous knocks on that door. The algorithm aims to minimize the expected time", "orig": "until all doors open. Crucially, it must act at any time without knowing whether or which other doors have already opened. In this work, we focus on scenarios where dependencies between doors are both positively correlated and acyclic.The fundamental distribution of a door describes the probability it opens in the best of conditions (with respect to other doors being open or closed). We show that if in two configurations of $d$ doors corresponding doors share the same fundamental distribution, then these configurations have the same optimal running time up to a universal constant, no matter what are the dependencies between doors and what are the distributions. We also identify algorithms that are optimal up", "generated": "to open all doors, which is a challenging task due to the interdependence of the doors.\n\nIn more detail, the dependent doors problem can be formalized as follows: at each time step, the algorithm selects a door to knock on, and the outcome is determined by the underlying probability distribution. The distribution is assumed to depend on the current state of the doors, which includes information about which doors are currently open and which ones are still closed. The algorithm does not receive any feedback about the outcome of its previous actions, except for the fact that a door is now open or remains closed. The goal is to find a knocking sequence that minim"}
{"full_text": "This study proposes a methodology using OpenAlex (OA) for tracking Open Access publications in the case of Argentina, a country where a self-archiving mandate has been in effect since 2013 ( Law 26.899, 2013). A sample of 167,240 papers by researchers from the National Council for Scientific and Technical Research (CONICET) was created and analyzed using statistical techniques. We estimate that OA is able to capture between 85-93% of authors for all disciplines, with the exception of Social Sciences and Humanities, where it only reaches an estimated 47%. The availability of papers in Open Access was calculated to be 41% for the period 1953-2021 and 46% when considering exclusively the post-law period (2014-2021). In both periods, gold Open Access made up the most common route. When comparing equal periods post and pre-law, we observed that the upward trend of gold Open Access was pre-existing to the legislation and the availability of closed articles in repositories increased by 5% to what is estimated based on existing trends. However, while the green route has had a positive evolution, it has been the publication in gold journals that has boosted access to Argentine production more rapidly. We concluded that the OA-based methodology, piloted here for the first time, is viable for tracking Open Access in Argentina since it yields percentages similar to other national and international studies. En este estudio se propone una metodolog\\'ia utilizando OpenAlex (OA) para monitorear el acceso abierto (AA) a las publicaciones cient\\'ificas para el caso de Argentina, pa\\'is donde rige el mandato de autoarchivo -Ley 26.899 (2013)-. Se conform\\'o una muestra con 167.240 art\\'iculos de investigadores del Consejo Nacional de Investigaciones Cient\\'ificas y T\\'ecnicas (CONICET) que se analizaron con t\\'ecnicas estad\\'isticas. Se estim\\'o que OA puede representar entre 85-93% de los autores para todas las disciplinas, excepto Ciencias Sociales y Humanidades, donde solo alcanza al 47%. Se calcul\\'o que 41% de los art\\'iculos publicados entre 1953-2021 incluidos en la fuente est\\'an en AA, porcentaje que sube a 46% al considerar exclusivamente el periodo post ley (2014-2021). En ambos periodos es la v\\'ia dorada la que representa mayor proporci\\'on. Al comparar periodos iguales post y pre ley, se observ\\'o que la tendencia en alza de la v\\'ia dorada era preexistente a la legislaci\\'on y la disponibilidad de art\\'iculos cerrados en repositorios aument\\'o un 5% a lo que se estima en base a tendencias existentes. Se concluye que si bien la v\\'ia verde ha tenido una evoluci\\'on positiva, ha sido la publicaci\\'on en revistas doradas lo que ha impulsado m\\'as r\\'apidamente el acceso a la producci\\'on argentina. Asimismo, que la metodolog\\'ia basada en OA, piloteada aqu\\'i por primera vez, es viable para monitorear el AA en Argentina ya que arroja porcentajes similares a otros estudios nacionales e internacionales.", "prompt": "This study proposes a methodology using OpenAlex (OA) for tracking Open Access publications in the case of Argentina, a country where a self-archiving mandate has been in effect since 2013 ( Law 26.899, 2013). A sample of 167,240 papers by researchers from the National Council for Scientific and Technical Research (CONICET) was created and analyzed using statistical techniques. We estimate that OA is able to capture between 85-93% of authors for all disciplines, with the exception of Social Sciences and Humanities, where it only reaches an estimated 47%. The availability of papers in Open Access was", "orig": "calculated to be 41% for the period 1953-2021 and 46% when considering exclusively the post-law period (2014-2021). In both periods, gold Open Access made up the most common route. When comparing equal periods post and pre-law, we observed that the upward trend of gold Open Access was pre-existing to the legislation and the availability of closed articles in repositories increased by 5% to what is estimated based on existing trends. However, while the green route has had a positive evolution, it has been the publication in gold journals that has boosted access to Argentine production more rapidly. We concluded that the", "generated": "found to vary significantly across different fields, with STEM fields (Science, Technology, Engineering, and Mathematics) showing a higher rate of OA publication compared to non-STEM fields.\n\nOur results suggest that the self-archiving mandate has had a positive impact on the availability of OA publications in Argentina, particularly in fields that are more reliant on public funding. However, the lower rate of OA publication in Social Sciences and Humanities raises concerns about the barriers to OA adoption in these disciplines. Possible explanations for this disparity include the lack of institutional repositories, limited awareness about OA among researchers, and the dominant role of commercial publishers in these fields.\n\nTo address these"}
{"full_text": "Fingerprinting-based positioning, one of the promising indoor positioning solutions, has been broadly explored owing to the pervasiveness of sensor-rich mobile devices, the prosperity of opportunistically measurable location-relevant signals and the progress of data-driven algorithms. One critical challenge is to controland improve the quality of the reference fingerprint map (RFM), which is built at the offline stage and applied for online positioning. The key concept concerningthe quality control of the RFM is updating the RFM according to the newly measured data. Though varies methods have been proposed for adapting the RFM, they approach the problem by introducing extra-positioning schemes (e.g. PDR orUGV) and directly adjust the RFM without distinguishing whether critical changes have occurred. This paper aims at proposing an extra-positioning-free solution by making full use of the redundancy of measurable features. Loosely inspired by random sampling consensus (RANSAC), arbitrarily sampled subset of features from the online measurement are used for generating multi-resamples, which areused for estimating the intermediate locations. In the way of resampling, it can mitigate the impact of the changed features on positioning and enables to retrieve accurate location estimation. The users location is robustly computed by identifying the candidate locations from these intermediate ones using modified Jaccardindex (MJI) and the feature-wise change belief is calculated according to the world model of the RFM and the estimated variability of features. In order to validate our proposed approach, two levels of experimental analysis have been carried out. On the simulated dataset, the average change detection accuracy is about 90%. Meanwhile, the improvement of positioning accuracy within 2 m is about 20% by dropping out the features that are detected as changed when performing positioning comparing to that of using all measured features for location estimation. On the long-term collected dataset, the average change detection accuracy is about 85%.", "prompt": "Fingerprinting-based positioning, one of the promising indoor positioning solutions, has been broadly explored owing to the pervasiveness of sensor-rich mobile devices, the prosperity of opportunistically measurable location-relevant signals and the progress of data-driven algorithms. One critical challenge is to controland improve the quality of the reference fingerprint map (RFM), which is built at the offline stage and applied for online positioning. The key concept concerningthe quality control of the RFM is updating the RFM according to the newly measured data. Though varies methods have been proposed for adapting the RFM, they approach the problem by introducing extra-positioning schemes (e", "orig": ".g. PDR orUGV) and directly adjust the RFM without distinguishing whether critical changes have occurred. This paper aims at proposing an extra-positioning-free solution by making full use of the redundancy of measurable features. Loosely inspired by random sampling consensus (RANSAC), arbitrarily sampled subset of features from the online measurement are used for generating multi-resamples, which areused for estimating the intermediate locations. In the way of resampling, it can mitigate the impact of the changed features on positioning and enables to retrieve accurate location estimation. The users location is robustly computed by identifying the candidate locations from these intermediate ones using modified J", "generated": ".g., crowdsourcing, active learning, and transfer learning) that can enhance the quality of the RFM. \n\nHowever, these methods may not be efficient in scenarios where the environment is constantly changing or where the newly measured data is limited. In such cases, it is essential to develop a more robust and adaptive method for updating the RFM. One potential approach is to leverage the spatial and temporal dependencies among the RFM data to identify the most critical regions that require updates. This can be achieved through the application of graph-based models that capture the relationships between different fingerprints and their corresponding locations. By analyzing the graph structure, it is possible to"}
{"full_text": "Technological progress in material synthesis, as well as artificial realization of condensed matter scenarios via ultra-cold atomic gases in optical lattices or epitaxial growth of thin films, is opening the gate to investigate a plethora of unprecedented strongly correlated electron systems. In a large subclass thereof, a metallic state of layered electrons undergoes an ordering transition below some temperature into unconventional states of matter driven by electronic correlations, such as magnetism, superconductivity, or other Fermi surface instabilities. While this type of phenomena has been a well-established direction of research in condensed matter for decades, the variety of today's accessible scenarios pose fundamental new challenges to describe them. A core complication is the multi-orbital nature of the low-energy electronic structure of these systems, such as the multi-d orbital nature of electrons in iron pnictides and transition-metal oxides in general, but also electronic states of matter on lattices with multiple sites per unit cell such as the honeycomb or kagome lattice. In this review, we propagate the functional renormalization group (FRG) as a suited approach to investigate multi-orbital Fermi surface instabilities. The primary goal of the review is to describe the FRG in explicit detail and render it accessible to everyone both at a technical and intuitive level. Summarizing recent progress in the field of multi-orbital Fermi surface instabilities, we illustrate how the unbiased fashion by which the FRG treats all kinds of ordering tendencies guarantees an adequate description of electronic phase diagrams and often allows to obtain parameter trends of sufficient accuracy to make qualitative predictions for experiments. This review includes detailed and illustrative illustrations of magnetism and, in particular, superconductivity for the iron pnictides from the viewpoint of FRG. Furthermore, it discusses candidate scenarios for topological bulk singlet superconductivity and exotic particle-hole condensates on hexagonal lattices such as sodium-doped cobaltates, graphene doped to van Hove Filling, and the kagome Hubbard model. In total, the FRG promises to be one of the most versatile and revealing numerical approaches to address unconventional Fermi surface instabilities in future fields of condensed matter research.", "prompt": "Technological progress in material synthesis, as well as artificial realization of condensed matter scenarios via ultra-cold atomic gases in optical lattices or epitaxial growth of thin films, is opening the gate to investigate a plethora of unprecedented strongly correlated electron systems. In a large subclass thereof, a metallic state of layered electrons undergoes an ordering transition below some temperature into unconventional states of matter driven by electronic correlations, such as magnetism, superconductivity, or other Fermi surface instabilities. While this type of phenomena has been a well-established direction of research in condensed matter for decades, the variety of today's accessible scenarios pose fundamental new challenges", "orig": "to describe them. A core complication is the multi-orbital nature of the low-energy electronic structure of these systems, such as the multi-d orbital nature of electrons in iron pnictides and transition-metal oxides in general, but also electronic states of matter on lattices with multiple sites per unit cell such as the honeycomb or kagome lattice. In this review, we propagate the functional renormalization group (FRG) as a suited approach to investigate multi-orbital Fermi surface instabilities. The primary goal of the review is to describe the FRG in explicit detail and render it accessible to everyone both at", "generated": ".\n\nThe investigation of these exotic states of matter is particularly intriguing in the context of high-temperature superconductors, where the interplay between magnetic and superconducting phases has sparked intense debate. The development of advanced synthesization techniques has enabled the creation of materials with tailored electronic structures, allowing researchers to probe the intricacies of these complex systems. For instance, the discovery of superconductivity in iron-based compounds has shed new light on the role of magnetic fluctuations in the emergence of unconventional superconductivity. Furthermore, the ability to engineer heterostructures and interfaces has opened up new avenues for exploring the interaction between competing orders and the resulting impact"}
{"full_text": "In robotic radiation therapy, high-energy photon beams from different directions are directed at a target within the patient. Target motion can be tracked by robotic ultrasound and then compensated by synchronous beam motion. However, moving the beams may result in beams passing through the ultrasound transducer or the robot carrying it. While this can be avoided by pausing the beam delivery, the treatment time would increase. Typically, the beams are delivered in an order which minimizes the robot motion and thereby the overall treatment time. However, this order can be changed, i.e., instead of pausing beams, other feasible beam could be delivered. We address this problem of dynamically ordering the beams by applying a model checking paradigm to select feasible beams. Since breathing patterns are complex and change rapidly, any offline model would be too imprecise. Thus, model checking must be conducted online, predicting the patient's current breathing pattern for a short amount of time and checking which beams can be delivered safely. Monitoring the treatment delivery online provides the option to reschedule beams dynamically in order to avoid pausing and hence to reduce treatment time. While human breathing patterns are complex and may change rapidly, we need a model which can be verified quickly and use approximation by a superposition of sine curves. Further, we simplify the 3D breathing motion into separate 1D models. We compensate the simplification by adding noise inside the model itself. In turn, we synchronize between the multiple models representing the different spatial directions, the treatment simulation, and corresponding verification queries. Our preliminary results show a 16.02 % to 37.21 % mean improvement on the idle time compared to a static beam schedule, depending on an additional safety margin. Note that an additional safety margin around the ultrasound robot can decrease idle times but also compromises plan quality by limiting the range of available beam directions. In contrast, the approach using online model checking maintains the plan quality. Further, we compare to a naive machine learning approach that does not achieve its goals while being harder to reason about.", "prompt": "In robotic radiation therapy, high-energy photon beams from different directions are directed at a target within the patient. Target motion can be tracked by robotic ultrasound and then compensated by synchronous beam motion. However, moving the beams may result in beams passing through the ultrasound transducer or the robot carrying it. While this can be avoided by pausing the beam delivery, the treatment time would increase. Typically, the beams are delivered in an order which minimizes the robot motion and thereby the overall treatment time. However, this order can be changed, i.e., instead of pausing beams, other feasible beam could be delivered. We address this problem of dynamically", "orig": "ordering the beams by applying a model checking paradigm to select feasible beams. Since breathing patterns are complex and change rapidly, any offline model would be too imprecise. Thus, model checking must be conducted online, predicting the patient's current breathing pattern for a short amount of time and checking which beams can be delivered safely. Monitoring the treatment delivery online provides the option to reschedule beams dynamically in order to avoid pausing and hence to reduce treatment time. While human breathing patterns are complex and may change rapidly, we need a model which can be verified quickly and use approximation by a superposition of sine curves. Further, we simplify the 3", "generated": "adjusting the beam delivery sequence in real-time to mitigate the interference between the beams and the ultrasound transducer.\n\nTo achieve this, we propose a novel algorithm that continuously monitors the target motion and the robot's position. This information is used to predict potential beam-transducer collisions and accordingly adjust the beam delivery sequence. The algorithm is designed to optimize the treatment time while ensuring that the beams are delivered with the required accuracy and precision. By dynamically reordering the beams, we can reduce the number of pauses and interruptions, resulting in a more efficient treatment process.\n\nThe proposed approach involves several key components, including a motion prediction module, a collision detection module,"}
{"full_text": "Context: Computational notebooks are a contemporary style of literate programming, in which users can communicate and transfer knowledge by interleaving executable code, output, and prose in a single rich document. A Domain-Specific Language (DSL) is an artificial software language tailored for a particular application domain. Usually, DSL users are domain experts that may not have a software engineering background. As a consequence, they might not be familiar with Integrated Development Environments (IDEs). Thus, the development of tools that offer different interfaces for interacting with a DSL is relevant. Inquiry: However, resources available to DSL designers are limited. We would like to leverage tools used to interact with general purpose languages in the context of DSLs. Computational notebooks are an example of such tools. Then, our main question is: What is an efficient and effective method of designing and implementing notebook interfaces for DSLs? By addressing this question we might be able to speed up the development of DSL tools, and ease the interaction between end-users and DSLs. Approach: In this paper, we present Bacat\\'a, a mechanism for generating notebook interfaces for DSLs in a language parametric fashion. We designed this mechanism in a way in which language engineers can reuse as many language components (e.g., language processors, type checkers, code generators) as possible. Knowledge: Our results show that notebook interfaces generated by Bacat\\'a can be automatically generated with little manual configuration. There are few considerations and caveats that should be addressed by language engineers that rely on language design aspects. The creation of a notebook for a DSL with Bacat\\'a becomes a matter of writing the code that wires existing language components in the Rascal language workbench with the Jupyter platform. Grounding: We evaluate Bacat\\'a by generating functional computational notebook interfaces for three different non-trivial DSLs, namely: a small subset of Halide (a DSL for digital image processing), SweeterJS (an extended version of JavaScript), and QL (a DSL for questionnaires). Additionally, it is relevant to generate notebook implementations rather than implementing them manually. We measured and compared the number of Source Lines of Code (SLOCs) that we reused from existing implementations of those languages. Importance: The adoption of notebooks by novice-programmers and end-users has made them very popular in several domains such as exploratory programming, data science, data journalism, and machine learning. Why are they popular? In (data) science, it is essential to make results reproducible as well as understandable. However, notebooks are only available for GPLs. This paper opens up the notebook metaphor for DSLs to improve the end-user experience when interacting with code and to increase DSLs adoption.", "prompt": "Context: Computational notebooks are a contemporary style of literate programming, in which users can communicate and transfer knowledge by interleaving executable code, output, and prose in a single rich document. A Domain-Specific Language (DSL) is an artificial software language tailored for a particular application domain. Usually, DSL users are domain experts that may not have a software engineering background. As a consequence, they might not be familiar with Integrated Development Environments (IDEs). Thus, the development of tools that offer different interfaces for interacting with a DSL is relevant. Inquiry: However, resources available to DSL designers are limited. We would like to leverage tools used", "orig": "to interact with general purpose languages in the context of DSLs. Computational notebooks are an example of such tools. Then, our main question is: What is an efficient and effective method of designing and implementing notebook interfaces for DSLs? By addressing this question we might be able to speed up the development of DSL tools, and ease the interaction between end-users and DSLs. Approach: In this paper, we present Bacat\\'a, a mechanism for generating notebook interfaces for DSLs in a language parametric fashion. We designed this mechanism in a way in which language engineers can reuse as many language components (e.g., language processors, type", "generated": "in computational notebooks to provide an interactive and user-friendly environment for DSL users.\n\nContinuation:\n\nOne such leverage is the adaptation of computational notebooks to create a DSL-agnostic environment, where domain experts can interact with the DSL without requiring extensive software engineering knowledge. This can be achieved by creating a DSL plugin for popular notebook platforms, allowing users to write and execute DSL code within a familiar environment. The plugin can provide features such as syntax highlighting, code completion, and error reporting, making it easier for domain experts to focus on the domain logic rather than the nuances of the DSL.\n\nFurthermore, the notebook environment can be extended to support collaboration and knowledge sharing"}
{"full_text": "The aim of this work is to study the asymptotic behavior of a structure made of plates of thickness $2\\delta$ when $\\delta\\to 0$. This study is carried on within the frame of linear elasticity by using the unfolding method. It is based on several decompositions of the structure displacements and on the passing to the limit in fixed domains. We begin with studying the displacements of a plate. We show that any displacement is the sum of an elementary displacement concerning the normal lines on the middle surface of the plate and a residual displacement linked to these normal lines deformations. An elementary displacement is linear with respect to the variable $x$3. It is written $U(^x)+R(^x)\\land x3e3$ where U is a displacement of the mid-surface of the plate. We show a priori estimates and convergence results when $\\delta \\to 0$. We characterize the limits of the unfolded displacements of a plate as well as the limits of the unfolded of the strained tensor. Then we extend these results to the structures made of plates. We show that any displacement of a structure is the sum of an elementary displacement of each plate and of a residual displacement. The elementary displacements of the structure (e.d.p.s.) coincide with elementary rods displacements in the junctions. Any e.d.p.s. is given by two functions belonging to $H1(S;R3)$ where S is the skeleton of the structure (the plates mid-surfaces set). One of these functions : U is the skeleton displacement. We show that U is the sum of an extensional displacement and of an inextensional one. The first one characterizes the membrane displacements and the second one is a rigid displacement in the direction of the plates and it characterizes the plates flexion. Eventually we pass to the limit as $\\delta \\to 0$ in the linearized elasticity system, on the one hand we obtain a variational problem that is satisfied by the limit extensional displacement, and on the other hand, a variational problem satisfied by the limit of inextensional displacements.", "prompt": "The aim of this work is to study the asymptotic behavior of a structure made of plates of thickness $2\\delta$ when $\\delta\\to 0$. This study is carried on within the frame of linear elasticity by using the unfolding method. It is based on several decompositions of the structure displacements and on the passing to the limit in fixed domains. We begin with studying the displacements of a plate. We show that any displacement is the sum of an elementary displacement concerning the normal lines on the middle surface of the plate and a residual displacement linked to these normal lines deformations. An elementary displacement is linear with respect to the", "orig": "variable $x$3. It is written $U(^x)+R(^x)\\land x3e3$ where U is a displacement of the mid-surface of the plate. We show a priori estimates and convergence results when $\\delta \\to 0$. We characterize the limits of the unfolded displacements of a plate as well as the limits of the unfolded of the strained tensor. Then we extend these results to the structures made of plates. We show that any displacement of a structure is the sum of an elementary displacement of each plate and of a residual displacement. The elementary displacements of the structure (e.d.p.s", "generated": "thickness of the plate, whereas the residual displacement is of higher order. This decomposition allows us to separate the effects of the plate's thickness on its overall deformation.\n\nFurthermore, we investigate the asymptotic behavior of the plate's stress tensor as the thickness approaches zero. We find that the stress tensor converges to a limit that is independent of the thickness, and that it can be expressed as a function of the elementary displacement. This result is crucial in understanding the behavior of the structure as a whole, since it allows us to model the interactions between adjacent plates.\n\nIn addition, we analyze the convergence of the structure's energy functional as the thickness tends"}
{"full_text": "The purpose of this study is to optimize the selection of prophylactic cardioverter defibrillator implantation candidates. Currently, the main criterion for implantation is a low Left Ventricular Ejection Fraction (LVEF) whose specificity is relatively poor. We designed two classifiers aimed to predict, from long term ECG recordings (Holter), whether a low-LVEF patient is likely or not to undergo ventricular arrhythmia in the next six months. One classifier is a single hidden layer neural network whose variables are the most relevant features extracted from Holter recordings, and the other classifier has a structure that capitalizes on the physiological decomposition of the arrhythmogenic factors into three disjoint groups: the myocardial substrate, the triggers and the autonomic nervous system (ANS). In this ad hoc network, the features were assigned to each group; one neural network classifier per group was designed and its complexity was optimized. The outputs of the classifiers were fed to a single neuron that provided the required probability estimate. The latter was thresholded for final discrimination A dataset composed of 186 pre-implantation 30-mn Holter recordings of patients equipped with an implantable cardioverter defibrillator (ICD) in primary prevention was used in order to design and test this classifier. 44 out of 186 patients underwent at least one treated ventricular arrhythmia during the six-month follow-up period. Performances of the designed classifier were evaluated using a cross-test strategy that consists in splitting the database into several combinations of a training set and a test set. The average arrhythmia prediction performances of the ad-hoc classifier are NPV = 77% $\\pm$ 13% and PPV = 31% $\\pm$ 19% (Negative Predictive Value $\\pm$ std, Positive Predictive Value $\\pm$ std). According to our study, improving prophylactic ICD-implantation candidate selection by automatic classification from ECG features may be possible, but the availability of a sizable dataset appears to be essential to decrease the number of False Negatives.", "prompt": "The purpose of this study is to optimize the selection of prophylactic cardioverter defibrillator implantation candidates. Currently, the main criterion for implantation is a low Left Ventricular Ejection Fraction (LVEF) whose specificity is relatively poor. We designed two classifiers aimed to predict, from long term ECG recordings (Holter), whether a low-LVEF patient is likely or not to undergo ventricular arrhythmia in the next six months. One classifier is a single hidden layer neural network whose variables are the most relevant features extracted from Holter recordings, and the other classifier has a structure that capitalizes on the physiological", "orig": "decomposition of the arrhythmogenic factors into three disjoint groups: the myocardial substrate, the triggers and the autonomic nervous system (ANS). In this ad hoc network, the features were assigned to each group; one neural network classifier per group was designed and its complexity was optimized. The outputs of the classifiers were fed to a single neuron that provided the required probability estimate. The latter was thresholded for final discrimination A dataset composed of 186 pre-implantation 30-mn Holter recordings of patients equipped with an implantable cardioverter defibrillator (ICD) in primary prevention was used in order to design and test this", "generated": "understanding of the underlying mechanisms of ventricular arrhythmias.\n\nThe neural network classifier is designed to learn patterns and relationships in the Holter data that are indicative of an increased risk of ventricular arrhythmias. The features used as inputs to the neural network include standard deviational measurements, such as mean Heart Rate (HR) and standard deviation of HR, as well as more advanced metrics, such as Poincar\u00e9 plot analysis and symbolic dynamics. These features are selected based on their ability to capture the complexity and variability of the heart's rhythm, which are known to be important predictors of ventricular arrhythmias.\n\nThe physiological-based"}
{"full_text": "Context: Domain-specific languages (DSLs) enable domain experts to specify tasks and problems themselves, while enabling static analysis to elucidate issues in the modelled domain early. Although language workbenches have simplified the design of DSLs and extensions to general purpose languages, static analyses must still be implemented manually. Inquiry: Moreover, static analyses, e.g., complexity metrics, dependency analysis, and declaration-use analysis, are usually domain-dependent and cannot be easily reused. Therefore, transferring existing static analyses to another DSL incurs a huge implementation overhead. However, this overhead is not always intrinsically necessary: in many cases, while the concepts of the DSL on which a static analysis is performed are domain-specific, the underlying algorithm employed in the analysis is actually domain-independent and thus can be reused in principle, depending on how it is specified. While current approaches either implement static analyses internally or with an external Visitor, the implementation is tied to the language's grammar and cannot be reused easily. Thus far, a commonly used approach that achieves reusable static analysis relies on the transformation into an intermediate representation upon which the analysis is performed. This, however, entails a considerable additional implementation effort. Approach: To remedy this, it has been proposed to map the necessary domain-specific concepts to the algorithm's domain-independent data structures, yet without a practical implementation and the demonstration of reuse. Thus, to make static analysis reusable again, we employ relational Reference Attribute Grammars (RAGs) by creating such a mapping to a domain-independent overlay structure using higher-order attributes. Knowledge: We describe how static analysis can be specified on analysis-specific data structures, how relational RAGs can help with the specification, and how a mapping from the domain-specific language can be performed. Furthermore, we demonstrate how a static analysis for a DSL can be externalized and reused in another general purpose language. Grounding: The approach was evaluated using the RAG system JastAdd. To illustrate reusability, we implemented two analyses with two addressed languages each: a cycle detection analysis used in a small state machine DSL and for detecting circular dependencies in Java types and packages, and an analysis of variable shadowing, applied to both Java and the Modelica modelling language. Thereby, we demonstrate the reuse of two analysis algorithms in three completely different domains. Additionally, we use the cycle detection analysis to evaluate the efficiency by comparing our external analysis to an internal reference implementation analysing all Java programs in the Qualitas Corpus and thereby are able to show that an externalized analysis incurs only minimal overhead. Importance: We make static analysis reusable, again, showing the practicality and efficiency of externalizing static analysis for both DSLs and general purpose languages using relational RAGs.", "prompt": "Context: Domain-specific languages (DSLs) enable domain experts to specify tasks and problems themselves, while enabling static analysis to elucidate issues in the modelled domain early. Although language workbenches have simplified the design of DSLs and extensions to general purpose languages, static analyses must still be implemented manually. Inquiry: Moreover, static analyses, e.g., complexity metrics, dependency analysis, and declaration-use analysis, are usually domain-dependent and cannot be easily reused. Therefore, transferring existing static analyses to another DSL incurs a huge implementation overhead. However, this overhead is not always intrinsically necessary: in many cases, while the concepts of", "orig": "the DSL on which a static analysis is performed are domain-specific, the underlying algorithm employed in the analysis is actually domain-independent and thus can be reused in principle, depending on how it is specified. While current approaches either implement static analyses internally or with an external Visitor, the implementation is tied to the language's grammar and cannot be reused easily. Thus far, a commonly used approach that achieves reusable static analysis relies on the transformation into an intermediate representation upon which the analysis is performed. This, however, entails a considerable additional implementation effort. Approach: To remedy this, it has been proposed to map the necessary domain-specific concepts to the algorithm's domain", "generated": "the analysis remain the same, the DSL-specific details differ.\n\nContinuation: It is essential to identify the root causes of this overhead and explore potential solutions to mitigate it. One possible approach lies in the development of a meta-language for static analyses, allowing domain experts to specify the analysis logic in a DSL-agnostic manner. This meta-language could provide a set of abstract constructs, enabling the definition of static analyses that are independent of specific DSLs. Such an approach would facilitate the reuse of existing static analyses across different DSLs, reducing the implementation overhead significantly. Furthermore, it would enable the creation of a library of reusable static analyses, empowering domain"}
{"full_text": "The problem of finding a reduced dimensionality representation of categorical variables while preserving their most relevant characteristics is fundamental for the analysis of complex data. Specifically, given a co-occurrence matrix of two variables, one often seeks a compact representation of one variable which preserves information about the other variable. We have recently introduced ``Sufficient Dimensionality Reduction' [GT-2003], a method that extracts continuous reduced dimensional features whose measurements (i.e., expectation values) capture maximal mutual information among the variables. However, such measurements often capture information that is irrelevant for a given task. Widely known examples are illumination conditions, which are irrelevant as features for face recognition, writing style which is irrelevant as a feature for content classification, and intonation which is irrelevant as a feature for speech recognition. Such irrelevance cannot be deduced apriori, since it depends on the details of the task, and is thus inherently ill defined in the purely unsupervised case. Separating relevant from irrelevant features can be achieved using additional side data that contains such irrelevant structures. This approach was taken in [CT-2002], extending the information bottleneck method, which uses clustering to compress the data. Here we use this side-information framework to identify features whose measurements are maximally informative for the original data set, but carry as little information as possible on a side data set. In statistical terms this can be understood as extracting statistics which are maximally sufficient for the original dataset, while simultaneously maximally ancillary for the side dataset. We formulate this tradeoff as a constrained optimization problem and characterize its solutions. We then derive a gradient descent algorithm for this problem, which is based on the Generalized Iterative Scaling method for finding maximum entropy distributions. The method is demonstrated on synthetic data, as well as on real face recognition datasets, and is shown to outperform standard methods such as oriented PCA.", "prompt": "The problem of finding a reduced dimensionality representation of categorical variables while preserving their most relevant characteristics is fundamental for the analysis of complex data. Specifically, given a co-occurrence matrix of two variables, one often seeks a compact representation of one variable which preserves information about the other variable. We have recently introduced ``Sufficient Dimensionality Reduction' [GT-2003], a method that extracts continuous reduced dimensional features whose measurements (i.e., expectation values) capture maximal mutual information among the variables. However, such measurements often capture information that is irrelevant for a given task. Widely known examples are illumination conditions, which are irrelevant as features for", "orig": "face recognition, writing style which is irrelevant as a feature for content classification, and intonation which is irrelevant as a feature for speech recognition. Such irrelevance cannot be deduced apriori, since it depends on the details of the task, and is thus inherently ill defined in the purely unsupervised case. Separating relevant from irrelevant features can be achieved using additional side data that contains such irrelevant structures. This approach was taken in [CT-2002], extending the information bottleneck method, which uses clustering to compress the data. Here we use this side-information framework to identify features whose measurements are maximally informative for the original data set", "generated": "object recognition tasks, yet may dominate the feature space if not properly addressed.\n\nTo combat this issue, we propose an extension to the Sufficient Dimensionality Reduction method, which we term ``Task-Driven Dimensionality Reduction'' (TDDR). TDDR incorporates knowledge about the specific task at hand, allowing for the extraction of features that are not only informative about the underlying variables but also relevant to the task. This is achieved through the use of a task-specific objective function, which guides the dimensionality reduction process to preserve only the information that is crucial for the task. In the context of object recognition, for instance, TDDR can be used"}
{"full_text": "When can we compute the diameter of a graph in quasi linear time? We address this question for the class of {\\em split graphs}, that we observe to be the hardest instances for deciding whether the diameter is at most two. We stress that although the diameter of a non-complete split graph can only be either $2$ or $3$, under the Strong Exponential-Time Hypothesis (SETH) we cannot compute the diameter of an $n$-vertex $m$-edge split graph in less than quadratic time -- in the size $n+m$ of the input. Therefore it is worth to study the complexity of diameter computation on {\\em subclasses} of split graphs, in order to better understand the complexity border. Specifically, we consider the split graphs with bounded {\\em clique-interval number} and their complements, with the former being a natural variation of the concept of interval number for split graphs that we introduce in this paper. We first discuss the relations between the clique-interval number and other graph invariants such as the classic interval number of graphs, the treewidth, the {\\em VC-dimension} and the {\\em stabbing number} of a related hypergraph. Then, in part based on these above relations, we almost completely settle the complexity of diameter computation on these subclasses of split graphs: - For the $k$-clique-interval split graphs, we can compute their diameter in truly subquadratic time if $k={\\cal O}(1)$, and even in quasi linear time if $k=o(\\log{n})$ and in addition a corresponding ordering of the vertices in the clique is given. However, under SETH this cannot be done in truly subquadratic time for any $k = \\omega(\\log{n})$. - For the {\\em complements} of $k$-clique-interval split graphs, we can compute their diameter in truly subquadratic time if $k={\\cal O}(1)$, and even in time ${\\cal O}(km)$ if a corresponding ordering of the vertices in the stable set is given. Again this latter result is optimal under SETH up to polylogarithmic factors. Our findings raise the question whether a $k$-clique interval ordering can always be computed in quasi linear time. We prove that it is the case for $k=1$ and for some subclasses such as bounded-treewidth split graphs, threshold graphs and comparability split graphs. Finally, we prove that some important subclasses of split graphs -- including the ones mentioned above -- have a bounded clique-interval number.", "prompt": "When can we compute the diameter of a graph in quasi linear time? We address this question for the class of {\\em split graphs}, that we observe to be the hardest instances for deciding whether the diameter is at most two. We stress that although the diameter of a non-complete split graph can only be either $2$ or $3$, under the Strong Exponential-Time Hypothesis (SETH) we cannot compute the diameter of an $n$-vertex $m$-edge split graph in less than quadratic time -- in the size $n+m$ of the input. Therefore it is worth to study the complexity of diameter computation", "orig": "on {\\em subclasses} of split graphs, in order to better understand the complexity border. Specifically, we consider the split graphs with bounded {\\em clique-interval number} and their complements, with the former being a natural variation of the concept of interval number for split graphs that we introduce in this paper. We first discuss the relations between the clique-interval number and other graph invariants such as the classic interval number of graphs, the treewidth, the {\\em VC-dimension} and the {\\em stabbing number} of a related hypergraph. Then, in part based on these above relations, we almost completely settle the complexity of", "generated": "on split graphs, as they provide a fundamental limit for the efficient computation of graph diameter.\n\nOur investigation leads us to explore the structural properties of split graphs that make them challenging for diameter computation. We find that the presence of certain types of induced subgraphs, such as cliques and cobipartite graphs, significantly affects the computational complexity of diameter computation on split graphs. By exploiting these structural insights, we develop a novel algorithm that computes the diameter of an $n$-vertex $m$-edge split graph in quasi-linear time, $\\tilde{O}(n+m)$, under certain restrictions on the graph's structure.\n\nOur"}
{"full_text": "On the basis of general relativity and quantum statistics, it was shown (Neslu\\v{s}an L.: 2009, Phys. Rev. D 80, 024015, arxiv:0808.3484) that the equation of state (ES) of extremely hot Fermi-Dirac gas in the surface layer of an ultra-relativistic compact object converges to the same form as the relativistic equation of thermodynamical equilibrium (RETE), which is the condition of stability of the object. The description of energy state of a gas particle was completed with the term corresponding with the potential-type energy. The necessity of such the term is set by the demand of convergence of the relativistic particle-impulse distribution law to its Maxwell-Boltzmann form in the classical limit. The identity of the ES and RETE, both applied to the gas in the object's surface layer, becomes perfect, yielding the stable object, when the object's physical radius is identical to its gravitational radius. In this state, the internal energy of gas particles in a volume of the object's surface layer increases over all limits in the frame of the volume and this opens the question if the horizon of events actually is an insuperable barrier. It seems to be possible that some matter can be temporarily lifted above the surface or, so far, be ejected from the object and can emit a radiation detectable by a distant observer. In our contribution, we demonstrate a general validity of the functional form of the potential-type energy found in our previous work. The consistency of the RETE with its non-relativistic approximation can occur only for this functional form. We also point out some observational consequences of the approximate identity of ES and RETE before the object collapses, in the proper time, to its gravitational radius as well as the possible observational consequences of the infinitely high internal energy in the surface layer of already collapsed object. In general, we propagate the idea that a lot of phenomena observed at the stellar-sized or supermassive black holes (or not-yet black holes) can be not necessarily related to the structures in a vicinity of the black hole, e.g. to an accretion disk, but they can be linked directly to the behaviour of the central, ultra-compact object.", "prompt": "On the basis of general relativity and quantum statistics, it was shown (Neslu\\v{s}an L.: 2009, Phys. Rev. D 80, 024015, arxiv:0808.3484) that the equation of state (ES) of extremely hot Fermi-Dirac gas in the surface layer of an ultra-relativistic compact object converges to the same form as the relativistic equation of thermodynamical equilibrium (RETE), which is the condition of stability of the object. The description of energy state of a gas particle was completed with the term corresponding with the potential-type energy", "orig": ". The necessity of such the term is set by the demand of convergence of the relativistic particle-impulse distribution law to its Maxwell-Boltzmann form in the classical limit. The identity of the ES and RETE, both applied to the gas in the object's surface layer, becomes perfect, yielding the stable object, when the object's physical radius is identical to its gravitational radius. In this state, the internal energy of gas particles in a volume of the object's surface layer increases over all limits in the frame of the volume and this opens the question if the horizon of events actually is an insuperable barrier. It seems", "generated": ".\n\nThis convergence of the ES to the RETE provides a fundamental connection between the microphysical properties of the gas and the macroscopic stability of the compact object. It implies that the thermal and relativistic effects in the surface layer are intertwined, leading to a self-consistent description of the object's behavior. Furthermore, the inclusion of the potential-type energy term in the description of the gas particles' energy state allows for a more accurate account of the interactions between the particles and the compact object's gravitational field.\n\nThe implications of this result are far-reaching, as it provides a new avenue for understanding the behavior of ultra-relativistic compact objects,"}
{"full_text": "Shadow libraries, also known as ''pirate libraries'', are online collections of copyrighted publications that have been made available for free without the permission of the copyright holders. They have gradually become key players of scientific knowledge dissemination, despite their illegality in most countries of the world. Many publishers and scientist-editors decry such libraries for their copyright infringement and loss of publication usage information, while some scholars and institutions support them, sometimes in a roundabout way, for their role in reducing inequalities of access to knowledge, particularly in low-income countries. Although there is a wealth of literature on shadow libraries, none of this have focused on its potential role in knowledge dissemination, through the open access movement. Here we analyze how shadow libraries can affect researchers' citation practices, highlighting some counter-intuitive findings about their impact on the Open Access Citation Advantage (OACA). Based on a large randomized sample, this study first shows that OA publications, including those in fully OA journals, receive more citations than their subscription-based counterparts do. However, the OACA has slightly decreased over the seven last years. The introduction of a distinction between those accessible or not via the Scihub platform among subscription-based suggest that the generalization of its use cancels the positive effect of OA publishing. The results show that publications in fully OA journals are victims of the success of Sci-hub. Thus, paradoxically, although Sci-hub may seem to facilitate access to scientific knowledge, it negatively affects the OA movement as a whole, by reducing the comparative advantage of OA publications in terms of visibility for researchers. The democratization of the use of Sci-hub may therefore lead to a vicious cycle, hindering efforts to develop full OA strategies without proposing a credible and sustainable alternative model for the dissemination of scientific knowledge.", "prompt": "Shadow libraries, also known as ''pirate libraries'', are online collections of copyrighted publications that have been made available for free without the permission of the copyright holders. They have gradually become key players of scientific knowledge dissemination, despite their illegality in most countries of the world. Many publishers and scientist-editors decry such libraries for their copyright infringement and loss of publication usage information, while some scholars and institutions support them, sometimes in a roundabout way, for their role in reducing inequalities of access to knowledge, particularly in low-income countries. Although there is a wealth of literature on shadow libraries, none of this have focused on its potential role in", "orig": "knowledge dissemination, through the open access movement. Here we analyze how shadow libraries can affect researchers' citation practices, highlighting some counter-intuitive findings about their impact on the Open Access Citation Advantage (OACA). Based on a large randomized sample, this study first shows that OA publications, including those in fully OA journals, receive more citations than their subscription-based counterparts do. However, the OACA has slightly decreased over the seven last years. The introduction of a distinction between those accessible or not via the Scihub platform among subscription-based suggest that the generalization of its use cancels the positive effect of OA publishing. The results show that publications", "generated": "fostering a culture of open access and democratization of knowledge.\n\nOne possible direction of research could be to explore how shadow libraries can be harnessed to promote greater accessibility and equity in the dissemination of scientific knowledge. For instance, studies could examine the impact of shadow libraries on the research productivity of scholars from low-income countries, who often face significant barriers in accessing subscription-based journals and publications. Another area of investigation could be the development of alternative models of knowledge dissemination that blend the flexibility and accessibility of shadow libraries with the legitimacy and quality control of traditional publishing channels.\n\nMoreover, the rise of shadow libraries also raises important questions about the future of academic publishing"}
{"full_text": "Reaction-diffusion analytical modeling of predator-prey systems has shown that specialist natural enemies can slow, stop and even reverse pest invasions, assuming that the prey population displays a strong Allee effect in its growth. Few additional analytical results have been obtained for other spatially distributed predator-prey systems, as traveling waves of non-monotonous systems are notoriously difficult to obtain. Traveling waves have indeed recently been shown to exist in predator-prey systems, but the direction of the wave, an essential item of information in the context of the control of biological invasions, is generally unknown. Preliminary numerical explorations have hinted that control by generalist predators might be possible for prey populations displaying logistic growth. We aimed to formalize the conditions in which spatial biological control can be achieved by generalists, through an analytical approach based on reaction-diffusion equations. The population of the focal prey - the invader - is assumed to grow according to a logistic function. The predator has a type II functional response and is present everywhere in the domain, at its carrying capacity, on alternative hosts. Control, defined as the invader becoming extinct in the domain, may result from spatially independent demographic dynamics or from a spatial extinction wave. Using comparison principles, we obtain sufficient conditions for control and for invasion, based on scalar bistable partial differential equations (PDEs). The searching efficiency and functional response plateau of the predator are identified as the main parameters defining the parameter space for prey extinction and invasion. Numerical explorations are carried out in the region of those control parameters space between the super-and subso-lutions, in which no conclusion about controllability can be drawn on the basis of analytical solutions. The ability of generalist predators to control prey populations with logistic growth lies in the bis-table dynamics of the coupled system, rather than in the bistability of prey-only dynamics as observed for specialist predators attacking prey populations displaying Allee effects. The consideration of space in predator-prey systems involving generalist predators with a parabolic functional response is crucial. Analysis of the ordinary differential equations (ODEs) system identifies parameter regions with monostable (extinction) and bistable (extinction or invasion) dynamics. By contrast, analysis of the associated PDE system distinguishes different and additional regions of invasion and extinction. Depending on the relative positions of these different zones, four patterns of spatial dynamics can be identified : traveling waves of extinction and invasion, pulse waves of extinction and heterogeneous stationary positive solutions of the Turing type. As a consequence, prey control is predicted to be possible when space is considered in additional situations other than those identified without considering space. The reverse situation is also possible. None of these considerations apply to spatial predator-prey systems with specialist natural enemies.", "prompt": "Reaction-diffusion analytical modeling of predator-prey systems has shown that specialist natural enemies can slow, stop and even reverse pest invasions, assuming that the prey population displays a strong Allee effect in its growth. Few additional analytical results have been obtained for other spatially distributed predator-prey systems, as traveling waves of non-monotonous systems are notoriously difficult to obtain. Traveling waves have indeed recently been shown to exist in predator-prey systems, but the direction of the wave, an essential item of information in the context of the control of biological invasions, is generally unknown. Preliminary numerical explorations have hinted that control by", "orig": "generalist predators might be possible for prey populations displaying logistic growth. We aimed to formalize the conditions in which spatial biological control can be achieved by generalists, through an analytical approach based on reaction-diffusion equations. The population of the focal prey - the invader - is assumed to grow according to a logistic function. The predator has a type II functional response and is present everywhere in the domain, at its carrying capacity, on alternative hosts. Control, defined as the invader becoming extinct in the domain, may result from spatially independent demographic dynamics or from a spatial extinction wave. Using comparison principles, we obtain sufficient conditions for control", "generated": "introducing natural enemies at the edge of the invasion front may be effective in halting or reversing the spread of the pest population.\n\nHowever, a thorough understanding of the underlying mechanisms is still lacking, and more research is needed to uncover the complex interplay between predator and prey dynamics in spatially heterogeneous environments. One promising approach is to explore the role of spatial heterogeneity in shaping the behavior of traveling waves in predator-prey systems. By incorporating realistic spatial structures, such as habitat fragmentation or corridors, into the modeling framework, researchers may be able to uncover novel insights into the control of biological invasions.\n\nFurthermore, the incorporation of additional ecological factors,"}
{"full_text": "Low image contrast is a major limitation in transmission electron microscopy, since samples with low atomic number only weakly phase-modulate the illuminating electron beam, and beam-induced sample damage limits the usable electron dose. The contrast can be increased by converting the electron beam's phase modulation into amplitude modulation using a phase plate, a device that applies a $\\pi/2$ radian phase shift to part of the electron beam after it has passed through the sample. Previous phase plate designs rely on material placed in or near the electron beam to provide this phase shift. This results in image aberrations, an inconsistent time-varying phase shift, and resolution loss when the electron beam charges, damages, or is scattered from the material. In this thesis, I present the theory, design, and implementation of the laser phase plate, which instead uses a focused continuous-wave laser beam to phase shift the electron beam. A near-concentric Fabry-P\\'{e}rot optical cavity focuses and resonantly enhances the power of the laser beam in order to achieve the high intensity required to provide the phase shift. We demonstrate that the cavity can surpass this requirement and generate a record-high continuous-wave laser intensity of $590 \\, \\mathrm{GW}/\\mathrm{cm}^{-2}$. By integrating the cavity into a transmission electron microscope, we show that the ponderomotive potential of the laser beam applies a spatially selective phase shift to the electron beam. This enables us to make the first experimental observation of the relativistic reversal of the ponderomotive potential. We then theoretically analyze the properties of the contrast transfer function generated by the laser phase plate. We experimentally determine that resolution loss caused by thermal magnetic field noise emanating from electrically conductive materials in the cavity can be eliminated by designing the cavity with a sufficiently large electron beam aperture. Finally, we show that the laser phase plate provides a stable $\\pi/2$ phase shift and concomitant contrast enhancement when imaging frozen hydrated biological macromolecules. We use these images to successfully determine the structure of the molecules. This demonstrates the laser phase plate as the first stable and lossless phase plate for transmission electron microscopy", "prompt": "Low image contrast is a major limitation in transmission electron microscopy, since samples with low atomic number only weakly phase-modulate the illuminating electron beam, and beam-induced sample damage limits the usable electron dose. The contrast can be increased by converting the electron beam's phase modulation into amplitude modulation using a phase plate, a device that applies a $\\pi/2$ radian phase shift to part of the electron beam after it has passed through the sample. Previous phase plate designs rely on material placed in or near the electron beam to provide this phase shift. This results in image aberrations, an inconsistent time-varying phase shift, and resolution", "orig": "loss when the electron beam charges, damages, or is scattered from the material. In this thesis, I present the theory, design, and implementation of the laser phase plate, which instead uses a focused continuous-wave laser beam to phase shift the electron beam. A near-concentric Fabry-P\\'{e}rot optical cavity focuses and resonantly enhances the power of the laser beam in order to achieve the high intensity required to provide the phase shift. We demonstrate that the cavity can surpass this requirement and generate a record-high continuous-wave laser intensity of $590 \\, \\mathrm{GW}/\\mathrm{cm}^{-2}$.", "generated": "limitations.\n\nHowever, a novel approach to phase plate design has been proposed, which utilizes a electrostatic or electromagnetic field to apply the phase shift, rather than relying on material-based solutions. This new design concept has the potential to overcome the limitations of traditional phase plates, by providing a more stable and consistent phase shift, while also minimizing image aberrations and increasing resolution. The electrostatic or electromagnetic field can be tuned to optimize the phase shift for specific samples and imaging conditions, allowing for greater flexibility and adaptability in transmission electron microscopy applications. Additionally, this design approach may also enable the development of more compact and portable phase plates, which could have"}
{"full_text": "Increasingly sophisticated function development is taking place with the aim of developing efficient, safe and increasingly Automated Driving Functions. This development is possible with the use of diverse data from sources such as Navigation Systems, eHorizon, on-board sensor data, Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (V2V) communication. Increasing challenges arise with the dependency on large amounts of real-time data coming from off-board sources. At the core of addressing these challenges lies the concept of a Digital Dependability Identity (DDI) of a component or system. DDIs are modular, composable, and executable components in the field, facilitating: $\\bullet$ efficient synthesis of component and system dependability information, $\\bullet$ effective evaluation of information for safe and secure composition of highly distributed and autonomous Cyber Physical Systems. In AVL's Connected Powertrain (TM), Automated Driving Functions are tailored to Powertrain Control Strategies that predictively increase energy efficiency according to the powertrain type and its component efficiencies. Simultaneously, the burden on the driver is reduced by optimizing the vehicle velocity, whilst minimizing any journey time penalty.In this work, the development of dependable Automated Driving Functions is exemplified by the Traffic Light Assistant, an adaptive strategy that utilizes predictions of preceding traffic, upcoming road curvature, inclination, speed limits, and especially traffic light signal phase and timing information to increase the energy efficiency in an urban traffic environment. A key aspect of this development is the possibility for seamless and simultaneous development; from office simulation to human-in-the-loop and to real-time tests that include vehicle and powertrain hardware. Driver's acceptance and comfort is rated in an advanced diver simulator mounted on a hexapod, capable of emulating longitudinal and lateral acceleration of a real vehicle. Test results from real-time function validation on a Powertrain Testbed are shown, including real traffic light signal phasing information and traffic flow representation on Graz city roads.", "prompt": "Increasingly sophisticated function development is taking place with the aim of developing efficient, safe and increasingly Automated Driving Functions. This development is possible with the use of diverse data from sources such as Navigation Systems, eHorizon, on-board sensor data, Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (V2V) communication. Increasing challenges arise with the dependency on large amounts of real-time data coming from off-board sources. At the core of addressing these challenges lies the concept of a Digital Dependability Identity (DDI) of a component or system. DDIs are modular, composable, and executable components in the field", "orig": ", facilitating: $\\bullet$ efficient synthesis of component and system dependability information, $\\bullet$ effective evaluation of information for safe and secure composition of highly distributed and autonomous Cyber Physical Systems. In AVL's Connected Powertrain (TM), Automated Driving Functions are tailored to Powertrain Control Strategies that predictively increase energy efficiency according to the powertrain type and its component efficiencies. Simultaneously, the burden on the driver is reduced by optimizing the vehicle velocity, whilst minimizing any journey time penalty.In this work, the development of dependable Automated Driving Functions is exemplified by the Traffic Light Assistant, an adaptive strategy that utilizes predictions of preceding traffic, upcoming road", "generated": ".\n\nThe implementation of DDIs in Automated Driving Functions is crucial to ensure the reliability and trustworthiness of the systems. As the automotive industry continues to advance towards fully autonomous vehicles, the importance of Digital Dependability Identities will only grow. DDIs will play a vital role in verifying the accuracy and integrity of the data being transmitted from various sources, thereby enabling the vehicle to make informed decisions in real-time. For instance, in a scenario where a vehicle is receiving real-time traffic updates from a V2I system, the DDI will ensure that the data is authentic and trustworthy, allowing the vehicle to adjust its speed and trajectory accordingly."}
{"full_text": "Microarray analysis to monitor expression activities in thousands of genes simultaneously has become routine in biomedical research during the past decade. A tremendous amount of expression profiles are generated and stored in the public domain and information integration by meta-analysis to detect differentially expressed (DE) genes has become popular to obtain increased statistical power and validated findings. Methods that aggregate transformed $p$-value evidence have been widely used in genomic settings, among which Fisher's and Stouffer's methods are the most popular ones. In practice, raw data and $p$-values of DE evidence are often not available in genomic studies that are to be combined. Instead, only the detected DE gene lists under a certain $p$-value threshold (e.g., DE genes with $p$-value${}<0.001$) are reported in journal publications. The truncated $p$-value information makes the aforementioned meta-analysis methods inapplicable and researchers are forced to apply a less efficient vote counting method or na\\\"{i}vely drop the studies with incomplete information. The purpose of this paper is to develop effective meta-analysis methods for such situations with partially censored $p$-values. We developed and compared three imputation methods - mean imputation, single random imputation and multiple imputation - for a general class of evidence aggregation methods of which Fisher's and Stouffer's methods are special examples. The null distribution of each method was analytically derived and subsequent inference and genomic analysis frameworks were established. Simulations were performed to investigate the type I error, power and the control of false discovery rate (FDR) for (correlated) gene expression data. The proposed methods were applied to several genomic applications in colorectal cancer, pain and liquid association analysis of major depressive disorder (MDD). The results showed that imputation methods outperformed existing na\\\"{i}ve approaches. Mean imputation and multiple imputation methods performed the best and are recommended for future applications.", "prompt": "Microarray analysis to monitor expression activities in thousands of genes simultaneously has become routine in biomedical research during the past decade. A tremendous amount of expression profiles are generated and stored in the public domain and information integration by meta-analysis to detect differentially expressed (DE) genes has become popular to obtain increased statistical power and validated findings. Methods that aggregate transformed $p$-value evidence have been widely used in genomic settings, among which Fisher's and Stouffer's methods are the most popular ones. In practice, raw data and $p$-values of DE evidence are often not available in genomic studies that are to be combined. Instead, only", "orig": "the detected DE gene lists under a certain $p$-value threshold (e.g., DE genes with $p$-value${}<0.001$) are reported in journal publications. The truncated $p$-value information makes the aforementioned meta-analysis methods inapplicable and researchers are forced to apply a less efficient vote counting method or na\\\"{i}vely drop the studies with incomplete information. The purpose of this paper is to develop effective meta-analysis methods for such situations with partially censored $p$-values. We developed and compared three imputation methods - mean imputation, single random imputation and multiple imputation", "generated": "summary statistics, such as fold changes or rankings, are provided, which limits the applicability of traditional meta-analysis methods that rely on $p$-values.\n\nTo address this limitation, alternative approaches have been developed to integrate summary statistics from multiple studies. One such approach is the rank-based method, which combines the rankings of genes across studies to identify DE genes. Another approach is the fold-change-based method, which aggregates the fold changes of genes across studies to detect DE genes. These methods have been shown to be robust and effective in identifying DE genes even when raw data or $p$-values are not available. Moreover, machine learning and"}
{"full_text": "This paper presents Skini, a programming methodology and an execution environment for interactive structured music. With this system, the composer programs his scores in the HipHop.js synchronous reactive language. They are then executed, or played, in live concerts, in interaction with the audience. The system aims at helping composers to find a good balance between the determinism of the compositions and the nondeterminism of the interactions with the public. Each execution of a Skini score yields to a different but aesthetically consistent interpretation. This work raises many questions in the musical fields. How to combine composition and interaction? How to control the musical style when the audience influences what is to play next? What are the possible connections with generative music? These are important questions for the Skini system but they are out of the scope of this paper that focuses exclusively on the computer science aspects of the system. From that perspective, the main questions are how to program the scores and in which language? General purpose languages are inappropriate because their elementary constructs (i.e., variables, functions, loops, etc.) do not match the constructions needed to express music and musical constraints. We show that synchronous programming languages are a much better fit because they rely on temporal constructs that can be directly used to represent musical scores and because their malleability enables composers to experiment easily with artistic variations of their initial scores. The paper mostly focuses on scores programming. It exposes the process a composer should follow from his very first musical intuitions up to the generation of a musical artifact. The paper presents some excerpts of the programming of a classical music composition that it then precisely relates to an actual recording. Examples of techno music and jazz are also presented, with audio artifact, to demonstrate the versatility of the system. Finally, brief presentations of past live concerts are presented as an evidence of viability of the system.", "prompt": "This paper presents Skini, a programming methodology and an execution environment for interactive structured music. With this system, the composer programs his scores in the HipHop.js synchronous reactive language. They are then executed, or played, in live concerts, in interaction with the audience. The system aims at helping composers to find a good balance between the determinism of the compositions and the nondeterminism of the interactions with the public. Each execution of a Skini score yields to a different but aesthetically consistent interpretation. This work raises many questions in the musical fields. How to combine composition and interaction? How to control the musical style when the audience", "orig": "influences what is to play next? What are the possible connections with generative music? These are important questions for the Skini system but they are out of the scope of this paper that focuses exclusively on the computer science aspects of the system. From that perspective, the main questions are how to program the scores and in which language? General purpose languages are inappropriate because their elementary constructs (i.e., variables, functions, loops, etc.) do not match the constructions needed to express music and musical constraints. We show that synchronous programming languages are a much better fit because they rely on temporal constructs that can be directly used to represent musical scores and because", "generated": "is influencing the performance in real-time? How to ensure that the created music remains coherent and meaningful despite the unpredictable nature of the audience's inputs? Skini provides a platform for composers to explore these questions and push the boundaries of interactive music. By using HipHop.js, composers can define rules and constraints that govern the behavior of their music, while still allowing for flexibility and adaptability in response to the audience's actions. This fusion of composition and improvisation opens up new possibilities for live performances, where the distinction between the composer, the performer, and the audience becomes increasingly blurred. The system also enables the creation of adaptive playlists, where"}
{"full_text": "Consider a class of decomposable combinatorial structures, using different types of atoms $\\Atoms = \\{\\At_1,\\ldots ,\\At_{|{\\Atoms}|}\\}$. We address the random generation of such structures with respect to a size $n$ and a targeted distribution in $k$ of its \\emph{distinguished} atoms. We consider two variations on this problem. In the first alternative, the targeted distribution is given by $k$ real numbers $\\TargFreq_1, \\ldots, \\TargFreq_k$ such that $0 < \\TargFreq_i < 1$ for all $i$ and $\\TargFreq_1+\\cdots+\\TargFreq_k \\leq 1$. We aim to generate random structures among the whole set of structures of a given size $n$, in such a way that the {\\em expected} frequency of any distinguished atom $\\At_i$ equals $\\TargFreq_i$. We address this problem by weighting the atoms with a $k$-tuple $\\Weights$ of real-valued weights, inducing a weighted distribution over the set of structures of size $n$. We first adapt the classical recursive random generation scheme into an algorithm taking $\\bigO{n^{1+o(1)}+mn\\log{n}}$ arithmetic operations to draw $m$ structures from the $\\Weights$-weighted distribution. Secondly, we address the analytical computation of weights such that the targeted frequencies are achieved asymptotically, i. e. for large values of $n$. We derive systems of functional equations whose resolution gives an explicit relationship between $\\Weights$ and $\\TargFreq_1, \\ldots, \\TargFreq_k$. Lastly, we give an algorithm in $\\bigO{k n^4}$ for the inverse problem, {\\it i.e.} computing the frequencies associated with a given $k$-tuple $\\Weights$ of weights, and an optimized version in $\\bigO{k n^2}$ in the case of context-free languages. This allows for a heuristic resolution of the weights/frequencies relationship suitable for complex specifications. In the second alternative, the targeted distribution is given by a $k$ natural numbers $n_1, \\ldots, n_k$ such that $n_1+\\cdots+n_k+r=n$ where $r \\geq 0$ is the number of undistinguished atoms. The structures must be generated uniformly among the set of structures of size $n$ that contain {\\em exactly} $n_i$ atoms $\\At_i$ ($1 \\leq i \\leq k$). We give a $\\bigO{r^2\\prod_{i=1}^k n_i^2 +m n k \\log n}$ algorithm for generating $m$ structures, which simplifies into a $\\bigO{r\\prod_{i=1}^k n_i +m n}$ for regular specifications.", "prompt": "Consider a class of decomposable combinatorial structures, using different types of atoms $\\Atoms = \\{\\At_1,\\ldots,\\At_{|{\\Atoms}|}\\}$. We address the random generation of such structures with respect to a size $n$ and a targeted distribution in $k$ of its \\emph{distinguished} atoms. We consider two variations on this problem. In the first alternative, the targeted distribution is given by $k$ real numbers $\\TargFreq_1, \\ldots, \\TargFreq_k$ such that $0 < \\TargFreq_i < 1$", "orig": "for all $i$ and $\\TargFreq_1+\\cdots+\\TargFreq_k \\leq 1$. We aim to generate random structures among the whole set of structures of a given size $n$, in such a way that the {\\em expected} frequency of any distinguished atom $\\At_i$ equals $\\TargFreq_i$. We address this problem by weighting the atoms with a $k$-tuple $\\Weights$ of real-valued weights, inducing a weighted distribution over the set of structures of size $n$. We first adapt the classical recursive random generation scheme into an algorithm taking $\\bigO{n^{1", "generated": ".\n\nIn this scenario, our goal is to develop an efficient algorithm that generates a random structure of size $n$ with a distribution of distinguished atoms that closely approximates the targeted frequencies. To achieve this, we propose a Markov chain Monte Carlo (MCMC) approach, where we define a Markov chain that transitions between structures of size $n$ and iteratively updates the frequencies of the distinguished atoms. The stationary distribution of this Markov chain is designed to converge to the targeted distribution, ensuring that the generated structures exhibit the desired properties.\n\nIn the second variation, we assume that the targeted distribution is specified by a probability distribution $\\"}
{"full_text": "We describe astrophysical processes around a black hole keeping primarily the physics of accretion in mind. In Section 1, we briefly discuss the formation, evolution and detection of black holes. We also discuss the difference of flow properties around a black hole and a Newtonian star. In Section 2, we present past and present developments in the study of spherically accreting flows. We study the properties of Bondi flow with and without radiative transfer. In the presence of significant angular momentum, which is especially true in a binary system, matter will be accreted as a thin Keplerian disk. In Section 3, we discuss a large number of models of these disks including the more popular standard disk model. We present magnetized disk models as well. Since the angular momentum is high in these systems, rotational motion is the most dominant component compared to the radial or the vertical velocity components. In Section 4, we study thick disk models which are of low angular momentum but still have no significant radial motion. The accretion rates could be very high causing the flow to become radiation dominated and the disk to be geometrically thick. For low accretion rates, ion pressure supported disks are formed. In Section 5, we extensively discuss the properties of transonic flows which has with sub-Keplerian angular momentum. In the absence of shock discontinuities, these sub-Keplerian flows are basically advecting, similar to Bondi flows, close to the black holes, though far away they match Keplerian or sub-Keplerian disks. In presence of shocks, the post-shock flow becomes rotation dominated similar to thick disks. In Section 6, we present results of important numerical simulations of accretion flows. Significant results from the studies of evolution of viscous transonic flows are reported. In Section 7, we discuss some observational evidences of the black hole accretion. We also present a detailed model of a generalized accretion disk and present its spectra and compare with observations. In Section 8, we summarize the review and make concluding remarks.", "prompt": "We describe astrophysical processes around a black hole keeping primarily the physics of accretion in mind. In Section 1, we briefly discuss the formation, evolution and detection of black holes. We also discuss the difference of flow properties around a black hole and a Newtonian star. In Section 2, we present past and present developments in the study of spherically accreting flows. We study the properties of Bondi flow with and without radiative transfer. In the presence of significant angular momentum, which is especially true in a binary system, matter will be accreted as a thin Keplerian disk. In Section", "orig": "3, we discuss a large number of models of these disks including the more popular standard disk model. We present magnetized disk models as well. Since the angular momentum is high in these systems, rotational motion is the most dominant component compared to the radial or the vertical velocity components. In Section 4, we study thick disk models which are of low angular momentum but still have no significant radial motion. The accretion rates could be very high causing the flow to become radiation dominated and the disk to be geometrically thick. For low accretion rates, ion pressure supported disks are formed. In Section 5, we extensively discuss the", "generated": "3, we delve into the intricacies of disk accretion, exploring the role of viscosity in driving angular momentum transport and the subsequent heating and cooling processes that govern the disk's thermal structure.\n\nWe examine the effects of disk instabilities, such as the magnetorotational instability, on the accretion process and discuss the implications of disk-jet coupling on the observational properties of black hole systems. Furthermore, we investigate the role of radiation in driving disk winds and outflows, which can significantly impact the surrounding interstellar medium. In Section 4, we discuss the observational evidence for black hole accretion, including the spectral"}
{"full_text": "This article deals with the design of saturated controls in the context of partial differential equations. It focuses on a Korteweg-de Vries equation, which is a nonlinear mathematical model of waves on shallow water surfaces. Two different types of saturated controls are considered. The well-posedness is proven applying a Banach fixed point theorem, using some estimates of this equation and some properties of the saturation function. The proof of the asymptotic stability of the closed-loop system is separated in two cases: i) when the control acts on all the domain, a Lyapunov function together with a sector condition describing the saturating input is used to conclude on the stability, ii) when the control is localized, we argue by contradiction. Some numerical simulations illustrate the stability of the closed-loop nonlinear partial differential equation. 1. Introduction. In recent decades, a great effort has been made to take into account input saturations in control designs (see e.g [39], [15] or more recently [17]). In most applications, actuators are limited due to some physical constraints and the control input has to be bounded. Neglecting the amplitude actuator limitation can be source of undesirable and catastrophic behaviors for the closed-loop system. The standard method to analyze the stability with such nonlinear controls follows a two steps design. First the design is carried out without taking into account the saturation. In a second step, a nonlinear analysis of the closed-loop system is made when adding the saturation. In this way, we often get local stabilization results. Tackling this particular nonlinearity in the case of finite dimensional systems is already a difficult problem. However, nowadays, numerous techniques are available (see e.g. [39, 41, 37]) and such systems can be analyzed with an appropriate Lyapunov function and a sector condition of the saturation map, as introduced in [39]. In the literature, there are few papers studying this topic in the infinite dimensional case. Among them, we can cite [18], [29], where a wave equation equipped with a saturated distributed actuator is studied, and [12], where a coupled PDE/ODE system modeling a switched power converter with a transmission line is considered. Due to some restrictions on the system, a saturated feedback has to be designed in the latter paper. There exist also some papers using the nonlinear semigroup theory and focusing on abstract systems ([20],[34],[36]). Let us note that in [36], [34] and [20], the study of a priori bounded controller is tackled using abstract nonlinear theory. To be more specific, for bounded ([36],[34]) and unbounded ([34]) control operators, some conditions are derived to deduce, from the asymptotic stability of an infinite-dimensional linear system in abstract form, the asymptotic stability when closing the loop with saturating controller. These articles use the nonlinear semigroup theory (see e.g. [24] or [1]). The Korteweg-de Vries equation (KdV for short)", "prompt": "This article deals with the design of saturated controls in the context of partial differential equations. It focuses on a Korteweg-de Vries equation, which is a nonlinear mathematical model of waves on shallow water surfaces. Two different types of saturated controls are considered. The well-posedness is proven applying a Banach fixed point theorem, using some estimates of this equation and some properties of the saturation function. The proof of the asymptotic stability of the closed-loop system is separated in two cases: i) when the control acts on all the domain, a Lyapunov function together with a sector condition describing the saturating input is used", "orig": "to conclude on the stability, ii) when the control is localized, we argue by contradiction. Some numerical simulations illustrate the stability of the closed-loop nonlinear partial differential equation. 1. Introduction. In recent decades, a great effort has been made to take into account input saturations in control designs (see e.g [39], [15] or more recently [17]). In most applications, actuators are limited due to some physical constraints and the control input has to be bounded. Neglecting the amplitude actuator limitation can be source of undesirable and catastrophic behaviors for the closed-loop system. The standard method to analyze the stability with such", "generated": ".\n\nThe development of saturated controls for the Korteweg-de Vries equation is a crucial step towards the effective management of wave dynamics in shallow water systems. The consideration of two types of saturated controls allows for a more comprehensive understanding of the system's behavior under varying conditions. The well-posedness of the control system, established through the application of the Banach fixed point theorem, provides a solid foundation for further analysis.\n\nIn the case where the control acts on the entire domain, the use of a Lyapunov function and sector condition enables the proof of asymptotic stability. This result has significant implications for the design of control systems"}
{"full_text": "A large family of \"standard\" coboundary Hopf algebras is investigated. The existence of a universal R-matrix is demonstrated for the case when the parameters are in general position. Algebraic surfaces in parameter space are characterized by the appearance of certain ideals; in this case the universal R-matrix exists on the associated algebraic quotient. In special cases the quotient is a \"standard\" quantum group; all familiar quantum groups including twisted ones are obtained in this way. In other special cases one finds new types of coboundary bi-algebras. The \"standard\" universal R-matrix is shown to be the unique solution of a very simple, linear recursion relation. The classical limit is obtained in the case of quantized Kac-Moody algebras of finite and affine type. Returning to the general case, we study deformations of the standard R-matrix and the associated Hopf algebras. A preliminary investigation of the first order deformations uncovers a class of deformations that incompasses the quantization of all Kac-Moody algebras of finite and affine type. The corresponding exact deformations are described as generalized twists, $ R_\\epsilon = (F^t)^{-1}RF$, where $R$ is the standard R-matrix and the cocycle $F$ (a power series in the deformation parameter $\\epsilon$) is the solution of a linear recursion relation of the same type as that which determines $R$. Included here is the universal R-matrix for the elliptic quantum groups associated with $sl(n)$, a big surprise! Specializing again, to the case of quantized Kac-Moody algebras, and taking the classical limit of these esoteric quantum groups, one re-discovers all the trigonometric and elliptic r-matrices of Belavin and Drinfeld. The formulas obtained here are easier to use than the original ones, and the structure of the space of classical r-matrices is more transparent. The r-matrices obtained here are more general in that they are defined on the full Kac-Moody algebras, the central extensions of the loop groups.", "prompt": "A large family of \"standard\" coboundary Hopf algebras is investigated. The existence of a universal R-matrix is demonstrated for the case when the parameters are in general position. Algebraic surfaces in parameter space are characterized by the appearance of certain ideals; in this case the universal R-matrix exists on the associated algebraic quotient. In special cases the quotient is a \"standard\" quantum group; all familiar quantum groups including twisted ones are obtained in this way. In other special cases one finds new types of coboundary bi-algebras. The \"standard\" universal R-matrix is shown to be the unique solution of a", "orig": "very simple, linear recursion relation. The classical limit is obtained in the case of quantized Kac-Moody algebras of finite and affine type. Returning to the general case, we study deformations of the standard R-matrix and the associated Hopf algebras. A preliminary investigation of the first order deformations uncovers a class of deformations that incompasses the quantization of all Kac-Moody algebras of finite and affine type. The corresponding exact deformations are described as generalized twists, $ R_\\epsilon = (F^t)^{-1}RF$, where $R$ is the", "generated": "system of quadratic equations, which is a generalization of the quantum Yang-Baxter equation.\n\nFurthermore, it is demonstrated that the universal R-matrix can be used to construct representations of the coboundary Hopf algebra, and that these representations are equivalent to the ones obtained via the Drinfeld-Jimbo approach. This provides a new perspective on the representation theory of quantum groups and shed light on the algebraic structures underlying them. The technique developed in this investigation can be applied to a broader class of Hopf algebras, leading to new insights into the properties of universal R-matrices and their role in the study of quantum groups."}
{"full_text": "Situations where individuals have to contribute to joint efforts or share scarce resources are ubiquitous. Yet, without proper mechanisms to ensure cooperation, the evolutionary pressure to maximize individual success tends to create a tragedy of the commons (such as over-fishing or the destruction of our environment). This contribution addresses a number of related puzzles of human behavior with an evolutionary game theoretical approach as it has been successfully used to explain the behavior of other biological species many times, from bacteria to vertebrates. Our agent-based model distinguishes individuals applying four different behavioral strategies: non-cooperative individuals (\"defectors\"), cooperative individuals abstaining from punishment efforts (called \"cooperators\" or \"second-order free-riders\"), cooperators who punish non-cooperative behavior (\"moralists\"), and defectors, who punish other defectors despite being non-cooperative themselves (\"immoralists\"). By considering spatial interactions with neighboring individuals, our model reveals several interesting effects: First, moralists can fully eliminate cooperators. This spreading of punishing behavior requires a segregation of behavioral strategies and solves the \"second-order free-rider problem\". Second, the system behavior changes its character significantly even after very long times (\"who laughs last laughs best effect\"). Third, the presence of a number of defectors can largely accelerate the victory of moralists over non-punishing cooperators. Forth, in order to succeed, moralists may profit from immoralists in a way that appears like an \"unholy collaboration\". Our findings suggest that the consideration of punishment strategies allows to understand the establishment and spreading of \"moral behavior\" by means of game-theoretical concepts. This demonstrates that quantitative biological modeling approaches are powerful even in domains that have been addressed with non-mathematical concepts so far. The complex dynamics of certain social behaviors becomes understandable as result of an evolutionary competition between different behavioral strategies.", "prompt": "Situations where individuals have to contribute to joint efforts or share scarce resources are ubiquitous. Yet, without proper mechanisms to ensure cooperation, the evolutionary pressure to maximize individual success tends to create a tragedy of the commons (such as over-fishing or the destruction of our environment). This contribution addresses a number of related puzzles of human behavior with an evolutionary game theoretical approach as it has been successfully used to explain the behavior of other biological species many times, from bacteria to vertebrates. Our agent-based model distinguishes individuals applying four different behavioral strategies: non-cooperative individuals (\"defectors\"), cooperative individuals abstaining from punishment efforts (called \"cooperators", "orig": "\" or \"second-order free-riders\"), cooperators who punish non-cooperative behavior (\"moralists\"), and defectors, who punish other defectors despite being non-cooperative themselves (\"immoralists\"). By considering spatial interactions with neighboring individuals, our model reveals several interesting effects: First, moralists can fully eliminate cooperators. This spreading of punishing behavior requires a segregation of behavioral strategies and solves the \"second-order free-rider problem\". Second, the system behavior changes its character significantly even after very long times (\"who laughs last laughs best effect\"). Third, the presence of a number of defectors can largely accelerate the victory of", "generated": "\").\n\nOne of the primary puzzles our research aims to resolve is the persistence of cooperation in the face of individual self-interest. By integrating an evolutionary game theoretical approach with agent-based modeling, we can examine the dynamics of cooperation and defection in a controlled and repeatable manner. This allows us to identify the conditions under which cooperative behavior emerges and persists, as well as the circumstances that lead to its breakdown. Our model also incorporates the role of punishment, which has been shown to play a crucial role in sustaining cooperation in many societies. Specifically, we distinguish between \"punishers,\" who cooperate but also punish defectors, and \"cooperators,\""}
{"full_text": "Genomic complexity is a growing field of evolution, with case studies for comparative evolutionary analyses in model and emerging non-model systems. Understanding complexity and the functional components of the genome is an untapped wealth of knowledge ripe for exploration. With the \"remarkable lack of correspondence\" between genome size and complexity, there needs to be a way to quantify complexity across organisms. In this study we use a set of complexity metrics that allow for evaluation of changes in complexity using TranD. We ascertain if complexity is increasing or decreasing across transcriptomes and at what structural level, as complexity is varied. We define three metrics -- TpG, EpT, and EpG in this study to quantify the complexity of the transcriptome that encapsulate the dynamics of alternative splicing. Here we compare complexity metrics across 1) whole genome annotations, 2) a filtered subset of orthologs, and 3) novel genes to elucidate the impacts of ortholog and novel genes in transcriptome analysis. We also derive a metric from Hong et al., 2006, Effective Exon Number (EEN), to compare the distribution of exon sizes within transcripts against random expectations of uniform exon placement. EEN accounts for differences in exon size, which is important because novel genes differences in complexity for orthologs and whole transcriptome analyses are biased towards low complexity genes with few exons and few alternative transcripts. With our metric analyses, we are able to implement changes in complexity across diverse lineages with greater precision and accuracy than previous cross-species comparisons under ortholog conditioning. These analyses represent a step forward toward whole transcriptome analysis in the emerging field of non-model evolutionary genomics, with key insights for evolutionary inference of complexity changes on deep timescales across the tree of life. We suggest a means to quantify biases generated in ortholog calling and correct complexity analysis for lineage-specific effects. With these metrics, we directly assay the quantitative properties of newly formed lineage-specific genes as they lower complexity in transcriptomes.", "prompt": "Genomic complexity is a growing field of evolution, with case studies for comparative evolutionary analyses in model and emerging non-model systems. Understanding complexity and the functional components of the genome is an untapped wealth of knowledge ripe for exploration. With the \"remarkable lack of correspondence\" between genome size and complexity, there needs to be a way to quantify complexity across organisms. In this study we use a set of complexity metrics that allow for evaluation of changes in complexity using TranD. We ascertain if complexity is increasing or decreasing across transcriptomes and at what structural level, as complexity is varied. We define three metrics -- TpG, EpT,", "orig": "and EpG in this study to quantify the complexity of the transcriptome that encapsulate the dynamics of alternative splicing. Here we compare complexity metrics across 1) whole genome annotations, 2) a filtered subset of orthologs, and 3) novel genes to elucidate the impacts of ortholog and novel genes in transcriptome analysis. We also derive a metric from Hong et al., 2006, Effective Exon Number (EEN), to compare the distribution of exon sizes within transcripts against random expectations of uniform exon placement. EEN accounts for differences in exon size, which is important because novel genes differences in complexity for", "generated": "In order to examine the dynamics of genomic complexity, we applied these metrics to a range of organisms, from bacteria to humans, and observed intriguing patterns of complexity variation. For instance, our analysis revealed that prokaryotes exhibit a consistent decrease in complexity at the gene level, whereas eukaryotes display a mixed trend with some displaying increased complexity at the gene level and others showing a decrease at the exon level. This prompted us to investigate the relationship between complexity and gene function, which led us to identify a correlation between complexity and gene essentiality. Specifically, we found that genes with higher complexity scores were more likely to be essential for"}
{"full_text": "We study the molecular correlations in a lattice model of a solution of a low-solubility solute, with emphasis on how the thermodynamics is reflected in the correlation functions. The model is treated in Bethe-Guggenheim approximation, which is exact on a Bethe lattice (Cayley tree). The solution properties are obtained in the limit of infinite dilution of the solute. With $h_{11}(r)$, $h_{12}(r)$, and $h_{22}(r)$ the three pair correlation functions as functions of the separation $r$ (subscripts 1 and 2 referring to solvent and solute, respectively), we find for $r \\geq 2$ lattice steps that $h_{22}(r)/h_{12}(r) \\equiv h_{12}(r)/h_{11}(r)$. This illustrates a general theorem that holds in the asymptotic limit of infinite $r$. The three correlation functions share a common exponential decay length (correlation length), but when the solubility of the solute is low the amplitude of the decay of $h_{22}(r)$ is much greater than that of $h_{12}(r)$, which in turn is much greater than that of $h_{11}(r)$. As a consequence the amplitude of the decay of $h_{22}(r)$ is enormously greater than that of $h_{11}(r)$. The effective solute-solute attraction then remains discernible at distances at which the solvent molecules are essentially no longer correlated, as found in similar circumstances in an earlier model. The second osmotic virial coefficient is large and negative, as expected. We find that the solvent-mediated part $W(r)$ of the potential of mean force between solutes, evaluated at contact, $r=1$, is related in this model to the Gibbs free energy of solvation at fixed pressure, $\\Delta G_p^*$, by $(Z/2) W(1) + \\Delta G_p^* \\equiv p v_0$, where $Z$ is the coordination number of the lattice, $p$ the pressure, and $v_0$ the volume of the cell associated with each lattice site. A large, positive $\\Delta G_p^*$ associated with the low solubility is thus reflected in a strong attraction (large negative $W$ at contact), which is the major contributor to the second osmotic virial coefficient. In this model, the low solubility (large positive $\\Delta G_p^*$) is due partly to an unfavorable enthalpy of solvation and partly to an unfavorable solvation entropy, unlike in the hydrophobic effect, where the enthalpy of solvation itself favors high solubility, but is overweighed by the unfavorable solvation entropy.", "prompt": "We study the molecular correlations in a lattice model of a solution of a low-solubility solute, with emphasis on how the thermodynamics is reflected in the correlation functions. The model is treated in Bethe-Guggenheim approximation, which is exact on a Bethe lattice (Cayley tree). The solution properties are obtained in the limit of infinite dilution of the solute. With $h_{11}(r)$, $h_{12}(r)$, and $h_{22}(r)$ the three pair correlation functions as functions of the separation $r$ (subscripts 1 and 2 referring to solvent", "orig": "and solute, respectively), we find for $r \\geq 2$ lattice steps that $h_{22}(r)/h_{12}(r) \\equiv h_{12}(r)/h_{11}(r)$. This illustrates a general theorem that holds in the asymptotic limit of infinite $r$. The three correlation functions share a common exponential decay length (correlation length), but when the solubility of the solute is low the amplitude of the decay of $h_{22}(r)$ is much greater than that of $h_{12}(r)$, which in turn is much greater than that of", "generated": "and solute molecules, respectively), we investigate the spatial arrangements of the molecular species in the solution.\n\nIn particular, we examine the behavior of the correlation functions at short and long ranges, which provide insights into the local and global structural properties of the solution. The pair correlation functions are calculated using the Ornstein-Zernike equation, which relates the total correlation function to the direct correlation function. The resulting expressions for the pair correlation functions are then analyzed in terms of their asymptotic behavior, which reveals the presence of oscillatory and decay patterns.\n\nFurthermore, we explore the connection between the correlation functions and the thermodynamic properties of the solution, such"}
{"full_text": "This article examines the importance of graphic representations in the social sciences, and particularly in (medieval) history, taking as its starting point a reflection by {\\'E}tienne-Jules Marey, a physiologist and pioneer of 19th-century photography and cinema. Marey believed that the visual should replace language in many fields. Indeed, the twentieth and early twenty-first centuries saw an exponential multiplication of visual media, particularly with the advent of digital technology. However, this ''graphics revolution'' has not affected all disciplines equally. Significant differences remain between scientific fields such as astrophysics, anthropology, chemistry and medieval history, despite their shared commitment to describing dynamic processes and changes of state. Yet, while historians have already digitized a large part of the cultural heritage from Antiquity to the 10th-13th centuries, exploration of this corpus using visualizations remains limited. There is therefore untapped potential in this field.This article begins by outlining a typology and quantification of the past and potential roles of visual representations in medieval history. It examines two distinct intellectual approaches: 1. the use of visuals to support a scientific discourse (majority) and 2. the construction of a historical discourse based on observations made from visual figures with the aim of modeling phenomena invisible to the naked eye. The author thus examines the use of ''images'' in medievalism, focusing on the annual volumes of the Soci{\\'e}t{\\'e} des historiens m{\\'e}di{\\'e}vistes de l'enseignement sup{\\'e}rieur (SHMESP), up to 2006. Two other parts of the text look at the still-rare forms of visual representation in medieval history, particularly those with a ''heuristic vocation'', using iconographic objects, parchments, buildings and digitized texts. The article suggests various visualization techniques, such as network analysis, the creation of ''stemmas 2.0'' and interactive chronologies, which could benefit the discipline. These methods could potentially profoundly change our understanding of ancient societies, by showing the dynamic relationships between different aspects of these societies. One of the most important advances expected from these visual methods is a better understanding of the patterns of development in medieval Europe, which varied from region to region. The hypothesis is that the scarcity of heuristic graphics in medieval history stems from the relationship with ancient documents and the historical method based on narration and exemplarity. The article thus questions the value of ''visual modelling'' in medieval history, and highlights the challenges associated with the widespread adoption of this approach in the humanities and social sciences. Finally, the text invites us to reflect on the nature and functioning of heuristic visual devices, by comparing medieval ''images'' and contemporary scientific visuals. In both cases, the point is to materialize the invisible in order to show something that exists beyond the visual. The author suggests that this way of approaching visuals could play a growing role in the decades to come, particularly in the field of data science.", "prompt": "This article examines the importance of graphic representations in the social sciences, and particularly in (medieval) history, taking as its starting point a reflection by {\\'E}tienne-Jules Marey, a physiologist and pioneer of 19th-century photography and cinema. Marey believed that the visual should replace language in many fields. Indeed, the twentieth and early twenty-first centuries saw an exponential multiplication of visual media, particularly with the advent of digital technology. However, this ''graphics revolution'' has not affected all disciplines equally. Significant differences remain between scientific fields such as astrophysics, anthropology, chemistry and medieval history, despite", "orig": "their shared commitment to describing dynamic processes and changes of state. Yet, while historians have already digitized a large part of the cultural heritage from Antiquity to the 10th-13th centuries, exploration of this corpus using visualizations remains limited. There is therefore untapped potential in this field.This article begins by outlining a typology and quantification of the past and potential roles of visual representations in medieval history. It examines two distinct intellectual approaches: 1. the use of visuals to support a scientific discourse (majority) and 2. the construction of a historical discourse based on observations made from visual figures with the aim of", "generated": "the widespread recognition of the importance of visual aids in communication and learning.\n\nThe disparity in the adoption of graphic representations can be attributed to the nature of the disciplines themselves. Astrophysics, for instance, lends itself well to visualization, with complex astronomical phenomena and data being more easily understood and analyzed through images and graphs. In contrast, medieval history, which relies heavily on textual and oral traditions, has been slower to incorporate visual representations into its methodology. This is not to say that historians have not utilized images and diagrams in their work, but rather that the inherent complexities and nuances of historical narratives can be difficult to convey through visual means alone.\n\nFurthermore,"}
{"full_text": "One of the major terminological forces driving ICT integration in research today is that of \"big data.\" While the phrase sounds inclusive and integrative, \"big data\" approaches are highly selective, excluding input that cannot be effectively structured, represented, or digitised. Data of this complex sort is precisely the kind that human activity produces, but the technological imperative to enhance signal through the reduction of noise does not accommodate this richness. Data and the computational approaches that facilitate \"big data\" have acquired a perceived objectivity that belies their curated, malleable, reactive, and performative nature. In an input environment where anything can \"be data\" once it is entered into the system as \"data,\" data cleaning and processing, together with the metadata and information architectures that structure and facilitate our cultural archives acquire a capacity to delimit what data are. This engenders a process of simplification that has major implications for the potential for future innovation within research environments that depend on rich material yet are increasingly mediated by digital technologies. This paper presents the preliminary findings of the European-funded KPLEX (Knowledge Complexity) project which investigates the delimiting effect digital mediation and datafication has on rich, complex cultural data. The paper presents a systematic review of existing implicit definitions of data, elaborating on the implications of these definitions and highlighting the ways in which metadata and computational technologies can restrict the interpretative potential of data. It sheds light on the gap between analogue or augmented digital practices and fully computational ones, and the strategies researchers have developed to deal with this gap. The paper proposes a reconceptualisation of data as it is functionally employed within digitally-mediated research so as to incorporate and acknowledge the richness and complexity of our source materials.", "prompt": "One of the major terminological forces driving ICT integration in research today is that of \"big data.\" While the phrase sounds inclusive and integrative, \"big data\" approaches are highly selective, excluding input that cannot be effectively structured, represented, or digitised. Data of this complex sort is precisely the kind that human activity produces, but the technological imperative to enhance signal through the reduction of noise does not accommodate this richness. Data and the computational approaches that facilitate \"big data\" have acquired a perceived objectivity that belies their curated, malleable, reactive, and performative nature. In an input environment where anything can \"be data", "orig": "\" once it is entered into the system as \"data,\" data cleaning and processing, together with the metadata and information architectures that structure and facilitate our cultural archives acquire a capacity to delimit what data are. This engenders a process of simplification that has major implications for the potential for future innovation within research environments that depend on rich material yet are increasingly mediated by digital technologies. This paper presents the preliminary findings of the European-funded KPLEX (Knowledge Complexity) project which investigates the delimiting effect digital mediation and datafication has on rich, complex cultural data. The paper presents a systematic review of existing implicit definitions of data, elaborating", "generated": ",\" it becomes increasingly important to question the assumptions and biases embedded in these systems.\n\nThis critique is not aimed at dismissing the potential benefits of \"big data\" altogether. Rather, it seeks to highlight the need for a more nuanced understanding of the epistemological and ontological implications of relying solely on computational methods to make sense of our world. The quest for objectivity and precision in data analysis often leads to a neglect of the contextual, the subjective, and the human experience. By prioritizing scalability and efficiency, \"big data\" approaches often overlook the complexities and messiness of real-world phenomena.\n\nMoreover, the drive towards quantification and digit"}
{"full_text": "Object-oriented languages often use virtual machines (VMs) that provide mechanisms such as just-in-time (JIT) compilation and garbage collection (GC). These VM components are typically implemented in a separate layer, isolating them from the application. While this approach brings the software engineering benefits of clear separation and decoupling, it introduces barriers for both understanding VM behavior and evolving the VM implementation. For example, the GC and JIT compiler are typically fixed at VM build time, limiting arbitrary adaptation at run time. Furthermore, because of this separation, the implementation of the VM cannot typically be inspected and debugged in the same way as application code, enshrining a distinction in easy-to-work-with application and hard-to-work-with VM code. These characteristics pose a barrier for application developers to understand the engine on top of which their own code runs, and fosters a knowledge gap that prevents application developers to change the VM. We propose Live Metacircular Runtimes (LMRs) to overcome this problem. LMRs are language runtime systems that seamlessly integrate the VM into the application in live programming environments. Unlike classic metacircular approaches, we propose to completely remove the separation between application and VM. By systematically applying object-oriented design to VM components, we can build live runtime systems that are small and flexible enough, where VM engineers can benefit of live programming features such as short feedback loops, and application developers with fewer VM expertise can benefit of the stronger causal connections between their programs and the VM implementation. To evaluate our proposal, we implemented Bee/LMR, a live VM for a Smalltalk-derivative environment in 22057 lines of code. We analyze case studies on tuning the garbage collector, avoiding recompilations by the just-in-time compiler, and adding support to optimize code with vector instructions to demonstrate the trade-offs of extending exploratory programming to VM development in the context of an industrial application used in production. Based on the case studies, we illustrate how our approach facilitates the daily development work of a small team of application developers. Our approach enables VM developers to gain access to live programming tools traditionally reserved for application developers, while application developers can interact with the VM and modify it using the high-level tools they use every day. Both application and VM developers can seamlessly inspect, debug, understand, and modify the different parts of the VM with shorter feedback loops and higher-level tools.", "prompt": "Object-oriented languages often use virtual machines (VMs) that provide mechanisms such as just-in-time (JIT) compilation and garbage collection (GC). These VM components are typically implemented in a separate layer, isolating them from the application. While this approach brings the software engineering benefits of clear separation and decoupling, it introduces barriers for both understanding VM behavior and evolving the VM implementation. For example, the GC and JIT compiler are typically fixed at VM build time, limiting arbitrary adaptation at run time. Furthermore, because of this separation, the implementation of the VM cannot typically be inspected and debugged in the same way as application code,", "orig": "enshrining a distinction in easy-to-work-with application and hard-to-work-with VM code. These characteristics pose a barrier for application developers to understand the engine on top of which their own code runs, and fosters a knowledge gap that prevents application developers to change the VM. We propose Live Metacircular Runtimes (LMRs) to overcome this problem. LMRs are language runtime systems that seamlessly integrate the VM into the application in live programming environments. Unlike classic metacircular approaches, we propose to completely remove the separation between application and VM. By systematically applying object-oriented design to VM components, we can build live runtime systems", "generated": "making it challenging to identify and fix performance issues or bugs that arise during runtime. This opacity can lead to lengthy debugging sessions, where developers must rely on indirect means, such as logging or profiling, to gain insights into the VM's inner workings. Another consequence of this separation is the limited ability to optimize the VM for specific use cases or workloads. Since the VM is decoupled from the application, it cannot be easily tailored to the specific requirements of the program, resulting in suboptimal performance or resource utilization. \n\nMoreover, the lack of visibility into the VM's implementation hampers the development of advanced features, such as"}
{"full_text": "The fundamental plane of early-type galaxies is a rather tight three-parameter correlation discovered more than twenty years ago. It has resisted a both global and precise physical interpretation despite a consequent number of works, observational, theoretical or using numerical simulations. It appears that its precise properties depend on the population of galaxies in study. Instead of selecting a priori these populations, we propose to objectively construct homologous populations from multivariate analyses. We have undertaken multivariate cluster and cladistic analyses of a sample of 56 low-redshift galaxy clusters containing 699 early-type galaxies, using four parameters: effective radius, velocity dispersion, surface brightness averaged over effective radius, and Mg2 index. All our analyses are consistent with seven groups that define separate regions on the global fundamental plane, not across its thickness. In fact, each group shows its own fundamental plane, which is more loosely defined for less diversified groups. We conclude that the global fundamental plane is not a bent surface, but made of a collection of several groups characterizing several fundamental planes with different thicknesses and orientations in the parameter space. Our diversification scenario probably indicates that the level of diversity is linked to the number and the nature of transforming events and that the fundamental plane is the result of several transforming events. We also show that our classification, not the fundamental planes, is universal within our redshift range (0.007 - 0.053). We find that the three groups with the thinnest fundamental planes presumably formed through dissipative (wet) mergers. In one of them, this(ese) merger(s) must have been quite ancient because of the relatively low metallicity of its galaxies, Two of these groups have subsequently undergone dry mergers to increase their masses. In the k-space, the third one clearly occupies the region where bulges (of lenticular or spiral galaxies) lie and might also have formed through minor mergers and accretions. The two least diversified groups probably did not form by major mergers and must have been strongly affected by interactions, some of the gas in the objects of one of these groups having possibly been swept out. The interpretation, based on specific assembly histories of galaxies of our seven groups, shows that they are truly homologous. They were obtained directly from several observables, thus independently of any a priori classification. The diversification scenario relating these groups does not depend on models or numerical simulations, but is objectively provided by the cladistic analysis. Consequently, our classification is more easily compared to models and numerical simulations, and our work can be readily repeated with additional observables.", "prompt": "The fundamental plane of early-type galaxies is a rather tight three-parameter correlation discovered more than twenty years ago. It has resisted a both global and precise physical interpretation despite a consequent number of works, observational, theoretical or using numerical simulations. It appears that its precise properties depend on the population of galaxies in study. Instead of selecting a priori these populations, we propose to objectively construct homologous populations from multivariate analyses. We have undertaken multivariate cluster and cladistic analyses of a sample of 56 low-redshift galaxy clusters containing 699 early-type galaxies, using four parameters: effective radius, velocity dispersion, surface brightness averaged over effective", "orig": "radius, and Mg2 index. All our analyses are consistent with seven groups that define separate regions on the global fundamental plane, not across its thickness. In fact, each group shows its own fundamental plane, which is more loosely defined for less diversified groups. We conclude that the global fundamental plane is not a bent surface, but made of a collection of several groups characterizing several fundamental planes with different thicknesses and orientations in the parameter space. Our diversification scenario probably indicates that the level of diversity is linked to the number and the nature of transforming events and that the fundamental plane is the result of several transforming events. We also show that our", "generated": "radius, and Mg2 line strength index.\n\nOur approach relies on the idea that galaxies belonging to the same population should exhibit similar behavior in the parameter space. By applying clustering algorithms, we are able to identify distinct groups of galaxies that share similar properties. These groups can be considered as homologous populations, which are not necessarily related to traditional morphological classification schemes. We have identified five distinct clusters in our sample, each corresponding to a specific region of the fundamental plane. These clusters exhibit different distributions of galaxy properties, such as velocity dispersion and surface brightness profiles. Furthermore, our cladistic analysis reveals that these clusters are not simply a result of"}
{"full_text": "We present DCL-PC: a logic for reasoning about how the abilities of agents and coalitions of agents are altered by transferring control from one agent to another. The logical foundation of DCL-PC is CL-PC, a logic for reasoning about cooperation in which the abilities of agents and coalitions of agents stem from a distribution of atomic Boolean variables to individual agents -- the choices available to a coalition correspond to assignments to the variables the coalition controls. The basic modal constructs of DCL-PC are of the form coalition C can cooperate to bring about phi. DCL-PC extends CL-PC with dynamic logic modalities in which atomic programs are of the form agent i gives control of variable p to agent j; as usual in dynamic logic, these atomic programs may be combined using sequence, iteration, choice, and test operators to form complex programs. By combining such dynamic transfer programs with cooperation modalities, it becomes possible to reason about how the power of agents and coalitions is affected by the transfer of control. We give two alternative semantics for the logic: a direct semantics, in which we capture the distributions of Boolean variables to agents; and a more conventional Kripke semantics. We prove that these semantics are equivalent, and then present an axiomatization for the logic. We investigate the computational complexity of model checking and satisfiability for DCL-PC, and show that both problems are PSPACE-complete (and hence no worse than the underlying logic CL-PC). Finally, we investigate the characterisation of control in DCL-PC. We distinguish between first-order control -- the ability of an agent or coalition to control some state of affairs through the assignment of values to the variables under the control of the agent or coalition -- and second-order control -- the ability of an agent to exert control over the control that other agents have by transferring variables to other agents. We give a logical characterisation of second-order control.", "prompt": "We present DCL-PC: a logic for reasoning about how the abilities of agents and coalitions of agents are altered by transferring control from one agent to another. The logical foundation of DCL-PC is CL-PC, a logic for reasoning about cooperation in which the abilities of agents and coalitions of agents stem from a distribution of atomic Boolean variables to individual agents -- the choices available to a coalition correspond to assignments to the variables the coalition controls. The basic modal constructs of DCL-PC are of the form coalition C can cooperate to bring about phi. DCL-PC extends CL-PC with dynamic logic modalities in which", "orig": "atomic programs are of the form agent i gives control of variable p to agent j; as usual in dynamic logic, these atomic programs may be combined using sequence, iteration, choice, and test operators to form complex programs. By combining such dynamic transfer programs with cooperation modalities, it becomes possible to reason about how the power of agents and coalitions is affected by the transfer of control. We give two alternative semantics for the logic: a direct semantics, in which we capture the distributions of Boolean variables to agents; and a more conventional Kripke semantics. We prove that these semantics are equivalent, and then present an axiomatization for the", "generated": "the transfer of control is explicitly modeled, enabling the expression of properties such as \"agent A transfers control of variable x to agent B, resulting in coalition C being able to bring about phi.\"\n\nThe introduction of dynamic logic modalities in DCL-PC allows for a more nuanced exploration of the dynamics of control transfer and its effects on the abilities of agents and coalitions. For instance, one can express scenarios in which an agent's ability to achieve a goal is contingent upon the transfer of control from another agent, or in which a coalition's ability to cooperate is hindered by the reluctance of one of its members to relinquish control. The"}
{"full_text": "Parametrized families of PDEs arise in various contexts such as inverse problems, control and optimization, risk assessment, and uncertainty quantification. In most of these applications, the number of parameters is large or perhaps even infinite. Thus, the development of numerical methods for these parametric problems is faced with the possible curse of dimensionality. This article is directed at (i) identifying and understanding which properties of parametric equations allow one to avoid this curse and (ii) developing and analyzing effective numerical methodd which fully exploit these properties and, in turn, are immune to the growth in dimensionality. The first part of this article studies the smoothness and approximability of the solution map, that is, the map $a\\mapsto u(a)$ where $a$ is the parameter value and $u(a)$ is the corresponding solution to the PDE. It is shown that for many relevant parametric PDEs, the parametric smoothness of this map is typically holomorphic and also highly anisotropic in that the relevant parameters are of widely varying importance in describing the solution. These two properties are then exploited to establish convergence rates of $n$-term approximations to the solution map for which each term is separable in the parametric and physical variables. These results reveal that, at least on a theoretical level, the solution map can be well approximated by discretizations of moderate complexity, thereby showing how the curse of dimensionality is broken. This theoretical analysis is carried out through concepts of approximation theory such as best $n$-term approximation, sparsity, and $n$-widths. These notions determine a priori the best possible performance of numerical methods and thus serve as a benchmark for concrete algorithms. The second part of this article turns to the development of numerical algorithms based on the theoretically established sparse separable approximations. The numerical methods studied fall into two general categories. The first uses polynomial expansions in terms of the parameters to approximate the solution map. The second one searches for suitable low dimensional spaces for simultaneously approximating all members of the parametric family. The numerical implementation of these approaches is carried out through adaptive and greedy algorithms. An a priori analysis of the performance of these algorithms establishes how well they meet the theoretical benchmarks.", "prompt": "Parametrized families of PDEs arise in various contexts such as inverse problems, control and optimization, risk assessment, and uncertainty quantification. In most of these applications, the number of parameters is large or perhaps even infinite. Thus, the development of numerical methods for these parametric problems is faced with the possible curse of dimensionality. This article is directed at (i) identifying and understanding which properties of parametric equations allow one to avoid this curse and (ii) developing and analyzing effective numerical methodd which fully exploit these properties and, in turn, are immune to the growth in dimensionality. The first part of this article studies", "orig": "the smoothness and approximability of the solution map, that is, the map $a\\mapsto u(a)$ where $a$ is the parameter value and $u(a)$ is the corresponding solution to the PDE. It is shown that for many relevant parametric PDEs, the parametric smoothness of this map is typically holomorphic and also highly anisotropic in that the relevant parameters are of widely varying importance in describing the solution. These two properties are then exploited to establish convergence rates of $n$-term approximations to the solution map for which each term is separable in the parametric and physical", "generated": "the underlying mathematical structure of parametric PDEs, focusing on the relationships between the parameters, the solution space, and the physical phenomena being modeled. By analyzing the properties of the parameter-to-solution map, we can identify the key features that enable the development of efficient numerical methods.\n\nIn particular, we explore the role of ellipticity, coercivity, and continuity in ensuring the well-posedness of the parametric problem and the convergence of numerical schemes. We also examine the importance of regularity and smoothness of the solution with respect to the parameters, which is crucial for the development of accurate and efficient algorithms. The second part of"}
{"full_text": "Mapping forest aboveground biomass (AGB) has become an important task, particularly for the reporting of carbon stocks and changes. AGB can be mapped using synthetic aperture radar data (SAR) or passive optical data. However, these data are insensitive to high AGB levels (\\textgreater{}150 Mg/ha, and \\textgreater{}300 Mg/ha for P-band), which are commonly found in tropical forests. Studies have mapped the rough variations in AGB by combining optical and environmental data at regional and global scales. Nevertheless, these maps cannot represent local variations in AGB in tropical forests. In this paper, we hypothesize that the problem of misrepresenting local variations in AGB and AGB estimation with good precision occurs because of both methodological limits (signal saturation or dilution bias) and a lack of adequate calibration data in this range of AGB values. We test this hypothesis by developing a calibrated regression model to predict variations in high AGB values (mean \\textgreater{}300 Mg/ha) in French Guiana by a methodological approach for spatial extrapolation with data from the optical geoscience laser altimeter system (GLAS), forest inventories, radar, optics, and environmental variables for spatial inter-and extrapolation. Given their higher point count, GLAS data allow a wider coverage of AGB values. We find that the metrics from GLAS footprints are correlated with field AGB estimations (R 2 =0.54, RMSE=48.3 Mg/ha) with no bias for high values. First, predictive models, including remote-sensing, environmental variables and spatial correlation functions, allow us to obtain \"wall-to-wall\" AGB maps over French Guiana with an RMSE for the in situ AGB estimates of ~51 Mg/ha and R${}^2$=0.48 at a 1-km grid size. We conclude that a calibrated regression model based on GLAS with dependent environmental data can produce good AGB predictions even for high AGB values if the calibration data fit the AGB range. We also demonstrate that small temporal and spatial mismatches between field data and GLAS footprints are not a problem for regional and global calibrated regression models because field data aim to predict large and deep tendencies in AGB variations from environmental gradients and do not aim to represent high but stochastic and temporally limited variations from forest dynamics. Thus, we advocate including a greater variety of data, even if less precise and shifted, to better represent high AGB values in global models and to improve the fitting of these models for high values.", "prompt": "Mapping forest aboveground biomass (AGB) has become an important task, particularly for the reporting of carbon stocks and changes. AGB can be mapped using synthetic aperture radar data (SAR) or passive optical data. However, these data are insensitive to high AGB levels (\\textgreater{}150 Mg/ha, and \\textgreater{}300 Mg/ha for P-band), which are commonly found in tropical forests. Studies have mapped the rough variations in AGB by combining optical and environmental data at regional and global scales. Nevertheless, these maps cannot represent local variations in AGB in tropical forests. In this paper, we hypothesize", "orig": "that the problem of misrepresenting local variations in AGB and AGB estimation with good precision occurs because of both methodological limits (signal saturation or dilution bias) and a lack of adequate calibration data in this range of AGB values. We test this hypothesis by developing a calibrated regression model to predict variations in high AGB values (mean \\textgreater{}300 Mg/ha) in French Guiana by a methodological approach for spatial extrapolation with data from the optical geoscience laser altimeter system (GLAS), forest inventories, radar, optics, and environmental variables for spatial inter-and extrapolation. Given their higher", "generated": "that the integration of LiDAR (Light Detection and Ranging) data with machine learning algorithms can provide more accurate and detailed maps of AGB in tropical forests, especially at high AGB levels.\n\nWe investigate the potential of LiDAR data to capture the structural complexity of tropical forests, which is closely linked to AGB. Our study area is located in the Amazon rainforest, where we have collected high-resolution LiDAR data and field measurements of AGB. We use a machine learning approach to relate LiDAR-derived metrics, such as canopy height and density, to AGB estimates obtained from field measurements. Our results show that"}
{"full_text": "Aims: The large abundances of CH$^+$ in the diffuse interstellar medium (ISM) are a long standing issue of our understanding of the thermodynamical and chemical states of the gas. We investigate, here, the formation of CH+ in turbulent and multiphase environments, where the heating of the gas is almost solely driven by the photoelectric effect. Methods: The diffuse ISM is simulated using the magnetohydrodynamic (MHD) code RAMSES which self-consistently computes the dynamical and thermal evolution of the gas along with the time-dependent evolutions of the abundances of H$^+$, H, and H$_2$. The rest of the chemistry, including the abundance of CH$^+$, is computed in post-processing, at equilibrium, under the constraint of out-ofequilibrium of H$^+$, H, and H$_2$. The comparison with the observations is performed taking into account an often neglected, yet paramount, piece of information, namely the length of the intercepted diffuse matter along the observed lines of sight. Results: The quasi totality of the mass of CH$^+$ originates from the unstable gas, in environments where the kinetic temperature is larger than 600 K, the density ranges between 0.6 and 10 cm$^{-3}$, the electronic fraction ranges between 3 x 10$^{-4}$ and 6 x 10$^{-3}$, and the molecular fraction is smaller than 0.4. Its formation is driven by warm and out-of-equilibrium H$_2$ initially formed in the cold neutral medium (CNM) and injected in more diffuse environments and even the warm neutral medium (WNM) through a combination of advection and thermal instability. The simulation which displays the tightest agreement with the HI-to-H$_2$ transition and the thermal pressure distribution observed in the Solar Neighborhood is found to naturally reproduce the observed abundances of CH$^+$, the dispersion of observations, the probability of occurrence of most of the lines of sight, the fraction of non-detections of CH$^+$, and the distribution of its line profiles. The amount of CH$^+$ and the statistical properties of the simulated lines of sight are set by the fraction of unstable gas rich in H$_2$ which is controlled, on Galactic scales, by the mean density of the diffuse ISM (or, equivalently, its total mass), the amplitude of the mean UV radiation field, and the strength of the turbulent forcing. Conclusions: This work offers a new and natural solution to an 80 years old chemical riddle. The almost ubiquitous presence of CH$^+$ in the diffuse ISM likely results from the exchanges of matter between the CNM and the WNM induced by the combination of turbulent advection and thermal instability, without the need to invoke ambipolar diffusion or regions of intermittent turbulent dissipation. Through two phase turbulent mixing, CH$^+$ might thus be a tracer of the H$_2$ mass loss rate of CNM clouds.", "prompt": "Aims: The large abundances of CH$^+$ in the diffuse interstellar medium (ISM) are a long standing issue of our understanding of the thermodynamical and chemical states of the gas. We investigate, here, the formation of CH+ in turbulent and multiphase environments, where the heating of the gas is almost solely driven by the photoelectric effect. Methods: The diffuse ISM is simulated using the magnetohydrodynamic (MHD) code RAMSES which self-consistently computes the dynamical and thermal evolution of the gas along with the time-dependent evolutions of the abundances of H$^+$,", "orig": "H, and H$_2$. The rest of the chemistry, including the abundance of CH$^+$, is computed in post-processing, at equilibrium, under the constraint of out-ofequilibrium of H$^+$, H, and H$_2$. The comparison with the observations is performed taking into account an often neglected, yet paramount, piece of information, namely the length of the intercepted diffuse matter along the observed lines of sight. Results: The quasi totality of the mass of CH$^+$ originates from the unstable gas, in environments where the kinetic temperature is larger than 600 K, the density ranges between 0.", "generated": "Continuation:\n\nCO, and CH$^+$, among other species. This code allows us to include a detailed treatment of the radiative transfer, turbulence, and magnetic fields, which are essential in understanding the formation of CH$^+$ in the ISM. We focus on the analysis of the impact of the photoelectric heating on the formation of CH$^+$ in a multiphase ISM, characterized by the coexistence of warm and cold gases. Our simulations demonstrate that the photoelectric effect is capable of sustaining a high temperature in the warm phase, which in turn enhances the formation of CH$^+$ through the reaction"}
{"full_text": "In this paper, we examine the price dynamics of on-line art auctions of modern Indian art using functional data analysis. The purpose here is not just to understand what determines the final prices of art objects, but also the price movement during the entire auction. We identify several factors, such as artist characteristics (established or emerging artist; prior sales history), art characteristics (size; painting medium--canvas or paper), competition characteristics (current number of bidders; current number of bids) and auction design characteristics (opening bid; position of the lot in the auction), that explain the dynamics of price movement in an on-line art auction. We find that the effects on price vary over the duration of the auction, with some of these effects being stronger at the beginning of the auction (such as the opening bid and historical prices realized). In some cases, the rate of change in prices (velocity) increases at the end of the auction (for canvas paintings and paintings by established artists). Our analysis suggests that the opening bid is positively related to on-line auction price levels of art at the beginning of the auction, but its effect declines toward the end of the auction. The order in which the lots appear in an art auction is negatively related to the current price level, with this relationship decreasing toward the end of the auction. This implies that lots that appear earlier have higher current prices during the early part of the auction, but that effect diminishes by the end of the auction. Established artists show a positive relationship with the price level at the beginning of the auction. Reputation or popularity of the artists and their investment potential as assessed by previous history of sales are positively related to the price levels at the beginning of the auction. The medium (canvas or paper) of the painting does not show any relationship with art auction price levels, but the size of the painting is negatively related to the current price during the early part of the auction. Important implications for auction design are drawn from the analysis.", "prompt": "In this paper, we examine the price dynamics of on-line art auctions of modern Indian art using functional data analysis. The purpose here is not just to understand what determines the final prices of art objects, but also the price movement during the entire auction. We identify several factors, such as artist characteristics (established or emerging artist; prior sales history), art characteristics (size; painting medium--canvas or paper), competition characteristics (current number of bidders; current number of bids) and auction design characteristics (opening bid; position of the lot in the auction), that explain the dynamics of price movement in an on-line art auction. We find", "orig": "that the effects on price vary over the duration of the auction, with some of these effects being stronger at the beginning of the auction (such as the opening bid and historical prices realized). In some cases, the rate of change in prices (velocity) increases at the end of the auction (for canvas paintings and paintings by established artists). Our analysis suggests that the opening bid is positively related to on-line auction price levels of art at the beginning of the auction, but its effect declines toward the end of the auction. The order in which the lots appear in an art auction is negatively related to the current price level, with this relationship decreasing toward", "generated": "that the price movement is influenced by a combination of these factors, and that the importance of each factor changes over the course of the auction.\n\nOur results show that the artist's reputation and prior sales history have a significant impact on the initial price movement, with artworks by established artists receiving higher opening bids and experiencing more rapid price increases in the early stages of the auction. However, as the auction progresses, the competition characteristics become more important, with the number of bidders and bids placed having a significant impact on the price movement. Interestingly, we find that the painting medium also plays a significant role, with artworks on canvas experiencing more rapid price increases"}
{"full_text": "A highly sheared dense aqueous suspension of granular cornstarch particles displays rich nonlinear rheology. We had previously demonstrated the growth and onset of interfacial instabilities when shear-thinning cornstarch suspensions were displaced by a Newtonian fluid, and had suggested methods to maximise displacement efficiency [Palak, R. Sathayanath, S. K. Kalpathy and R. Bandyopadhyay, Colloids Surf. A Physicochem. Eng. Asp., 629 (2021) 127405]. In the present work, we explore the miscible displacement of a dense aqueous cornstarch suspension in its discontinuous shear-thickening regime in a quasi-two-dimensional radial Hele-Shaw cell. We systematically study the growth kinetics of the inner interface between water and the cornstarch suspension, and also of the outer interface between the suspension and air. In addition to the growth of interfacial instabilities at the inner interface, we observe a transient withdrawal of the suspension and the formation of fingering instabilities at the outer interface. We demonstrate that these `reverse fingering' instabilities are extremely sensitive to the injection flow rate of water, the gap of the Hele-Shaw cell and the concentration of the displaced cornstarch suspension, {and emerge irrespective of immiscibility between the fluid pair. We believe that as the cornstarch suspension dilates due to the high shear rate imposed by the displacing fluid, the outer suspension-air interface responds with a restoring force, resulting in the penetration of air into the suspension and the formation of reverse fingers. We note that the growth of reverse fingers significantly reduces the displacement efficiency of the suspension. Finally, we demonstrate a correlation in the growth of inner and outer interfacial patterns by computing the velocity with which stresses propagate in the confined dense suspension. Our findings are useful in understanding the flow of granular materials through constrained geometries and can be extended to study stress propagation in shear-thickening materials due to a sudden imposition of high shear rate, such as in impact behaviour.", "prompt": "A highly sheared dense aqueous suspension of granular cornstarch particles displays rich nonlinear rheology. We had previously demonstrated the growth and onset of interfacial instabilities when shear-thinning cornstarch suspensions were displaced by a Newtonian fluid, and had suggested methods to maximise displacement efficiency [Palak, R. Sathayanath, S. K. Kalpathy and R. Bandyopadhyay, Colloids Surf. A Physicochem. Eng. Asp., 629 (2021) 127405]. In the present work, we explore the miscible displacement of a dense aque", "orig": "ous cornstarch suspension in its discontinuous shear-thickening regime in a quasi-two-dimensional radial Hele-Shaw cell. We systematically study the growth kinetics of the inner interface between water and the cornstarch suspension, and also of the outer interface between the suspension and air. In addition to the growth of interfacial instabilities at the inner interface, we observe a transient withdrawal of the suspension and the formation of fingering instabilities at the outer interface. We demonstrate that these `reverse fingering' instabilities are extremely sensitive to the injection flow rate of water, the gap of the Hele-Shaw cell and the concentration", "generated": "ous suspension of cornstarch particles by a less viscous Newtonian fluid.\n\nOur study focuses on the influence of various parameters such as the initial concentration of cornstarch, the viscosity ratio of the displacing fluid, and the shear rate on the displacement dynamics. We employ a combination of experimental and numerical techniques to investigate the complex interplay between the rheological properties of the cornstarch suspension and the flow dynamics during the displacement process. The experimental setup consists of a horizontal Hele-Shaw cell, where the dense cornstarch suspension is initially placed and displaced by a less viscous fluid pumped at a controlled flow rate. The displacement"}
{"full_text": "Domain-specific languages (DSLs) are touted as both easy to embed in programs and easy to optimize. Yet these goals are often in tension. Embedded or internal DSLs fit naturally with a host language, while inheriting the host's performance characteristics. External DSLs can use external optimizers and languages but sit apart from the host. We present Sham, a toolkit designed to enable internal DSLs with high performance. Sham provides a domain-specific language (embedded in Racket) for implementing other high-performance DSLs, with transparent compilation to assembly code at runtime. Sham is well suited as both a compilation target for other embedded DSLs and for transparently replacing DSL support code with faster versions. Sham provides seamless inter-operation with its host language without requiring any additional effort from its users. Sham also provides a framework for defining language syntax which implements Sham's own language interface as well. We validate Sham's design on a series of case studies, ranging from Krishnamurthi's classic automata DSL to a sound synthesis DSL and a probabilistic programming language. All of these are existing DSLs where we replaced the backend using Sham, resulting in major performance gains. We present an example-driven description of how Sham can smoothly enhance an existing DSL into a high-performance one. When compared to existing approaches for implementing high-performance DSLs, Sham's design aims for both simplicity and programmer control. This makes it easier to port our techniques to other languages and frameworks, or borrow Sham's innovations \"\\`a la carte\" without adopting the whole approach. Sham builds a sophisticated and powerful DSL construction toolkit atop fundamental language features including higher-order functions, data structures, and a foreign-function interface (FFI), all readily available in other languages. Furthermore, Sham's approach allows DSL developers to simply write functions, either using Sham or generating Sham, without needing to work through complex staging or partial evaluation systems.", "prompt": "Domain-specific languages (DSLs) are touted as both easy to embed in programs and easy to optimize. Yet these goals are often in tension. Embedded or internal DSLs fit naturally with a host language, while inheriting the host's performance characteristics. External DSLs can use external optimizers and languages but sit apart from the host. We present Sham, a toolkit designed to enable internal DSLs with high performance. Sham provides a domain-specific language (embedded in Racket) for implementing other high-performance DSLs, with transparent compilation to assembly code at runtime. Sham is well suited as both a compilation target for other embedded DSLs and for", "orig": "transparently replacing DSL support code with faster versions. Sham provides seamless inter-operation with its host language without requiring any additional effort from its users. Sham also provides a framework for defining language syntax which implements Sham's own language interface as well. We validate Sham's design on a series of case studies, ranging from Krishnamurthi's classic automata DSL to a sound synthesis DSL and a probabilistic programming language. All of these are existing DSLs where we replaced the backend using Sham, resulting in major performance gains. We present an example-driven description of how Sham can smoothly enhance an existing DSL into a high-performance one. When compared to", "generated": "building high-performance systems from the ground up.\n\nBy leveraging Sham's ability to compile to assembly code at runtime, developers can create internal DSLs that not only integrate seamlessly with their host language but also benefit from performance optimizations that would be difficult or impossible to achieve with traditional interpreted or compiled DSLs. This allows for the creation of high-performance DSLs that can be used in a wide range of applications, from scientific computing and data analysis to systems programming and embedded systems.\n\nSham's transparent compilation process also enables developers to take advantage of the lowest-level optimizations available on modern CPUs, such as SIMD instructions and cache optimization. This is particularly important for"}
{"full_text": "Disaggregated memory is promising for improving memory utilization in computer clusters in which memory demands significantly vary across computer nodes under utilization. It allows applications with high memory demands to use memory in other computer nodes. However, disaggregated memory is not easy to use for implementing data structures in C++ because the C++ standard does not provide an adequate abstraction to use it efficiently in a high-level, modular manner. Because accessing remote memory involves high latency, disaggregated memory is often used as a far-memory system, which forms a kind of swap memory where part of local memory is used as a cache area, while the remaining memory is not subject to swapping. To pursue performance, programmers have to be aware of this nonuniform memory view and place data appropriately to minimize swapping. In this work, we model the address space of memory-disaggregated systems as the far-memory model, present the collective allocator abstraction, which enables us to specify object placement aware of memory address subspaces, and apply it to programming aware of the far-memory model. The far-memory model provides a view of the nonuniform memory space while hiding the details. In the model, the virtual address space is divided into two subspaces; one is subject to swapping and the other is not. The swapping subspace is further divided into even-sized pages, which are units of swapping. The collective allocator abstraction forms an allocator as a collection of sub-allocators, each of which owns a distinct subspace, where every allocation is done via sub-allocators. It enables us to control object placement at allocation time by selecting an appropriate sub-allocator according to different criteria, such as subspace characteristics and object collocation. It greatly facilitates implementing container data structures aware of the far-memory model. We develop an allocator based on the collective allocator abstraction by extending the C++ standard allocator for container data structures on the far-memory model and experimentally demonstrate that it facilitates implementing containers equipped with object placement strategies aware of spatial locality under the far-memory model in a high-level, modular manner. More specifically, we have successfully implemented B-trees and skip lists with the combined use of two placement strategies. The modifications therein for the original implementations are fairly modest: addition is mostly due to specifying object placement; deletion and modification are at most 1.2 % and 3.2 % of lines of the original code, respectively. We have experimentally confirmed that the modified implementations successfully have data layouts suppressing swapping. We forecast that the collective allocator abstraction would be a key to high-level integration with different memory hardware technologies because it straightforwardly accommodates new interfaces for subspaces.", "prompt": "Disaggregated memory is promising for improving memory utilization in computer clusters in which memory demands significantly vary across computer nodes under utilization. It allows applications with high memory demands to use memory in other computer nodes. However, disaggregated memory is not easy to use for implementing data structures in C++ because the C++ standard does not provide an adequate abstraction to use it efficiently in a high-level, modular manner. Because accessing remote memory involves high latency, disaggregated memory is often used as a far-memory system, which forms a kind of swap memory where part of local memory is used as a cache area, while the remaining memory is not subject to swapping", "orig": ". To pursue performance, programmers have to be aware of this nonuniform memory view and place data appropriately to minimize swapping. In this work, we model the address space of memory-disaggregated systems as the far-memory model, present the collective allocator abstraction, which enables us to specify object placement aware of memory address subspaces, and apply it to programming aware of the far-memory model. The far-memory model provides a view of the nonuniform memory space while hiding the details. In the model, the virtual address space is divided into two subspaces; one is subject to swapping and the other is not. The swapping subspace is further divided", "generated": ".\n\nTo address the limitations of using disaggregated memory in C++, researchers have proposed various solutions, such as developing custom memory allocators that can manage remote memory allocation and deallocation efficiently. These allocators can provide a high-level abstraction for accessing remote memory, allowing developers to write modular and scalable code that can take advantage of the benefits of disaggregated memory. For instance, a custom allocator can provide a caching mechanism that stores frequently accessed data in local memory, reducing the latency associated with remote memory access. Additionally, some researchers have explored the use of parallel algorithms and data structures that can effectively utilize the distributed memory architecture of disaggregated memory systems."}
{"full_text": "Solar flare prediction is a central problem in space weather forecasting and has captivated the attention of a wide spectrum of researchers due to recent advances in both remote sensing as well as machine learning and deep learning approaches. The experimental findings based on both machine and deep learning models reveal significant performance improvements for task specific datasets. Along with building models, the practice of deploying such models to production environments under operational settings is a more complex and often time-consuming process which is often not addressed directly in research settings. We present a set of new heuristic approaches to train and deploy an operational solar flare prediction system for $\\geq$M1.0-class flares with two prediction modes: full-disk and active region-based. In full-disk mode, predictions are performed on full-disk line-of-sight magnetograms using deep learning models whereas in active region-based models, predictions are issued for each active region individually using multivariate time series data instances. The outputs from individual active region forecasts and full-disk predictors are combined to a final full-disk prediction result with a meta-model. We utilized an equal weighted average ensemble of two base learners' flare probabilities as our baseline meta learner and improved the capabilities of our two base learners by training a logistic regression model. The major findings of this study are: (i) We successfully coupled two heterogeneous flare prediction models trained with different datasets and model architecture to predict a full-disk flare probability for next 24 hours, (ii) Our proposed ensembling model, i.e., logistic regression, improves on the predictive performance of two base learners and the baseline meta learner measured in terms of two widely used metrics True Skill Statistic (TSS) and Heidke Skill core (HSS), and (iii) Our result analysis suggests that the logistic regression-based ensemble (Meta-FP) improves on the full-disk model (base learner) by $\\sim9\\%$ in terms TSS and $\\sim10\\%$ in terms of HSS. Similarly, it improves on the AR-based model (base learner) by $\\sim17\\%$ and $\\sim20\\%$ in terms of TSS and HSS respectively. Finally, when compared to the baseline meta model, it improves on TSS by $\\sim10\\%$ and HSS by $\\sim15\\%$.", "prompt": "Solar flare prediction is a central problem in space weather forecasting and has captivated the attention of a wide spectrum of researchers due to recent advances in both remote sensing as well as machine learning and deep learning approaches. The experimental findings based on both machine and deep learning models reveal significant performance improvements for task specific datasets. Along with building models, the practice of deploying such models to production environments under operational settings is a more complex and often time-consuming process which is often not addressed directly in research settings. We present a set of new heuristic approaches to train and deploy an operational solar flare prediction system for $\\geq$M1.0-class flares with", "orig": "two prediction modes: full-disk and active region-based. In full-disk mode, predictions are performed on full-disk line-of-sight magnetograms using deep learning models whereas in active region-based models, predictions are issued for each active region individually using multivariate time series data instances. The outputs from individual active region forecasts and full-disk predictors are combined to a final full-disk prediction result with a meta-model. We utilized an equal weighted average ensemble of two base learners' flare probabilities as our baseline meta learner and improved the capabilities of our two base learners by training a logistic regression model. The major findings of this study are:", "generated": "high-temporal resolution and reliability. The proposed approaches leverage ensemble methods and attention-based neural networks to improve the prediction accuracy and robustness. Our system is designed to integrate seamlessly with existing space weather forecasting infrastructure, enabling real-time monitoring and alert systems for solar flares. The deployment of the system is facilitated through a containerized architecture, ensuring portability, scalability, and maintainability. The system is trained on a large dataset of solar flare events, leveraging bothground-based and space-based observations. The dataset is curated to include a diverse range of flare events, including X-class and M-class flares, to ensure generalizability of the"}
{"full_text": "VAEs, or variational autoencoders, are autoencoders that explicitly learn the distribution of the input image space rather than assuming no prior information about the distribution. This allows it to classify similar samples close to each other in the latent space's distribution. VAEs classically assume the latent space is normally distributed, though many distribution priors work, and they encode this assumption through a K-L divergence term in the loss function. While VAEs learn the distribution of the latent space and naturally make each dimension in the latent space as disjoint from the others as possible, they do not group together similar features -- the image space feature represented by one unit of the representation layer does not necessarily have high correlation with the feature represented by a neighboring unit of the representation layer. This makes it difficult to interpret VAEs since the representation layer is not structured in a way that is easy for humans to parse. We aim to make a more interpretable VAE by partitioning the representation layer into disjoint sets of units. Partitioning the representation layer into disjoint sets of interconnected units yields a prior that features of the input space to this new VAE, which we call a partition VAE or PVAE, are grouped together by correlation -- for example, if our image space were the space of all ping ping game images (a somewhat complex image space we use to test our architecture) then we would hope the partitions in the representation layer each learned some large feature of the image like the characteristics of the ping pong table or the characteristics and position of the players or the ball. We also add to the PVAE a cost-saving measure: subresolution. Because we do not have access to GPU training environments for long periods of time and Google Colab Pro costs money, we attempt to decrease the complexity of the PVAE by outputting an image with dimensions scaled down from the input image by a constant factor, thus forcing the model to output a smaller version of the image. We then increase the resolution to calculate loss and train by interpolating through neighboring pixels. We train a tuned PVAE on MNIST and Sports10 to test its effectiveness.", "prompt": "VAEs, or variational autoencoders, are autoencoders that explicitly learn the distribution of the input image space rather than assuming no prior information about the distribution. This allows it to classify similar samples close to each other in the latent space's distribution. VAEs classically assume the latent space is normally distributed, though many distribution priors work, and they encode this assumption through a K-L divergence term in the loss function. While VAEs learn the distribution of the latent space and naturally make each dimension in the latent space as disjoint from the others as possible, they do not group together similar features -- the image space feature", "orig": "represented by one unit of the representation layer does not necessarily have high correlation with the feature represented by a neighboring unit of the representation layer. This makes it difficult to interpret VAEs since the representation layer is not structured in a way that is easy for humans to parse. We aim to make a more interpretable VAE by partitioning the representation layer into disjoint sets of units. Partitioning the representation layer into disjoint sets of interconnected units yields a prior that features of the input space to this new VAE, which we call a partition VAE or PVAE, are grouped together by correlation -- for example, if our image space were", "generated": "extraction is often dispersed across multiple dimensions.\n\nHowever, this limitation can be addressed by incorporating techniques from disentangled representation learning, which aim to learn a set of independent factors that explain the variability in the data. By doing so, VAEs can learn to group similar features together in the latent space, allowing for more interpretable and disentangled representations. For instance, in the context of facial image generation, a disentangled VAE might learn to separate features such as face shape, skin tone, and facial expression into distinct dimensions, making it easier to manipulate and generate new images.\n\nFurthermore, VAEs have also been extended"}
{"full_text": "Scaling temporal dynamics in functional MRI (fMRI) signals have been evidenced for a decade as intrinsic characteristics of ongoing brain activity (Zarahn et al., 1997). Recently, scaling properties were shown to fluctuate across brain networks and to be modulated between rest and task (He, 2011): notably, Hurst exponent, quantifying long memory, decreases under task in activating and deactivating brain regions. In most cases, such results were obtained: First, from univariate (voxelwise or regionwise) analysis, hence focusing on specific cognitive systems such as Resting-State Networks (RSNs) and raising the issue of the specificity of this scale-free dynamics modulation in RSNs. Second, using analysis tools designed to measure a single scaling exponent related to the second order statistics of the data, thus relying on models that either implicitly or explicitly assume Gaussianity and (asymptotic) self-similarity, while fMRI signals may significantly depart from those either of those two assumptions (Ciuciu et al., 2008; Wink et al., 2008). To address these issues, the present contribution elaborates on the analysis of the scaling properties of fMRI temporal dynamics by proposing two significant variations. First, scaling properties are technically investigated using the recently introduced Wavelet Leader-based Multifractal formalism (WLMF; Wendt et al., 2007). This measures a collection of scaling exponents, thus enables a richer and more versatile description of scale invariance (beyond correlation and Gaussianity), referred to as multifractality. Also, it benefits from improved estimation performance compared to tools previously used in the literature. Second, scaling properties are investigated in both RSN and non-RSN structures (e.g., artifacts), at a broader spatial scale than the voxel one, using a multivariate approach, namely the Multi-Subject Dictionary Learning (MSDL) algorithm (Varoquaux et al., 2011) that produces a set of spatial components that appear more sparse than their Independent Component Analysis (ICA) counterpart. These tools are combined and applied to a fMRI dataset comprising 12 subjects with resting-state and activation runs (Sadaghiani et al., 2009). Results stemming from those analysis confirm the already reported task-related decrease of long memory in functional networks, but also show that it occurs in artifacts, thus making this feature not specific to functional networks. Further, results indicate that most fMRI signals appear multifractal at rest except in non-cortical regions. Task-related modulation of multifractality appears only significant in functional networks and thus can be considered as the key property disentangling functional networks from artifacts. These finding are discussed in the light of the recent literature reporting scaling dynamics of EEG microstate sequences at rest and addressing non-stationarity issues in temporally independent fMRI modes.", "prompt": "Scaling temporal dynamics in functional MRI (fMRI) signals have been evidenced for a decade as intrinsic characteristics of ongoing brain activity (Zarahn et al., 1997). Recently, scaling properties were shown to fluctuate across brain networks and to be modulated between rest and task (He, 2011): notably, Hurst exponent, quantifying long memory, decreases under task in activating and deactivating brain regions. In most cases, such results were obtained: First, from univariate (voxelwise or regionwise) analysis, hence focusing on specific cognitive systems such as Resting-State Networks (RSNs) and raising", "orig": "the issue of the specificity of this scale-free dynamics modulation in RSNs. Second, using analysis tools designed to measure a single scaling exponent related to the second order statistics of the data, thus relying on models that either implicitly or explicitly assume Gaussianity and (asymptotic) self-similarity, while fMRI signals may significantly depart from those either of those two assumptions (Ciuciu et al., 2008; Wink et al., 2008). To address these issues, the present contribution elaborates on the analysis of the scaling properties of fMRI temporal dynamics by proposing two significant variations. First, scaling properties", "generated": "questions about the universality of these observations across the brain.\n\nHowever, the study of scaling properties in fMRI signals has been mainly limited to examining individual brain regions or networks in isolation, neglecting the complex interactions between them. A more comprehensive understanding of the brain's intrinsic dynamics would require a multivariate approach, taking into account the intricate relationships between different brain regions and networks. In particular, it has been suggested that the brain's functional organization can be better understood by examining the dynamics of the entire brain network, rather than focusing on individual components (Bullmore & Sporns, 2009). Furthermore, recent studies have demonstrated that mult"}
{"full_text": "Let $G$ be a simple, undirected, finite graph with vertex set $V(G)$ and edge set $E(G)$. A $k$-dimensional box is a Cartesian product of closed intervals $[a_1,b_1]\\times [a_2,b_2]\\times...\\times [a_k,b_k]$. The {\\it boxicity} of $G$, $\\boxi(G)$ is the minimum integer $k$ such that $G$ can be represented as the intersection graph of $k$-dimensional boxes, i.e. each vertex is mapped to a $k$-dimensional box and two vertices are adjacent in $G$ if and only if their corresponding boxes intersect. Let $\\poset=(S,P)$ be a poset where $S$ is the ground set and $P$ is a reflexive, anti-symmetric and transitive binary relation on $S$. The dimension of $\\poset$, $\\dim(\\poset)$ is the minimum integer $t$ such that $P$ can be expressed as the intersection of $t$ total orders. Let $G_\\poset$ be the \\emph{underlying comparability graph} of $\\poset$, i.e. $S$ is the vertex set and two vertices are adjacent if and only if they are comparable in $\\poset$. It is a well-known fact that posets with the same underlying comparability graph have the same dimension. The first result of this paper links the dimension of a poset to the boxicity of its underlying comparability graph. In particular, we show that for any poset $\\poset$, $\\boxi(G_\\poset)/(\\chi(G_\\poset)-1) \\le \\dim(\\poset)\\le 2\\boxi(G_\\poset)$, where $\\chi(G_\\poset)$ is the chromatic number of $G_\\poset$ and $\\chi(G_\\poset)\\ne1$. It immediately follows that if $\\poset$ is a height-2 poset, then $\\boxi(G_\\poset)\\le \\dim(\\poset)\\le 2\\boxi(G_\\poset)$ since the underlying comparability graph of a height-2 poset is a bipartite graph. The second result of the paper relates the boxicity of a graph $G$ with a natural partial order associated with the \\emph{extended double cover} of $G$, denoted as $G_c$: Note that $G_c$ is a bipartite graph with partite sets $A$ and $B$ which are copies of $V(G)$ such that corresponding to every $u\\in V(G)$, there are two vertices $u_A\\in A$ and $u_B\\in B$ and $\\{u_A,v_B\\}$ is an edge in $G_c$ if and only if either $u=v$ or $u$ is adjacent to $v$ in $G$. Let $\\poset_c$ be the natural height-2 poset associated with $G_c$ by making $A$ the set of minimal elements and $B$ the set of maximal elements. We show that $\\frac{\\boxi(G)}{2} \\le \\dim(\\poset_c) \\le 2\\boxi(G)+4$. These results have some immediate and significant consequences. The upper bound $\\dim(\\poset)\\le 2\\boxi(G_\\poset)$ allows us to derive hitherto unknown upper bounds for poset dimension such as $\\dim(\\poset)\\le 2\\tw(G_\\poset)+4$, since boxicity of any graph is known to be at most its $\\tw+2$. In the other direction, using the already known bounds for partial order dimension we get the following: (1) The boxicity of any graph with maximum degree $\\Delta$ is $O(\\Delta\\log^2\\Delta)$ which is an improvement over the best known upper bound of $\\Delta^2+2$. (2) There exist graphs with boxicity $\\Omega(\\Delta\\log\\Delta)$. This disproves a conjecture that the boxicity of a graph is $O(\\Delta)$. (3) There exists no polynomial-time algorithm to approximate the boxicity of a bipartite graph on $n$ vertices with a factor of $O(n^{0.5-\\epsilon})$ for any $\\epsilon>0$, unless $NP=ZPP$.", "prompt": "Let $G$ be a simple, undirected, finite graph with vertex set $V(G)$ and edge set $E(G)$. A $k$-dimensional box is a Cartesian product of closed intervals $[a_1,b_1]\\times [a_2,b_2]\\times...\\times [a_k,b_k]$. The {\\it boxicity} of $G$, $\\boxi(G)$ is the minimum integer $k$ such that $G$ can be represented as the intersection graph of $k$-dimensional boxes, i.e. each vertex is mapped to a $k$-dimension", "orig": "al box and two vertices are adjacent in $G$ if and only if their corresponding boxes intersect. Let $\\poset=(S,P)$ be a poset where $S$ is the ground set and $P$ is a reflexive, anti-symmetric and transitive binary relation on $S$. The dimension of $\\poset$, $\\dim(\\poset)$ is the minimum integer $t$ such that $P$ can be expressed as the intersection of $t$ total orders. Let $G_\\poset$ be the \\emph{underlying comparability graph} of $\\poset$, i.e. $S", "generated": "al box in a way that two vertices are adjacent in $G$ if and only if their corresponding boxes intersect.\n\nContinuing this concept, the boxicity of a graph is a measure of how complex the graphic structure is. A graph with a low boxicity can be represented in a lower-dimensional space, whereas a graph with a high boxicity requires a higher-dimensional space to capture its structural relationships. This concept has applications in various fields, including computer science, biology, and social network analysis. For instance, in computer science, boxicity can be used to optimize the layout of circuits and networks, while in biology, it can be used"}
{"full_text": "We have shown, in a series of articles, that a classical description of a large number of economic agents can be replaced by a statistical fields formalism. To better understand the accumulation and allocation of capital among different sectors, the present paper applies this statistical fields description to a large number of heterogeneous agents divided into two groups. The first group is composed of a large number of firms in different sectors that collectively own the entire physical capital. The second group, investors, holds the entire financial capital and allocates it between firms across sectors according to investment preferences, expected returns, and stock prices variations on financial markets. In return, firms pay dividends to their investors. Financial capital is thus a function of dividends and stock valuations, whereas physical capital is a function of the total capital allocated by the financial sector. Whereas our previous work focused on the background fields that describe potential long-term equilibria, here we compute the transition functions of individual agents and study their probabilistic dynamics in the background field, as a function of their initial state. We show that capital accumulation depends on various factors. The probability associated with each firm's trajectories is the result of several contradictory effects: the firm tends to shift towards sectors with the greatest long-term return, but must take into account the impact of its shift on its attractiveness for investors throughout its trajectory. Since this trajectory depends largely on the average capital of transition sectors, a firm's attractiveness during its relocation depends on the relative level of capital in those sectors. Thus, an under-capitalized firm reaching a high-capital sector will experience a loss of attractiveness, and subsequently, in investors. Moreover, the firm must also consider the effects of competition in the intermediate sectors. An under-capitalized firm will tend to be ousted out towards sectors with lower average capital, while an over-capitalized firm will tend to shift towards higher averagecapital sectors. For investors, capital allocation depends on their short and long-term returns. These returns are not independent: in the short-term, returns are composed of both the firm's dividends and the increase in its stock prices. In the long-term, returns are based on the firm's growth expectations, but also, indirectly, on expectations of higher stock prices. Investors' capital allocation directly depends on the volatility of stock prices and {\\ldots}rms'dividends. Investors will tend to reallocate their capital to maximize their short and long-term returns. The higher their level of capital, the stronger the reallocation will be.", "prompt": "We have shown, in a series of articles, that a classical description of a large number of economic agents can be replaced by a statistical fields formalism. To better understand the accumulation and allocation of capital among different sectors, the present paper applies this statistical fields description to a large number of heterogeneous agents divided into two groups. The first group is composed of a large number of firms in different sectors that collectively own the entire physical capital. The second group, investors, holds the entire financial capital and allocates it between firms across sectors according to investment preferences, expected returns, and stock prices variations on financial markets. In return, firms pay dividends to", "orig": "their investors. Financial capital is thus a function of dividends and stock valuations, whereas physical capital is a function of the total capital allocated by the financial sector. Whereas our previous work focused on the background fields that describe potential long-term equilibria, here we compute the transition functions of individual agents and study their probabilistic dynamics in the background field, as a function of their initial state. We show that capital accumulation depends on various factors. The probability associated with each firm's trajectories is the result of several contradictory effects: the firm tends to shift towards sectors with the greatest long-term return, but must take into account the impact of its shift", "generated": "investors, who then reallocate their financial resources based on the performance of the firms and the overall market conditions.\n\nThis framework allows us to model the complex interactions between firms and investors, taking into account the inherent uncertainties and risks associated with investment decisions. By using statistical fields, we can capture the emergent behavior of the system, such as the formation of clusters and patterns in the allocation of capital, and the propagation of shocks and perturbations through the economy. Furthermore, our approach enables us to investigate the role of various factors, such as government policies, technological innovations, and changes in consumer preferences, on the accumulation and allocation of capital."}
{"full_text": "Context: Database-backed applications often run queries with more authority than necessary. Since programs can access more data than they legitimately need, flaws in security checks at the application level can enable malicious or buggy code to view or modify data in violation of intended access control policies. Inquiry: Although database management systems provide tools to control access to data, these tools are not well-suited for modern applications which often have many users and consist of many different software components. First, databases are unaware of application users, and creating a new database user for each application user is impractical for applications with many users. Second, different components of the same application may require different authority, which would require creating different database users for different software components. Thus, it is difficult to use existing tools to properly limit the authority an application has when executing queries. For this reason, we consider a new, language-based approach to application-specific database security. Approach: Prior work has addressed the difficulty of running applications with least privilege using capability-based security and software contracts, which we adapt to the setting of database-backed applications. Knowledge: This paper's main contribution is the design and implementation of ShillDB, a language for writing secure database-backed applications. ShillDB enables reasoning about database access at the language level through capabilities, which limit which database tables a program can access, and contracts, which limit what operations a program can perform on those tables. ShillDB contracts are expressed as part of function interfaces, making it easy to specify different access control policies for different components. Contracts act as executable security documentation for ShillDB programs and are enforced by the language runtime. Further, ShillDB provides database access control guarantees independent of (and in addition to) the security mechanisms of the underlying database management system. Grounding: We have implemented a prototype of ShillDB and have used it to implement the backend for a lending library reservation system with contracts for each endpoint to evaluate the performance and usability of ShillDB. Further, we benchmark individual database operations in ShillDB to better understand the language's performance overhead. Importance: Our experience indicates that ShillDB is a practical language for enforcing database access control policies in realistic, multi-user applications and has reasonable performance overhead. ShillDB allows developers to reason about security at the component level, safely compose components, and reuse third-party components with their own application-specific database security policies.", "prompt": "Context: Database-backed applications often run queries with more authority than necessary. Since programs can access more data than they legitimately need, flaws in security checks at the application level can enable malicious or buggy code to view or modify data in violation of intended access control policies. Inquiry: Although database management systems provide tools to control access to data, these tools are not well-suited for modern applications which often have many users and consist of many different software components. First, databases are unaware of application users, and creating a new database user for each application user is impractical for applications with many users. Second, different components of the same application may require different", "orig": "authority, which would require creating different database users for different software components. Thus, it is difficult to use existing tools to properly limit the authority an application has when executing queries. For this reason, we consider a new, language-based approach to application-specific database security. Approach: Prior work has addressed the difficulty of running applications with least privilege using capability-based security and software contracts, which we adapt to the setting of database-backed applications. Knowledge: This paper's main contribution is the design and implementation of ShillDB, a language for writing secure database-backed applications. ShillDB enables reasoning about database access at the language level through capabilities, which", "generated": "levels of access to the same data.\n\nContinuation: Furthermore, as applications become increasingly complex and distributed, it becomes even more challenging to manage and enforce access control policies at the database level. Traditional Role-Based Access Control (RBAC) models, which rely on static roles and permissions, are insufficient to capture the dynamic and context-dependent nature of modern applications. Moreover, the separation of duties between development, deployment, and management teams can lead to a lack of visibility and control over data access, making it difficult to identify and remediate security vulnerabilities.\n\nTo address these limitations, new approaches to data access control are needed. One promising direction is the"}
{"full_text": "AT 2000ch is a highly variable massive star and supernova imposter in NGC 3432 first detected in 2000. It is similar and often compared to SN 2009ip, and it is therefore expected to undergo a core-collapse supernova -- a SN imposter of similar brightness -- in the near future. We characterize the long-term variability of AT 2000ch in the radio and optical regimes with archival data reaching back to the year 1984. We use these newly reduced observations in addition to observations in the literature to restrict the mass-loss rates of AT 2000ch at multiple epochs based on different approaches, and to infer the general properties of its circumstellar nebula with respect to the detected radio brightness. We extend the known optical light curve of AT 2000ch up to the beginning of 2022 by performing point spread function photometry on archival data from the Palomar Transient Factory and the Zwicky Transient Facility. We reduced archival radio continuum observations obtained with the Very Large Array using standard calibration and imaging methods and complemented these with pre-reduced \\changes observations as well as observations obtained with the Westerbork Synthesis Radio Telescope and LOw Frequency ARray. For the analysis of AT 2000ch, we consider the optical light curve and color evolution, its radio continuum brightness at different frequencies and times, and the corresponding spectral indices. We estimated mass-loss rates and optical depths based on radio continuum brightnesses and Ha fluxes. We report two newly detected outbursts of AT 2000ch similar to those found in the 2000s and 13 re-brightening events, of which at least four are not conclusively detected because of insufficient sampling of the light curve. The dates of all outbursts and significant, well-sampled re-brightening events are consistent with a period of $\\sim 201 \\pm 12\\,$days over a total time-span of two decades. Such a behavior has never been found for any SN imposter, especially not for candidate SN~2009ip analogs. During 2010 to 2012 and 2014 to 2018, we only have a few detections, which is insufficient to come to any conclusion as to a possible less eruptive phase of the transient. We find steady dimming after the most recent re-brightening events and possible evidence of porosity in the circumstellar envelope, suggesting AT~2000ch may currently be in transition to a state of relative calm. We identified a second, unrelated source at a projected distance of $\\sim 23\\,$pc ($\\sim0.5^{\\prime\\prime}$) that has contaminated the optical measurements of AT~2000ch at its minimum luminosity over the last two decades probably on a $5\\%-10\\,\\%$ level, but this does not affect our overall findings and is negligible during re-brightening. We are able to restrict the mass-loss rate of AT~2000ch to range between several $10^{-6}\\,\\textrm{M}_{\\odot}/\\textrm{yr}$ and several $10^{-5}\\,\\textrm{M}_{\\odot}/\\textrm{yr}$. The fresh ejecta appear to be optically thick to radio continuum emission at least within the first $\\sim 25\\,$days after significant re-brightening.", "prompt": "AT 2000ch is a highly variable massive star and supernova imposter in NGC 3432 first detected in 2000. It is similar and often compared to SN 2009ip, and it is therefore expected to undergo a core-collapse supernova -- a SN imposter of similar brightness -- in the near future. We characterize the long-term variability of AT 2000ch in the radio and optical regimes with archival data reaching back to the year 1984. We use these newly reduced observations in addition to observations in the literature to restrict the mass-loss rates of AT 2000ch at multiple epochs based on different", "orig": "approaches, and to infer the general properties of its circumstellar nebula with respect to the detected radio brightness. We extend the known optical light curve of AT 2000ch up to the beginning of 2022 by performing point spread function photometry on archival data from the Palomar Transient Factory and the Zwicky Transient Facility. We reduced archival radio continuum observations obtained with the Very Large Array using standard calibration and imaging methods and complemented these with pre-reduced \\changes observations as well as observations obtained with the Westerbork Synthesis Radio Telescope and LOw Frequency ARray. For the analysis of AT 2000ch", "generated": "models of radio emission.\n\nOur analysis reveals that AT 2000ch has experienced significant mass loss over the past few decades, with periods of intense activity interspersed with quieter phases. The star's radio luminosity has increased by an order of magnitude since the 1980s, suggesting a rapid escalation of its mass-loss rate in recent years. This conclusion is supported by optical spectroscopy, which indicates a pronounced increase in the star's wind density and velocity over the same period.\n\nThe implications of these findings are significant, as they suggest that AT 2000ch is on the cusp of a catastrophic event, potentially a core-collapse"}
{"full_text": "Model checking verifies that a model of a system satisfies a given property, and otherwise produces a counter-example explaining the violation. The verified properties are formally expressed in temporal logics. Some temporal logics, such as CTL, are branching: they allow to express facts about the whole computation tree of the model, rather than on each single linear computation. This branching aspect is even more critical when dealing with multi-modal logics, i.e. logics expressing facts about systems with several transition relations. A prominent example is CTLK, a logic that reasons about temporal and epistemic properties of multi-agent systems. In general, model checkers produce linear counter-examples for failed properties, composed of a single computation path of the model. But some branching properties are only poorly and partially explained by a linear counter-example. This paper proposes richer counter-example structures called tree-like annotated counter-examples (TLACEs), for properties in Action-Restricted CTL (ARCTL), an extension of CTL quantifying paths restricted in terms of actions labeling transitions of the model. These counter-examples have a branching structure that supports more complete description of property violations. Elements of these counter-examples are annotated with parts of the property to give a better understanding of their structure. Visualization and browsing of these richer counter-examples become a critical issue, as the number of branches and states can grow exponentially for deeply-nested properties. This paper formally defines the structure of TLACEs, characterizes adequate counter-examples w.r.t. models and failed properties, and gives a generation algorithm for ARCTL properties. It also illustrates the approach with examples in CTLK, using a reduction of CTLK to ARCTL. The proposed approach has been implemented, first by extending the NuSMV model checker to generate and export branching counter-examples, secondly by providing an interactive graphical interface to visualize and browse them.", "prompt": "Model checking verifies that a model of a system satisfies a given property, and otherwise produces a counter-example explaining the violation. The verified properties are formally expressed in temporal logics. Some temporal logics, such as CTL, are branching: they allow to express facts about the whole computation tree of the model, rather than on each single linear computation. This branching aspect is even more critical when dealing with multi-modal logics, i.e. logics expressing facts about systems with several transition relations. A prominent example is CTLK, a logic that reasons about temporal and epistemic properties of multi-agent systems. In general, model checkers produce", "orig": "linear counter-examples for failed properties, composed of a single computation path of the model. But some branching properties are only poorly and partially explained by a linear counter-example. This paper proposes richer counter-example structures called tree-like annotated counter-examples (TLACEs), for properties in Action-Restricted CTL (ARCTL), an extension of CTL quantifying paths restricted in terms of actions labeling transitions of the model. These counter-examples have a branching structure that supports more complete description of property violations. Elements of these counter-examples are annotated with parts of the property to give a better understanding of their structure. Visualization and browsing of these richer", "generated": "counter-examples that are crucial for understanding and debugging the behavior of complex systems.\n\nIn the context of multi-agent systems, model checking becomes even more powerful when combined with epistemic logics. Epistemic logics allow us to reason about the knowledge and beliefs of agents in the system, which is essential for understanding how they interact and make decisions. For instance, CTLK enables us to express properties such as \"agent A knows that agent B knows the password\" or \"agent A believes that the door is open\". By model checking these properties, we can verify whether the system satisfies the desired requirements, such as security or coordination."}
{"full_text": "Jerome H. Friedman was born in Yreka, California, USA, on December 29, 1939. He received his high school education at Yreka High School, then spent two years at Chico State College before transferring to the University of California at Berkeley in 1959. He completed an undergraduate degree in physics in 1962 and a Ph.D. in high-energy particle physics in 1968 and was a post-doctoral research physicist at the Lawrence Berkeley Laboratory during 1968-1972. In 1972, he moved to Stanford Linear Accelerator Center (SLAC) as head of the Computation Research Group, retaining this position until 2006. In 1981, he was appointed half time as Professor in the Department of Statistics, Stanford University, remaining half time with his SLAC appointment. He has held visiting appointments at CSIRO in Sydney, CERN and the Department of Statistics at Berkeley, and has had a very active career as a commercial consultant. Jerry became Professor Emeritus in the Department of Statistics in 2007. Apart from some 30 publications in high-energy physics early in his career, Jerry has published over 70 research articles and books in statistics and computer science, including co-authoring the pioneering books Classification and Regression Trees and The Elements of Statistical Learning. Many of his publications have hundreds if not thousands of citations (e.g., the CART book has over 21,000). Much of his software is incorporated in commercial products, including at least one popular search engine. Many of his methods and algorithms are essential inclusions in modern statistical and data mining packages. Honors include the following: the Rietz Lecture (1999) and the Wald Lectures (2009); election to the American Academy of Arts and Sciences (2005) and the US National Academy of Sciences (2010); a Fellow of the American Statistical Association; Paper of the Year (JASA 1980, 1985; Technometrics 1998, 1992); Statistician of the Year (ASA, Chicago Chapter, 1999); ACM Data Mining Lifetime Innovation Award (2002), Emanuel & Carol Parzen Award for Statistical Innovation (2004); Noether Senior Lecturer (American Statistical Association, 2010); and the IEEE Computer Society Data Mining Research Contribution Award (2012).", "prompt": "Jerome H. Friedman was born in Yreka, California, USA, on December 29, 1939. He received his high school education at Yreka High School, then spent two years at Chico State College before transferring to the University of California at Berkeley in 1959. He completed an undergraduate degree in physics in 1962 and a Ph.D. in high-energy particle physics in 1968 and was a post-doctoral research physicist at the Lawrence Berkeley Laboratory during 1968-1972. In 1972, he moved to Stanford Linear Accelerator Center (SLAC) as head of", "orig": "the Computation Research Group, retaining this position until 2006. In 1981, he was appointed half time as Professor in the Department of Statistics, Stanford University, remaining half time with his SLAC appointment. He has held visiting appointments at CSIRO in Sydney, CERN and the Department of Statistics at Berkeley, and has had a very active career as a commercial consultant. Jerry became Professor Emeritus in the Department of Statistics in 2007. Apart from some 30 publications in high-energy physics early in his career, Jerry has published over 70 research articles and books in statistics and computer science, including co-authoring the", "generated": "the experimental physics group, where he led a team of researchers in groundbreaking experiments on particle physics.\n\nDuring his time at SLAC, Friedman's work focused on the study of hadronic interactions, particularly in the domain of deep inelastic scattering. His pioneering research in this area led to the discovery of quarks, a fundamental constituent of matter, and earned him the 1990 Nobel Prize in Physics, along with his colleagues Henry W. Kendall and Richard E. Taylor. The trio's work laid the foundation for the development of the Standard Model of particle physics, a cornerstone of modern physics.\n\nFriedman's contributions to physics went beyond his"}
{"full_text": "We consider the identification of heterogeneous linear elastic moduli in the context of time-harmonic elastodynamics. This inverse problem is formulated as the minimization of the modified error in constitutive equation (MECE), an energy-based cost functional defined as an weighted additive combination $\\mathcal{E}+\\kappa\\mathcal{D}$ of the error in constitutive equation (ECE) $\\mathcal{E}$, expressed using an energy seminorm, and a quadratic error term $\\mathcal{D}$ incorporating the kinematical measurements. MECE-based identification are known from existing computational evidence to enjoy attractive properties such as improved convexity, robustness to resonant frequencies, and tolerance to incompletely specified boundary conditions (BCs). The main goal of this work is to develop theoretical foundations, in a continuous setting, allowing to explain and justify some of the aforementioned beneficial properties, in particular addressing the general case where BCs may be underspecified. A specific feature of MECE formulations is that forward and adjoint solutions are governed by a fully coupled system, whose mathematical properties play a fundamental role in the qualitative and computational aspects of MECE minimization. We prove that this system has a unique and stable solution at any frequency, provided data is abundant enough (in a sense made precise therein) to at least compensate for any missing information on BCs. As a result, our formulation leads in such situations to a well-defined solution even though the relevant forward problem is not \\emph{a priori} clearly defined. This result has practical implications such as applicability of MECE to partial interior data (with important practical applications including ultrasound elastography), convergence of finite element discretizations and differentiability of the reduced MECE functional. In addition, we establish that usual least squares and pure ECE formulations are limiting cases of MECE formulations for small and large values of $\\kappa$, respectively. For the latter case, which corresponds to exact enforcement of kinematic data, we furthermore show that the reduced MECE Hessian is asymptotically positive for any parameter perturbation supported on the measurement region, thereby corroborating existing computational evidence on convexity improvement brought by MECE functionals. Finally, numerical studies that support and illustrate our theoretical findings, including a parameter reconstruction example using interior data, are presented.", "prompt": "We consider the identification of heterogeneous linear elastic moduli in the context of time-harmonic elastodynamics. This inverse problem is formulated as the minimization of the modified error in constitutive equation (MECE), an energy-based cost functional defined as an weighted additive combination $\\mathcal{E}+\\kappa\\mathcal{D}$ of the error in constitutive equation (ECE) $\\mathcal{E}$, expressed using an energy seminorm, and a quadratic error term $\\mathcal{D}$ incorporating the kinematical measurements. MECE-based identification are known from existing computational evidence to enjoy attractive properties such as", "orig": "improved convexity, robustness to resonant frequencies, and tolerance to incompletely specified boundary conditions (BCs). The main goal of this work is to develop theoretical foundations, in a continuous setting, allowing to explain and justify some of the aforementioned beneficial properties, in particular addressing the general case where BCs may be underspecified. A specific feature of MECE formulations is that forward and adjoint solutions are governed by a fully coupled system, whose mathematical properties play a fundamental role in the qualitative and computational aspects of MECE minimization. We prove that this system has a unique and stable solution at any frequency, provided data is abundant enough (", "generated": "robustness to noisy data, improved accuracy, and computational efficiency.\n\nMoreover, the MECE functional has been shown to be a promising approach for identifying heterogeneous materials properties in elastodynamics, as it takes into account both the kinematical and dynamical behavior of the system. The weighted combination of the ECE and quadratic error terms allows for balancing the importance of accurate constitutive modeling and reliable kinematical data. This flexibility in the formulation enables the user to adapt the identification process to the specific problem at hand, depending on the quality and availability of the experimental data.\n\nFurthermore, the MECE-based approach has been successfully applied to various problems in"}
{"full_text": "Gas behavior in systems at microscale has been receiving significant attention from researchers in the last two decades [1-4]. Today, there is an enhanced emphasis on developing new experimental techniques to capture the local temperature profiles in gases at rarefied conditions. The main underlying reason behind this focus is the interesting physics exhibited by gases at these rarefied conditions, especially in the transition regime. There is the onset of local thermodynamic disequilibrium, which manifests as velocity slip and temperature jump [1-4] at the wall. However, there is limited experimental evidence on understanding these aforementioned phenomena. With the advances in experimental facilities, it is today possible, at least in principle, to map the local temperature profiles in gases at rarefied conditions. Molecular tagging approach is one such technique which has shown the potential to map the temperature profile in low pressure conditions [5]. In molecular tagging approach, a very small percentage of tracer molecules are introduced into the gas of interest, referred as carrier gas. In gas flow studies, the typical tracers employed are acetone and biacetyl. These tracer molecules, assumed to be in equilibrium with the carrier gas, are excited with a source of energy at a specific wavelength, typically a laser. The excited molecules are unstable and tend to de-excite in a radiative and non-radiative manner, which is manifested as fluorescence and phosphorescence. Following the deformation with time of a tagged line permits to obtain the flow velocity. In addition, the dependence of the phosphorescence and fluorescence intensity to the gas temperature could also allow to use this technique for local temperature measurements. The objective of this study is to develop an experimental setup capable of simultaneously mapping the wall and fluid near-wall temperatures with the final goal to measure temperature jump at the wall when rarefied conditions are reached. The originality of this setup shown in Figure 1 is to couple surface temperature measurements using an infrared camera with Molecular Tagging Thermometry (MTT) for gas temperature measurements. The bottom wall of the channel will be made of Sapphire substrate of 650 $\\mu$m thickness coated with a thin film of Indium Tin Oxide (ITO). The average roughness of this ITO layer is about 3 nm. The top wall of the channel will be made of SU8 and bonded with the bottom wall with a layer of PDMS. The channel will be filled in with acetone vapor,", "prompt": "Gas behavior in systems at microscale has been receiving significant attention from researchers in the last two decades [1-4]. Today, there is an enhanced emphasis on developing new experimental techniques to capture the local temperature profiles in gases at rarefied conditions. The main underlying reason behind this focus is the interesting physics exhibited by gases at these rarefied conditions, especially in the transition regime. There is the onset of local thermodynamic disequilibrium, which manifests as velocity slip and temperature jump [1-4] at the wall. However, there is limited experimental evidence on understanding these aforementioned phenomena. With the advances in experimental facilities, it is", "orig": "today possible, at least in principle, to map the local temperature profiles in gases at rarefied conditions. Molecular tagging approach is one such technique which has shown the potential to map the temperature profile in low pressure conditions [5]. In molecular tagging approach, a very small percentage of tracer molecules are introduced into the gas of interest, referred as carrier gas. In gas flow studies, the typical tracers employed are acetone and biacetyl. These tracer molecules, assumed to be in equilibrium with the carrier gas, are excited with a source of energy at a specific wavelength, typically a laser. The excited molecules are unstable and tend to de", "generated": "now possible to design and develop novel diagnostic tools that can accurately measure the temperature and velocity distributions of gases in microscale systems. For instance, high-resolution microscopy techniques, such as laser-induced fluorescence (LIF) and molecular tagging velocimetry (MTV), have shown great promise in capturing the spatial and temporal variations of gas flows at the microscale. Moreover, advances in numerical simulations, particularly in the development of direct simulation Monte Carlo (DSMC) and lattice Boltzmann methods (LBM), have enabled researchers to model and predict the behavior of gases at rarefied conditions with greater accuracy. These experimental and numerical advancements have"}
{"full_text": "This short book is the result of various master and summer school courses I have taught. The objective is to introduce the readers to mathematical control theory, both in finite and infinite dimension. In the finite-dimensional context, we consider controlled ordinary differential equations (ODEs); in this context, existence and uniqueness issues are easily resolved thanks to the Picard-Lindel\\''of (Cauchy-Lipschitz) theorem. In infinite dimension, in view of dealing with controlled partial differential equations (PDEs), the concept of well-posed system is much more difficult and requires to develop a bunch of functional analysis tools, in particular semigroup theory -- and this, just for the setting in which the control system is written and makes sense. This is why I have splitted the book into two parts, the first being devoted to finite-dimensional control systems, and the second to infinite-dimensional ones. In spite of this splitting, it may be nice to learn basics of control theory for finite-dimensional linear autonomous control systems (e.g., the Kalman condition) and then to see in the second part how some results are extended to infinite dimension, where matrices are replaced by operators, and exponentials of matrices are replaced by semigroups. For instance, the reader will see how the Gramian controllability condition is expressed in infinite dimension, and leads to the celebrated Hilbert Uniqueness Method (HUM). Except the very last section, in the second part I have only considered linear autonomous control systems (the theory is already quite complicated), providing anyway several references to other textbooks for the several techniques existing to treat some particular classes of nonlinear PDEs. In contrast, in the first part on finite-dimensional control theory, there are much less difficulties to treat general nonlinear control systems, and I give here some general results on controllability, optimal control and stabilization. Of course, whether in finite or infinite dimension, there exist much finer results and methods in the literature, established however for specific classes of control systems. Here, my objective is to provide the reader with an introduction to control theory and to the main tools allowing to treat general control systems. I hope this will serve as motivation to go deeper into the theory or numerical aspects that are not covered here.", "prompt": "This short book is the result of various master and summer school courses I have taught. The objective is to introduce the readers to mathematical control theory, both in finite and infinite dimension. In the finite-dimensional context, we consider controlled ordinary differential equations (ODEs); in this context, existence and uniqueness issues are easily resolved thanks to the Picard-Lindel\\''of (Cauchy-Lipschitz) theorem. In infinite dimension, in view of dealing with controlled partial differential equations (PDEs), the concept of well-posed system is much more difficult and requires to develop a bunch of functional analysis tools, in particular sem", "orig": "igroup theory -- and this, just for the setting in which the control system is written and makes sense. This is why I have splitted the book into two parts, the first being devoted to finite-dimensional control systems, and the second to infinite-dimensional ones. In spite of this splitting, it may be nice to learn basics of control theory for finite-dimensional linear autonomous control systems (e.g., the Kalman condition) and then to see in the second part how some results are extended to infinite dimension, where matrices are replaced by operators, and exponentials of matrices are replaced by semigroups. For instance, the reader will see how the", "generated": "igroup theory, Sobolev spaces, and interpolation theory.\n\nAs we delve deeper into the realm of infinite-dimensional control systems, we encounter a multitude of challenges. The lack of a general existence and uniqueness theorem for nonlinear PDEs forces us to rely on more subtle and nuanced approaches. One such approach is the use of semigroup theory, which provides a powerful framework for analyzing the dynamics of infinite-dimensional systems. By exploiting the properties of semigroups, we can establish the well-posedness of certain classes of PDEs and develop effective control strategies.\n\nAnother crucial aspect of infinite-dimensional control theory is the study of Sobolev spaces"}
{"full_text": "In this article we address the first part of the programme presented in \\cite{Teleman_arXiv_III}, \\S 2; we construct the local $K$- theory level of the index formula. Our construction is sufficiently general to encompass the algebra of pseudo-differential operators of order zero on smooth manifolds, elliptic pseudo-differential operators of order zero, their abstract symbol (see Introduction \\S 2.) and their local $K$- theory analytical and topological index classes, see \\cite{Teleman_arXiv_III}, \\S 5, Definition 5 and 6. Our definitions are sufficiently general to apply to exact sequences of singular integral operators, which are of interest in the case of the index theorem on Lipschitz and quasi-conformal manifolds, see \\cite{Teleman_IHES}, \\cite{Teleman_Acta}, \\cite{Donaldson_Sullivan}, \\cite{Connes_Sullivan_Teleman}. In this article we introduce localised algebras (Definition 3) $\\mathit{A}$ and in \\S 6 we define their local algebraic $K$-theory. A localised algebra $\\mathit{A}$ is an algebra in which a decreasing filtration by vector sub-spaces $\\mathit{A}_{\\mu}$ is introduced. The filtration $\\mathit{A}_{\\mu}$ induces a filtration on the space of matrices $\\mathbb{M}(\\mathit{A}_{\\mu})$. Although we define solely $K^{loc}_{\\ast}(\\mathit{A})$ for $\\ast= 0, \\; 1$, we expect our construction could be extended in higher degrees. We stress that our construction of $K^{loc}_{0}(\\mathit{A})$ uses exclusively idempotent matrices and that the use of finite projective modules is totally avoided. (Idempotent matrices, rather than projective modules, contain less arbitrariness in the description of the $K_{0}$ classes and allow a better filtration control). The group $K^{loc}_{0}(\\mathit{A})$ is by definition the quotient space of the space of the Grothendieck completion of the space of idempotent matrices through three equivalence relations: -i) stabilisation $\\sim_{s}$, -2) local conjugation $\\sim_{l}$, {\\em and} -3) projective limit with respect to the filtration. By definition, the $K_{1}^{loc} (\\mathit{A})$ is the projective limit of the local $K_{1}(\\mathit{A}_{\\mu})$ groups. The group $K_{1}(\\mathit{A}_{\\mu})$ is by definition the quotient of $\\mathbb{GL}(\\mathit{A}_{\\mu})$ modulo the equivalence relation generated by: -1) stabilisation $\\sim_{s}$, --2) local conjugation $\\sim_{l}$ and -3) $\\sim_{\\mathbb{O}(\\mathit{A}_{\\mu})}$, where $\\mathbb{O}(\\mathit{A}_{\\mu})$ is the sub-module generated by elements of the form $ u \\oplus u^{-1} $, for any $u \\in \\mathbb{GL}(\\mathit{A}_{\\mu})$. The class of any invertible element $u$ modulo conjugation (inner auto-morphisms) we call the Jordan canonical form of $u$. The local conjugation preserves the local Jordan canonical form of invertible elements. The equivalence relation $\\sim_{\\mathbb{O}(\\mathit{A}_{\\mu})}$ insures existence of opposite elements in $K_{1}(\\mathit{A}_{\\mu})$ and $K_{1}^{loc}(\\mathit{A})$. Our definition of $K^{loc}_{1}(\\mathit{A})$ does not use the commutator sub-group $[\\mathbb{GL}(\\mathit{A}), \\mathbb{GL}(\\mathit{A})]$ nor elementary matrices in its construction. We define short exact sequences of localised algebras. To get the corresponding (open) six terms exact sequence (Theorem 51) one has to take the tensor product of the expected six terms exact sequence by $\\mathbb{Z}[\\frac{1}{2}]$. We expect the factor $\\otimes_{\\mathbb{Z}[\\frac{1}{2}}]$ to have important consequences. Our work shows that the basic structure of $K_{1}$ resides in the {\\em additive} sub-group generated by elements of the form $u \\oplus u^{-1}$, $u \\in \\mathbb{GL}(\\mathit{A})$, rather than in the {\\em multiplicativ} commutator sub-group $[\\mathbb{GL}(\\mathit{A}), \\mathbb{GL}(\\mathit{A})]$. Even into the case of trivially filtered algebras, $\\mathit{A}_{\\mu} = \\mathit{A}$, for all $\\mu \\in \\mathbb{N}$, the introduced group $K^{loc}_{1}(\\mathit{A})$ should provide more information than the classical group $K_{1}(\\mathit{A})$.", "prompt": "In this article we address the first part of the programme presented in \\cite{Teleman_arXiv_III}, \\S 2; we construct the local $K$- theory level of the index formula. Our construction is sufficiently general to encompass the algebra of pseudo-differential operators of order zero on smooth manifolds, elliptic pseudo-differential operators of order zero, their abstract symbol (see Introduction \\S 2.) and their local $K$- theory analytical and topological index classes, see \\cite{Teleman_arXiv_III}, \\S 5, Definition 5 and 6. Our", "orig": "definitions are sufficiently general to apply to exact sequences of singular integral operators, which are of interest in the case of the index theorem on Lipschitz and quasi-conformal manifolds, see \\cite{Teleman_IHES}, \\cite{Teleman_Acta}, \\cite{Donaldson_Sullivan}, \\cite{Connes_Sullivan_Teleman}. In this article we introduce localised algebras (Definition 3) $\\mathit{A}$ and in \\S 6 we define their local algebraic $K$-theory. A localised algebra $\\mathit{A}$ is an algebra in", "generated": "approach is based on the concept of \"nice\" subalgebras of the algebra of pseudo-differential operators, which allows us to define a family of topological invariants that can be used to compute the local $K$-theory index classes.\n\nIn particular, we show that the local $K$-theory index classes of elliptic pseudo-differential operators of order zero can be expressed in terms of the topological invariants of their abstract symbols. This result is a crucial step towards the proof of the index formula, as it provides a way to connect the analytical properties of the operators with their topological properties.\n\nMoreover, we"}
{"full_text": "The Gaussian graphical model, a popular paradigm for studying relationship among variables in a wide range of applications, has attracted great attention in recent years. This paper considers a fundamental question: When is it possible to estimate low-dimensional parameters at parametric square-root rate in a large Gaussian graphical model? A novel regression approach is proposed to obtain asymptotically efficient estimation of each entry of a precision matrix under a sparseness condition relative to the sample size. When the precision matrix is not sufficiently sparse, or equivalently the sample size is not sufficiently large, a lower bound is established to show that it is no longer possible to achieve the parametric rate in the estimation of each entry. This lower bound result, which provides an answer to the delicate sample size question, is established with a novel construction of a subset of sparse precision matrices in an application of Le Cam's lemma. Moreover, the proposed estimator is proven to have optimal convergence rate when the parametric rate cannot be achieved, under a minimal sample requirement. The proposed estimator is applied to test the presence of an edge in the Gaussian graphical model or to recover the support of the entire model, to obtain adaptive rate-optimal estimation of the entire precision matrix as measured by the matrix $\\ell_q$ operator norm and to make inference in latent variables in the graphical model. All of this is achieved under a sparsity condition on the precision matrix and a side condition on the range of its spectrum. This significantly relaxes the commonly imposed uniform signal strength condition on the precision matrix, irrepresentability condition on the Hessian tensor operator of the covariance matrix or the $\\ell_1$ constraint on the precision matrix. Numerical results confirm our theoretical findings. The ROC curve of the proposed algorithm, Asymptotic Normal Thresholding (ANT), for support recovery significantly outperforms that of the popular GLasso algorithm.", "prompt": "The Gaussian graphical model, a popular paradigm for studying relationship among variables in a wide range of applications, has attracted great attention in recent years. This paper considers a fundamental question: When is it possible to estimate low-dimensional parameters at parametric square-root rate in a large Gaussian graphical model? A novel regression approach is proposed to obtain asymptotically efficient estimation of each entry of a precision matrix under a sparseness condition relative to the sample size. When the precision matrix is not sufficiently sparse, or equivalently the sample size is not sufficiently large, a lower bound is established to show that it is no longer possible to achieve the parametric rate", "orig": "in the estimation of each entry. This lower bound result, which provides an answer to the delicate sample size question, is established with a novel construction of a subset of sparse precision matrices in an application of Le Cam's lemma. Moreover, the proposed estimator is proven to have optimal convergence rate when the parametric rate cannot be achieved, under a minimal sample requirement. The proposed estimator is applied to test the presence of an edge in the Gaussian graphical model or to recover the support of the entire model, to obtain adaptive rate-optimal estimation of the entire precision matrix as measured by the matrix $\\ell_q$ operator norm and to make inference in latent", "generated": ".\n\nThe proposed regression approach leverages the conditional independence structure of the Gaussian graphical model to reduce the dimensionality of the parameter space. By introducing a novel penalty term that encourages sparsity in the precision matrix, the approach is able to adapt to the underlying structure of the data and achieve optimal estimation rates. Theoretical guarantees are provided to establish the asymptotic efficiency of the proposed estimator, and extensive simulations demonstrate its superior performance compared to existing methods.\n\nFurthermore, the paper provides insights into the fundamental limits of estimation in large Gaussian graphical models. The lower bound established in the paper highlights the importance of considering the interplay between the sparsity of the precision"}
{"full_text": "Let $(\\{X_i(t)\\}_{i\\in \\mathbb{Z}^d})_{t\\geq 0}$ be the system of interacting diffusions on $[0,\\infty)$ defined by the following collection of coupled stochastic differential equations: \\begin{eqnarray}dX_i(t)=\\sum\\limits_{j\\in \\mathbb{Z}^d}a(i,j)[X_j(t)-X_i(t)] dt+\\sqrt{bX_i(t)^2} dW_i(t), \\eqntext{i\\in \\mathbb{Z}^d,t\\geq 0.}\\end{eqnarray} Here, $a(\\cdot,\\cdot)$ is an irreducible random walk transition kernel on $\\mathbb{Z}^d\\times \\mathbb{Z}^d$, $b\\in (0,\\infty)$ is a diffusion parameter, and $(\\{W_i(t)\\}_{i\\in \\mathbb{Z}^d})_{t\\geq 0}$ is a collection of independent standard Brownian motions on $\\mathbb{R}$. The initial condition is chosen such that $\\{X_i(0)\\}_{i\\in \\mathbb{Z}^d}$ is a shift-invariant and shift-ergodic random field on $[0,\\infty)$ with mean $\\Theta\\in (0,\\infty)$ (the evolution preserves the mean). We show that the long-time behavior of this system is the result of a delicate interplay between $a(\\cdot,\\cdot)$ and $b$, in contrast to systems where the diffusion function is subquadratic. In particular, let $\\hat{a}(i,j)={1/2}[a(i,j)+a(j,i)]$, $i,j\\in \\mathbb{Z}^d$, denote the symmetrized transition kernel. We show that: (A) If $\\hat{a}(\\cdot,\\cdot)$ is recurrent, then for any $b>0$ the system locally dies out. (B) If $\\hat{a}(\\cdot,\\cdot)$ is transient, then there exist $b_*\\geq b_2>0$ such that: (B1)d The system converges to an equilibrium $\\nu_{\\Theta}$ (with mean $\\Theta$) if $0<b<b_*$. (B2) The system locally dies out if $b>b_*$. (B3) $\\nu_{\\Theta}$ has a finite 2nd moment if and only if $0<b<b_2$. (B4) The 2nd moment diverges exponentially fast if and only if $b>b_2$. The equilibrium $\\nu_{\\Theta}$ is shown to be associated and mixing for all $0<b<b_*$. We argue in favor of the conjecture that $b_*>b_2$. We further conjecture that the system locally dies out at $b=b_*$. For the case where $a(\\cdot,\\cdot)$ is symmetric and transient we further show that: (C) There exists a sequence $b_2\\geq b_3\\geq b_4\\geq ... >0$ such that: (C1) $\\nu_{\\Theta}$ has a finite $m$th moment if and only if $0<b<b_m$. (C2) The $m$th moment diverges exponentially fast if and only if $b>b_m$. (C3) $b_2\\leq (m-1)b_m<2$. uad(C4) $\\lim_{m\\to\\infty}(m-1)b_m=c=\\sup_{m\\geq 2}(m-1)b_m$. The proof of these results is based on self-duality and on a representation formula through which the moments of the components are related to exponential moments of the collision local time of random walks. Via large deviation theory, the latter lead to variational expressions for $b_*$ and the $b_m$'s, from which sharp bounds are deduced. The critical value $b_*$ arises from a stochastic representation of the Palm distribution of the system. The special case where $a(\\cdot,\\cdot)$ is simple random walk is commonly referred to as the parabolic Anderson model with Brownian noise. This case was studied in the memoir by Carmona and Molchanov [Parabolic Anderson Problem and Intermittency (1994) Amer. Math. Soc., Providence, RI], where part of our results were already established.", "prompt": "Let $(\\{X_i(t)\\}_{i\\in \\mathbb{Z}^d})_{t\\geq 0}$ be the system of interacting diffusions on $[0,\\infty)$ defined by the following collection of coupled stochastic differential equations: \\begin{eqnarray}dX_i(t)=\\sum\\limits_{j\\in \\mathbb{Z}^d}a(i,j)[X_j(t)-X_i(t)] dt+\\sqrt{bX_i(t)^2} dW_i(t), \\eqntext{i\\in \\mathbb{Z}^d,t", "orig": "\\geq 0.}\\end{eqnarray} Here, $a(\\cdot,\\cdot)$ is an irreducible random walk transition kernel on $\\mathbb{Z}^d\\times \\mathbb{Z}^d$, $b\\in (0,\\infty)$ is a diffusion parameter, and $(\\{W_i(t)\\}_{i\\in \\mathbb{Z}^d})_{t\\geq 0}$ is a collection of independent standard Brownian motions on $\\mathbb{R}$. The initial condition is chosen such that $\\{X_i(0)\\}_{i\\", "generated": "Here, we assume that $a(i,j)\\geq 0$ is some symmetric function that satisfies $\\sum_{j\\in \\mathbb{Z}^d}a(i,j)=1$ for all $i\\in \\mathbb{Z}^d$, and $b>0$ is some constant. The collection of independent Brownian motions is denoted by $(W_i(t))_{i\\in \\mathbb{Z}^d,t\\geq 0}$. We consider a natural initial condition $X_i(0)=x_i\\geq 0$ for all $i"}
{"full_text": "We build in this paper a numerical solution procedure to compute the flow induced by a spherical flame expanding from a point source at a constant expansion velocity, with an instantaneous chemical reaction. The solution is supposed to be self-similar and the flow is split in three zones: an inner zone composed of burnt gases at rest, an intermediate zone where the solution is regular and the initial atmosphere composed of fresh gases at rest. The intermediate zone is bounded by the reactive shock (inner side) and the so-called precursor shock (outer side), for which Rankine-Hugoniot conditions are written; the solution in this zone is governed by two ordinary differential equations which are solved numerically. We show that, for any admissible precursor shock speed, the construction combining this numerical resolution with the exploitation of jump conditions is unique, and yields decreasing pressure, density and velocity profiles in the intermediate zone. In addition, the reactive shock speed is larger than the velocity on the outer side of the shock, which is consistent with the fact that the difference of these two quantities is the so-called flame velocity, i.e. the (relative) velocity at which the chemical reaction progresses in the fresh gases. Finally, we also observe numerically that the function giving the flame velocity as a function of the precursor shock speed is increasing; this allows to embed the resolution in a Newton-like procedure to compute the flow for a given flame speed (instead of for a given precursor shock speed). The resulting numerical algorithm is applied to stoichiometric hydrogen-air mixtures. Key words. spherical flames, reactive Euler equations, Riemann problems burnt zone (constant state) intermediate zone (regular solution) unburnt zone (constant initial state) W b W 2 W 1 W 0 reactive shock, r = $\\sigma$ r t. precursor shock, r = $\\sigma$ p t. W = ($\\rho$, u, p): local fluid state. Fig. 1.1. Structure of the solution. 1. Problem position. We address the flame propagation in a reactive infinite atmosphere of initial constant composition. The ignition is supposed to occur at a single point (chosen to be the origin of R 3) and the flow is supposed to satisfy a spherical symmetry property: the density $\\rho$, the pressure p, the internal energy e and the entropy s only depend on the distance r to the origin and the velocity reads u = ur/r, where r stands for the position vector. The flame is supposed to be infinitely thin and to move at a constant speed. The flow is governed by the Euler equations, and we seek a solution with the following structure:", "prompt": "We build in this paper a numerical solution procedure to compute the flow induced by a spherical flame expanding from a point source at a constant expansion velocity, with an instantaneous chemical reaction. The solution is supposed to be self-similar and the flow is split in three zones: an inner zone composed of burnt gases at rest, an intermediate zone where the solution is regular and the initial atmosphere composed of fresh gases at rest. The intermediate zone is bounded by the reactive shock (inner side) and the so-called precursor shock (outer side), for which Rankine-Hugoniot conditions are written; the solution in this zone is governed by two ordinary differential", "orig": "equations which are solved numerically. We show that, for any admissible precursor shock speed, the construction combining this numerical resolution with the exploitation of jump conditions is unique, and yields decreasing pressure, density and velocity profiles in the intermediate zone. In addition, the reactive shock speed is larger than the velocity on the outer side of the shock, which is consistent with the fact that the difference of these two quantities is the so-called flame velocity, i.e. the (relative) velocity at which the chemical reaction progresses in the fresh gases. Finally, we also observe numerically that the function giving the flame velocity as a function of the precursor shock", "generated": "equations, which are derived from the conservation laws of mass, momentum, and energy.\n\nThe numerical solution is obtained using a finite difference method, where the spatial domain is discretized into a finite number of points and the time derivatives are approximated using a second-order accurate scheme. The resulting algebraic system is then solved iteratively using a Newton-Raphson method. To validate the numerical solution, we compare it with available analytical solutions for specific cases, such as the Taylor-Sedov solution for the blast wave problem, and with experimental data available in the literature.\n\nOne of the main challenges in solving this problem is the treatment of the singular"}
{"full_text": "Atmospheric composition measurements taken at many high-altitude stations around the world, aim to collect data representative of the free troposphere and of an intercontinental scale. However, the high-altitude environment favours vertical mixing and the transportation of air masses at local or regional scales, which has a potential influence on the composition of the sampled air masses. Mixing processes, source-receptor pathways, and atmospheric chemistry may strongly depend on local and regional weather regimes, and these should be characterized specifically for each station. The Pic du Midi (PDM) isa mountaintop observatory (2850 m a.s.l.) on the north side of the Pyrenees. PDM is associated with the Centre de Recherches Atmosph{\\'e}riques (CRA), a site in the foothills ar 600 m a.s.l. 28 km north-east of the PDM. The two centers make up the Pyrenean Platform for the Observation of the Atmosphere (P2OA). Data measured at PDM and CRA were combined to form a5-year hourly dataset of 23 meteorological variables notably: temperature, humidity, cloud cover, wind at several altitudes. The dataset was classified using hierarchical clustering, with the aim of grouping together the days which had similar meteorological characteristics. To complete the clustering, we computed several diagnostic tools, in order to provide additional information and study specific phenomena (foehn, precipitation, atmospheric vertical structure, and thermally driven circulations). This classification resulted in six clusters: three highly populated clusters which correspond to the most frequent meteorological conditions (fair weather, mixed weather and disturbed weather, respectively); a small cluster evidencing clear characteristics of winter northwesterly windstorms; and two small clusters characteristic of south foehn (south- to southwesterly large-scaleflow, associated with warm and dry downslope flow on the lee side of the chain). The diagnostic tools applied to the six clusters provided results in line with the conclusions tentatively drawn from 23 meteorological variables. This, to some extent,validates the approach of hierarchical clustering of local data to distinguish weather regimes. Then statistics of atmospheric composition at PDM were analysed and discussed for each cluster. Radon measurements, notably, revealed that the regional background in the lower troposphere dominates the influence of diurnal thermal flows when daily averaged concentrations are considered. Differences between clusters were demonstrated by the anomalies of CO, CO$_2$, CH$_4$, O$_3$ and aerosol number concentration, and interpretations in relation with chemical sinks and sources are proposed.", "prompt": "Atmospheric composition measurements taken at many high-altitude stations around the world, aim to collect data representative of the free troposphere and of an intercontinental scale. However, the high-altitude environment favours vertical mixing and the transportation of air masses at local or regional scales, which has a potential influence on the composition of the sampled air masses. Mixing processes, source-receptor pathways, and atmospheric chemistry may strongly depend on local and regional weather regimes, and these should be characterized specifically for each station. The Pic du Midi (PDM) isa mountaintop observatory (2850 m a.s.l.) on the north side of", "orig": "the Pyrenees. PDM is associated with the Centre de Recherches Atmosph{\\'e}riques (CRA), a site in the foothills ar 600 m a.s.l. 28 km north-east of the PDM. The two centers make up the Pyrenean Platform for the Observation of the Atmosphere (P2OA). Data measured at PDM and CRA were combined to form a5-year hourly dataset of 23 meteorological variables notably: temperature, humidity, cloud cover, wind at several altitudes. The dataset was classified using hierarchical clustering, with the aim of grouping together the days which", "generated": "the Pyrenees mountains, and its unique location makes it an ideal site for studying the long-range transport of atmospheric pollutants and the impact of climate change on atmospheric composition.\n\nSituated in the vicinity of the French-Spanish border, PDM is influenced by a variety of weather patterns, including Atlantic westerlies, Mediterranean cyclones, and polar cold fronts. The observatory's high altitude and proximity to the Atlantic Ocean also make it susceptible to the uptake of marine aerosols and the transport of pollutants from North America and Europe. As a result, PDM offers a unique opportunity to investigate the complex interplay between large-scale atmospheric circulation"}
{"full_text": "A Boolean logic driven Markov process (BDMP) is a dependability analysis model that defines a continuous-time Markov chain (CTMC). This formalism has high expressive power, yet it remains readable because its graphical representation stays close to standard fault trees. The size of a BDMP is roughly speaking proportional to the size of the system it models, whereas the size of the CTMC specified by this BDMP suffers from exponential growth. Thus quantifying large BDMPs can be a challenging task. The most general method to quantify them is Monte Carlo simulation, but this may be intractable for highly reliable systems. On the other hand, some subcategories of BDMPs can be processed with much more efficient methods. For example, BDMPs without repairs can be translated into dynamic fault trees, a formalism accepted as an input of the STORM model checker, that performs numerical calculations on sparse matrices, or they can be processed with the tool FIGSEQ that explores paths going to a failure state and calculates their probabilities. BDMPs with repairs can be quantified by FIGSEQ (BDMPs capturing quickly and completely repairable behaviors are solved by a different algorithm), and by the I&AB (Initiator and All Barriers) method, recently published and implemented in a prototype version of RISKSPECTRUM PSA. This tool, based exclusively on Boolean representations looks for and quantifies minimal cut sets of the system, i.e., minimal combinations of component failures that induce the loss of the system. This allows a quick quantification of large models with repairable components, standby redundancies and some other types of dependencies between omponents. All these quantification methods have been tried on a benchmark whose definition was published at the MARS 2017 workshop: the model of emergency power supplies of a nuclear power plant. In this paper, after a recall of the theoretical principles of the various quantification methods, we compare their performances on that benchmark.", "prompt": "A Boolean logic driven Markov process (BDMP) is a dependability analysis model that defines a continuous-time Markov chain (CTMC). This formalism has high expressive power, yet it remains readable because its graphical representation stays close to standard fault trees. The size of a BDMP is roughly speaking proportional to the size of the system it models, whereas the size of the CTMC specified by this BDMP suffers from exponential growth. Thus quantifying large BDMPs can be a challenging task. The most general method to quantify them is Monte Carlo simulation, but this may be intractable for highly reliable systems. On the other", "orig": "hand, some subcategories of BDMPs can be processed with much more efficient methods. For example, BDMPs without repairs can be translated into dynamic fault trees, a formalism accepted as an input of the STORM model checker, that performs numerical calculations on sparse matrices, or they can be processed with the tool FIGSEQ that explores paths going to a failure state and calculates their probabilities. BDMPs with repairs can be quantified by FIGSEQ (BDMPs capturing quickly and completely repairable behaviors are solved by a different algorithm), and by the I&AB (Initiator and All Barriers) method, recently published and", "generated": "hand, exact methods, such as the state-space exploration approach, can be computationally expensive and impractical for large systems.\n\nOne possible solution to this problem is to use approximate methods that can efficiently quantify large BDMPs with reasonable accuracy. These methods include, for example, the use of bounding techniques, which provide upper and lower bounds on the reliability measures, and surrogate models, which approximate the complex behavior of the system using simpler models. Another approach is to use the concept of hierarchical modeling, where the system is decomposed into smaller sub-systems, each of which can be modeled and quantified separately. This allows for a more efficient"}
{"full_text": "Context: Software development tools that interact with running programs such as debuggers, profilers, and dynamic analysis frameworks are presumed to demand difficult tradeoffs among implementation complexity (cost), functionality, usability, and performance. Among the many consequences, tools are often delivered late (if ever), have limited functionality, require non-standard configurations, and impose serious performance costs on running programs. Inquiry: Can flexible tool support become a practical, first class, intrinsic requirement for a modern highperformance programming language implementation framework? Approach: We extended the Truffle Language Implementation Framework, which together with the GraalVM execution environment makes possible very high performance language implementations. Truffle's new Instrumentation Framework is language-agnostic and designed to derive high performance from the same technologies as do language implementations. Truffle Instrumentation includes: (1) low overhead capture of execution events by dynamically adding \"wrapper\" nodes to executing ASTs; (2) extensions to the Language Implementation Framework that allow per-language specialization, primarily for visual display of values and names, among others; and (3) versatile APIs and support services for implementing many kinds of tools without VM modification. Knowledge: It is now possible for a client in a production environment to insert (dynamically, with thread safety) an instrumentation probe that captures and reports abstractly specified execution events. A probe in fully optimized code imposes very low overhead until actually used to access (or modify) execution state. Event capture has enabled construction of numerous GraalVM services and tools that work for all implemented languages, either singly or in combination. Instrumentation has also proved valuable for implementing some traditionally tricky language features, as well as some GraalVM services such as placing bounds on resources consumed by running programs. Grounding: Tools for debugging (via multiple clients), profiling, statement counting, dynamic analysis, and others are now part of GraalVM or are in active development. Third parties have also used Truffle Instrumentation for innovative tool implementations. Importance: Experience with Truffle Instrumentation validates the notion that addressing developer tools support as a forethought can change expectations about the availability of practical, efficient tools for high-performance languages. Tool development becomes a natural part of language implementation, requiring little additional effort and offering the advantage of early and continuous availability.", "prompt": "Context: Software development tools that interact with running programs such as debuggers, profilers, and dynamic analysis frameworks are presumed to demand difficult tradeoffs among implementation complexity (cost), functionality, usability, and performance. Among the many consequences, tools are often delivered late (if ever), have limited functionality, require non-standard configurations, and impose serious performance costs on running programs. Inquiry: Can flexible tool support become a practical, first class, intrinsic requirement for a modern highperformance programming language implementation framework? Approach: We extended the Truffle Language Implementation Framework, which together with the GraalVM execution environment makes possible very high performance language implementations. Tr", "orig": "uffle's new Instrumentation Framework is language-agnostic and designed to derive high performance from the same technologies as do language implementations. Truffle Instrumentation includes: (1) low overhead capture of execution events by dynamically adding \"wrapper\" nodes to executing ASTs; (2) extensions to the Language Implementation Framework that allow per-language specialization, primarily for visual display of values and names, among others; and (3) versatile APIs and support services for implementing many kinds of tools without VM modification. Knowledge: It is now possible for a client in a production environment to insert (dynamically, with thread safety) an instrumentation probe that captures and", "generated": "uffle's architecture, centered around the concept of abstract syntax trees (ASTs), allows for the creation of high-performance language implementations with a focus on dynamic languages.\n\nWe leveraged Truffle's modular design to incorporate a novel tooling infrastructure that integrates seamlessly with the language implementation framework. This infrastructure enables the creation of flexible, high-performance tools that can be easily composed and customized to suit specific use cases. By doing so, we demonstrated that it is possible to provide flexible tool support as a first-class, intrinsic requirement for modern high-performance programming language implementation frameworks. Our approach eliminates the need for tradeoffs between implementation complexity, functionality, usability, and"}
{"full_text": "This paper demonstrates the usefulness of distributed local verification of proofs, as a tool for the design of self-stabilizing algorithms.In particular, it introduces a somewhat generalized notion of distributed local proofs, and utilizes it for improving the time complexity significantly, while maintaining space optimality. As a result, we show that optimizing the memory size carries at most a small cost in terms of time, in the context of Minimum Spanning Tree (MST). That is, we present algorithms that are both time and space efficient for both constructing an MST and for verifying it.This involves several parts that may be considered contributions in themselves.First, we generalize the notion of local proofs, trading off the time complexity for memory efficiency. This adds a dimension to the study of distributed local proofs, which has been gaining attention recently. Specifically, we design a (self-stabilizing) proof labeling scheme which is memory optimal (i.e., $O(\\log n)$ bits per node), and whose time complexity is $O(\\log ^2 n)$ in synchronous networks, or $O(\\Delta \\log ^3 n)$ time in asynchronous ones, where $\\Delta$ is the maximum degree of nodes. This answers an open problem posed by Awerbuch and Varghese (FOCS 1991). We also show that $\\Omega(\\log n)$ time is necessary, even in synchronous networks. Another property is that if $f$ faults occurred, then, within the requireddetection time above, they are detected by some node in the $O(f\\log n)$ locality of each of the faults.Second, we show how to enhance a known transformer that makes input/output algorithms self-stabilizing. It now takes as input an efficient construction algorithm and an efficient self-stabilizing proof labeling scheme, and produces an efficient self-stabilizing algorithm. When used for MST, the transformer produces a memory optimal self-stabilizing algorithm, whose time complexity, namely, $O(n)$, is significantly better even than that of previous algorithms. (The time complexity of previous MST algorithms that used $\\Omega(\\log^2 n)$ memory bits per node was $O(n^2)$, and the time for optimal space algorithms was $O(n|E|)$.) Inherited from our proof labelling scheme, our self-stabilising MST construction algorithm also has the following two properties: (1) if faults occur after the construction ended, then they are detected by some nodes within $O(\\log ^2 n)$ time in synchronous networks, or within $O(\\Delta \\log ^3 n)$ time in asynchronous ones, and (2) if $f$ faults occurred, then, within the required detection time above, they are detected within the $O(f\\log n)$ locality of each of the faults. We also show how to improve the above two properties, at the expense of some increase in the memory.", "prompt": "This paper demonstrates the usefulness of distributed local verification of proofs, as a tool for the design of self-stabilizing algorithms.In particular, it introduces a somewhat generalized notion of distributed local proofs, and utilizes it for improving the time complexity significantly, while maintaining space optimality. As a result, we show that optimizing the memory size carries at most a small cost in terms of time, in the context of Minimum Spanning Tree (MST). That is, we present algorithms that are both time and space efficient for both constructing an MST and for verifying it.This involves several parts that may be considered contributions in themselves.First, we generalize the notion of", "orig": "local proofs, trading off the time complexity for memory efficiency. This adds a dimension to the study of distributed local proofs, which has been gaining attention recently. Specifically, we design a (self-stabilizing) proof labeling scheme which is memory optimal (i.e., $O(\\log n)$ bits per node), and whose time complexity is $O(\\log ^2 n)$ in synchronous networks, or $O(\\Delta \\log ^3 n)$ time in asynchronous ones, where $\\Delta$ is the maximum degree of nodes. This answers an open problem posed by Awerbuch and Varghese (FOCS 1991", "generated": "distributed local proofs, extending the concept to accommodate a broader range of verification tasks.\n\nSpecifically, we introduce a novel framework that enables the design of distributed algorithms that can efficiently verify the correctness of local computations, without requiring excessive communication or storage overhead. This framework is particularly useful in the context of MST construction, where the verification of local computations is crucial for ensuring the correctness of the overall algorithm. By leveraging this framework, we are able to develop algorithms that can efficiently construct and verify MSTs, while minimizing both the time and space complexity.\n\nOne of the key insights of our approach is that the cost of verification can be significantly reduced by distributing the"}
{"full_text": "As the nearest known AGB star (d=64pc) and one of the brightest (mK-2), L2 Pup is a particularly interesting benchmark object to monitor the final stages of stellar evolution. We report new lucky imaging observations of this star with the VLT/NACO adaptive optics system in twelve narrow band filters covering the 1.0-4.0 microns wavelength range. These diffraction limited images reveal an extended circumstellar dust lane in front of the star, that exhibits a high opacity in the J band and becomes translucent in the H and K bands. In the L band, extended thermal emission from the dust is detected. We reproduce these observations using Monte-Carlo radiative transfer modeling of a dust disk with the RADMC-3D code. We also present new interferometric observations with the VLTI/VINCI and MIDI instruments. We measure in the K band an upper limit to the limb-darkened angular diameter of theta_LD = 17.9 +/- 1.6 mas, converting to a maximum linear radius of R = 123 +/- 14 Rsun. Considering the geometry of the extended K band emission in the NACO images, this upper limit is probably close to the actual angular diameter of the star. The position of L2 Pup in the Herzsprung-Russell diagram indicates that this star has a mass around 2 Msun and is probably experiencing an early stage of the asymptotic giant branch. We do not detect any stellar companion of L2 Pup in our adaptive optics and interferometric observations, and we attribute its apparent astrometric wobble in the Hipparcos data to variable lighting effects on its circumstellar material. We however do not exclude the presence of a binary companion, as the large loop structure extending to more than 10 AU to the North-East of the disk in our L band images may be the result of interaction between the stellar wind of L2 Pup and a hidden secondary object. The geometric configuration that we propose, with a large dust disk seen almost edge-on, appears particularly favorable to test and develop our understanding of the formation of bipolar nebulae.", "prompt": "As the nearest known AGB star (d=64pc) and one of the brightest (mK-2), L2 Pup is a particularly interesting benchmark object to monitor the final stages of stellar evolution. We report new lucky imaging observations of this star with the VLT/NACO adaptive optics system in twelve narrow band filters covering the 1.0-4.0 microns wavelength range. These diffraction limited images reveal an extended circumstellar dust lane in front of the star, that exhibits a high opacity in the J band and becomes translucent in the H and K bands. In the L band, extended thermal emission from", "orig": "the dust is detected. We reproduce these observations using Monte-Carlo radiative transfer modeling of a dust disk with the RADMC-3D code. We also present new interferometric observations with the VLTI/VINCI and MIDI instruments. We measure in the K band an upper limit to the limb-darkened angular diameter of theta_LD = 17.9 +/- 1.6 mas, converting to a maximum linear radius of R = 123 +/- 14 Rsun. Considering the geometry of the extended K band emission in the NACO images, this upper limit is probably close to the actual angular diameter of the star. The", "generated": "the dust is detected, indicating a high dust temperature in this region.\n\nThe presence of this circumstellar dust lane suggests that L2 Pup is undergoing significant mass loss, which is a characteristic trait of AGB stars in their final stages of evolution. The high opacity in the J band is consistent with the presence of small, cool dust grains, while the translucency in the H and K bands indicates that these grains are not evenly distributed around the star. The thermal emission detected in the L band implies that the dust is heated by the star's radiation, and that the dust temperature is likely to be in excess of 1000 K"}
{"full_text": "Ingram Olkin was born on July 23, 1924 in Waterbury, Connecticut. His family moved to New York in 1934 and he graduated from DeWitt Clinton High School in 1941. He served three years in the Air Force during World War II and obtained a B.S. in mathematics at the City College of New York in 1947. After receiving an M.A. in mathematical statistics from Columbia in 1949, he completed his graduate studies in the Department of Statistics at the University of North Carolina in 1951. His dissertation was written under the direction of S. N. Roy and Harold Hotelling. He joined the Department of Mathematics at Michigan State University in 1951 as an Assistant Professor, subsequently being promoted to Professor. In 1960, he took a position as Chair of the Department of Statistics at the University of Minnesota. He moved to Stanford University in 1961 to take a joint position as Professor of Statistics and Professor of Education; he was also Chair of the Department of Statistics from 1973--1976. In 2007, Ingram became Professor Emeritus. Ingram was Editor of the Annals of Mathematical Statistics (1971--1972) and served as the first editor of the Annals of Statistics from 1972--1974. He was a primary force in the founding of the Journal of Educational Statistics, for which he was also Associate Editor during 1977--1985. In 1984, he was President of the Institute of Mathematical Statistics. Among his many professional activities, he has served as Chair of the Committee of Presidents of Statistical Societies (COPSS), Chair of the Committee on Applied and Theoretical Statistics of the National Research Council, Chair of the Management Board of the American Education Research Association, and as Trustee for the National Institute of Statistical Sciences. He has been honored by the American Statistical Association (ASA) with a Wilks Medal (1992) and a Founder's Award (1992). The American Psychological Association gave him a Lifetime Contribution Award (1997) and he was elected to the National Academy of Education in 2005. He received the COPSS Elizabeth L. Scott Award in 1998 and delivered the R. A. Fisher Lecture in 2000. In 2003, the City University of New York gave him a Townsend Harris Medal. An author of 5 books, an editor of 10 books, and an author of more than 200 publications, Ingram has made major contributions to statistics and education. His research has focused on multivariate analysis, majorization and inequalities, distribution theory, and meta-analysis. A volume in celebration of Ingram's 65th birthday contains a brief biography and an interview [Gleser, Perlman, Press and Sampson (1989)]. Ingram was chosen in 1997 to participate in the American Statistical Association Distinguished Statistician Video Series and a videotaped conversation and a lecture (Olkin, 1997) are available from the ASA (1997, DS041, DS042).", "prompt": "Ingram Olkin was born on July 23, 1924 in Waterbury, Connecticut. His family moved to New York in 1934 and he graduated from DeWitt Clinton High School in 1941. He served three years in the Air Force during World War II and obtained a B.S. in mathematics at the City College of New York in 1947. After receiving an M.A. in mathematical statistics from Columbia in 1949, he completed his graduate studies in the Department of Statistics at the University of North Carolina in 1951. His dissertation was written under the direction of S. N. Roy and", "orig": "Harold Hotelling. He joined the Department of Mathematics at Michigan State University in 1951 as an Assistant Professor, subsequently being promoted to Professor. In 1960, he took a position as Chair of the Department of Statistics at the University of Minnesota. He moved to Stanford University in 1961 to take a joint position as Professor of Statistics and Professor of Education; he was also Chair of the Department of Statistics from 1973--1976. In 2007, Ingram became Professor Emeritus. Ingram was Editor of the Annals of Mathematical Statistics (1971--1972) and served as the first editor of the Ann", "generated": "focused on the topic of multivariate analysis.\n\nOlkin's academic career took off shortly after completing his Ph.D. He held positions at the University of Minnesota and Michigan State University before moving to Stanford University in 1961, where he spent the majority of his career. During his time at Stanford, Olkin made significant contributions to the field of statistics, publishing numerous papers on topics such as multivariate analysis, non-parametric statistics, and stochastic processes. He was particularly known for his work on the theory of majorization, which has had a lasting impact on the field.\n\nIn addition to his academic pursuits, Olkin was also an accomplished"}
{"full_text": "Bromochloro alkanes (BCAs) have been manufactured for use as flame retardants for decades and preliminary environmental risk screening suggests they are likely to behave similarly to polychlorinated alkanes (PCAs), subclasses of which are restricted as Stockholm Convention Persistent Organic Pollutants (POPs). BCAs have rarely been studied in the environment, though some evidence suggests they may migrate from treated-consumer materials into indoor dust, resulting in human exposure via inadvertent ingestion. In this study, BCA-C14 mixture standards were synthesized and used to validate an analytical method. This method relies on chloride-enhanced liquid chromatography-electrospray ionization-Orbitrap-high resolution mass spectrometry (LC-ESI-Orbitrap-HRMS) and a novel CP-Seeker integration software package for homologue detection and integration. Dust sample preparation via ultrasonic extraction, acidified silica clean-up and fractionation on neutral silica cartridges was found to be suitable for BCAs, with absolute recovery of individual homologues averaging 66 to 78% and coefficients of variation $\\le$10% in replicated spiking experiments (n=3). In addition, a total of 59 indoor dust samples from six countries including Australia (n=10), Belgium (n=10), Colombia (n=10), Japan (n=10), Thailand (n=10) and the United States of America (n=9) were analysed for BCAs. BCAs were detected in seven samples from the USA, with carbon chain lengths of C8, C10, C12, C14, C16, C18, C24 to C28, C30 and C31 observed overall, though not detected in samples from any other countries. Bromination of detected homologues in the indoor dust samples ranged from Br1-4 as well as Br7, while chlorine numbers ranged from Cl2-11. BCA-C18 were the most frequently detected, observed in each of the USA samples, while the most prevalent halogenation degrees were homologues of Br2 and Cl4-5. Broad estimations of BCA concentrations in the dust samples indicated that levels may approach those of other flame retardants in at least some instances. These findings suggest that development of quantification strategies and further investigation of environmental occurrence and health implications are needed.", "prompt": "Bromochloro alkanes (BCAs) have been manufactured for use as flame retardants for decades and preliminary environmental risk screening suggests they are likely to behave similarly to polychlorinated alkanes (PCAs), subclasses of which are restricted as Stockholm Convention Persistent Organic Pollutants (POPs). BCAs have rarely been studied in the environment, though some evidence suggests they may migrate from treated-consumer materials into indoor dust, resulting in human exposure via inadvertent ingestion. In this study, BCA-C14 mixture standards were synthesized and used to validate an analytical method. This method relies on chloride-enhanced liquid chromat", "orig": "ography-electrospray ionization-Orbitrap-high resolution mass spectrometry (LC-ESI-Orbitrap-HRMS) and a novel CP-Seeker integration software package for homologue detection and integration. Dust sample preparation via ultrasonic extraction, acidified silica clean-up and fractionation on neutral silica cartridges was found to be suitable for BCAs, with absolute recovery of individual homologues averaging 66 to 78% and coefficients of variation $\\le$10% in replicated spiking experiments (n=3). In addition, a total of 59 indoor dust samples from six countries including Australia (n=", "generated": "ography tandem mass spectrometry (Cl-ELC-MS/MS) to detect and quantify BCA congeners in environmental samples.\n\nThe developed method was applied to analyze indoor dust samples collected from various locations in the United States. The results showed that BCAs were detectable in all samples, with concentrations ranging from 10 to 1000 ng/g. The most abundant congener was BCA-C12, which accounted for more than 50% of the total BCA concentration in most samples. The distribution of BCA congeners varied among samples, suggesting that the migration of BCAs from treated materials into indoor dust is influenced"}
{"full_text": "The equioscillation theorem interleaves the Haar condition, the existence and uniqueness and strong uniqueness of the optimal Chebyshev approximation and its characterization by the equioscillation condition in a way that cannot extend to multivariate approximation: Rice~[\\emph{Transaction of the AMS}, 1963] says ''A form of alternation is still present for functions of several variables. However, there is apparently no simple method of distinguishing between the alternation of a best approximation and the alternation of other approximating functions. This is due to the fact that there is no natural ordering of the critical points.'' In addition, in the context of multivariate approximation the Haar condition is typically not satisfied and strong uniqueness may hold or not. The present paper proposes an multivariate equioscillation theorem, which includes such a simple alternation condition on error extrema, existence and a sufficient condition for strong uniqueness. To this end, the relationship between the properties interleaved in the univariate equioscillation theorem is clarified: first, a weak Haar condition is proposed, which simply implies existence. Second, the equioscillation condition is shown to be equivalent to the optimality condition of convex optimization, hence characterizing optimality independently from uniqueness. It is reformulated as the synchronized oscillations between the error extrema and the components of a related Haar matrix kernel vector, in a way that applies to multivariate approximation. Third, an additional requirement on the involved Haar matrix and its kernel vector, called strong equioscillation, is proved to be sufficient for the strong uniqueness of the solution. These three disconnected conditions give rise to a multivariate equioscillation theorem, where existence, characterization by equioscillation and strong uniqueness are separated, without involving the too restrictive Haar condition. Remarkably, relying on optimality condition of convex optimization gives rise to a quite simple proof. Instances of multivariate problems with strongly unique, non-strong but unique and non-unique solutions are presented to illustrate the scope of the theorem.", "prompt": "The equioscillation theorem interleaves the Haar condition, the existence and uniqueness and strong uniqueness of the optimal Chebyshev approximation and its characterization by the equioscillation condition in a way that cannot extend to multivariate approximation: Rice~[\\emph{Transaction of the AMS}, 1963] says ''A form of alternation is still present for functions of several variables. However, there is apparently no simple method of distinguishing between the alternation of a best approximation and the alternation of other approximating functions. This is due to the fact that there is no natural ordering of the critical points.'' In addition,", "orig": "in the context of multivariate approximation the Haar condition is typically not satisfied and strong uniqueness may hold or not. The present paper proposes an multivariate equioscillation theorem, which includes such a simple alternation condition on error extrema, existence and a sufficient condition for strong uniqueness. To this end, the relationship between the properties interleaved in the univariate equioscillation theorem is clarified: first, a weak Haar condition is proposed, which simply implies existence. Second, the equioscillation condition is shown to be equivalent to the optimality condition of convex optimization, hence characterizing optimality independently from uniqueness. It", "generated": "this limitation is further complicated by the fact that the notion of \"best\" approximation becomes increasingly ambiguous in higher dimensions. In the univariate case, the Chebyshev norm provides a natural metric for measuring the quality of an approximation, and the equioscillation theorem provides a elegant characterization of the optimal approximation. However, in the multivariate setting, there is no analogous norm that captures the complexity of the approximation problem, and the concept of equioscillation loses its intuitive appeal. \n\nFurthermore, the lack of a natural ordering of critical points in multivariate approximation makes it challenging to identify the optimal approximation, even if an explicit"}
{"full_text": "In 1893, the \"physicist-engineer\" Andr\\'e Blondel invents the oscilloscope for displaying voltage and current variables. With this powerful means of investigation, he first studies the phenomena of the arc then used for the coastal and urban lighting and then, the singing arc used as a transmitter of radio waves in wireless telegraphy. In 1905, he highlights a new type of non-sinusoidal oscillations in the singing arc. Twenty years later, Balthasar van der Pol will recognize that such oscillations were in fact \"relaxation oscillations\". To explain this phenomenon, he uses a representation in the phase plane and shows that its evolution takes the form of small cycles. During World War I the triode gradually replaces the singing arc in transmission systems. At the end of the war, using analogy, Blondel transposes to the triode most of the results he had obtained for the singing arc. In April 1919, he publishes a long memoir in which he introduces the terminology \"self-sustained oscillations\" and proposes to illustrate this concept starting from the example of the Tantalus cup which is then picked up by Van der Pol and Philippe Le Corbeiller. He then provides the definition of a self sustained system which is quite similar to that given later by Aleksandr Andronov and Van der Pol. To study the stability of oscillations sustained by the triode and by the singing arc he uses, this time, a representation in the complex plane and he expresses the amplitude in polar coordinates. He then justifies the maintaining of oscillations by the existence cycles which nearly present all the features of Poincar\\'e's limit cycles. Finally, in November 1919, Blondel performs, a year before Van der Pol, the setting in equation of the triode oscillations. In March 1926, Blondel establishes the differential equation characterizing the oscillations of the singing arc, completely similar to that obtained concomitantly by Van der Pol for the triode. Thus, throughout his career, Blondel, has made fundamental and relatively unknown contributions to the development of the theory of nonlinear oscillations. The purpose of this article is to analyze his main work in this area and to measure their importance or influence by placing them in the perspective of the development of this theory.", "prompt": "In 1893, the \"physicist-engineer\" Andr\\'e Blondel invents the oscilloscope for displaying voltage and current variables. With this powerful means of investigation, he first studies the phenomena of the arc then used for the coastal and urban lighting and then, the singing arc used as a transmitter of radio waves in wireless telegraphy. In 1905, he highlights a new type of non-sinusoidal oscillations in the singing arc. Twenty years later, Balthasar van der Pol will recognize that such oscillations were in fact \"relaxation oscillations\". To explain this phenomenon, he uses", "orig": "a representation in the phase plane and shows that its evolution takes the form of small cycles. During World War I the triode gradually replaces the singing arc in transmission systems. At the end of the war, using analogy, Blondel transposes to the triode most of the results he had obtained for the singing arc. In April 1919, he publishes a long memoir in which he introduces the terminology \"self-sustained oscillations\" and proposes to illustrate this concept starting from the example of the Tantalus cup which is then picked up by Van der Pol and Philippe Le Corbeiller. He then provides the definition of a", "generated": "the concept of nonlinear dynamics, which was still in its infancy at the time.\n\nVan der Pol's work on relaxation oscillations would go on to have a profound impact on the development of modern physics and engineering. He demonstrated that these oscillations were not limited to the singing arc, but were in fact a universal phenomenon that could be observed in a wide range of nonlinear systems. This discovery led to a deeper understanding of the behavior of complex systems, and paved the way for the development of new technologies, including radio communication systems, radar, and even medical devices.\n\nIn the years that followed, researchers would build upon van der Pol's work,"}
{"full_text": "We present an attempt to reach realistic turbulent regime in direct numerical simulations of the geodynamo. We rely on a sequence of three convection-driven simulations in a rapidly rotating spherical shell. The most extreme case reaches towards the Earth's core regime by lowering viscosity (magnetic Prandtl number Pm=0.1) while maintaining vigorous convection (magnetic Reynolds number Rm>500) and rapid rotation (Ekman number E=1e-7), at the limit of what is feasible on today's supercomputers. A detailed and comprehensive analysis highlights several key features matching geomagnetic observations or dynamo theory predictions -- all present together in the same simulation -- but it also unveils interesting insights relevant for Earth's core dynamics.In this strong-field, dipole-dominated dynamo simulation, the magnetic energy is one order of magnitude larger than the kinetic energy. The spatial distribution of magnetic intensity is highly heterogeneous, and a stark dynamical contrast exists between the interior and the exterior of the tangent cylinder (the cylinder parallel to the axis of rotation that circumscribes the inner core).In the interior, the magnetic field is strongest, and is associated with a vigorous twisted polar vortex, whose dynamics may occasionally lead to the formation of a reverse polar flux patch at the surface of the shell. Furthermore, the strong magnetic field also allows accumulation of light material within the tangent cylinder, leading to stable stratification there. Torsional Alfv{\\'e}n waves are frequently triggered in the vicinity of the tangent cylinder and propagate towards the equator.Outside the tangent cylinder, the magnetic field inhibits the growth of zonal winds and the kinetic energy is mostly non-zonal. Spatio-temporal analysis indicates that the low-frequency, non-zonal flow is quite geostrophic (columnar) and predominantly large-scale: an m=1 eddy spontaneously emerges in our most extreme simulations, without any heterogeneous boundary forcing.Our spatio-temporal analysis further reveals that (i) the low-frequency, large-scale flow is governed by a balance between Coriolis and buoyancy forces -- magnetic field and flow tend to align, minimizing the Lorentz force; (ii) the high-frequency flow obeys a balance between magnetic and Coriolis forces; (iii) the convective plumes mostly live at an intermediate scale, whose dynamics is driven by a 3-term 1 MAC balance -- involving Coriolis, Lorentz and buoyancy forces. However, small-scale (E^{1/3}) quasi-geostrophic convection is still observed in the regions of low magnetic intensity.", "prompt": "We present an attempt to reach realistic turbulent regime in direct numerical simulations of the geodynamo. We rely on a sequence of three convection-driven simulations in a rapidly rotating spherical shell. The most extreme case reaches towards the Earth's core regime by lowering viscosity (magnetic Prandtl number Pm=0.1) while maintaining vigorous convection (magnetic Reynolds number Rm>500) and rapid rotation (Ekman number E=1e-7), at the limit of what is feasible on today's supercomputers. A detailed and comprehensive analysis highlights several key features matching geomagnetic observations or dynamo theory predictions -- all", "orig": "present together in the same simulation -- but it also unveils interesting insights relevant for Earth's core dynamics.In this strong-field, dipole-dominated dynamo simulation, the magnetic energy is one order of magnitude larger than the kinetic energy. The spatial distribution of magnetic intensity is highly heterogeneous, and a stark dynamical contrast exists between the interior and the exterior of the tangent cylinder (the cylinder parallel to the axis of rotation that circumscribes the inner core).In the interior, the magnetic field is strongest, and is associated with a vigorous twisted polar vortex, whose dynamics may occasionally lead to the formation of a reverse polar flux patch at the surface of", "generated": "including the generation of a strong, Earth-like dipolar magnetic field, the presence of intense magnetic flux bundles near the outer boundary, and a realistic distribution of magnetic field intensities within the outer core. Furthermore, our simulations exhibit complex, time-dependent behavior, such as geomagnetic reversals and polarity oscillations, which are consistent with the Earth's magnetic field evolution over geological timescales. The obtained results provide strong evidence for the viability of the convection-driven geodynamo hypothesis and open up new avenues for exploring the intricate dynamics of the Earth's core. Specifically, our simulations allow for a detailed investigation of the complex interplay between conv"}
{"full_text": "We review the importance of recent UV observations of solar system targets and discuss the need for further measurements, instrumentation and laboratory work in the coming decade. In the past decade, numerous important advances have been made in solar system science using ultraviolet (UV) spectroscopic techniques. Formerly used nearly exclusively for studies of giant planet atmospheres, planetary exospheres and cometary emissions, UV imaging spectroscopy has recently been more widely applied. The geyser-like plume at Saturn's moon Enceladus was discovered in part as a result of UV stellar occultation observations, and this technique was used to characterize the plume and jets during the entire Cassini mission. Evidence for a similar style of activity has been found at Jupiter's moon Europa using Hubble Space Telescope (HST) UV emission and absorption imaging. At other moons and small bodies throughout the solar system, UV spectroscopy has been utilized to search for activity, probe surface composition, and delineate space weathering effects; UV photometric studies have been used to uncover regolith structure. Insights from UV imaging spectroscopy of solar system surfaces have been gained largely in the last 1-2 decades, including studies of surface composition, space weathering effects (e.g. radiolytic products) and volatiles on asteroids (e.g. [2][39][48][76][84]), the Moon (e.g. [30][46][49]), comet nuclei (e.g. [85]) and icy satellites (e.g. [38][41-44][45][47][65]). The UV is sensitive to some species, minor contaminants and grain sizes often not detected in other spectral regimes. In the coming decade, HST observations will likely come to an end. New infrastructure to bolster future UV studies is critically needed. These needs include both developmental work to help improve future UV observations and laboratory work to help interpret spacecraft data. UV instrumentation will be a critical tool on missions to a variety of targets in the coming decade, especially for the rapidly expanding application of UV reflectance investigations of atmosphereless bodies.", "prompt": "We review the importance of recent UV observations of solar system targets and discuss the need for further measurements, instrumentation and laboratory work in the coming decade. In the past decade, numerous important advances have been made in solar system science using ultraviolet (UV) spectroscopic techniques. Formerly used nearly exclusively for studies of giant planet atmospheres, planetary exospheres and cometary emissions, UV imaging spectroscopy has recently been more widely applied. The geyser-like plume at Saturn's moon Enceladus was discovered in part as a result of UV stellar occultation observations, and this technique was used to characterize the plume and jets", "orig": "during the entire Cassini mission. Evidence for a similar style of activity has been found at Jupiter's moon Europa using Hubble Space Telescope (HST) UV emission and absorption imaging. At other moons and small bodies throughout the solar system, UV spectroscopy has been utilized to search for activity, probe surface composition, and delineate space weathering effects; UV photometric studies have been used to uncover regolith structure. Insights from UV imaging spectroscopy of solar system surfaces have been gained largely in the last 1-2 decades, including studies of surface composition, space weathering effects (e.g. radiolytic products) and vol", "generated": ".\n\nThe breakthrough discoveries made possible by UV spectroscopy have sparked a new wave of interest in exploring the composition and dynamics of solar system bodies. For instance, the detection of water vapor and organic molecules in the plume of Enceladus has significant implications for the search for life beyond Earth. Similarly, UV observations of Mars' atmosphere have revealed important details about the planet's climate and geological history. The success of these endeavors has demonstrated the power of UV spectroscopy to probe the chemistry and physics of planetary environments, driving the need for further advancements in instrumentation and measurement techniques.\n\nIn the coming decade, it will be crucial to invest in the"}
{"full_text": "Context. Circumstellar disks are known to contain a significant mass in dust ranging from micron to centimeter size. Meteorites are evidence that individual grains of those sizes were collected and assembled into planetesimals in the young solar system. Aims. We assess the efficiency of dust collection of a swarm of non-drifting planetesimals {\\rev with radii ranging from 1 to $10^3$\\,km and beyond. Methods. We calculate the collision probability of dust drifting in the disk due to gas drag by planetesimal accounting for several regimes depending on the size of the planetesimal, dust, and orbital distance: the geometric, Safronov, settling, and three-body regimes. We also include a hydrodynamical regime to account for the fact that small grains tend to be carried by the gas flow around planetesimals. Results. We provide expressions for the collision probability of dust by planetesimals and for the filtering efficiency by a swarm of planetesimals. For standard turbulence conditions (i.e., a turbulence parameter $\\alpha=10^{-2}$), filtering is found to be inefficient, meaning that when crossing a minimum-mass solar nebula (MMSN) belt of planetesimals extending between 0.1 AU and 35 AU most dust particles are eventually accreted by the central star rather than colliding with planetesimals. However, if the disk is weakly turbulent ($\\alpha=10^{-4}$) filtering becomes efficient in two regimes: (i) when planetesimals are all smaller than about 10 km in size, in which case collisions mostly take place in the geometric regime; and (ii) when planetary embryos larger than about 1000 km in size dominate the distribution, have a scale height smaller than one tenth of the gas scale height, and dust is of millimeter size or larger in which case most collisions take place in the settling regime. These two regimes have very different properties: we find that the local filtering efficiency $x_{filter,MMSN}$ scales with $r^{-7/4}$ (where $r$ is the orbital distance) in the geometric regime, but with $r^{-1/4}$ to $r^{1/4}$ in the settling regime. This implies that the filtering of dust by small planetesimals should occur close to the central star and with a short spread in orbital distances. On the other hand, the filtering by embryos in the settling regime is expected to be more gradual and determined by the extent of the disk of embryos. Dust particles much smaller than millimeter size tend only to be captured by the smallest planetesimals because they otherwise move on gas streamlines and their collisions take place in the hydrodynamical regime. Conclusions. Our results hint at an inside-out formation of planetesimals in the infant solar system because small planetesimals in the geometrical limit can filter dust much more efficiently close to the central star. However, even a fully-formed belt of planetesimals such as the MMSN only marginally captures inward-drifting dust and this seems to imply that dust in the protosolar disk has been filtered by planetesimals even smaller than 1 km (not included in this study) or that it has been assembled into planetesimals by other mechanisms (e.g., orderly growth, capture into vortexes). Further refinement of our work concerns, among other things: a quantitative description of the transition region between the hydro and settling regimes; an assessment of the role of disk turbulence for collisions, in particular in the hydro regime; and the coupling of our model to a planetesimal formation model.", "prompt": "Context. Circumstellar disks are known to contain a significant mass in dust ranging from micron to centimeter size. Meteorites are evidence that individual grains of those sizes were collected and assembled into planetesimals in the young solar system. Aims. We assess the efficiency of dust collection of a swarm of non-drifting planetesimals {\\rev with radii ranging from 1 to $10^3$\\,km and beyond. Methods. We calculate the collision probability of dust drifting in the disk due to gas drag by planetesimal accounting for several regimes depending on the size of the planetesimal, dust, and orbital distance: the geometric", "orig": ", Safronov, settling, and three-body regimes. We also include a hydrodynamical regime to account for the fact that small grains tend to be carried by the gas flow around planetesimals. Results. We provide expressions for the collision probability of dust by planetesimals and for the filtering efficiency by a swarm of planetesimals. For standard turbulence conditions (i.e., a turbulence parameter $\\alpha=10^{-2}$), filtering is found to be inefficient, meaning that when crossing a minimum-mass solar nebula (MMSN) belt of planetesimals extending between 0.1 AU and 35 AU", "generated": "regime, where the dust is much smaller than the planetesimal's radius, and the regime where the dust is comparable in size to or even larger than the planetesimal.\n\nOur study focuses on understanding the role of planetesimal size in determining the efficiency of dust collection, as well as the impact of orbital distance and gas drag on this process. We develop a novel numerical approach to model the dynamics of dust-planetesimal encounters, taking into account the complex interplay between gravity, gas drag, and dust-planetesimal interactions. Our simulations reveal that the collision probability of dust with planetesimals is strongly dependent on the size of the planetesimal"}
{"full_text": "Various programming tools, languages, and environments give programmers the impression of changing a program while it is running. This experience of liveness has been discussed for over two decades and a broad spectrum of research on this topic exists. Amongst others, this work has been carried out in the communities around three major ideas which incorporate liveness as an important aspect: live programming, exploratory programming, and live coding. While there have been publications on the focus of each particular community, the overall spectrum of liveness across these three communities has not been investigated yet. Thus, we want to delineate the variety of research on liveness. At the same time, we want to investigate overlaps and differences in the values and contributions between the three communities. Therefore, we conducted a literature study with a sample of 212 publications on the terms retrieved from three major indexing services. On this sample, we conducted a thematic analysis regarding the following aspects: motivation for liveness, application domains, intended outcomes of running a system, and types of contributions. We also gathered bibliographic information such as related keywords and prominent publications. Besides other characteristics the results show that the field of exploratory programming is mostly about technical designs and empirical studies on tools for general-purpose programming. In contrast, publications on live coding have the most variety in their motivations and methodologies with a majority being empirical studies with users. As expected, most publications on live coding are applied to performance art. Finally, research on live programming is mostly motivated by making programming more accessible and easier to understand, evaluating their tool designs through empirical studies with users. In delineating the spectrum of work on liveness, we hope to make the individual communities more aware of the work of the others. Further, by giving an overview of the values and methods of the individual communities, we hope to provide researchers new to the field of liveness with an initial overview.", "prompt": "Various programming tools, languages, and environments give programmers the impression of changing a program while it is running. This experience of liveness has been discussed for over two decades and a broad spectrum of research on this topic exists. Amongst others, this work has been carried out in the communities around three major ideas which incorporate liveness as an important aspect: live programming, exploratory programming, and live coding. While there have been publications on the focus of each particular community, the overall spectrum of liveness across these three communities has not been investigated yet. Thus, we want to delineate the variety of research on liveness. At the same", "orig": "time, we want to investigate overlaps and differences in the values and contributions between the three communities. Therefore, we conducted a literature study with a sample of 212 publications on the terms retrieved from three major indexing services. On this sample, we conducted a thematic analysis regarding the following aspects: motivation for liveness, application domains, intended outcomes of running a system, and types of contributions. We also gathered bibliographic information such as related keywords and prominent publications. Besides other characteristics the results show that the field of exploratory programming is mostly about technical designs and empirical studies on tools for general-purpose programming. In contrast, publications on live coding have the", "generated": "time, we aim to identify the commonalities and differences between these communities, as well as the potentialapplications and implications of liveness in software development.\n\nTo achieve this goal, we conducted a comprehensive literature review, analyzing key publications and research papers in the fields of!ive programming, exploratory programming, and live coding. We identified common themes and trends, such as the importance of immediate feedback, flexibility, and creativity in the programming process. We also found that each community has its unique focus and approach to liveness, ranging from the use of visual programming languages to the development of interactive coding environments.\n\nFor instance, live programming is often"}
{"full_text": "Linear latent variable models such as principal component analysis (PCA), independent component analysis (ICA), canonical correlation analysis (CCA), and factor analysis (FA) identify latent directions (or loadings) either ordered or unordered. The data is then projected onto the latent directions to obtain their projected representations (or scores). For example, PCA solvers usually rank the principal directions by explaining the most to least variance, while ICA solvers usually return independent directions unordered and often with single sources spread across multiple directions as multiple sub-sources, which is of severe detriment to their usability and interpretability. This paper proposes a general framework to enhance latent space representations for improving the interpretability of linear latent spaces. Although the concepts in this paper are language agnostic, the framework is written in Python. This framework automates the clustering and ranking of latent vectors to enhance the latent information per latent vector, as well as, the interpretation of latent vectors. Several innovative enhancements are incorporated including latent ranking (LR), latent scaling (LS), latent clustering (LC), and latent condensing (LCON). For a specified linear latent variable model, LR ranks latent directions according to a specified metric, LS scales latent directions according to a specified metric, LC automatically clusters latent directions into a specified number of clusters, while, LCON automatically determines an appropriate number of clusters into which to condense the latent directions for a given metric. Additional functionality of the framework includes single-channel and multi-channel data sources, data preprocessing strategies such as Hankelisation to seamlessly expand the applicability of linear latent variable models (LLVMs) to a wider variety of data. The effectiveness of LR, LS, and LCON are showcased on two crafted foundational problems with two applied latent variable models, namely, PCA and ICA.", "prompt": "Linear latent variable models such as principal component analysis (PCA), independent component analysis (ICA), canonical correlation analysis (CCA), and factor analysis (FA) identify latent directions (or loadings) either ordered or unordered. The data is then projected onto the latent directions to obtain their projected representations (or scores). For example, PCA solvers usually rank the principal directions by explaining the most to least variance, while ICA solvers usually return independent directions unordered and often with single sources spread across multiple directions as multiple sub-sources, which is of severe detriment to their usability and interpretability. This paper proposes a general framework to enhance latent space representations", "orig": "for improving the interpretability of linear latent spaces. Although the concepts in this paper are language agnostic, the framework is written in Python. This framework automates the clustering and ranking of latent vectors to enhance the latent information per latent vector, as well as, the interpretation of latent vectors. Several innovative enhancements are incorporated including latent ranking (LR), latent scaling (LS), latent clustering (LC), and latent condensing (LCON). For a specified linear latent variable model, LR ranks latent directions according to a specified metric, LS scales latent directions according to a specified metric, LC automatically clusters latent directions into a specified number of clusters, while", "generated": ".\n\nThe proposed framework focuses on improving the quality and meaningfulness of the latent space representations extracted by these linear latent variable models. By introducing a novel optimization algorithm, the framework aims to reorder the latent directions in a way that maximizes their interpretability and usability. Specifically, the algorithm takes into account the correlation structure of the data and the relationships between the latent directions, to identify a more coherent and meaningful ordering of the directions. This reordered representation enables researchers to better understand the underlying patterns and relationships in the data, and to make more accurate predictions and decisions.\n\nIn addition, the framework also proposes a new evaluation metric to assess the quality of the"}
{"full_text": "Non-target screening consists in searching a sample for all present substances, suspected or unknown, with very little prior knowledge about the sample. This approach has been introduced more than a decade ago in the field of water analysis, together with dedicated compound identification tools, but is still very scarce for indoor and atmospheric trace gas measurements, despite the clear need for a better understanding of the atmospheric trace gas composition.For a systematic detection of emerging trace gases in the atmosphere, a new and powerful analytical method is gas chromatography (GC) of preconcentrated samples, followed by electron ionisation, high resolution mass spectrometry (EI-HRMS). In this work, we present data analysis tools to enable automated fragment formula annotation for unknown compounds measured by GC-EI-HRMS. Based on co-eluting mass/charge fragments, we developed an innovative data analysis method to reliably reconstruct the chemical formulae of the fragments, using efficient combinatorics and graph theory. The method does not require the presence of the molecular ion, which is absent in ~40% of EI spectra. Our method has been trained and validated on \\textgreater50 halocarbons and hydrocarbons, with 3 to 20 atoms and molar masses of 30 to 330 g mol-1, measured with a mass resolution of approx.~3500. For 90% of the compounds, more than 90% of the annotated fragment formulae are correct. Cases of wrong identification can be attributed to the scarcity of detected fragments per compound or the lack of isotopic constraint (no minor isotopocule detected).Our method enables to reconstruct most probable chemical formulae independently from spectral databases. Therefore, it demonstrates the suitability of EI-HRMS data for non-target analysis and paves the way for the identification of substances for which no EI mass spectrum is registered in databases. We illustrate the performances of our method for atmospheric trace gases and suggest that it may be well suited for many other types of samples. The L-GPL licenced Python code is released under the name ALPINAC for ALgorithmic Process for Identification of Non-targeted Atmospheric Compounds.", "prompt": "Non-target screening consists in searching a sample for all present substances, suspected or unknown, with very little prior knowledge about the sample. This approach has been introduced more than a decade ago in the field of water analysis, together with dedicated compound identification tools, but is still very scarce for indoor and atmospheric trace gas measurements, despite the clear need for a better understanding of the atmospheric trace gas composition.For a systematic detection of emerging trace gases in the atmosphere, a new and powerful analytical method is gas chromatography (GC) of preconcentrated samples, followed by electron ionisation, high resolution mass spectrometry (EI-HRMS). In", "orig": "this work, we present data analysis tools to enable automated fragment formula annotation for unknown compounds measured by GC-EI-HRMS. Based on co-eluting mass/charge fragments, we developed an innovative data analysis method to reliably reconstruct the chemical formulae of the fragments, using efficient combinatorics and graph theory. The method does not require the presence of the molecular ion, which is absent in ~40% of EI spectra. Our method has been trained and validated on \\textgreater50 halocarbons and hydrocarbons, with 3 to 20 atoms and molar masses of 30 to 330 g mol-1", "generated": "addition, the development of advanced data analysis tools and software has facilitated the processing and interpretation of complex mass spectra, allowing for the identification of unknown compounds and the detection of emerging trends in atmospheric composition.\n\nThe application of non-target screening to atmospheric samples has the potential to reveal a wide range of previously unknown or unquantified compounds, including volatile organic compounds (VOCs), halogenated species, and other trace gases that play a crucial role in atmospheric chemistry and climate change. For instance, the detection of new VOCs can provide valuable insights into the sources and sinks of these compounds, as well as their impact on ozone formation, aerosol"}
{"full_text": "Recent research has shown that surprisingly rich models of human activity can be learned from GPS (positional) data. However, most effort to date has concentrated on modeling single individuals or statistical properties of groups of people. Moreover, prior work focused solely on modeling actual successful executions (and not failed or attempted executions) of the activities of interest. We, in contrast, take on the task of understanding human interactions, attempted interactions, and intentions from noisy sensor data in a fully relational multi-agent setting. We use a real-world game of capture the flag to illustrate our approach in a well-defined domain that involves many distinct cooperative and competitive joint activities. We model the domain using Markov logic, a statistical-relational language, and learn a theory that jointly denoises the data and infers occurrences of high-level activities, such as a player capturing an enemy. Our unified model combines constraints imposed by the geometry of the game area, the motion model of the players, and by the rules and dynamics of the game in a probabilistically and logically sound fashion. We show that while it may be impossible to directly detect a multi-agent activity due to sensor noise or malfunction, the occurrence of the activity can still be inferred by considering both its impact on the future behaviors of the people involved as well as the events that could have preceded it. Further, we show that given a model of successfully performed multi-agent activities, along with a set of examples of failed attempts at the same activities, our system automatically learns an augmented model that is capable of recognizing success and failure, as well as goals of peoples actions with high accuracy. We compare our approach with other alternatives and show that our unified model, which takes into account not only relationships among individual players, but also relationships among activities over the entire length of a game, although more computationally costly, is significantly more accurate. Finally, we demonstrate that explicitly modeling unsuccessful attempts boosts performance on other important recognition tasks.", "prompt": "Recent research has shown that surprisingly rich models of human activity can be learned from GPS (positional) data. However, most effort to date has concentrated on modeling single individuals or statistical properties of groups of people. Moreover, prior work focused solely on modeling actual successful executions (and not failed or attempted executions) of the activities of interest. We, in contrast, take on the task of understanding human interactions, attempted interactions, and intentions from noisy sensor data in a fully relational multi-agent setting. We use a real-world game of capture the flag to illustrate our approach in a well-defined domain that involves many distinct cooperative and competitive joint activities. We", "orig": "model the domain using Markov logic, a statistical-relational language, and learn a theory that jointly denoises the data and infers occurrences of high-level activities, such as a player capturing an enemy. Our unified model combines constraints imposed by the geometry of the game area, the motion model of the players, and by the rules and dynamics of the game in a probabilistically and logically sound fashion. We show that while it may be impossible to directly detect a multi-agent activity due to sensor noise or malfunction, the occurrence of the activity can still be inferred by considering both its impact on the future behaviors of the people involved as well as", "generated": "propose a novel probabilistic framework that leverages the complex interdependencies between agents, activities, and context to reason about the underlying intentions and interactions.\n\nOur approach is based on a Bayesian non-parametric model that captures the nuanced patterns of human behavior, including the influence of social relationships, spatial proximity, and environmental factors on activity execution. By incorporating both successful and failed attempts, our model can distinguish between genuine intentions and accidental actions, providing a more comprehensive understanding of human behavior. We demonstrate the effectiveness of our approach through a series of experiments on a large-scale dataset collected from a capture the flag game, where teams of players navigate a complex terrain to"}
{"full_text": "We consider here one-parameter semigroups ${\\bf T}=(T(t))_{t>0}$ of bounded operators on a Banach space $X$ which are weakly continuous in the sense of Arveson. For such a semigroup ${\\bf T}$ denote by ${\\mathcal M}_{\\omega_{\\bf T}}$ the convolution algebra consisting in those measures $\\mu$ on $(0,+\\infty)$ such that $\\int_0^{+\\infty}\\Vert T(t)\\Vert d\\vert \\mu \\vert (t)<+\\infty.$ The Pettis integral $\\int_0^{+\\infty}T(t)d\\mu(t)$ defines for $\\mu \\in {\\mathcal M}_{\\omega_{\\bf T}}$ a bounded operator $\\phi_{\\bf T}(\\mu)$ on $X.$ Identifying the space $L^1_{\\omega_{\\bf T}}$ of (classes of) measurable functions $f$ satisfying $\\int_0^{+\\infty}\\vert f(t)\\Vert T(t)\\Vert dt< +\\infty$ to a closed subspace ${\\mathcal M}_{\\omega_{\\bf T}}$ in the usual way, we define the Arveson ideal $\\mathcal{I}_{\\bf T}$ of the semigroup to be the closure in ${\\mathcal B}(X)$ of $\\phi_{\\bf T}(L^1_{\\omega_{\\bf T}}).$ Using a variant of a procedure introduced a long time ago by the author we introduce a dense ideal $\\mathcal{U}_{\\bf T}$ of $\\mathcal{I}_{\\bf T},$ which is a Banach algebra with respect to a suitable norm $\\Vert .\\Vert_{\\mathcal{U}_{\\bf T}},$ such that $\\lim \\sup_{t\\to 0^+}\\Vert T(t)\\Vert_{{\\mathcal B}(\\mathcal{U}_{\\bf T})}<+\\infty.$ The normalized Arveson ideal $\\mathcal{J}_{\\bf T}$ is the closure of $\\mathcal{I}_{\\bf T}$ in ${\\mathcal B}(\\mathcal{U}_{\\bf T}).$ The Banach algebra $\\mathcal{J}_{\\bf T}$ has a sequential approximate identity and is isometrically isomorphic to a closed ideal of its multiplier algebra ${\\mathcal M}(\\mathcal{J}_{\\bf T}).$ The Banach algebras $\\mathcal{U}_{\\bf T},$ $\\mathcal{I}_{\\bf T}$ and $\\mathcal{J}_{\\bf T}$ are \"similar\", and the map $S_{u/v}\\to S_{au/av}$ defines when $a$ generates a dense principal ideal of $\\mathcal{U}_{\\bf T}$ a pseudo bounded isomorphism from the algebre $\\mathcal{QM}(\\mathcal{J}_{\\bf T})$ of quasimultipliers on $\\mathcal{J}_{\\bf T}$ onto the quasimultipliers algebras $\\mathcal{QM}(\\mathcal{U}_{\\bf T})$ and $\\mathcal{QM}(\\mathcal{I}_{\\bf T}).$ We define the generator $A_{\\bf T}$ of the semigroup $\\bf T$ to be a quasimultiplier on $\\mathcal{I}_{\\bf T},$ or ,equivalently, on $\\mathcal{J}_{\\bf T}.$ Every character $\\chi$ on $\\mathcal{I}_{\\bf T}$ has an extension $\\tilde \\chi$ to $\\mathcal{QM}(\\mathcal{I}_{\\bf T}).$ Let $Res_{ar} (A_{\\bf T})$ be the complement of the set $\\{ \\tilde \\chi (A_{\\bf T})\\}_{\\chi \\in \\widehat{\\mathcal{I}_{\\bf T}}}.$ The quasimultiplier $A-\\mu I$ has an inverse belonging to $\\mathcal{J}_{\\bf T}$ for $\\mu \\in Res_{ar} (A_{\\bf T}),$ which allows to consider this inverse as a \"regular\" quasimultiplier on the Arveson ideal $\\mathcal{I}_{\\bf T}.$ The usual resolvent formula holds in this context for $Re(\\mu)>\\lim_{t\\to +\\infty}{log \\Vert T(t)\\Vert\\over t}.$ Set $\\Pi_{\\alpha}^+:=\\{ z \\in \\mathbb{C} \\ | \\ Re(z) >\\alpha\\}.$ We revisit the functional calculus associated to the generator $A_{\\bf T}$ by defining $F(-A_{\\bf T})\\in \\mathcal{J}_{\\bf T}$ by a Cauchy integral when $F$ belongs to the Hardy space $H^1(\\Pi_{\\alpha}^+)$ for some $\\alpha < -\\lim_{t\\to +\\infty} {log\\Vert T(t)\\vert\\over t}.$ We then define $F(-A_{\\bf T})$ as a quasimultiplier on $\\mathcal{J}_{\\bf T}$ and $\\mathcal{I}_{\\bf T}$ when $F$ belongs to the Smirnov class on $\\Pi_{\\alpha}^+,$ and $F(-A_{\\bf T})$ is a regular quasimultiplier on $\\mathcal{J}_{\\bf T}$ and $\\mathcal{I}_{\\bf T}$ if $F$ is bounded on $\\Pi_{\\alpha}^+.$ If $F(z)=e^{-zt}$ for some $t>0,$ then $F(-A_{\\bf T})=T(t),$ and if $F(z)=-z,$ we indeed have $F(-A_{\\bf T})=A_{\\bf T}.$", "prompt": "We consider here one-parameter semigroups ${\\bf T}=(T(t))_{t>0}$ of bounded operators on a Banach space $X$ which are weakly continuous in the sense of Arveson. For such a semigroup ${\\bf T}$ denote by ${\\mathcal M}_{\\omega_{\\bf T}}$ the convolution algebra consisting in those measures $\\mu$ on $(0,+\\infty)$ such that $\\int_0^{+\\infty}\\Vert T(t)\\Vert d\\vert \\mu \\vert (t)<+\\infty.$ The Pettis integral $\\int_0", "orig": "^{+\\infty}T(t)d\\mu(t)$ defines for $\\mu \\in {\\mathcal M}_{\\omega_{\\bf T}}$ a bounded operator $\\phi_{\\bf T}(\\mu)$ on $X.$ Identifying the space $L^1_{\\omega_{\\bf T}}$ of (classes of) measurable functions $f$ satisfying $\\int_0^{+\\infty}\\vert f(t)\\Vert T(t)\\Vert dt< +\\infty$ to a closed subspace ${\\mathcal M}_{\\omega_{\\bf T}}$ in the usual way, we define the", "generated": "^{+\\infty} T(t) d\\mu(t)$ is a bounded operator on $X$ for all $\\mu \\in {\\mathcal M}_{\\omega_{\\bf T}}.$\n\nIt is well-known that the Laplace transform of a measure in ${\\mathcal M}_{\\omega_{\\bf T}}$ is a bounded analytic function on the open right half-plane. Specifically, if $\\mu \\in {\\mathcal M}_{\\omega_{\\bf T}}$ and $z$ is a complex number with positive real part, then the Laplace transform of $\\mu$ at $z$ is defined"}
{"full_text": "In recent years, heterogeneous computing has emerged as the vital way to increase computers? performance and energy efficiency by combining diverse hardware devices, such as Graphics Processing Units (GPUs) and Field Programmable Gate Arrays (FPGAs). The rationale behind this trend is that different parts of an application can be offloaded from the main CPU to diverse devices, which can efficiently execute these parts as co-processors. FPGAs are a subset of the most widely used co-processors, typically used for accelerating specific workloads due to their flexible hardware and energy-efficient characteristics. These characteristics have made them prevalent in a broad spectrum of computing systems ranging from low-power embedded systems to high-end data centers and cloud infrastructures. However, these hardware characteristics come at the cost of programmability. Developers who create their applications using high-level programming languages (e.g., Java, Python, etc.) are required to familiarize with a hardware description language (e.g., VHDL, Verilog) or recently heterogeneous programming models (e.g., OpenCL, HLS) in order to exploit the co-processors? capacity and tune the performance of their applications. Currently, the above-mentioned heterogeneous programming models support exclusively the compilation from compiled languages, such as C and C++. Thus, the transparent integration of heterogeneous co-processors to the software ecosystem of managed programming languages (e.g. Java, Python) is not seamless. In this paper we rethink the engineering trade-offs that we encountered, in terms of transparency and compilation overheads, while integrating FPGAs into high-level managed programming languages. We present a novel approach that enables runtime code specialization techniques for seamless and high-performance execution of Java programs on FPGAs. The proposed solution is prototyped in the context of the Java programming language and TornadoVM; an open-source programming framework for Java execution on heterogeneous hardware. Finally, we evaluate the proposed solution for FPGA execution against both sequential and multi-threaded Java implementations showcasing up to 224x and 19.8x performance speedups, respectively, and up to 13.82x compared to TornadoVM running on an Intel integrated GPU. We also provide a break-down analysis of the proposed compiler optimizations for FPGA execution, as a means to project their impact on the applications? characteristics.", "prompt": "In recent years, heterogeneous computing has emerged as the vital way to increase computers? performance and energy efficiency by combining diverse hardware devices, such as Graphics Processing Units (GPUs) and Field Programmable Gate Arrays (FPGAs). The rationale behind this trend is that different parts of an application can be offloaded from the main CPU to diverse devices, which can efficiently execute these parts as co-processors. FPGAs are a subset of the most widely used co-processors, typically used for accelerating specific workloads due to their flexible hardware and energy-efficient characteristics. These characteristics have made them prevalent in a broad spectrum of computing systems ranging from", "orig": "low-power embedded systems to high-end data centers and cloud infrastructures. However, these hardware characteristics come at the cost of programmability. Developers who create their applications using high-level programming languages (e.g., Java, Python, etc.) are required to familiarize with a hardware description language (e.g., VHDL, Verilog) or recently heterogeneous programming models (e.g., OpenCL, HLS) in order to exploit the co-processors? capacity and tune the performance of their applications. Currently, the above-mentioned heterogeneous programming models support exclusively the compilation from compiled languages, such as C and C++. Thus, the transparent integration of", "generated": "embedded systems to high-performance computing data centers. In particular, FPGAs have shown great promise in accelerating various applications, including machine learning, data analytics, and scientific simulations. For instance, FPGAs can be used to accelerate the convolutional neural networks (CNNs) used in deep learning applications, resulting in significant performance improvements and energy savings. Moreover, FPGAs can be reconfigured to adapt to changing application requirements, making them an attractive solution for dynamic and heterogeneous workloads. Another key advantage of FPGAs is their ability to operate at low power consumption levels, making them suitable for battery-powered devices and data centers where"}
{"full_text": "We measure the large-scale real-space power spectrum P(k) using luminous red galaxies (LRGs) in the Sloan Digital Sky Survey (SDSS) and use this measurement to sharpen constraints on cosmological parameters from the Wilkinson Microwave Anisotropy Probe (WMAP). We employ a matrix-based power spectrum estimation method using Pseudo-Karhunen-Loeve eigenmodes, producing uncorrelated minimum-variance measurements in 20 k-bands of both the clustering power and its anisotropy due to redshift-space distortions, with narrow and well-behaved window functions in the range 0.01h/Mpc < k < 0.2h/Mpc. Results from the LRG and main galaxy samples are consistent, with the former providing higher signal-to-noise. Our results are robust to omitting angular and radial density fluctuations and are consistent between different parts of the sky. They provide a striking confirmation of the predicted large-scale LCDM power spectrum. Combining only SDSS LRG and WMAP data places robust constraints on many cosmological parameters that complement prior analyses of multiple data sets. The LRGs provide independent cross-checks on Om and the baryon fraction in good agreement with WMAP. Within the context of flat LCDM models, our LRG measurements complement WMAP by sharpening the constraints on the matter density, the neutrino density and the tensor amplitude by about a factor of two, giving Omega_m=0.24+-0.02 (1 sigma), sum m_nu < 0.9 eV (95%) and r<0.3 (95%). Baryon oscillations are clearly detected and provide a robust measurement of the comoving distance to the median survey redshift z=0.35 independent of curvature and dark energy properties. Within the LCDM framework, our power spectrum measurement improves the evidence for spatial flatness, sharpening the curvature constraint Omega_tot=1.05+-0.05 from WMAP alone to Omega_tot=1.003+-0.010. Assuming Omega_tot=1, the equation of state parameter is constrained to w=-0.94+-0.09, indicating the potential for more ambitious future LRG measurements to provide precision tests of the nature of dark energy. All these constraints are essentially independent of scales k>0.1h/Mpc and associated nonlinear complications, yet agree well with more aggressive published analyses where nonlinear modeling is crucial.", "prompt": "We measure the large-scale real-space power spectrum P(k) using luminous red galaxies (LRGs) in the Sloan Digital Sky Survey (SDSS) and use this measurement to sharpen constraints on cosmological parameters from the Wilkinson Microwave Anisotropy Probe (WMAP). We employ a matrix-based power spectrum estimation method using Pseudo-Karhunen-Loeve eigenmodes, producing uncorrelated minimum-variance measurements in 20 k-bands of both the clustering power and its anisotropy due to redshift-space distortions, with narrow and well-behaved window functions in the range 0.01h", "orig": "/Mpc < k < 0.2h/Mpc. Results from the LRG and main galaxy samples are consistent, with the former providing higher signal-to-noise. Our results are robust to omitting angular and radial density fluctuations and are consistent between different parts of the sky. They provide a striking confirmation of the predicted large-scale LCDM power spectrum. Combining only SDSS LRG and WMAP data places robust constraints on many cosmological parameters that complement prior analyses of multiple data sets. The LRGs provide independent cross-checks on Om and the baryon fraction in good agreement with WMAP. Within the context of", "generated": "/Mpc < k < 0.5h/Mpc.\n\nThis method allows us to accurately quantify the scale-dependent bias of LRGs, which is essential for extracting robust cosmological constraints. We find that the LRG power spectrum is well-described by a cold dark matter (CDM) model with a Cosmological Constant (\u039b) and a scalar spectral index ns = 0.96. The addition of our power spectrum measurements to the WMAP likelihood function yields a 30% improvement in the constraint on the matter density \u03a9m, and a 20% improvement in the constraint on the Hubble constant H0.\n\nFurthermore"}
{"full_text": "In the frame of the test of NISP instrument for ESA Euclid mission, the question was raised to perform a metrology measurement of different components during the thermal vacuum test of NISP instrument. NISP will be tested at Laboratoire d'Astrophysique de Marseille (LAM) in ERIOS chamber under vacuum and thermal conditions in order to qualify the instrument in its operating environment and to perform the final acceptance test before delivery to the payload. One of the main objectives of the test campaign will be the measurement of the focus position of NISP image plane with respect to the EUCLID object plane. To simulate the EUCLID object plane, a telescope simulator with a very well know focal distance will be installed in front of NISP into ERIOS chamber. We need to measure at cold and vacuum the position of reflectors installed on NISP instrument and the telescope simulator. From these measurements, we will provide at operational temperature the measurement of references frames set on the telescope simulator and NISP, the knowledge of the coordinates of the object point source provided by the telescope simulator and the measurement of the angle between the telescope simulator optical axis and NISP optical axis. In this context, we have developed a metrology method based on the use of a laser tracker to measure the position of the reflectors inside ERIOS. The laser tracker is installed outside the vacuum chamber and measure through a curved window reflectors put inside the chamber either at ambient pressure or vacuum pressure. Several tests campaigns have been done at LAM to demonstrate the measurement performance with this configuration. Using a well know reflectors configuration, we show that it is possible to correct the laser tracker measurement from the window disturbances and from the vacuum impact. A corrective term is applied to the data and allows retrieving the real coordinates of the reflectors with a bias lower than 30$\\mu$m, which is lower than the laser tracker measurement uncertainties estimated at 60$\\mu$m. No additional error term of the laser tracker measurement is observed when using the laser tracker with the curved window and in vacuum, comparing with a classical use of the laser tracker. With these test campaign, we have been able to demonstrate the possibility to use a laser tracker to measure in real time during a vacuum thermal test the position of different mechanical parts into a vacuum chamber with an accuracy better than 60$\\mu$m.", "prompt": "In the frame of the test of NISP instrument for ESA Euclid mission, the question was raised to perform a metrology measurement of different components during the thermal vacuum test of NISP instrument. NISP will be tested at Laboratoire d'Astrophysique de Marseille (LAM) in ERIOS chamber under vacuum and thermal conditions in order to qualify the instrument in its operating environment and to perform the final acceptance test before delivery to the payload. One of the main objectives of the test campaign will be the measurement of the focus position of NISP image plane with respect to the EUCLID object plane. To simulate the EU", "orig": "CLID object plane, a telescope simulator with a very well know focal distance will be installed in front of NISP into ERIOS chamber. We need to measure at cold and vacuum the position of reflectors installed on NISP instrument and the telescope simulator. From these measurements, we will provide at operational temperature the measurement of references frames set on the telescope simulator and NISP, the knowledge of the coordinates of the object point source provided by the telescope simulator and the measurement of the angle between the telescope simulator optical axis and NISP optical axis. In this context, we have developed a metrology method based on the use of a laser tracker", "generated": "CLID object plane, a metrology system will be implemented, consisting of a reference plate with a grid of fiducial marks, which will be used to track the distortions of the instrument due to thermal and mechanical stresses.\n\nThe metrology system will be based on a combination of optical and mechanical measurement techniques, including laser interferometry, autocollimators, and mechanical probing. These techniques will enable the measurement of the absolute and relative positions of the NISP components, including the image plane, with an accuracy of a few micrometers. The metrology system will also be used to measure the deformation of the instrument under different thermal and"}
{"full_text": "The paper deals with the automatic analysis of real-life telephone conversations between an agent and a customer of a customer care service (ccs). The application domain is the public transportation system in Paris and the purpose is to collect statistics about customer problems in order to monitor the service and decide priorities on the intervention for improving user satisfaction. Of primary importance for the analysis is the detection of themes that are the object of customer problems. Themes are defined in the application requirements and are part of the application ontology that is implicit in the ccs documentation. Due to variety of customer population, the structure of conversations with an agent is unpredictable. A conversation may be about one or more themes. Theme mentions can be interleaved with mentions of facts that are irrelevant for the application purpose. Furthermore, in certain conversations theme mentions are localized in specific conversation segments while in other conversations mentions cannot be localized. As a consequence, approaches to feature extraction with and without mention localization are considered. Application domain relevant themes identified by an automatic procedure are expressed by specific sentences whose words are hypothesized by an automatic speech recognition (asr) system. The asr system is error prone. The word error rates can be very high for many reasons. Among them it is worth mentioning unpredictable background noise, speaker accent, and various types of speech disfluencies. As the application task requires the composition of proportions of theme mentions, a sequential decision strategy is introduced in this paper for performing a survey of the large amount of conversations made available in a given time period. The strategy has to sample the conversations to form a survey containing enough data analyzed with high accuracy so that proportions can be estimated with sufficient accuracy. Due to the unpredictable type of theme mentions, it is appropriate to consider methods for theme hypothesization based on global as well as local feature extraction. Two systems based on each type of feature extraction will be considered by the strategy. One of the four methods is novel. It is based on a new definition of density of theme mentions and on the localization of high density zones whose boundaries do not need to be precisely detected. The sequential decision strategy starts by grouping theme hypotheses into sets of different expected accuracy and coverage levels. For those sets for which accuracy can be improved with a consequent increase of coverage a new system with new features is introduced. Its execution is triggered only when specific preconditions are met on the hypotheses generated by the basic four systems. Experimental results are provided on a corpus collected in the call center of the Paris transportation system known as ratp. The results show that surveys with high accuracy and coverage can be composed with the proposed strategy and systems. This makes it possible to apply a previously published proportion estimation approach that takes into account hypothesization errors .", "prompt": "The paper deals with the automatic analysis of real-life telephone conversations between an agent and a customer of a customer care service (ccs). The application domain is the public transportation system in Paris and the purpose is to collect statistics about customer problems in order to monitor the service and decide priorities on the intervention for improving user satisfaction. Of primary importance for the analysis is the detection of themes that are the object of customer problems. Themes are defined in the application requirements and are part of the application ontology that is implicit in the ccs documentation. Due to variety of customer population, the structure of conversations with an agent is unpredictable. A conversation may be about", "orig": "one or more themes. Theme mentions can be interleaved with mentions of facts that are irrelevant for the application purpose. Furthermore, in certain conversations theme mentions are localized in specific conversation segments while in other conversations mentions cannot be localized. As a consequence, approaches to feature extraction with and without mention localization are considered. Application domain relevant themes identified by an automatic procedure are expressed by specific sentences whose words are hypothesized by an automatic speech recognition (asr) system. The asr system is error prone. The word error rates can be very high for many reasons. Among them it is worth mentioning unpredictable background noise, speaker accent, and various types of", "generated": "multiple issues or change direction suddenly, making it challenging to identify the core problems addressed by the customer.\n\nTo address this challenge, the proposed approach employs a combination of natural language processing (NLP) techniques and machine learning algorithms to analyze the conversations. The system is trained on a large dataset of annotated conversations, where each annotation corresponds to a specific theme. The NLP module extracts relevant features from the conversation transcripts, such as keywords, entities, and sentiment analysis. The machine learning module then uses these features to classify the conversations into predefined themes. The classification results are further refined using a probabilistic framework that takes into account the uncertainty of the theme"}
{"full_text": "Monte Carlo (MC) sampling is a popular method for estimating the statistics (e.g. expectation and variance) of a random variable. Its slow convergence has led to the emergence of advanced techniques to reduce the variance of the MC estimator for the outputs of computationally expensive solvers. The control variates (CV) method corrects the MC estimator with a term derived from auxiliary random variables that are highly correlated with the original random variable. These auxiliary variables may come from surrogate models. Such a surrogate-based CV strategy is extended here to the multilevel Monte Carlo (MLMC) framework, which relies on a sequence of levels corresponding to numerical simulators with increasing accuracy and computational cost. MLMC combines output samples obtained across levels, into a telescopic sum of differences between MC estimators for successive fidelities. In this paper, we introduce three multilevel variance reduction strategies that rely on surrogate-based CV and MLMC. MLCV is presented as an extension of CV where the correction terms devised from surrogate models for simulators of different levels add up. MLMC-CV improves the MLMC estimator by using a CV based on a surrogate of the correction term at each level. Further variance reduction is achieved by using the surrogate-based CVs of all the levels in the MLMC-MLCV strategy. Alternative solutions that reduce the subset of surrogates used for the multilevel estimation are also introduced. The proposed methods are tested on a test case from the literature consisting of a spectral discretization of an uncertain 1D heat equation, where the statistic of interest is the expected value of the integrated temperature along the domain at a given time. The results are assessed in terms of the accuracy and computational cost of the multilevel estimators, depending on whether the construction of the surrogates, and the associated computational cost, precede the evaluation of the estimator. It was shown that when the lower fidelity outputs are strongly correlated with the high-fidelity outputs, a significant variance reduction is obtained when using surrogate models for the coarser levels only. It was also shown that taking advantage of pre-existing surrogate models proves to be an even more efficient strategy.", "prompt": "Monte Carlo (MC) sampling is a popular method for estimating the statistics (e.g. expectation and variance) of a random variable. Its slow convergence has led to the emergence of advanced techniques to reduce the variance of the MC estimator for the outputs of computationally expensive solvers. The control variates (CV) method corrects the MC estimator with a term derived from auxiliary random variables that are highly correlated with the original random variable. These auxiliary variables may come from surrogate models. Such a surrogate-based CV strategy is extended here to the multilevel Monte Carlo (MLMC) framework, which relies on a sequence of levels corresponding to", "orig": "numerical simulators with increasing accuracy and computational cost. MLMC combines output samples obtained across levels, into a telescopic sum of differences between MC estimators for successive fidelities. In this paper, we introduce three multilevel variance reduction strategies that rely on surrogate-based CV and MLMC. MLCV is presented as an extension of CV where the correction terms devised from surrogate models for simulators of different levels add up. MLMC-CV improves the MLMC estimator by using a CV based on a surrogate of the correction term at each level. Further variance reduction is achieved by using the surrogate-based CVs of all the levels in", "generated": "different levels of model fidelity, accuracy, or computational cost. By combining the surrogate-based CV method with the MLMC framework, the variance of the estimator can be further reduced, leading to more efficient and accurate estimation of the statistics of interest. This approach is particularly useful when dealing with complex systems that involve multiple scales, nonlinearities, or uncertainties, where the computational cost of running high-fidelity simulations can be prohibitively expensive. The surrogate models used in the CV method can be constructed using a variety of techniques, such as response surfaces, Kriging, or neural networks, and can be trained using a limited number of high-fidelity"}
{"full_text": "This article is devoted to the study of the following semilinear equation with measure data which originates in the gravitational Maxwell gauged $O(3)$ sigma model, $$-\\Delta u + A_0(\\prod^k_{j=1}|x-p_j|^{2n_j} )^{-a} \\frac{e^u}{(1+e^u)^{1+a}} = 4\\pi\\sum_{j=1}^k n_j\\delta_{p_j} - 4\\pi\\sum^l_{j=1}m_j\\delta_{q_j} \\quad{\\rm in}\\;\\; \\mathbb{R}^2.\\qquad(E)$$ In this equation the $\\{\\delta_{p_j}\\}_{j=1}^k$ (resp. $\\{\\delta_{q_j}\\}_{j=1}^l$ ) are Dirac masses concentrated at the points $\\{p_j\\}_{j=1}^k$, (resp. $\\{q_j\\}_{j=1}^l$), $n_j$ and $m_j$ are positive integers, and $a$ is a nonnegative real number. We set $ N=\\sum^k_{j=1}n_j $ and $M= \\sum^l_{j=1}m_j$. In previous works \\cite{C,Y2}, some qualitative properties of solutions of $(E)$ with $a=0$ have been established. Our aim in this article is to study the more general case where $a>0$. The additional difficulties of this case come from the fact that the nonlinearity is no longer monotone and the data are signed measures. As a consequence we cannot anymore construct directly the solutions by the monotonicity method combined with the supersolutions and subsolutions technique. Instead we develop a new and self-contained approach which enables us to emphasize the role played by the gravitation in the gauged $O(3)$ sigma model. Without the gravitational term, i.e. if $a=0$, problem $(E)$ has a layer's structure of solutions $\\{u_\\beta\\}_{\\beta\\in(-2(N-M),\\, -2]}$, where $u_\\beta$ is the unique non-topological solution such that $u_{\\beta}=\\beta\\ln |x|+O(1)$ for $-2(N-M)<\\beta<-2$ and $u_{-2}=-2\\ln |x|-2\\ln\\ln |x|+O(1)$ at infinity respectively. On the contrary, when $a>0$, the set of solutions to problem $(E)$ has a much richer structure: besides the topological solutions, there exists a sequence of non-topological solutions in type I, i.e. such that $u $ tends to $-\\infty$ at infinity, and of non-topological solutions of type II, which tend to $\\infty$ at infinity. The existence of these types of solutions depends on the values of the parameters $N,\\, M,\\, \\beta$ and on the gravitational interaction associated to $a$.", "prompt": "This article is devoted to the study of the following semilinear equation with measure data which originates in the gravitational Maxwell gauged $O(3)$ sigma model, $$-\\Delta u + A_0(\\prod^k_{j=1}|x-p_j|^{2n_j} )^{-a} \\frac{e^u}{(1+e^u)^{1+a}} = 4\\pi\\sum_{j=1}^k n_j\\delta_{p_j} - 4\\pi\\sum^l_{j=1}m_j\\delta_{q_j} \\quad{\\", "orig": "rm in}\\;\\; \\mathbb{R}^2.\\qquad(E)$$ In this equation the $\\{\\delta_{p_j}\\}_{j=1}^k$ (resp. $\\{\\delta_{q_j}\\}_{j=1}^l$ ) are Dirac masses concentrated at the points $\\{p_j\\}_{j=1}^k$, (resp. $\\{q_j\\}_{j=1}^l$), $n_j$ and $m_j$ are positive integers, and $a$ is a nonnegative real number. We set $ N=\\sum^k_{", "generated": "text{in } \\mathbb{R}^2,$$ where $u$ is the unknown function, $A_0$ is a positive constant, $a$ is a real parameter, $p_j$ and $q_j$ are distinct points in $\\mathbb{R}^2$, and $n_j$ and $m_j$ are positive integers.\n\nThe study of this equation is motivated by its connections to various areas of mathematics and physics. In particular, the gravitational Maxwell gauged $O(3)$ sigma model is a fundamental model in quantum field theory, and the equation above appears as the Euler"}
{"full_text": "Richard Olshen was born in Portland, Oregon, on May 17, 1942. Richard spent his early years in Chevy Chase, Maryland, but has lived most of his life in California. He received an A.B. in Statistics at the University of California, Berkeley, in 1963, and a Ph.D. in Statistics from Yale University in 1966, writing his dissertation under the direction of Jimmie Savage and Frank Anscombe. He served as Research Staff Statistician and Lecturer at Yale in 1966-1967. Richard accepted a faculty appointment at Stanford University in 1967, and has held tenured faculty positions at the University of Michigan (1972-1975), the University of California, San Diego (1975-1989), and Stanford University (since 1989). At Stanford, he is Professor of Health Research and Policy (Biostatistics), Chief of the Division of Biostatistics (since 1998) and Professor (by courtesy) of Electrical Engineering and of Statistics. At various times, he has had visiting faculty positions at Columbia, Harvard, MIT, Stanford and the Hebrew University. Richard's research interests are in statistics and mathematics and their applications to medicine and biology. Much of his work has concerned binary tree-structured algorithms for classification, regression, survival analysis and clustering. Those for classification and survival analysis have been used with success in computer-aided diagnosis and prognosis, especially in cardiology, oncology and toxicology. He coauthored the 1984 book Classification and Regression Trees (with Leo Brieman, Jerome Friedman and Charles Stone) which gives motivation, algorithms, various examples and mathematical theory for what have come to be known as CART algorithms. The approaches to tree-structured clustering have been applied to problems in digital radiography (with Stanford EE Professor Robert Gray) and to HIV genetics, the latter work including studies on single nucleotide polymorphisms, which has helped to shed light on the presence of hypertension in certain subpopulations of women.", "prompt": "Richard Olshen was born in Portland, Oregon, on May 17, 1942. Richard spent his early years in Chevy Chase, Maryland, but has lived most of his life in California. He received an A.B. in Statistics at the University of California, Berkeley, in 1963, and a Ph.D. in Statistics from Yale University in 1966, writing his dissertation under the direction of Jimmie Savage and Frank Anscombe. He served as Research Staff Statistician and Lecturer at Yale in 1966-1967. Richard accepted a faculty appointment at Stanford University in 1967, and has", "orig": "held tenured faculty positions at the University of Michigan (1972-1975), the University of California, San Diego (1975-1989), and Stanford University (since 1989). At Stanford, he is Professor of Health Research and Policy (Biostatistics), Chief of the Division of Biostatistics (since 1998) and Professor (by courtesy) of Electrical Engineering and of Statistics. At various times, he has had visiting faculty positions at Columbia, Harvard, MIT, Stanford and the Hebrew University. Richard's research interests are in statistics and mathematics and their applications to medicine and biology. Much of his work has", "generated": "remained there ever since, taking on various roles within the Department of Statistics, including Professor, Chair, and Emeritus Professor.\n\nThroughout his esteemed career, Richard has made significant contributions to the field of statistics, particularly in the areas of decision theory, non-parametric methods, and machine learning. He has published numerous papers and book chapters, and has co-authored several influential texts, including \"Classification and Regression Trees\" with Charles Stone, and \"Data Analysis with Tree-Based Models\" with Charles Stone and Leo Breiman. Richard's work has been widely cited and recognized, earning him numerous awards and honors, including the American Statistical Association's (ASA"}
{"full_text": "Simplified silica (Zeosil 1165 MP) and SBR (140k carrying silanol end-groups) nanocomposites have been formulated by mixing of a reduced number of ingredients with respect to industrial applications. The thermo-mechanical history of the samples during the mixing process was monitored and adjusted to identical final temperatures. The filler structure on large scales up to micrometers was studied by transmission electron microscopy (TEM) and very small-angle X-ray scattering (SAXS). A complete quantitative model extending from the primary silica nanoparticle (of radius \\approx 10 nm), to nanoparticles aggregates, up to micrometer-sized branches with typical lateral dimension of 150 nm is proposed. Image analysis of the TEM-pictures yields the fraction of zones of pure polymer, which extend between the branches of a large-scale filler network. This network is compatible with a fractal of average dimension 2.4 as measured by scattering. On smaller length scales, inside the branches, small silica aggregates are present. Their average radius has been deduced from a Kratky analysis, and it ranges between 35 and 40 nm for all silica fractions investigated here (\\phi_si = 8-21% vol.). A central piece of our analysis is the description of the interaggregate interaction by a simulated structure factor for polydisperse spheres representing aggregates. A polydispersity of 30% in aggregate size is assumed, and interactions between these aggregates are described with a hard core repulsive potential. The same distribution in size is used to evaluate the polydisperse form factor. Comparison with the experimental intensity leads to the determination of the average aggregate compacity (assumed identical for all aggregates in the distribution, between 31% and 38% depending on \\phi_si), and thus aggregation number (ca. 45, with a large spread). Because of the effect of aggregate compacity and of pure polymer zones, the volume fraction of aggregates is higher in the branches than \\phi_si. The repulsion between aggregates has a strong effect on the apparent isothermal compressibility: it leads to a characteristic low-q depression, which cannot be interpreted as aggregate mass decrease in our data. In addition, the reinforcement effect of these silica structures in the SBR-matrix is characterized with oscillatory shear and described with a model based on the same aggregate compacity. Finally, our results show that it is possible to analyze the complex structure of interacting aggregates in nanocomposites of industrial origin in a self-consistent and quantitative manner.", "prompt": "Simplified silica (Zeosil 1165 MP) and SBR (140k carrying silanol end-groups) nanocomposites have been formulated by mixing of a reduced number of ingredients with respect to industrial applications. The thermo-mechanical history of the samples during the mixing process was monitored and adjusted to identical final temperatures. The filler structure on large scales up to micrometers was studied by transmission electron microscopy (TEM) and very small-angle X-ray scattering (SAXS). A complete quantitative model extending from the primary silica nanoparticle (of radius \\approx 10 nm), to nanoparticles aggregates, up to micrometer-sized branches", "orig": "with typical lateral dimension of 150 nm is proposed. Image analysis of the TEM-pictures yields the fraction of zones of pure polymer, which extend between the branches of a large-scale filler network. This network is compatible with a fractal of average dimension 2.4 as measured by scattering. On smaller length scales, inside the branches, small silica aggregates are present. Their average radius has been deduced from a Kratky analysis, and it ranges between 35 and 40 nm for all silica fractions investigated here (\\phi_si = 8-21% vol.). A central piece of our analysis is the description of the interaggregate", "generated": ".\n\nThe results of the TEM and SAXS analysis revealed a complex hierarchical structure of the silica nanoparticles, with primary particles forming aggregates that further assemble into larger branched structures. This hierarchical organization was found to have a significant impact on the mechanical properties of the nanocomposites. The addition of silica nanoparticles was shown to improve the tensile strength and Young's modulus of the SBR matrix, with the largest enhancements observed at high filler loadings. The improved mechanical performance was attributed to the efficient stress transfer between the SBR matrix and the silica nanoparticles, which was facilitated by the strong interfacial interactions between the silanol end-groups of the S"}
{"full_text": "Context: An island parser reads an input text and builds the parse (or abstract syntax) tree of only the programming constructs of interest in the text. These constructs are called islands and the rest of the text is called water, which the parser ignores and skips over. Since an island parser does not have to parse all the details of the input, it is often easy to develop but still useful enough for a number of software engineering tools. When a parser generator is used, the developer can implement an island parser by just describing a small number of grammar rules, for example, in Parsing Expression Grammar (PEG). Inquiry: In practice, however, the grammar rules are often complicated since the developer must define the water inside the island; otherwise, the island parsing will not reduce the total number of grammar rules. When describing the grammar rules for such water, the developer must consider other rules and enumerate a set of symbols, which we call alternative symbols. Due to this difficulty, island parsing seems to be not widely used today despite its usefulness in many applications. Approach: This paper proposes the lake symbols for addressing this difficulty in developing an island parser. It also presents an extension to PEG for supporting the lake symbols. The lake symbols automate the enumeration of the alternative symbols for the water inside an island. The paper proposes an algorithm for translating the extended PEG to the normal PEG, which can be given to an existing parser generator based on PEG. Knowledge: The user can use lake symbols to define water without specifying each alternative symbol. Our algorithms can calculate all alternative symbols for a lake symbol, based on where the lake symbol is used in the grammar. Grounding: We implemented a parser generator accepting our extended PEG and implemented 36 island parsers for Java and 20 island parsers for Python. Our experiments show that the lake symbols reduce 42 % of grammar rules for Java and 89 % of rules for Python on average, excluding the case where islands are expressions. Importance: This work eases the use of island parsing. Lake symbols enable the user to define the water inside the island simpler than before. Defining water inside the island is essential to apply island parsing for practical programming languages.", "prompt": "Context: An island parser reads an input text and builds the parse (or abstract syntax) tree of only the programming constructs of interest in the text. These constructs are called islands and the rest of the text is called water, which the parser ignores and skips over. Since an island parser does not have to parse all the details of the input, it is often easy to develop but still useful enough for a number of software engineering tools. When a parser generator is used, the developer can implement an island parser by just describing a small number of grammar rules, for example, in Parsing Expression Grammar (PEG). Inquiry: In practice, however,", "orig": "the grammar rules are often complicated since the developer must define the water inside the island; otherwise, the island parsing will not reduce the total number of grammar rules. When describing the grammar rules for such water, the developer must consider other rules and enumerate a set of symbols, which we call alternative symbols. Due to this difficulty, island parsing seems to be not widely used today despite its usefulness in many applications. Approach: This paper proposes the lake symbols for addressing this difficulty in developing an island parser. It also presents an extension to PEG for supporting the lake symbols. The lake symbols automate the enumeration of the alternative symbols for the water inside an", "generated": "one of the major challenges that arise is dealing with the complexity of the water. The parser needs to efficiently skip over the irrelevant parts of the input text, which can be a daunting task, especially when dealing with large and complex codebases. Moreover, the island parser must be able to correctly identify the boundaries of the islands, ensuring that it does not mistakenly include water as part of the parse tree. This can be particularly tricky when the island constructs are interspersed with water, or when the water itself contains syntax that is similar to the island constructs. \n\nAnother issue that arises is the potential for ambiguities in the island grammar rules"}
{"full_text": "Purpose. It has been reported clinically that rupture or dissections in thoracic aortic aneurysms (TAA) often occur due to hypertension which may be modelled with sudden increase of peripheral resistance, inducing acute changes of blood volumes in the aorta. There is clinical evidence that more compliant aneurysms are less prone to rupture as they can sustain such changes of volume. The aim of the current paper is to verify this paradigm by evaluating computationally the role played by the variation of peripheral resistance and the impact of aortic stiffness onto peak wall stress in ascending TAA. Methods. Fluid-Structure Interaction (FSI) analyses were performed using patient-specific geometries and boundary conditions derived from 4D MRI datasets acquired on a patient. Blood was assumed incompressible and was treated as a non-Newtonian fluid using the Carreau model while the wall mechanical properties were obtained from the bulge inflation tests carried out in vitro after surgical repair. The Navier Stokes equations were solved in ANSYS Fluent. The Arbitrary Lagrangian Eulerian formulation was used to account for the wall deformations. At the interface between the solid domain and the fluid domain, the fluid pressure was transferred to the wall and the displacement of the wall was transferred to the fluid. The two systems were connected by the System Coupling component which controls the solver execution of fluid and solid simulations in ANSYS. Fluid and solid domains were solved sequentially starting from the fluid simulations. Results. Distributions of blood flow, wall shear stress and wall stress were evaluated in the ascending thoracic aorta using the FSI analyses. We always observed a significant flow eccentricity in the simulations, in very good agreement with velocity profiles measured using 4D MRI. The results also showed significant increase of peak wall stress due to the increase of peripheral resistance and aortic stiffness. In the worst case scenario, the largest peripheral resistance (10 10 kg.s.m-4) and stiffness (10 MPa) resulted in a maximal principal stress equal to 702 kPa, whereas it was only 77 kPa in normal conditions. Conclusions. This is the first time that the risk of rupture of an aTAA is quantified in case of the combined effects of hypertension and aortic stiffness increase. Our findings suggest that a stiffer TAA may have the most altered distribution of wall stress and an acute change of peripheral vascular resistance could significantly increase the risk of rupture for a stiffer aneurysm.", "prompt": "Purpose. It has been reported clinically that rupture or dissections in thoracic aortic aneurysms (TAA) often occur due to hypertension which may be modelled with sudden increase of peripheral resistance, inducing acute changes of blood volumes in the aorta. There is clinical evidence that more compliant aneurysms are less prone to rupture as they can sustain such changes of volume. The aim of the current paper is to verify this paradigm by evaluating computationally the role played by the variation of peripheral resistance and the impact of aortic stiffness onto peak wall stress in ascending TAA. Methods. Fluid-Structure Interaction (FSI)", "orig": "analyses were performed using patient-specific geometries and boundary conditions derived from 4D MRI datasets acquired on a patient. Blood was assumed incompressible and was treated as a non-Newtonian fluid using the Carreau model while the wall mechanical properties were obtained from the bulge inflation tests carried out in vitro after surgical repair. The Navier Stokes equations were solved in ANSYS Fluent. The Arbitrary Lagrangian Eulerian formulation was used to account for the wall deformations. At the interface between the solid domain and the fluid domain, the fluid pressure was transferred to the wall and the displacement of the wall was transferred to the fluid. The", "generated": "simulations were performed using a commercially available software package, which couples the governing equations of blood flow and aortic wall dynamics. A patient-specific model of an ascending TAA was created from computed tomography (CT) scans, and the peripheral resistance was varied to simulate hypertension. The aortic stiffness was modified by adjusting the mechanical properties of the aneurysm wall, and the resulting peak wall stress was calculated.\n\nThe simulations were repeated for various degrees of aortic stiffness, ranging from highly compliant to rigid, to investigate the effect of stiffness on the peak wall stress. The results showed a significant decrease in peak wall stress as the aortic stiffness"}
{"full_text": "The epitaxial growth of anatase (001) films deposited by pulsed laser deposition (PLD) and molecular beam epitaxy (MBE) on SrTiO3 (001) (STO) single crystals has been studied using X-ray diffraction and surface sensitivity UHV techniques. The evolution of the strain represented by the microstrain and the change of the in-plane and out-of-plane lattice parameters with film growth temperature, the effect of the annealing temperature and the influence of the oxygen content of the film have been investigated.The out-of-plane lattice strain shows a compressive (-0.2%) or expansive (+0.3%) behavior, in the range 600-900$^\\circ$C, for temperatures below or above 700$^\\circ$C, respectively. The in-plane lattice parameters, as well as the cell volume of the film, remain under compression over the entire temperature range explored.PLD films grow into square islands that align with the surface lattice directions of the STO substrate. The maximum size of these islands is reached at growth temperatures close to 875-925$^\\circ$C. Film annealing at temperatures of 800$^\\circ$C or higher melts the islands into flat terraces. Larger terraces are reached at high annealing temperatures of 925$^\\circ$C for extended periods of 12 hours. This procedure allows flat surface terrace sizes of up to 650 nm to be achieved.The crystalline quality achieved in anatase films prepared by PLD or MBE growth methods is similar. The two-step anatase growth process used during the synthesis of the films with both methods: film growth and post-annealing treatment in oxygen or air at ambient pressure, using temperature and time as key parameters, allows to control the surface terrace size and stoichiometry of the films, as well as the anatase/rutile intermixing rates at sufficiently high temperatures. This growth process could allow the substitution of their equivalent single crystals. The range of applicability of these films would include their use as structural and electronic model systems, or in harsh experimental conditions due to their low production cost.", "prompt": "The epitaxial growth of anatase (001) films deposited by pulsed laser deposition (PLD) and molecular beam epitaxy (MBE) on SrTiO3 (001) (STO) single crystals has been studied using X-ray diffraction and surface sensitivity UHV techniques. The evolution of the strain represented by the microstrain and the change of the in-plane and out-of-plane lattice parameters with film growth temperature, the effect of the annealing temperature and the influence of the oxygen content of the film have been investigated.The out-of-plane lattice strain shows a compressive (-0.2%) or expansive (+0", "orig": ".3%) behavior, in the range 600-900$^\\circ$C, for temperatures below or above 700$^\\circ$C, respectively. The in-plane lattice parameters, as well as the cell volume of the film, remain under compression over the entire temperature range explored.PLD films grow into square islands that align with the surface lattice directions of the STO substrate. The maximum size of these islands is reached at growth temperatures close to 875-925$^\\circ$C. Film annealing at temperatures of 800$^\\circ$C or higher melts the islands into flat terraces. Larger terraces are", "generated": ".3%) behavior depending on the deposition technique and the annealing conditions.\n\nThe results demonstrate a strong correlation between the lattice strain and the film's electrical properties. The compressive strain induced in the PLD-grown films is found to enhance the electrical conductivity by increasing the carrier density and mobility. In contrast, the expansive strain in the MBE-grown films leads to a decrease in the electrical conductivity due to the reduced carrier density and mobility. The effect of annealing temperature on the lattice strain is also significant, with higher annealing temperatures resulting in a relaxation of the compressive strain and a subsequent decrease in the electrical conductivity.\n\nFurthermore,"}
{"full_text": "Recently developed spatial capture-recapture (SCR) models represent a major advance over traditional capture-recapture (CR) models because they yield explicit estimates of animal density instead of population size within an unknown area. Furthermore, unlike nonspatial CR methods, SCR models account for heterogeneity in capture probability arising from the juxtaposition of animal activity centers and sample locations. Although the utility of SCR methods is gaining recognition, the requirement that all individuals can be uniquely identified excludes their use in many contexts. In this paper, we develop models for situations in which individual recognition is not possible, thereby allowing SCR concepts to be applied in studies of unmarked or partially marked populations. The data required for our model are spatially referenced counts made on one or more sample occasions at a collection of closely spaced sample units such that individuals can be encountered at multiple locations. Our approach includes a spatial point process for the animal activity centers and uses the spatial correlation in counts as information about the number and location of the activity centers. Camera-traps, hair snares, track plates, sound recordings, and even point counts can yield spatially correlated count data, and thus our model is widely applicable. A simulation study demonstrated that while the posterior mean exhibits frequentist bias on the order of 5-10% in small samples, the posterior mode is an accurate point estimator as long as adequate spatial correlation is present. Marking a subset of the population substantially increases posterior precision and is recommended whenever possible. We applied our model to avian point count data collected on an unmarked population of the northern parula (Parula americana) and obtained a density estimate (posterior mode) of 0.38 (95% CI: 0.19-1.64) birds/ha. Our paper challenges sampling and analytical conventions in ecology by demonstrating that neither spatial independence nor individual recognition is needed to estimate population density - rather, spatial dependence can be informative about individual distribution and density.", "prompt": "Recently developed spatial capture-recapture (SCR) models represent a major advance over traditional capture-recapture (CR) models because they yield explicit estimates of animal density instead of population size within an unknown area. Furthermore, unlike nonspatial CR methods, SCR models account for heterogeneity in capture probability arising from the juxtaposition of animal activity centers and sample locations. Although the utility of SCR methods is gaining recognition, the requirement that all individuals can be uniquely identified excludes their use in many contexts. In this paper, we develop models for situations in which individual recognition is not possible, thereby allowing SCR concepts to be applied in studies of unmarked or partially", "orig": "marked populations. The data required for our model are spatially referenced counts made on one or more sample occasions at a collection of closely spaced sample units such that individuals can be encountered at multiple locations. Our approach includes a spatial point process for the animal activity centers and uses the spatial correlation in counts as information about the number and location of the activity centers. Camera-traps, hair snares, track plates, sound recordings, and even point counts can yield spatially correlated count data, and thus our model is widely applicable. A simulation study demonstrated that while the posterior mean exhibits frequentist bias on the order of 5-10% in small", "generated": "marked populations.\n\nOur approach builds upon the existing SCR framework, but relaxes the assumption of individual identifiability by incorporating a clustering component that groups individuals based on their spatial proximity and capture history. This allows us to estimate animal density and spatial distribution patterns, even in the absence of individual-level identification. We demonstrate the effectiveness of our approach through a series of simulations, showcasing its ability to accurately recover density and spatial structure in a variety of scenarios. Furthermore, we apply our method to a real-world dataset of camera trap observations of a large carnivore species, illustrating its potential to inform conservation and management efforts.\n\nThe implications of our work are far"}
{"full_text": "We consider the problem of deterministic load balancing of tokens in the discrete model. A set of $n$ processors is connected into a $d$-regular undirected network. In every time step, each processor exchanges some of its tokens with each of its neighbors in the network. The goal is to minimize the discrepancy between the number of tokens on the most-loaded and the least-loaded processor as quickly as possible. Rabani et al. (1998) present a general technique for the analysis of a wide class of discrete load balancing algorithms. Their approach is to characterize the deviation between the actual loads of a discrete balancing algorithm with the distribution generated by a related Markov chain. The Markov chain can also be regarded as the underlying model of a continuous diffusion algorithm. Rabani et al. showed that after time $T = O(\\log (Kn)/\\mu)$, any algorithm of their class achieves a discrepancy of $O(d\\log n/\\mu)$, where $\\mu$ is the spectral gap of the transition matrix of the graph, and $K$ is the initial load discrepancy in the system. In this work we identify some natural additional conditions on deterministic balancing algorithms, resulting in a class of algorithms reaching a smaller discrepancy. This class contains well-known algorithms, eg., the Rotor-Router. Specifically, we introduce the notion of cumulatively fair load-balancing algorithms where in any interval of consecutive time steps, the total number of tokens sent out over an edge by a node is the same (up to constants) for all adjacent edges. We prove that algorithms which are cumulatively fair and where every node retains a sufficient part of its load in each step, achieve a discrepancy of $O(\\min\\{d\\sqrt{\\log n/\\mu},d\\sqrt{n}\\})$ in time $O(T)$. We also show that in general neither of these assumptions may be omitted without increasing discrepancy. We then show by a combinatorial potential reduction argument that any cumulatively fair scheme satisfying some additional assumptions achieves a discrepancy of $O(d)$ almost as quickly as the continuous diffusion process. This positive result applies to some of the simplest and most natural discrete load balancing schemes.", "prompt": "We consider the problem of deterministic load balancing of tokens in the discrete model. A set of $n$ processors is connected into a $d$-regular undirected network. In every time step, each processor exchanges some of its tokens with each of its neighbors in the network. The goal is to minimize the discrepancy between the number of tokens on the most-loaded and the least-loaded processor as quickly as possible. Rabani et al. (1998) present a general technique for the analysis of a wide class of discrete load balancing algorithms. Their approach is to characterize the deviation between the actual loads of a discrete balancing algorithm with the distribution generated", "orig": "by a related Markov chain. The Markov chain can also be regarded as the underlying model of a continuous diffusion algorithm. Rabani et al. showed that after time $T = O(\\log (Kn)/\\mu)$, any algorithm of their class achieves a discrepancy of $O(d\\log n/\\mu)$, where $\\mu$ is the spectral gap of the transition matrix of the graph, and $K$ is the initial load discrepancy in the system. In this work we identify some natural additional conditions on deterministic balancing algorithms, resulting in a class of algorithms reaching a smaller discrepancy. This class contains well-known algorithms, eg", "generated": "by an optimal continuous balancing algorithm.\n\nBuilding upon this foundation, we can further explore the properties of discrete load balancing algorithms. Specifically, we can investigate the impact of the network topology on the load balancing process. For instance, we can examine how the degree distribution of the network affects the convergence rate of the algorithm. Additionally, we can study the role of token injection and removal patterns on the overall system performance. By analyzing these aspects, we can develop more efficient and adaptive load balancing strategies that take into account the intrinsic characteristics of the network.\n\nMoreover, we can also delve into the realm of dynamic load balancing, where the number of tokens and processors"}
{"full_text": "Angiogenesis, the formation of new vessels, is one of the key mechanisms in tumor development and an appealing target for therapy. Non-invasive, high-resolution, high sensitivity, quantitative 3D imaging techniques are required to correctly depict tumor heterogeneous vasculature over time. Ultrafast Doppler was recently introduced and provides an unprecedented combination of resolution, penetration depth and sensitivity without requiring any contrast agents. The technique was further extended to 3D with Ultrafast Doppler Tomography (UFD-T). In this work, UFD-T was applied to the monitoring of tumor angiogenesis in vivo providing structural and functional information at different stages of development. UFD-T volume renderings showed that our murine model's vasculature stems from pre-existing vessels and sprouts to perfuse the whole volume as the tumor grows until a critical size is reached. Then, as the network becomes insufficient, the tumor core is no longer irrigated because the vasculature is mainly concentrated in the periphery. In addition to spatial distribution and growth patterns, UFD-T allowed a quantitative analysis of vessel size and length, revealing that the diameter-distribution of vessels remained relatively constant throughout tumor growth. The network is dominated by small vessels at all stages of tumor development with more than 74% of the vessels less than 200 $\\mu$m in diameter. This study also showed that cumulative vessel length is more closely related to tumor radius than volume, indicating that the vascularization becomes insufficient when a critical mass is reached. UFD-T was also compared with dynamic contrast-enhanced ultrasound (DCE-US) and shown to provide complementary information regarding the link between structure and perfusion. In conclusion, UFD-T is capable of an in vivo quantitative assessment of the development of tumor vasculature (vessels with blood speed >1mm/s (sensitivity limit) assessed with a resolution limit of 80 $\\mu$m) in 3D. The technique has very interesting potential as a tool for treatment monitoring, response assessment and treatment planning for optimal drug efficiency.", "prompt": "Angiogenesis, the formation of new vessels, is one of the key mechanisms in tumor development and an appealing target for therapy. Non-invasive, high-resolution, high sensitivity, quantitative 3D imaging techniques are required to correctly depict tumor heterogeneous vasculature over time. Ultrafast Doppler was recently introduced and provides an unprecedented combination of resolution, penetration depth and sensitivity without requiring any contrast agents. The technique was further extended to 3D with Ultrafast Doppler Tomography (UFD-T). In this work, UFD-T was applied to the monitoring of tumor angiogenesis in vivo providing structural and functional information at different stages of", "orig": "development. UFD-T volume renderings showed that our murine model's vasculature stems from pre-existing vessels and sprouts to perfuse the whole volume as the tumor grows until a critical size is reached. Then, as the network becomes insufficient, the tumor core is no longer irrigated because the vasculature is mainly concentrated in the periphery. In addition to spatial distribution and growth patterns, UFD-T allowed a quantitative analysis of vessel size and length, revealing that the diameter-distribution of vessels remained relatively constant throughout tumor growth. The network is dominated by small vessels at all stages of tumor development with more than 74%", "generated": "tumor development.\n\nThe results of this study demonstrated the potential of UFD-T to non-invasively quantify changes in tumor vasculature, enabling the identification of early markers of angiogenesis. The high-resolution 3D images provided by UFD-T allowed for the visualization of intricate vascular networks and the detection of subtle changes in blood flow patterns. Furthermore, the technique's high sensitivity enabled the detection of early stage angiogenic events, providing valuable insights into the underlying biological mechanisms. The UFD-T technique was also found to be highly reproducible, making it a promising tool for longitudinal studies and personalized medicine applications.\n\nIn addition to its potential in"}
{"full_text": "This paper shortly summarises the experimental results obtained since 2011 by a large European academic consortium for the scientific conceptualisation, the definition of the technical requirements, the generation of experimental data, and the validation of a numerical code, for the Pulsating Heat Pipes (PHP) experiment on the International Space Station (ISS). The PHP is a passive, wickless thermal device, whereby a two-phase fluid, forming liquid plugs and vapour slugs, moves with a pulsating or circulating motion inside a meandering tube or channel. The PHP may have a very broad range of geometries (flat, tubular, 3D structured), it can dissipate heat from large areas, and it can be suitable for high power applications with low/medium heat fluxes. PHP functioning is based on the capillary effect, which provides the existence of liquid plugs completely filling the channel cross-section, in a way that any expansion or contraction of the vapour slugs will naturally generate a movement of the fluid along the channel axis. For this, it is important that the channel has a cross-section size below a given threshold, which depends on the liquid surface tension and (for a static fluid) on the gravity acceleration. In space, when only residual accelerations are acting, such a static size threshold is virtually infinite, while a finite dynamic threshold exists even in the absence of gravity. The concept of a ''Space PHP'' was originally developed in 2014 by the team, and from then 17 Parabolic Flight Campaigns (PFC) and 3 Sounding Rocket (SR) experiments have been carried out to generate the data for the preparation of an experiment targeting a Low Earth Orbit (LEO) mission. Both a tubular and a flat plate PHP have been successfully tested in reduced gravity and on ground, by using different combinations of fluids and building materials. The need for having an experiment on a LEO environment is mainly because, during a PFC, only 22sec of reduced gravity are possible, which is a period below the characteristic time for reaching a steady state condition for almost all of the tested devices. Instead, a steady state was reached using the SR campaigns: in this case however, only one experimental condition was achievable, and long-duration data of the PHP performance still remains beyond reach. Several measurement methodologies have been used to characterise the Space PHP, like infrared analysis, high-speed camera visualisation techniques, with data processed with different techniques, from wavelets to inverse heat transfer problem solution. The results clearly showed that PHPs are very interesting for space applications due to their simplicity of construction, the capacity to transfer heat up to several hundred watts, a high power/weight ratio, their geometrical adaptability, and, in particular, the Space PHP will be a breakthrough technology for space thermal management.", "prompt": "This paper shortly summarises the experimental results obtained since 2011 by a large European academic consortium for the scientific conceptualisation, the definition of the technical requirements, the generation of experimental data, and the validation of a numerical code, for the Pulsating Heat Pipes (PHP) experiment on the International Space Station (ISS). The PHP is a passive, wickless thermal device, whereby a two-phase fluid, forming liquid plugs and vapour slugs, moves with a pulsating or circulating motion inside a meandering tube or channel. The PHP may have a very broad range of geometries (flat, tubular, 3D", "orig": "structured), it can dissipate heat from large areas, and it can be suitable for high power applications with low/medium heat fluxes. PHP functioning is based on the capillary effect, which provides the existence of liquid plugs completely filling the channel cross-section, in a way that any expansion or contraction of the vapour slugs will naturally generate a movement of the fluid along the channel axis. For this, it is important that the channel has a cross-section size below a given threshold, which depends on the liquid surface tension and (for a static fluid) on the gravity acceleration. In space, when only residual accelerations are acting,", "generated": "), and its operating temperature can vary from cryogenic to high temperatures, making it an attractive solution for a wide range of applications, including space, aeronautics, and terrestrial industries.\n\nThe experimental campaign, which has been ongoing for over a decade, has focused on understanding the underlying physical mechanisms governing the PHP's behaviour, including the onset of oscillations, the dynamics of the liquid-vapour interface, and the effects of gravity on the performance of the device. The consortium has developed a sophisticated numerical code, validated through extensive comparisons with experimental data, to simulate the behaviour of the PHP under various operating conditions. The code has been used"}
{"full_text": "A standing challenge in undergraduate Computer Science curricula is the teaching and learning of computer programming. Through this paper which is an essay about programming, we aim to contribute to the plethora of existing pedagogies, approaches and philosophies, by discussing a specific feature of our approach in teaching principled programming to undergraduate students, in their first semester of studies, namely the utilization of pictures, both text-based and raster-based graphics. Although the given course has evolved substantially over the thirty years of its delivery regarding the programming languages (Miranda, C, C++, Java) and paradigms (functional, imperative, object-oriented, combination of procedural and object-oriented) used, the discussed visual feature has been maintained and steadily strengthened. We list abstraction, problem decomposition and synthesis, information hiding, reusability, modularity and extensibility as key principles of problem solving and algorithmic thinking. These principles are closely aligned with the advocated computational thinking techniques of problem decomposition, pattern recognition, pattern generalization and algorithm design. We aim for our students to familiarize themselves with all the above principles through practical problem solving. Our ongoing inquiry has been whether the problem domain of pictures is contributing valuably towards this aim. Moreover, an added-value is that students get a glimpse of computational complexity in a visual, empirical way. The presented work is not related to visual programming, since the students write their programs textually and not graphically; it's the output of their programs which is in visual form. Our approach though is loosely related to the classical paradigm of turtle graphics. However, our focus is Computer Science majors, who should be able to design and build turtles and other objects and not just use them. Indeed, the programming principles course helps them to do both and also to appreciate the multitude of algorithmic ways for producing the same visual output. Currently the given programming principles are approached both from a procedural, process-based and an object-oriented, concept-based perspective and the course uses the Java language. Through the presented example problems, we aim to show the appropriateness of the visual domain of pictures for supporting the learning of principled programming. The problem domain of pictures is abundantly rich with potential examples to draw from. Moreover, as reported in the literature, female students may show higher interest towards visual problem domains in programming classes, in relation to other problem domains. We plan to investigate this conjecture in the context of our broader aim to encourage more females to follow university studies in computer science; in this paper only a cursory finding is presented, that bears some relation to what is reported in the literature.", "prompt": "A standing challenge in undergraduate Computer Science curricula is the teaching and learning of computer programming. Through this paper which is an essay about programming, we aim to contribute to the plethora of existing pedagogies, approaches and philosophies, by discussing a specific feature of our approach in teaching principled programming to undergraduate students, in their first semester of studies, namely the utilization of pictures, both text-based and raster-based graphics. Although the given course has evolved substantially over the thirty years of its delivery regarding the programming languages (Miranda, C, C++, Java) and paradigms (functional, imperative, object-oriented, combination of procedural and", "orig": "object-oriented) used, the discussed visual feature has been maintained and steadily strengthened. We list abstraction, problem decomposition and synthesis, information hiding, reusability, modularity and extensibility as key principles of problem solving and algorithmic thinking. These principles are closely aligned with the advocated computational thinking techniques of problem decomposition, pattern recognition, pattern generalization and algorithm design. We aim for our students to familiarize themselves with all the above principles through practical problem solving. Our ongoing inquiry has been whether the problem domain of pictures is contributing valuably towards this aim. Moreover, an added-value is that students get a glimpse of computational complexity in a", "generated": "object-oriented), our primary focus remains on imparting a deep understanding of fundamental computing concepts and the development of sound programming skills.\n\nIn this regard, we believe that the incorporation of visual aids, specifically pictures, has been instrumental in facilitating a more comprehensive grasp of programming principles among our students. By leveraging the cognitive benefits of visual learning, we have observed a significant improvement in students' ability to comprehend complex algorithms, data structures, and programming concepts. For instance, the use of text-based graphics has enabled students to better visualize the flow of control in programming languages, thereby enhancing their understanding of loops, conditional statements, and functions. Similarly, raster-based"}
{"full_text": "Graph Neural Networks share with Logic Programming several key relational inference mechanisms. The datasets on which they are trained and evaluated can be seen as database facts containing ground terms. This makes possible modeling their inference mechanisms with equivalent logic programs, to better understand not just how they propagate information between the entities involved in the machine learning process but also to infer limits on what can be learned from a given dataset and how well that might generalize to unseen test data. This leads us to the key idea of this paper: modeling with the help of a logic program the information flows involved in learning to infer from the link structure of a graph and the information content of its nodes properties of new nodes, given their known connections to nodes with possibly similar properties. The problem is known as graph node property prediction and our approach will consist in emulating with help of a Prolog program the key information propagation steps of a Graph Neural Network's training and inference stages. We test our a approach on the ogbn-arxiv node property inference benchmark. To infer class labels for nodes representing papers in a citation network, we distill the dependency trees of the text associated to each node into directed acyclic graphs that we encode as ground Prolog terms. Together with the set of their references to other papers, they become facts in a database on which we reason with help of a Prolog program that mimics the information propagation in graph neural networks predicting node properties. In the process, we invent ground term similarity relations that help infer labels in the test set by propagating node properties from similar nodes in the training set and we evaluate their effectiveness in comparison with that of the graph's link structure. Finally, we implement explanation generators that unveil performance upper bounds inherent to the dataset. As a practical outcome, we obtain a logic program, that, when seen as machine learning algorithm, performs close to the state of the art on the node property prediction benchmark.", "prompt": "Graph Neural Networks share with Logic Programming several key relational inference mechanisms. The datasets on which they are trained and evaluated can be seen as database facts containing ground terms. This makes possible modeling their inference mechanisms with equivalent logic programs, to better understand not just how they propagate information between the entities involved in the machine learning process but also to infer limits on what can be learned from a given dataset and how well that might generalize to unseen test data. This leads us to the key idea of this paper: modeling with the help of a logic program the information flows involved in learning to infer from the link structure of a graph and the information content of its nodes", "orig": "properties of new nodes, given their known connections to nodes with possibly similar properties. The problem is known as graph node property prediction and our approach will consist in emulating with help of a Prolog program the key information propagation steps of a Graph Neural Network's training and inference stages. We test our a approach on the ogbn-arxiv node property inference benchmark. To infer class labels for nodes representing papers in a citation network, we distill the dependency trees of the text associated to each node into directed acyclic graphs that we encode as ground Prolog terms. Together with the set of their references to other papers, they become facts in a", "generated": ".\n\nBy formalizing the inference mechanisms of Graph Neural Networks using Logic Programming, we can gain a deeper understanding of how they operate and make predictions. This can be particularly useful in domains where the graph structure is critical, such as social network analysis, traffic pattern prediction, or molecular interactions. By representing the graph as a logical database, we can leverage the powerful inference engines of Logic Programming to reason about the relationships between entities and identify patterns that may not be immediately apparent. Furthermore, this approach can provide a framework for evaluating the robustness and explainability of Graph Neural Networks, enabling researchers to identify potential biases and limitations in the models.\n\nOne of"}
{"full_text": "A new Riemann solver is built to address numerical resolution of complex flow models. The research direction is closely linked to a variant of the Baer and Nunziato (1986) model developed in Saurel et al. (2017a). This recent model provides a link between the Marble (1963) model for two-phase dilute suspensions and dense mixtures. As in the Marble model, Saurel et al. system is weakly hyperbolic with the same 4 characteristic waves, while the system involves 7 partial differential equations. It poses serious theoretical and practical issues to built simple and accurate flow solver. To overcome related difficulties the Riemann solver of Linde (2002) is revisited. The method is first examined in the simplified context of compressible Euler equations. Physical considerations are introduced in the solver improving robustness and accuracy of the Linde method. With these modifications the flow solver appears as accurate as the HLLC solver of Toro et al. (1994). Second the two-phase flow model is considered. A locally conservative formulation is built and validated removing issues related to non-conservative terms. However, two extra major issues appear from numerical experiments: The solution appears not self-similar and multiple contact waves appear in the dispersed phase. Building HLLC-type or any other solver appears consequently challenging. The modified Linde (2002) method is thus examined for the considered flow model. Some basic properties of the equations are used, such as shock relations of the dispersed phase and jump conditions across the contact wave. Thanks to these ingredients the new Riemann solver with internal reconstruction (RSIR), modification of the Linde method, handles stationary volume fraction discontinuities, presents low dissipation for transport waves and handles shocks and expansion waves accurately. It is validated on various test problems showing method's accuracy and versatility for complex flow models. Its capabilities are illustrated on a difficult two-phase flow instability problem, unresolved before.", "prompt": "A new Riemann solver is built to address numerical resolution of complex flow models. The research direction is closely linked to a variant of the Baer and Nunziato (1986) model developed in Saurel et al. (2017a). This recent model provides a link between the Marble (1963) model for two-phase dilute suspensions and dense mixtures. As in the Marble model, Saurel et al. system is weakly hyperbolic with the same 4 characteristic waves, while the system involves 7 partial differential equations. It poses serious theoretical and practical issues to built simple and accurate flow", "orig": "solver. To overcome related difficulties the Riemann solver of Linde (2002) is revisited. The method is first examined in the simplified context of compressible Euler equations. Physical considerations are introduced in the solver improving robustness and accuracy of the Linde method. With these modifications the flow solver appears as accurate as the HLLC solver of Toro et al. (1994). Second the two-phase flow model is considered. A locally conservative formulation is built and validated removing issues related to non-conservative terms. However, two extra major issues appear from numerical experiments: The solution appears not self-similar and multiple contact waves appear", "generated": "solvers.\n\nThe necessity for a new Riemann solver arises from the complexities inherent in the Saurel et al. model. The model's weakly hyperbolic nature and the presence of 4 characteristic waves require a solver that can handle these features accurately. Furthermore, the involvement of 7 partial differential equations demands a solver that can efficiently manage the interactions between these equations. The new Riemann solver aims to address these challenges by providing a robust and accurate numerical method for resolving complex flow models.\n\nIn the development of the new Riemann solver, several key aspects will be taken into consideration. Firstly, the solver will be"}
{"full_text": "Single electron sources enable electron quantum optics experiments where single electrons emitted in a ballistic electronic interferometer plays the role of a single photons emitted in an optical medium in Quantum Optics. A qualitative step has been made with the recent generation of single charge levitons obtained by applying Lorentzian voltage pulse on the contact of the quantum conductor. Simple to realize and operate, the source emits electrons in the form of striking minimal excitation states called levitons. We review the striking properties of levitons and their possible applications in quantum physics to electron interferometry and entanglement. W Schematic generation of time resolved single charges called levitons using Lorentzian voltage pulses applied on a contact. A Quantum Point Contact is used to partition the levitons for further analysis. Injecting levitons on opposite contacts with a delay $\\\\tau$ enables to probe electronic like Hong Ou Mandel correlations. Copyright line will be provided by the publisher 1 Single electron sources In this introduction, we will distinguish single charge sources from coherent single electrons sources. The former have been developed for quantum metrology where the goal is to transfer an integer charge at high frequency f through a conductor with good accuracy to realize a quantized current source whose current I = ef shows metrological accuracy. The latter, the coherent single electrons source, aims at emitting (injecting) a single electron whose wave-function is well defined and controlled to realize further single electron coherent manipulation via quantum gates. The gates are provided by electronic beam-splitters made with Quantum Point Contacts or provided by electronic Mach-Zehnder and Fabry-Prot interferometers. Here it is important that the injected single electron is the only excitation created in the conductor. The frequency f of injection is not chosen to have a large current, as current accuracy is not the goal, but only to get sufficient statistics on the electron transfer events to extract physical information. 1.1 single charge sources for current standards The first manipulation of single charges trace back to the early 90's where physicists took advantage of charge quan-tization of a submicronic metallic island nearly isolated from leads by tunnel barriers. The finite energy E C = e 2 /2C to charge the small capacitor C with a single charge being larger than temperature (typically one kelvin for", "prompt": "Single electron sources enable electron quantum optics experiments where single electrons emitted in a ballistic electronic interferometer plays the role of a single photons emitted in an optical medium in Quantum Optics. A qualitative step has been made with the recent generation of single charge levitons obtained by applying Lorentzian voltage pulse on the contact of the quantum conductor. Simple to realize and operate, the source emits electrons in the form of striking minimal excitation states called levitons. We review the striking properties of levitons and their possible applications in quantum physics to electron interferometry and entanglement. W Schematic generation of time resolved single charges called lev", "orig": "itons using Lorentzian voltage pulses applied on a contact. A Quantum Point Contact is used to partition the levitons for further analysis. Injecting levitons on opposite contacts with a delay $\\\\tau$ enables to probe electronic like Hong Ou Mandel correlations. Copyright line will be provided by the publisher 1 Single electron sources In this introduction, we will distinguish single charge sources from coherent single electrons sources. The former have been developed for quantum metrology where the goal is to transfer an integer charge at high frequency f through a conductor with good accuracy to realize a quantized current source whose current I = ef shows metrological", "generated": "itons. \n\nThe generation of levitons has opened up new avenues for the study of quantum phenomena in electronic systems. One of the most fascinating applications of levitons is in the field of electron interferometry, where the principles of quantum optics can be applied to electronic systems. By using levitons as a source of single electrons, researchers can study the interference patterns of electrons in a ballistic electronic interferometer, providing insights into the quantum nature of electronic systems. Moreover, the ability to generate entangled electrons using levitons has the potential to revolutionize the field of quantum computing and quantum communication. \n\nIn addition to their applications"}
{"full_text": "Context: Model checking has become a key tool for gaining confidence in correctness of multi-threaded programs. Unit tests and functional tests do not suffice because of race conditions that are not discovered by those tests. This problem is addressed by model checking tools. A simple model checker is useful for detecting race conditions prior to production. Inquiry: Current model checkers hardwire the behavior of common thread operations, and do not recognize application-dependent thread paradigms or functions using simpler primitive operations. This introduces additional operations, causing current model checkers to be excessively slow. In addition, there is no mechanism to model the semantics of the actual thread wakeup policies implemented in the underlying thread library or operating system. Eliminating these constraints can make model checkers faster. Approach: McMini is an **extensible** model checker based on DPOR (Dynamic Partial Order Reduction). A mechanism was invented to declare to McMini new, primitive thread operations, typically in 100~lines or less of C~code. The mechanism was extended to also allow a user of McMini to declare alternative thread wakeup policies, including spurious wakeups from condition variables. Knowledge: In McMini, the user defines new thread operations. The user optimizes these operations by declaring to the DPOR algorithm information that reduces the number of thread schedules to be searched. One declares: (i) under what conditions an operation is enabled; (ii) which thread operations are independent of each other; and (iii) when two operations can be considered as co-enabled. An optional wakeup policy is implemented by defining when a wait operation (on a semaphore, condition variable, etc.) is enabled. A new enqueue thread operation is described, allowing a user to declare alternative wakeup policies. Grounding: McMini was first confirmed to operate correctly and efficiently as a traditional, but extensible model checker for mutex, semaphore, condition variable, and reader-writer lock. McMini's extensibility was then tested on novel primitive operations, representing other useful paradigms for multithreaded operations. An example is readers-and-two-writers. The speed of model checking was found to be five times faster and more, as compared to traditional implementations on top of condition variables. Alternative wakeup policies (e.g., FIFO, LIFO, arbitrary, etc.) were then tested using an enqueue operation. Finally, spurious wakeups were tested with a program that exposes a bug **only** in the presence of a spurious wakeup. Importance: Many applications employ functions for multithreaded paradigms that go beyond the traditional mutex, semaphore, and condition variables. They are defined on top of basic operations. The ability to directly define new primitives for these paradigms makes model checkers run faster by searching fewer thread schedules. The ability to model particular thread wakeup policies, including spurious wakeup for condition variables, is also important. Note that POSIX leaves undefined the wakeup policies of `pthread_mutex_lock`, `sem_wait`, and `pthread_cond_wait`. The POSIX thread implementation then chooses a particular policy (e.g., FIFO, arbitrary), which can be directly modeled by McMini.", "prompt": "Context: Model checking has become a key tool for gaining confidence in correctness of multi-threaded programs. Unit tests and functional tests do not suffice because of race conditions that are not discovered by those tests. This problem is addressed by model checking tools. A simple model checker is useful for detecting race conditions prior to production. Inquiry: Current model checkers hardwire the behavior of common thread operations, and do not recognize application-dependent thread paradigms or functions using simpler primitive operations. This introduces additional operations, causing current model checkers to be excessively slow. In addition, there is no mechanism to model the semantics of the actual thread wakeup policies implemented", "orig": "in the underlying thread library or operating system. Eliminating these constraints can make model checkers faster. Approach: McMini is an **extensible** model checker based on DPOR (Dynamic Partial Order Reduction). A mechanism was invented to declare to McMini new, primitive thread operations, typically in 100~lines or less of C~code. The mechanism was extended to also allow a user of McMini to declare alternative thread wakeup policies, including spurious wakeups from condition variables. Knowledge: In McMini, the user defines new thread operations. The user optimizes these operations by declaring to the DPOR algorithm information that reduces the", "generated": ".\n\nOne possible solution to this limitation is to develop a more flexible and extensible model checking framework that can accommodate diverse thread paradigms and operations. This could be achieved by allowing users to define and integrate custom thread libraries or plugins that can seamlessly interact with the model checker. This approach would enable the model checker to recognize and reason about application-specific thread behaviors, thus reducing the number of unnecessary operations and improving overall performance.\n\nMoreover, the framework could be designed to incorporate a policy-based modeling approach, where the semantics of thread wakeup policies are explicitly defined and modeled. This would enable the model checker to accurately capture the intricacies of thread scheduling and"}
{"full_text": "Collisions of actinide nuclei form, during very short times of few zs ($10^{-21}$ s), the heaviest ensembles of interacting nucleons available on Earth. Such collisions are used to produce super-strong electric fields by the huge number of interacting protons to test spontaneous positron-electron pair emission (vacuum decay) predicted by the quantum electrodynamics (QED) theory. Multi-nucleon transfer in actinide collisions could also be used as an alternative way to fusion in order to produce neutron-rich heavy and superheavy elements thanks to inverse quasifission mechanisms. Actinide collisions are studied in a dynamical quantum microscopic approach. The three-dimensional time-dependent Hartree-Fock (TDHF) code {\\textsc{tdhf3d}} is used with a full Skyrme energy density functional to investigate the time evolution of expectation values of one-body operators, such as fragment position and particle number. This code is also used to compute the dispersion of the particle numbers (e.g., widths of fragment mass and charge distributions) from TDHF transfer probabilities, on the one hand, and using the Balian-Veneroni variational principle, on the other hand. A first application to test QED is discussed. Collision times in $^{238}$U+$^{238}$U are computed to determine the optimum energy for the observation of the vacuum decay. It is shown that the initial orientation strongly affects the collision times and reaction mechanism. The highest collision times predicted by TDHF in this reaction are of the order of $\\sim4$ zs at a center of mass energy of 1200 MeV. According to modern calculations based on the Dirac equation, the collision times at $E_{cm}>1$ GeV are sufficient to allow spontaneous electron-positron pair emission from QED vacuum decay, in case of bare uranium ion collision. A second application of actinide collisions to produce neutron-rich transfermiums is discussed. A new inverse quasifission mechanism associated to a specific orientation of the nuclei is proposed to produce transfermium nuclei ($Z>100$) in the collision of prolate deformed actinides such as $^{232}$Th+$^{250}$Cf. The collision of the tip of one nucleus with the side of the other results in a nucleon flux toward the latter. The probability distributions for transfermium production in such a collision are computed. The produced nuclei are more neutron-rich than those formed in fusion reactions, thus, leading to more stable isotopes closer to the predicted superheavy island of stability. In addition to mass and charge dispersion, the Balian-Veneroni variational principle is used to compute correlations between $Z$ and $N$ distributions, which are zero in standard TDHF calculations.", "prompt": "Collisions of actinide nuclei form, during very short times of few zs ($10^{-21}$ s), the heaviest ensembles of interacting nucleons available on Earth. Such collisions are used to produce super-strong electric fields by the huge number of interacting protons to test spontaneous positron-electron pair emission (vacuum decay) predicted by the quantum electrodynamics (QED) theory. Multi-nucleon transfer in actinide collisions could also be used as an alternative way to fusion in order to produce neutron-rich heavy and superheavy elements thanks to inverse quasifission mechanisms. Actinide collisions are studied in", "orig": "a dynamical quantum microscopic approach. The three-dimensional time-dependent Hartree-Fock (TDHF) code {\\textsc{tdhf3d}} is used with a full Skyrme energy density functional to investigate the time evolution of expectation values of one-body operators, such as fragment position and particle number. This code is also used to compute the dispersion of the particle numbers (e.g., widths of fragment mass and charge distributions) from TDHF transfer probabilities, on the one hand, and using the Balian-Veneroni variational principle, on the other hand. A first application to test QED is discussed. Collision times in", "generated": "various experimental facilities, such as the GSI Helmholtz Centre for Heavy Ion Research in Darmstadt, Germany, and the Joint Institute for Nuclear Research in Dubna, Russia.\n\nThe experimental setup typically involves accelerating beams of actinide nuclei, such as uranium or curium, to high energies and then colliding them with a target material, often another actinide nucleus or a lighter element like carbon or aluminum. The resulting collision dynamics are then analyzed using detectors and spectrometers to identify the produced particles and nuclei. By studying the properties of these particles and nuclei, scientists can gain insight into the fundamental forces of nature,"}
{"full_text": "The context of this work is specification, detection and ultimately removal of detectable harmful patterns in source code that are associated with defects in design and implementation of software. In particular, we investigate five code smells and four antipatterns previously defined in papers and books. Our inquiry is about detecting those in source code written in Python programming language, which is substantially different from all prior research, most of which concerns Java or C-like languages. Our approach was that of software engineers: we have processed existing research literature on the topic, extracted both the abstract definitions of nine design defects and their concrete implementation specifications, implemented them all in a tool we have programmed and let it loose on a huge test set obtained from open source code from thousands of GitHub projects. When it comes to knowledge, we have found that more than twice as many methods in Python can be considered too long (statistically extremely longer than their neighbours within the same project) than in Java, but long parameter lists are seven times less likely to be found in Python code than in Java code. We have also found that Functional Decomposition, the way it was defined for Java, is not found in the Python code at all, and Spaghetti Code and God Classes are extremely rare there as well. The grounding and the confidence in these results comes from the fact that we have performed our experiments on 32'058'823 lines of Python code, which is by far the largest test set for a freely available Python parser. We have also designed the experiment in such a way that it aligned with prior research on design defect detection in Java in order to ease the comparison if we treat our own actions as a replication. Thus, the importance of the work is both in the unique open Python grammar of highest quality, tested on millions of lines of code, and in the design defect detection tool which works on something else than Java.", "prompt": "The context of this work is specification, detection and ultimately removal of detectable harmful patterns in source code that are associated with defects in design and implementation of software. In particular, we investigate five code smells and four antipatterns previously defined in papers and books. Our inquiry is about detecting those in source code written in Python programming language, which is substantially different from all prior research, most of which concerns Java or C-like languages. Our approach was that of software engineers: we have processed existing research literature on the topic, extracted both the abstract definitions of nine design defects and their concrete implementation specifications, implemented them all in a tool we have", "orig": "programmed and let it loose on a huge test set obtained from open source code from thousands of GitHub projects. When it comes to knowledge, we have found that more than twice as many methods in Python can be considered too long (statistically extremely longer than their neighbours within the same project) than in Java, but long parameter lists are seven times less likely to be found in Python code than in Java code. We have also found that Functional Decomposition, the way it was defined for Java, is not found in the Python code at all, and Spaghetti Code and God Classes are extremely rare there as well. The grounding and the confidence in", "generated": "designed to identify and flag these defects in Python code.\n\nWe began by conducting a comprehensive review of the existing literature on code smells and antipatterns, focusing on the five smells and four antipatterns that have been widely recognized as detrimental to software quality. We then extracted the abstract definitions of these defects, which provided a solid foundation for our implementation. The next step was to create concrete implementation specifications for each defect, taking into account the unique characteristics of the Python programming language. This involved developing a deep understanding of Python's syntax, semantics, and idioms, as well as its strengths and weaknesses.\n\nOur tool, which we have"}
{"full_text": "(l) I have enough evidence to render the sentence S probable. (la) So, relative to what I know, it is rational of me to believe S. (2) Now that I have more evidence, S may no longer be probable. (2a) So now, relative to what I know, it is not rational of me to believe S. These seem a perfectly ordinary, common sense, pair of situations. Generally and vaguely, I take them to embody what I shall call probabilistic inference. This form of inference is clearly non-monotonic. Relatively few people have taken this form of inference, based on high probability, to serve as a foundation for non-monotonic logic or for a logical or defeasible inference. There are exceptions: Jane Nutter [16] thinks that sometimes probability has something to do with non-monotonic reasoning. Judea Pearl [ 17] has recently been exploring the possibility. There are any number of people whom one might call probability enthusiasts who feel that probability provides all the answers by itself, with no need of help from logic. Cheeseman [1], Henrion [5] and others think it useful to look at a distribution of probabilities over a whole algebra of statements, to update that distribution in the light of new evidence, and to use the latest updated distribution of probability over the algebra as a basis for planning and decision making. A slightly weaker form of this approach is captured by Nilsson [15], where one assumes certain probabilities for certain statements, and infers the probabilities, or constraints on the probabilities of other statement. None of this corresponds to what I call probabilistic inference. All of the inference that is taking place, either in Bayesian updating, or in probabilistic logic, is strictly deductive. Deductive inference, particularly that concerned with the distribution of classical probabilities or chances, is of great importance. But this is not to say that there is no important role for what earlier logicians have called \"ampliative\" or \"inductive\" or \"scientific\" inference, in which the conclusion goes beyond the premises, asserts more than do the premises. This depends on what David Israel [6] has called \"real rules of inference\". It is characteristic of any such logic or inference procedure that it can go wrong: that statements accepted at one point may be rejected at a later point. Research underlying the results reported here has been partially supported by the Signals Warfare Center of the United States Army.", "prompt": "(l) I have enough evidence to render the sentence S probable. (la) So, relative to what I know, it is rational of me to believe S. (2) Now that I have more evidence, S may no longer be probable. (2a) So now, relative to what I know, it is not rational of me to believe S. These seem a perfectly ordinary, common sense, pair of situations. Generally and vaguely, I take them to embody what I shall call probabilistic inference. This form of inference is clearly non-monotonic. Relatively few people have taken this form of inference, based on high", "orig": "probability, to serve as a foundation for non-monotonic logic or for a logical or defeasible inference. There are exceptions: Jane Nutter [16] thinks that sometimes probability has something to do with non-monotonic reasoning. Judea Pearl [ 17] has recently been exploring the possibility. There are any number of people whom one might call probability enthusiasts who feel that probability provides all the answers by itself, with no need of help from logic. Cheeseman [1], Henrion [5] and others think it useful to look at a distribution of probabilities over a whole algebra of statements, to update that distribution", "generated": "probability, as a central concern in their theories of rationality.\n\nIn fact, many philosophers have traditionally focused on deductive logic, where the conclusion follows with absolute certainty from the premises. However, in everyday life, we often deal with uncertain and incomplete information, and probabilistic inference seems to be a more fitting framework for understanding how we reason in such situations. It acknowledges that our beliefs and conclusions can change as new evidence becomes available, and that it is rational to revise our opinions in light of new information.\n\nOne of the key aspects of probabilistic inference is that it is sensitive to the context and the total evidence available. This means that"}
{"full_text": "Background: Recent studies reported postural balance disorders in patients and soccer players with groin pain (GP) compared to controls. Since postural balance asymmetry identified after an initial injury contributes for subsequent injuries, identification of this asymmetry in soccer players with GP may highlight the risk of sustaining subsequent noncontact lower extremity musculoskeletal injuries in these players. Therefore, the aims of this study were to (i) examine static and dynamic unipedal postural balance asymmetry in soccer players with GP compared to healthy ones, and (ii) quantify the risk of subsequent noncontact lower extremity injuries in these players. Research question: Do soccer players with GP exhibit higher static and dynamic unipedal postural balance asymmetry, and higher risk of sustaining subsequent injuries compared to controls. Methods: In this prospective case control study, 27 soccer players with non-time loss GP (GP group: GPG), and 27 healthy ones (control group: CG) were enrolled. Static and dynamic unipedal postural balance asymmetry were evaluated with a force platform using symmetry index (SI), and Y-balance test (Y-BT), respectively. Additionally, subsequent noncontact lower extremity musculoskeletal injuries were tracked for 10 months. Results: The GPG revealed higher (p < 0.01) SI in eyes closed condition, higher (p < 0.001) side-to-side asymmetry in anterior, posteromedial and posterolateral reach distances and in composite Y-BT score compared to CG. They showed lower (p < 0.001) composite score for injured limb and higher (p < 0.001) side-to-side asymmetry in posteromedial reach distance compared to the cutoff values of 89.6 % and 4 cm, respectively. Moreover, GPG exhibited higher odds (OR= 7.48; 95 % CI = 2.15, 26.00; p < 0.01) of sustaining subsequent injuries compared to CG. Significance: The Y-BT should be instituted into existing pre-participation physical examinations to screen for soccer players with non-time loss GP at an elevated risk of sustaining subsequent injuries. This could help coaches and clinicians make valid return to play decisions.", "prompt": "Background: Recent studies reported postural balance disorders in patients and soccer players with groin pain (GP) compared to controls. Since postural balance asymmetry identified after an initial injury contributes for subsequent injuries, identification of this asymmetry in soccer players with GP may highlight the risk of sustaining subsequent noncontact lower extremity musculoskeletal injuries in these players. Therefore, the aims of this study were to (i) examine static and dynamic unipedal postural balance asymmetry in soccer players with GP compared to healthy ones, and (ii) quantify the risk of subsequent noncontact lower extremity injuries in these players. Research question: Do soccer", "orig": "players with GP exhibit higher static and dynamic unipedal postural balance asymmetry, and higher risk of sustaining subsequent injuries compared to controls. Methods: In this prospective case control study, 27 soccer players with non-time loss GP (GP group: GPG), and 27 healthy ones (control group: CG) were enrolled. Static and dynamic unipedal postural balance asymmetry were evaluated with a force platform using symmetry index (SI), and Y-balance test (Y-BT), respectively. Additionally, subsequent noncontact lower extremity musculoskeletal injuries were tracked for 10 months. Results: The GPG revealed higher", "generated": "players with GP exhibit greater postural balance asymmetry compared to healthy soccer players, and is this asymmetry associated with an increased risk of subsequent noncontact lower extremity injuries?\n\nMethodology: This prospective cohort study included 30 male soccer players with GP and 30 healthy male soccer players as controls. Postural balance was assessed using a force plate, and the static and dynamic unipedal balance tests were conducted. The players were then followed for a period of six months, during which any noncontact lower extremity musculoskeletal injuries were recorded. The relationships between postural balance asymmetry and injury risk were analyzed using regression analysis.\n\nP"}
{"full_text": "Background Weight loss trajectories after bariatric surgery vary widely between individuals, and predicting weight loss before the operation remains challenging. We aimed to develop a model using machine learning to provide individual preoperative prediction of 5-year weight loss trajectories after surgery. Methods In this multinational retrospective observational study we enrolled adult participants (aged $\\ge$18 years) from ten prospective cohorts (including ABOS [NCT01129297], BAREVAL [NCT02310178], the Swedish Obese Subjects study, and a large cohort from the Dutch Obesity Clinic [Nederlandse Obesitas Kliniek]) and two randomised trials (SleevePass [NCT00793143] and SM-BOSS [NCT00356213]) in Europe, the Americas, and Asia, with a 5 year followup after Roux-en-Y gastric bypass, sleeve gastrectomy, or gastric band. Patients with a previous history of bariatric surgery or large delays between scheduled and actual visits were excluded. The training cohort comprised patients from two centres in France (ABOS and BAREVAL). The primary outcome was BMI at 5 years. A model was developed using least absolute shrinkage and selection operator to select variables and the classification and regression trees algorithm to build interpretable regression trees. The performances of the model were assessed through the median absolute deviation (MAD) and root mean squared error (RMSE) of BMI. Findings10 231 patients from 12 centres in ten countries were included in the analysis, corresponding to 30 602 patient-years. Among participants in all 12 cohorts, 7701 (75$\\bullet$3%) were female, 2530 (24$\\bullet$7%) were male. Among 434 baseline attributes available in the training cohort, seven variables were selected: height, weight, intervention type, age, diabetes status, diabetes duration, and smoking status. At 5 years, across external testing cohorts the overall mean MAD BMI was 2$\\bullet$8 kg/m${}^2$ (95% CI 2$\\bullet$6-3$\\bullet$0) and mean RMSE BMI was 4$\\bullet$7 kg/m${}^2$ (4$\\bullet$4-5$\\bullet$0), and the mean difference between predicted and observed BMI was-0$\\bullet$3 kg/m${}^2$ (SD 4$\\bullet$7). This model is incorporated in an easy to use and interpretable web-based prediction tool to help inform clinical decision before surgery. InterpretationWe developed a machine learning-based model, which is internationally validated, for predicting individual 5-year weight loss trajectories after three common bariatric interventions.", "prompt": "Background Weight loss trajectories after bariatric surgery vary widely between individuals, and predicting weight loss before the operation remains challenging. We aimed to develop a model using machine learning to provide individual preoperative prediction of 5-year weight loss trajectories after surgery. Methods In this multinational retrospective observational study we enrolled adult participants (aged $\\ge$18 years) from ten prospective cohorts (including ABOS [NCT01129297], BAREVAL [NCT02310178], the Swedish Obese Subjects study, and a large cohort from the Dutch Obesity Clinic [Nederlandse Obesitas Kliniek]) and two randomised trials (Slee", "orig": "vePass [NCT00793143] and SM-BOSS [NCT00356213]) in Europe, the Americas, and Asia, with a 5 year followup after Roux-en-Y gastric bypass, sleeve gastrectomy, or gastric band. Patients with a previous history of bariatric surgery or large delays between scheduled and actual visits were excluded. The training cohort comprised patients from two centres in France (ABOS and BAREVAL). The primary outcome was BMI at 5 years. A model was developed using least absolute shrinkage and selection operator to select variables and the classification and regression trees algorithm to build interpre", "generated": "ve vs. Bypass, [NCT01779114], and SM-BOSS [NCT02563567]). \n\nOur goal was to create a predictive model that would enable healthcare providers to identify patients who are most likely to benefit from bariatric surgery, thereby optimizing treatment strategies and improving overall patient outcomes. To achieve this, we collected a comprehensive set of preoperative variables, including demographic, anthropometric, and comorbidity data, as well as information on patients' eating habits, physical activity levels, and psychological profiles. We then applied a machine learning algorithm to these data, using a range of modeling techniques, including decision"}
{"full_text": "In this paper we introduce the word {\\em fresco} to denote a monogenic geometric (a,b)-module. This \"basic object\" (generalized Brieskorn module with one generator) corresponds to the formal germ of the minimal filtered (regular) differential equation. Such an equation is satisfied by a relative de Rham cohomology class at a critical value of a holomorphic function on a smooth complex manifold. In [B.09] the first structure theorems are proved. Then in [B.10] we introduced the notion of {\\em theme} which corresponds in the \\ $[\\lambda]-$primitive case to frescos having a unique Jordan-H{\\\"o}lder sequence (a unique Jordan block for the monodromy). Themes correspond to asymptotic expansion of a given vanishing period, so to an image of a fresco in the module of asymptotic expansions. For a fixed relative de Rham cohomology class (for instance given by a smooth differential form $d-$closed and $df-$closed) each choice of a vanishing cycle in the spectral eigenspace of the monodromy for the eigenvalue \\ $exp(2i\\pi.\\lambda)$ \\ produces a \\ $[\\lambda]-$primitive theme, which is a quotient of the fresco associated to the given relative de Rham class itself. \\\\ We show that for any fresco there exists an {\\em unique} Jordan-H{\\\"o}lder sequence, called the {\\em principal J-H. sequence}, with corresponding quotients giving the opposite of the roots of the Bernstein polynomial in increasing order. We study the semi-simple part of a given fresco and we characterize the semi-simplicity of a fresco by the fact for any given order on the roots of its Bernstein polynomial we may find a J-H. sequence making them appear with this order. Then we construct a numerical invariant, called the \\ $\\beta-$invariant, and we show that it produces numerical criteria in order to give a necessary and sufficient condition on a fresco to be semi-simple. We show that these numerical invariants define a natural algebraic stratification on the set of isomorphism classes of fresco with given fundamental invariants (or equivalently with given roots of the Bernstein polynomial).", "prompt": "In this paper we introduce the word {\\em fresco} to denote a monogenic geometric (a,b)-module. This \"basic object\" (generalized Brieskorn module with one generator) corresponds to the formal germ of the minimal filtered (regular) differential equation. Such an equation is satisfied by a relative de Rham cohomology class at a critical value of a holomorphic function on a smooth complex manifold. In [B.09] the first structure theorems are proved. Then in [B.10] we introduced the notion of {\\em theme} which corresponds in the \\ $[\\lambda]-$primitive case", "orig": "to frescos having a unique Jordan-H{\\\"o}lder sequence (a unique Jordan block for the monodromy). Themes correspond to asymptotic expansion of a given vanishing period, so to an image of a fresco in the module of asymptotic expansions. For a fixed relative de Rham cohomology class (for instance given by a smooth differential form $d-$closed and $df-$closed) each choice of a vanishing cycle in the spectral eigenspace of the monodromy for the eigenvalue \\ $exp(2i\\pi.\\lambda)$ \\ produces a \\ $[\\lambda]-$primitive theme", "generated": "to a specific parameterized family of frescoes, which are classified by their characteristic invariants.\n\nAs we delved deeper into the study of frescoes and themes, we discovered a rich algebraic structure underlying these objects. Specifically, we found that the set of all frescoes forms a category, with morphisms given by certain natural transformations between the corresponding modules. Furthermore, the themes can be organized into a hierarchical structure, with the most primitive themes serving as building blocks for the more complex ones. This hierarchical structure is reflected in the algebraic properties of the corresponding frescoes, and provides a powerful tool for classifying and analyzing them.\n\nOne"}
{"full_text": "Yuan Shih Chow was born in Hubei province in China, on September 1, 1924. The eldest child of a local militia and political leader, he grew up in war and turmoil. His hometown was on the front line during most of the Japanese invasion and occupation of China. When he was 16, Y. S. Chow journeyed, mostly on foot, to Chongqing (Chung-King), the wartime Chinese capital, to finish his high school education. When the Communist party gained power in China, Y. S. Chow had already followed his university job to Taiwan. In Taiwan, he taught mathematics as an assistant at National Taiwan University until he came to the United States in 1954. At the University of Illinois, he studied under J. L. Doob and received his Ph.D. in 1958. He served as a staff mathematician and adjunct faculty at the IBM Watson Research Laboratory and Columbia University from 1959 to 1962. He was a member of the Statistics Department at Purdue University from 1962 to 1968. From 1968 until his retirement in 1993, Y. S. Chow served as Professor of Mathematical Statistics at Columbia University. At different times, he was a visiting professor at the University of California at Berkeley, University of Heidelberg (Germany) and the National Central University, Taiwan. He served as Director of the Institute of Mathematics of Academia Sinica, Taiwan, and Director of the Center of Applied Statistics at Nankai University, Tianjin, China. He was instrumental in establishing the Institute of Statistics of Academia Sinica in Taiwan. He is currently Professor Emeritus at Columbia University. Y. S. Chow is a fellow of the Institute of Mathematical Statistics, a member of the International Statistical Institute and a member of Taiwan's Academia Sinica. He has numerous publications, including Great Expectations: The Theory of Optimal Stopping (1971), in collaboration with Herbert Robbins and David Siegmund, and Probability Theory (1978), in collaboration with Henry Teicher. Y. S. Chow has a strong interest in mathematics education. He taught high school mathematics for one year in 1947 and wrote a book on high school algebra in collaboration with J. H. Teng and M. L. Chu. In 1992, Y. S. Chow, together with I. S. Chang and W. C. Ho, established the Chinese Institute of Probability and Statistics in Taiwan. This conversation took place in the fall of 2003 in Dobbs Ferry, New York.", "prompt": "Yuan Shih Chow was born in Hubei province in China, on September 1, 1924. The eldest child of a local militia and political leader, he grew up in war and turmoil. His hometown was on the front line during most of the Japanese invasion and occupation of China. When he was 16, Y. S. Chow journeyed, mostly on foot, to Chongqing (Chung-King), the wartime Chinese capital, to finish his high school education. When the Communist party gained power in China, Y. S. Chow had already followed his university job to Taiwan. In Taiwan, he", "orig": "taught mathematics as an assistant at National Taiwan University until he came to the United States in 1954. At the University of Illinois, he studied under J. L. Doob and received his Ph.D. in 1958. He served as a staff mathematician and adjunct faculty at the IBM Watson Research Laboratory and Columbia University from 1959 to 1962. He was a member of the Statistics Department at Purdue University from 1962 to 1968. From 1968 until his retirement in 1993, Y. S. Chow served as Professor of Mathematical Statistics at Columbia University. At different times, he was", "generated": "pursued his passion for mathematics, earning a PhD from National Taiwan University in 1956. \n\nHis academic excellence and dedication earned him a scholarship to study at the University of California, Berkeley, where he spent two years refining his skills in mathematical statistics. Upon his return to Taiwan, Chow became a prominent figure in the field, taking up a professorship at National Taiwan University. He went on to publish numerous papers on probability theory, statistical inference, and mathematical statistics, earning him international recognition and respect. In the 1960s, Chow played a crucial role in establishing the Institute of Statistical Science at Academia Sinica, Taiwan's highest"}
{"full_text": "The analysis of practical probabilistic models on the computer demands a convenient representation for the available knowledge and an efficient algorithm to perform inference. An appealing representation is the influence diagram, a network that makes explicit the random variables in a model and their probabilistic dependencies. Recent advances have developed solution procedures based on the influence diagram. In this paper, we examine the fundamental properties that underlie those techniques, and the information about the probabilistic structure that is available in the influence diagram representation. The influence diagram is a convenient representation for computer processing while also being clear and non-mathematical. It displays probabilistic dependence precisely, in a way that is intuitive for decision makers and experts to understand and communicate. As a result, the same influence diagram can be used to build, assess and analyze a model, facilitating changes in the formulation and feedback from sensitivity analysis. The goal in this paper is to determine arbitrary conditional probability distributions from a given probabilistic model. Given qualitative information about the dependence of the random variables in the model we can, for a specific conditional expression, specify precisely what quantitative information we need to be able to determine the desired conditional probability distribution. It is also shown how we can find that probability distribution by performing operations locally, that is, over subspaces of the joint distribution. In this way, we can exploit the conditional independence present in the model to avoid having to construct or manipulate the full joint distribution. These results are extended to include maximal processing when the information available is incomplete, and optimal decision making in an uncertain environment. Influence diagrams as a computer-aided modeling tool were developed by Miller, Merkofer, and Howard [5] and extended by Howard and Matheson [2]. Good descriptions of how to use them in modeling are in Owen [7] and Howard and Matheson [2]. The notion of solving a decision problem through influence diagrams was examined by Olmsted [6] and such an algorithm was developed by Shachter [8]. The latter paper also shows how influence diagrams can be used to perform a variety of sensitivity analyses. This paper extends those results by developing a theory of the properties of the diagram that are used by the algorithm, and the information needed to solve arbitrary probability inference problems. Section 2 develops the notation and the framework for the paper and the relationship between influence diagrams and joint probability distributions. The general probabilistic inference problem is posed in Section 3. In Section 4 the transformations on the diagram are developed and then put together into a solution procedure in Section 5. In Section 6, this procedure is used to calculate the information requirement to solve an inference problem and the maximal processing that can be performed with incomplete information. Section 7 contains a summary of results.", "prompt": "The analysis of practical probabilistic models on the computer demands a convenient representation for the available knowledge and an efficient algorithm to perform inference. An appealing representation is the influence diagram, a network that makes explicit the random variables in a model and their probabilistic dependencies. Recent advances have developed solution procedures based on the influence diagram. In this paper, we examine the fundamental properties that underlie those techniques, and the information about the probabilistic structure that is available in the influence diagram representation. The influence diagram is a convenient representation for computer processing while also being clear and non-mathematical. It displays probabilistic dependence precisely, in a way that is", "orig": "intuitive for decision makers and experts to understand and communicate. As a result, the same influence diagram can be used to build, assess and analyze a model, facilitating changes in the formulation and feedback from sensitivity analysis. The goal in this paper is to determine arbitrary conditional probability distributions from a given probabilistic model. Given qualitative information about the dependence of the random variables in the model we can, for a specific conditional expression, specify precisely what quantitative information we need to be able to determine the desired conditional probability distribution. It is also shown how we can find that probability distribution by performing operations locally, that is, over subspaces of the joint distribution.", "generated": "intuitive and easy to comprehend, making it an ideal tool for experts and non-experts alike. The diagram's graphical structure allows for a visual representation of the relationships between variables, enabling users to identify patterns and connections that might be obscured by mathematical equations. Furthermore, the influence diagram's probabilistic dependencies can be easily quantified, enabling the calculation of conditional probabilities and the execution of Bayesian inference. This representation also facilitates the identification of conditional independence, a crucial aspect of probabilistic modeling, which can significantly reduce the complexity of inference algorithms. Additionally, the influence diagram's modular structure enables the decomposition of complex models into smaller, more manageable sub"}
{"full_text": "The celebrated H\\\"{o}rmander condition is a sufficient (and nearly necessary) condition for a second-order linear Kolmogorov partial differential equation (PDE) with smooth coefficients to be hypoelliptic. As a consequence, the solutions of Kolmogorov PDEs are smooth at all positive times if the coefficients of the PDE are smooth and satisfy H\\\"{o}rmander's condition even if the initial function is only continuous but not differentiable. First-order linear Kolmogorov PDEs with smooth coefficients do not have this smoothing effect but at least preserve regularity in the sense that solutions are smooth if their initial functions are smooth. In this article, we consider the intermediate regime of nonhypoelliptic second-order Kolmogorov PDEs with smooth coefficients. The main observation of this article is that there exist counterexamples to regularity preservation in that case. More precisely, we give an example of a second-order linear Kolmogorov PDE with globally bounded and smooth coefficients and a smooth initial function with compact support such that the unique globally bounded viscosity solution of the PDE is not even locally H\\\"{o}lder continuous. From the perspective of probability theory, the existence of this example PDE has the consequence that there exists a stochastic differential equation (SDE) with globally bounded and smooth coefficients and a smooth function with compact support which is mapped by the corresponding transition semigroup to a function which is not locally H\\\"{o}lder continuous. In other words, degenerate noise can have a roughening effect. A further implication of this loss of regularity phenomenon is that numerical approximations may converge without any arbitrarily small polynomial rate of convergence to the true solution of the SDE. More precisely, we prove for an example SDE with globally bounded and smooth coefficients that the standard Euler approximations converge to the exact solution of the SDE in the strong and numerically weak sense, but at a rate that is slower then any power law.", "prompt": "The celebrated H\\\"{o}rmander condition is a sufficient (and nearly necessary) condition for a second-order linear Kolmogorov partial differential equation (PDE) with smooth coefficients to be hypoelliptic. As a consequence, the solutions of Kolmogorov PDEs are smooth at all positive times if the coefficients of the PDE are smooth and satisfy H\\\"{o}rmander's condition even if the initial function is only continuous but not differentiable. First-order linear Kolmogorov PDEs with smooth coefficients do not have this smoothing effect but at least preserve regularity", "orig": "in the sense that solutions are smooth if their initial functions are smooth. In this article, we consider the intermediate regime of nonhypoelliptic second-order Kolmogorov PDEs with smooth coefficients. The main observation of this article is that there exist counterexamples to regularity preservation in that case. More precisely, we give an example of a second-order linear Kolmogorov PDE with globally bounded and smooth coefficients and a smooth initial function with compact support such that the unique globally bounded viscosity solution of the PDE is not even locally H\\\"{o}lder continuous. From the perspective of probability theory,", "generated": ".\n\nIt's fascinating to note that the H\\\"{o}rmander condition has far-reaching implications for the study of stochastic processes. The connection between Kolmogorov PDEs and stochastic processes lies in the fact that the transition probability density of a Markov process satisfies a Kolmogorov forward equation or backward equation, which is a type of Kolmogorov PDE. The H\\\"{o}rmander condition plays a crucial role in ensuring the smoothness of the transition probability density, and subsequently, the smoothness of the sample paths of the Markov process. This has significant consequences for"}
{"full_text": "Reactive programming is a popular paradigm to program event-driven applications, and it is often proposed as a paradigm to write distributed applications. One such type of application is *prosumer* applications, which are distributed applications that both produce and consume many events. We analyse the problems that occur when using a reactive programming language or framework to implement prosumer applications. We find that the assumption of an open network, which means prosumers of various types spontaneously join and leave the network, can cause a lot of code complexity or run-time inefficiency. At the basis of these issues lies *acquaintance management*: the ability to discover prosumers as they join and leave the network, and correctly maintaining this state throughout the reactive program. Most existing reactive programming languages and frameworks have limited support for managing acquaintances, resulting in accidental complexity of the code or inefficient computations. In this paper we present acquaintance management for reactive programs. First, we design an *acquaintance discovery* mechanism to create a *flock* that automatically discovers prosumers on the network. An important aspect of flocks is their integration with reactive programs, such that a reactive program can correctly and efficiently maintain its state. To this end we design an *acquaintance maintenance* mechanism: a new type of operator for functional reactive programming languages that we call `deploy-*`. The `deploy-*` operator enables correct and efficient reactions to time-varying collections of discovered prosumers. The proposed mechanisms are implemented in a reactive programming language called Stella, which serves as a linguistic vehicle to demonstrate the ideas of our approach. Our implementation of acquaintance management results in computationally efficient and idiomatic reactive code. We evaluate our approach quantitatively via benchmarks that show that our implementation is efficient: computations will efficiently update whenever a new prosumer is discovered, or a connected prosumer is dropped. To evaluate the distributed capabilities of our prototype implementation, we implement a use-case that simulates the bike-sharing infrastructure of Brussels, and we run it on a Raspberry Pi cluster computer. We consider our work to be an important step to use functional reactive programming to build distributed systems for open networks, in other words, distributed reactive programs that involve many prosumer devices and sensors that spontaneously join and leave the network.", "prompt": "Reactive programming is a popular paradigm to program event-driven applications, and it is often proposed as a paradigm to write distributed applications. One such type of application is *prosumer* applications, which are distributed applications that both produce and consume many events. We analyse the problems that occur when using a reactive programming language or framework to implement prosumer applications. We find that the assumption of an open network, which means prosumers of various types spontaneously join and leave the network, can cause a lot of code complexity or run-time inefficiency. At the basis of these issues lies *acquaintance management*: the ability to discover prosum", "orig": "ers as they join and leave the network, and correctly maintaining this state throughout the reactive program. Most existing reactive programming languages and frameworks have limited support for managing acquaintances, resulting in accidental complexity of the code or inefficient computations. In this paper we present acquaintance management for reactive programs. First, we design an *acquaintance discovery* mechanism to create a *flock* that automatically discovers prosumers on the network. An important aspect of flocks is their integration with reactive programs, such that a reactive program can correctly and efficiently maintain its state. To this end we design an *acquaintance maintenance* mechanism: a", "generated": "ers that are currently available on the network, establish communication channels with them, and adapt to changes in their availability.\n\nAs we delve deeper into the problems of implementing prosumer applications using reactive programming, we encounter several challenges. For instance, the dynamic nature of an open network necessitates continuous monitoring of prosumer availability, which can lead to excessive message overhead and network congestion. Moreover, the need to establish and tear down communication channels between prosumers can result in significant computational overhead, particularly when dealing with large-scale networks. Furthermore, the reactive programming paradigm's emphasis on event-driven interactions can make it difficult to manage the complexity of acquaintance management,"}
{"full_text": "We consider a class of mean field games in which the agents interact through both their states and controls, and we focus on situations in which a generic agent tries to adjust her speed (control) to an average speed (the average is made in a neighborhood in the state space). In such cases, the monotonicity assumptions that are frequently made in the theory of mean field games do not hold, and uniqueness cannot be expected in general. Such model lead to systems of forward-backward nonlinear nonlocal parabolic equations; the latter are supplemented with various kinds of boundary conditions, in particular Neumann-like boundary conditions stemming from reflection conditions on the underlying controled stochastic processes. The present work deals with numerical approximations of the above mentioned systems. After describing the finite difference scheme, we propose an iterative method for solving the systems of nonlinear equations that arise in the discrete setting; it combines a continuation method, Newton iterations and inner loops of a bigradient like solver. The numerical method is used for simulating two examples. We also make experiments on the behaviour of the iterative algorithm when the parameters of the model vary. The theory of mean field games, (MFGs for short), aims at studying deterministic or stochastic differential games (Nash equilibria) as the number of agents tends to infinity. It supposes that the rational agents are indistinguishable and individually have a negligible influence on the game, and that each individual strategy is influenced by some averages of quantities depending on the states (or the controls as in the present work) of the other agents. MFGs have been introduced in the pioneering works of J-M. Lasry and P-L. Lions [17, 18, 19]. Independently and at approximately the same time, the notion of mean field games arose in the engineering literature, see the works of M.Y. Huang, P.E. Caines and R.Malham{\\'e} [14, 15]. The present work deals with numerical approximations of mean field games in which the agents interact through both their states and controls; it follows a more theoretical work by the second author, [16], which is devoted to the mathematical analysis of the related systems of nonlocal partial differential equations. There is not much literature on MFGs in which the agents also interact through their controls, see [13, 12, 8, 10, 7, 16]. To stress the fact that the latter situation is considered, we will sometimes use the terminology mean field games of control and the acronym MFGC.", "prompt": "We consider a class of mean field games in which the agents interact through both their states and controls, and we focus on situations in which a generic agent tries to adjust her speed (control) to an average speed (the average is made in a neighborhood in the state space). In such cases, the monotonicity assumptions that are frequently made in the theory of mean field games do not hold, and uniqueness cannot be expected in general. Such model lead to systems of forward-backward nonlinear nonlocal parabolic equations; the latter are supplemented with various kinds of boundary conditions, in particular Neumann-like boundary conditions stemming from reflection conditions on the underlying", "orig": "controled stochastic processes. The present work deals with numerical approximations of the above mentioned systems. After describing the finite difference scheme, we propose an iterative method for solving the systems of nonlinear equations that arise in the discrete setting; it combines a continuation method, Newton iterations and inner loops of a bigradient like solver. The numerical method is used for simulating two examples. We also make experiments on the behaviour of the iterative algorithm when the parameters of the model vary. The theory of mean field games, (MFGs for short), aims at studying deterministic or stochastic differential games (Nash equilibria) as the number of agents tends", "generated": "stochastic process.\n\nOne of the primary challenges in analyzing these systems is the lack of a priori estimates, which makes it difficult to establish the existence of solutions. Moreover, the nonlocal nature of the equations, arising from the averaging process in the neighborhood of the state space, introduces an additional layer of complexity. To tackle these issues, we employ a combination of probabilistic and analytic techniques, including the use of backward stochastic differential equations (BSDEs) and the theory of viscosity solutions.\n\nThe Neumann-like boundary conditions, in particular, require a careful treatment, as they involve a reflection condition on the underlying process. This reflection condition can"}
{"full_text": "We define the equivariant Cox ring of a normal variety with algebraic group action. We study algebraic and geometric aspects of this object and show how it is related to the ordinary Cox ring. Then, we specialize to the case of normal rational varieties of complexity one under the action of a connected reductive group G. We show that the G-equivariant Cox ring is then a finitely generated integral normal G-algebra. Under a mild additional condition, we give a presentation by generators and relations of its subalgebra of U-invariants, where U is the unipotent part of a Borel subgroup of G. The ordinary Cox ring is also finitely generated and canonically isomorphic to the U-equivariant Cox ring, so that it inherits a canonical structure of U-algebra. Relying on a work of Hausen and Herppich, we prove that the subalgebra of U-invariants of the Cox ring is a finitely generated Cox ring of a variety of complexity one under the action of a torus. This yields in particular that this latter algebra is a complete intersection.Characterizing the log terminality of singularities in a finitely generated Cox ring is an interesting question, particularly since the work of Gongyo, Okawa, Sannai and Takagi characterizing varieties of Fano type via singularities of Cox rings ([13]). We provide a criterion of combinatorial nature for the Cox ring of an almost homogeneous G-variety of complexity one to have log terminal singularities.Iteration of Cox rings has been introduced by Arzhantsev, Braun, Hausen and Wrobel in [1]. For log terminal quasicones with a torus action of complexity one, they proved that the iteration sequence is finite with a finitely generated factorial master Cox ring. We prove that the iteration sequence is finite for equivariant and ordinary Cox rings of normal rational G-varieties of complexity one satisfying a mild additional condition (e.g. complete varieties or almost homogeneous varieties). In the almost homogeneous case, we prove that the equivariant and ordinary master Cox rings are finitely generated and factorial.", "prompt": "We define the equivariant Cox ring of a normal variety with algebraic group action. We study algebraic and geometric aspects of this object and show how it is related to the ordinary Cox ring. Then, we specialize to the case of normal rational varieties of complexity one under the action of a connected reductive group G. We show that the G-equivariant Cox ring is then a finitely generated integral normal G-algebra. Under a mild additional condition, we give a presentation by generators and relations of its subalgebra of U-invariants, where U is the unipotent part of a Borel subgroup of G. The", "orig": "ordinary Cox ring is also finitely generated and canonically isomorphic to the U-equivariant Cox ring, so that it inherits a canonical structure of U-algebra. Relying on a work of Hausen and Herppich, we prove that the subalgebra of U-invariants of the Cox ring is a finitely generated Cox ring of a variety of complexity one under the action of a torus. This yields in particular that this latter algebra is a complete intersection.Characterizing the log terminality of singularities in a finitely generated Cox ring is an interesting question, particularly since the work of Gongyo, Ok", "generated": "geometric significance of this result lies in the fact that it provides a comprehensive framework for understanding the algebraic and geometric properties of the variety, as well as the action of the group on it.\n\nFurthermore, we explore the implications of this result on the study of algebraic invariants of the variety, such as the Picard group and the Neron-Severi group. We demonstrate how the equivariant Cox ring can be used to compute these invariants and to describe their geometric significance. Additionally, we discuss the connections between the equivariant Cox ring and other important algebraic and geometric objects, such as the Chow ring and the Hilbert scheme"}
{"full_text": "The individual data collected throughout patient follow-up constitute crucial information for assessing the risk of a clinical event, and eventually for adapting a therapeutic strategy. Joint models and landmark models have been proposed to compute individual dynamic predictions from repeated measures to one or two markers. However, they hardly extend to the case where the complete patient history includes much more repeated markers possibly. Our objective was thus to propose a solution for the dynamic prediction of a health event that may exploit repeated measures of a possibly large number of markers. We combined a landmark approach extended to endogenous markers history with machine learning methods adapted to survival data. Each marker trajectory is modeled using the information collected up to landmark time, and summary variables that best capture the individual trajectories are derived. These summaries and additional covariates are then included in different prediction methods. To handle a possibly large dimensional history, we rely on machine learning methods adapted to survival data, namely regularized regressions and random survival forests, to predict the event from the landmark time, and we show how they can be combined into a superlearner. Then, the performances are evaluated by cross-validation using estimators of Brier Score and the area under the Receiver Operating Characteristic curve adapted to censored data. We demonstrate in a simulation study the benefits of machine learning survival methods over standard survival models, especially in the case of numerous and/or nonlinear relationships between the predictors and the event. We then applied the methodology in two prediction contexts: a clinical context with the prediction of death for patients with primary biliary cholangitis, and a public health context with the prediction of death in the general elderly population at different ages. Our methodology, implemented in R, enables the prediction of an event using the entire longitudinal patient history, even when the number of repeated markers is large. Although introduced with mixed models for the repeated markers and methods for a single right censored time-to-event, our method can be used with any other appropriate modeling technique for the markers and can be easily extended to competing risks setting.", "prompt": "The individual data collected throughout patient follow-up constitute crucial information for assessing the risk of a clinical event, and eventually for adapting a therapeutic strategy. Joint models and landmark models have been proposed to compute individual dynamic predictions from repeated measures to one or two markers. However, they hardly extend to the case where the complete patient history includes much more repeated markers possibly. Our objective was thus to propose a solution for the dynamic prediction of a health event that may exploit repeated measures of a possibly large number of markers. We combined a landmark approach extended to endogenous markers history with machine learning methods adapted to survival data. Each marker trajectory is modeled using the information collected", "orig": "up to landmark time, and summary variables that best capture the individual trajectories are derived. These summaries and additional covariates are then included in different prediction methods. To handle a possibly large dimensional history, we rely on machine learning methods adapted to survival data, namely regularized regressions and random survival forests, to predict the event from the landmark time, and we show how they can be combined into a superlearner. Then, the performances are evaluated by cross-validation using estimators of Brier Score and the area under the Receiver Operating Characteristic curve adapted to censored data. We demonstrate in a simulation study the benefits of machine learning survival methods", "generated": "from the patient's past observations, allowing for a personalized characterization of the patient's profile.\n\nBy integrating these individual marker trajectories, we can obtain a comprehensive picture of the patient's health status over time. This enables clinicians to identify high-risk patients and take proactive measures to prevent or mitigate the occurrence of the clinical event. Moreover, our approach can also facilitate the identification of novel biomarkers that are strongly associated with the risk of the clinical event, which can inform the development of new diagnostic tests or therapeutic strategies. In addition, our methodology can be applied to a wide range of clinical settings, including chronic diseases such as diabetes, cardiovascular disease, and"}
{"full_text": "Electroactive polymers (EAP) are smart materials that can be used as actuators, sensors or energy harvesters in many fields. We had previously studied an ionic metal-polymer composites (IPMC), which consists in an ionic polymer film such as Nafion saturated with water and coated on both sides with a thin layer of metal acting as electrodes. This system bends when it is subject to an electric field orthogonal to the film and can thus be used as an actuator. Conversely, the deflection of the film generates a potential difference between the electrodes ; the same system can therefore be used as a sensor.We have developed a ''continuous medium'' model for this system. The thermodynamics of linear irreversible processes had enabled us to establish its constitutive equations.We are currently interested in a system of close properties based on PEDOT, a semiconductor EAP. The central part of the device consists in two interpenetrating polymers playing the role of an ions reservoir. The PEDOT is polymerized on each side and forms an interpenetrating network with the two other polymers. A pseudo trilayer is obtained, the two outer layers containing the PEDOT acting as electrodes. It is then saturated with an ionic liquid. When the blade thus obtained is placed in an electric field orthogonal to its faces, the PEDOT undergoes a reduction reaction (or dedoping) on the side of the negative electrode, which attracts cations from the central part and therefore swells ; the blade ultimately bends towards the positive electrode.We have first adapted our model to this two-components system : the cations on the one hand, and the three polymers and the anions on the other hand. We have written its balance equations and thermodynamic relations first at the microscopic scale for each phase, then at the macroscopic scale for the whole material using an averaging technique. The thermodynamics of linear irreversible processes then provides its constitutive relations : a Kelvin - Voigt type stress-strain relation and generalized Fourier's and Darcy's laws. The equations obtained were applied to the case of a cantilevered blade subject to a continuous potential difference at constant temperature. The numerical resolution of the equations system enabled us to draw the profiles of the different quantities, which are very steep functions near the electrodes. We also evaluated the tip displacement and the force that must be exerted on the free end of the beam to prevent its displacement (blocking force). The results obtained are in good agreement with the experimental data published in the literature.", "prompt": "Electroactive polymers (EAP) are smart materials that can be used as actuators, sensors or energy harvesters in many fields. We had previously studied an ionic metal-polymer composites (IPMC), which consists in an ionic polymer film such as Nafion saturated with water and coated on both sides with a thin layer of metal acting as electrodes. This system bends when it is subject to an electric field orthogonal to the film and can thus be used as an actuator. Conversely, the deflection of the film generates a potential difference between the electrodes ; the same system can therefore be used as a sensor", "orig": ".We have developed a ''continuous medium'' model for this system. The thermodynamics of linear irreversible processes had enabled us to establish its constitutive equations.We are currently interested in a system of close properties based on PEDOT, a semiconductor EAP. The central part of the device consists in two interpenetrating polymers playing the role of an ions reservoir. The PEDOT is polymerized on each side and forms an interpenetrating network with the two other polymers. A pseudo trilayer is obtained, the two outer layers containing the PEDOT acting as electrodes. It is then saturated with an ionic liquid. When the", "generated": ".\n\nThe potential applications of EAP-based systems are vast and varied. In the field of robotics, for instance, IPMC-based actuators could be used to create soft, flexible robots that can navigate through tight spaces or interact with their environment in a more delicate manner. The same technology could also be used in medical devices, such as catheters or prosthetics, where its ability to mimic natural movement and flexibility would be particularly valuable. As sensors, IPMC-based systems could be used to detect even the slightest changes in their environment, making them ideal for applications such as structural health monitoring or biomedical sensing.\n\nAnother promising area of research involves the"}
{"full_text": "Recent technological advances have enabled the recording of neurons in intact circuits with a high spatial and temporal resolution, creating the need for modeling with the same precision. In particular, the development of ultra-fast two-photon microscopy combined with fluorescence-based genetically-encoded Ca2+-indicators allows capture of full-dendritic arbor and somatic responses associated with synaptic input and action potential output. The complexity of dendritic arbor structures and distributed patterns of activity over time results in the generation of incredibly rich 4D datasets that are challenging to analyze (Sakaki, 2020). Interpreting neural activity from fluorescence-based Ca2+ biosensors is challenging due to non-linear interactions between several factors influencing intracellular calcium ion concentration and its binding to sensors, including the ionic dynamics driven by diffusion, electrical gradients and voltage-gated conductance.To investigate those dynamics, we designed a model based on a Cable-like equation coupled to the Nernst-Planck equations for ionic fluxes in electrolytes. We employ this model to simulate signal propagation and ionic electrodiffusion across a dendritic arbor. Using these simulation results, we then designed an algorithm to detect synapses from Ca2+ imaging datasets. We finally apply this algorithm to experimental Ca2+-indicator datasets from neurons expressing jGCaMP7s (Dana et al., 2019), using full-dendritic arbor sampling in vivo in the Xenopus laevis optic tectum using fast random-access two-photon microscopy.Our model reproduces the dynamics of visual stimulus-evoked jGCaMP7s-mediated calcium signals observed experimentally, and the resulting algorithm allows prediction of the location of synapses across the dendritic arbor.Our study provides a way to predict synaptic activity and location on dendritic arbors, from fluorescence data in the full dendritic arbor of a neuron recorded in the intact and awake developing vertebrate brain.", "prompt": "Recent technological advances have enabled the recording of neurons in intact circuits with a high spatial and temporal resolution, creating the need for modeling with the same precision. In particular, the development of ultra-fast two-photon microscopy combined with fluorescence-based genetically-encoded Ca2+-indicators allows capture of full-dendritic arbor and somatic responses associated with synaptic input and action potential output. The complexity of dendritic arbor structures and distributed patterns of activity over time results in the generation of incredibly rich 4D datasets that are challenging to analyze (Sakaki, 2020). Interpreting neural activity from fluorescence-based Ca2+ biosensors", "orig": "is challenging due to non-linear interactions between several factors influencing intracellular calcium ion concentration and its binding to sensors, including the ionic dynamics driven by diffusion, electrical gradients and voltage-gated conductance.To investigate those dynamics, we designed a model based on a Cable-like equation coupled to the Nernst-Planck equations for ionic fluxes in electrolytes. We employ this model to simulate signal propagation and ionic electrodiffusion across a dendritic arbor. Using these simulation results, we then designed an algorithm to detect synapses from Ca2+ imaging datasets. We finally apply this algorithm to experimental Ca2+-indicator datasets from", "generated": "requires sophisticated computational tools and statistical models to extract meaningful information from these datasets.\n\nTo tackle this challenge, researchers have turned to machine learning and artificial intelligence approaches to develop novel algorithms and models that can efficiently process and interpret the vast amounts of data generated by these advances. For instance, convolutional neural networks (CNNs) have been successfully applied to analyze the spatial patterns of dendritic activity, while recurrent neural networks (RNNs) have been used to model the temporal dynamics of neural responses. Moreover, graph theory and network analysis techniques have been employed to study the connectivity patterns of neural circuits and how they relate to neural computations.\n\nFurthermore, the"}
{"full_text": "Context: This paper presents the concept of open programming language interpreters and the implementation of a framework-level metaobject protocol (MOP) to support them. Inquiry: We address the problem of dynamic interpreter adaptation to tailor the interpreter's behavior on the task to be solved and to introduce new features to fulfill unforeseen requirements. Many languages provide a MOP that to some degree supports reflection. However, MOPs are typically language-specific, their reflective functionality is often restricted, and the adaptation and application logic are often mixed which hardens the understanding and maintenance of the source code. Our system overcomes these limitations. Approach: We designed and implemented a system to support open programming language interpreters. The prototype implementation is integrated in the Neverlang framework. The system exposes the structure, behavior and the runtime state of any Neverlang-based interpreter with the ability to modify it. Knowledge: Our system provides a complete control over interpreter's structure, behavior and its runtime state. The approach is applicable to every Neverlang-based interpreter. Adaptation code can potentially be reused across different language implementations. Grounding: Having a prototype implementation we focused on feasibility evaluation. The paper shows that our approach well addresses problems commonly found in the research literature. We have a demonstrative video and examples that illustrate our approach on dynamic software adaptation, aspect-oriented programming, debugging and context-aware interpreters. Importance: To our knowledge, our paper presents the first reflective approach targeting a general framework for language development. Our system provides full reflective support for free to any Neverlang-based interpreter. We are not aware of any prior application of open implementations to programming language interpreters in the sense defined in this paper. Rather than substituting other approaches, we believe our system can be used as a complementary technique in situations where other approaches present serious limitations.", "prompt": "Context: This paper presents the concept of open programming language interpreters and the implementation of a framework-level metaobject protocol (MOP) to support them. Inquiry: We address the problem of dynamic interpreter adaptation to tailor the interpreter's behavior on the task to be solved and to introduce new features to fulfill unforeseen requirements. Many languages provide a MOP that to some degree supports reflection. However, MOPs are typically language-specific, their reflective functionality is often restricted, and the adaptation and application logic are often mixed which hardens the understanding and maintenance of the source code. Our system overcomes these limitations. Approach: We designed and implemented", "orig": "a system to support open programming language interpreters. The prototype implementation is integrated in the Neverlang framework. The system exposes the structure, behavior and the runtime state of any Neverlang-based interpreter with the ability to modify it. Knowledge: Our system provides a complete control over interpreter's structure, behavior and its runtime state. The approach is applicable to every Neverlang-based interpreter. Adaptation code can potentially be reused across different language implementations. Grounding: Having a prototype implementation we focused on feasibility evaluation. The paper shows that our approach well addresses problems commonly found in the research literature. We have a demonstrative video and examples that illustrate our approach", "generated": "a novel MOP that allows for dynamic and flexible interpreter adaptation, enabling the creation of open programming language interpreters. Our approach decouples the adaptation and application logic, resulting in a clear separation of concerns and a more modular architecture. This, in turn, facilitates the understanding and maintenance of the source code. The proposed MOP provides a set of APIs and hooks that enable developers to dynamically modify the interpreter's behavior, introducing new features and adapting to changing requirements. We have implemented our approach in a prototype system, which we have evaluated through a series of experiments and case studies. The results demonstrate the effectiveness and flexibility of our approach, showcasing"}
{"full_text": "Scientific progress increasingly depends on data management, particularly to clean and curate data so that it can be systematically analyzed and reused. A wealth of techniques for managing and curating data (and its provenance) have been proposed, largely in the database community. In particular, a number of influential papers have proposed collecting provenance information explaining where a piece of data was copied from, or what other records were used to derive it. Most of these techniques, however, exist only as research prototypes and are not available in mainstream database systems. This means scientists must either implement such techniques themselves or (all too often) go without. This is essentially a code reuse problem: provenance techniques currently cannot be implemented reusably, only as ad hoc, usually unmaintained extensions to standard databases. An alternative, relatively unexplored approach is to support such techniques at a higher abstraction level, using metaprogramming or reflection techniques. Can advanced programming techniques make it easier to transfer provenance research results into practice? We build on a recent approach called language-integrated provenance, which extends language-integrated query techniques with source-to-source query translations that record provenance. In previous work, a proof of concept was developed in a research programming language called Links, which supports sophisticated Web and database programming. In this paper, we show how to adapt this approach to work in Haskell building on top of the Database-Supported Haskell (DSH) library. Even though it seemed clear in principle that Haskell's rich programming features ought to be sufficient, implementing language-integrated provenance in Haskell required overcoming a number of technical challenges due to interactions between these capabilities. Our implementation serves as a proof of concept showing how this combination of metaprogramming features can, for the first time, make data provenance facilities available to programmers as a library in a widely-used, general-purpose language. In our work we were successful in implementing forms of provenance known as where-provenance and lineage. We have tested our implementation using a simple database and query set and established that the resulting queries are executed correctly on the database. Our implementation is publicly available on GitHub. Our work makes provenance tracking available to users of DSH at little cost. Although Haskell is not widely used for scientific database development, our work suggests which languages features are necessary to support provenance as library. We also highlight how combining Haskell's advanced type programming features can lead to unexpected complications, which may motivate further research into type system expressiveness.", "prompt": "Scientific progress increasingly depends on data management, particularly to clean and curate data so that it can be systematically analyzed and reused. A wealth of techniques for managing and curating data (and its provenance) have been proposed, largely in the database community. In particular, a number of influential papers have proposed collecting provenance information explaining where a piece of data was copied from, or what other records were used to derive it. Most of these techniques, however, exist only as research prototypes and are not available in mainstream database systems. This means scientists must either implement such techniques themselves or (all too often) go without. This is essentially", "orig": "a code reuse problem: provenance techniques currently cannot be implemented reusably, only as ad hoc, usually unmaintained extensions to standard databases. An alternative, relatively unexplored approach is to support such techniques at a higher abstraction level, using metaprogramming or reflection techniques. Can advanced programming techniques make it easier to transfer provenance research results into practice? We build on a recent approach called language-integrated provenance, which extends language-integrated query techniques with source-to-source query translations that record provenance. In previous work, a proof of concept was developed in a research programming language called Links, which supports sophisticated Web and", "generated": "a vicious cycle, where the lack of accessible data management tools hinders scientific progress, which in turn further emphasizes the need for efficient data management.\n\nThe consequences of this situation are far-reaching. Data quality issues, for instance, can significantly impact the validity of research findings. Moreover, the inability to track data provenance can lead to difficulties in reproducing results, which is a fundamental principle of scientific inquiry. The absence of mainstream data management tools also means that scientists must divert a significant portion of their time and resources to developing ad-hoc solutions, rather than focusing on their core research questions. This not only slows down the scientific discovery process but"}
{"full_text": "Aims. The comparative study of several molecular species at the origin of the gas phase chemistry in the diffuse interstellar medium (ISM) is a key input in unraveling the coupled chemical and dynamical evolution of the ISM. Methods. The lowest rotational lines of HCO+, HCN, HNC, and CN were observed at the IRAM-30m telescope in absorption against the \\lambda 3 mm and \\lambda 1.3 mm continuum emission of massive star-forming regions in the Galactic plane. The absorption lines probe the gas over kiloparsecs along these lines of sight. The excitation temperatures of HCO+ are inferred from the comparison of the absorptions in the two lowest transitions. The spectra of all molecular species on the same line of sight are decomposed into Gaussian velocity components. Most appear in all the spectra of a given line of sight. For each component, we derived the central opacity, the velocity dispersion, and computed the molecular column density. We compared our results to the predictions of UV-dominated chemical models of photodissociation regions (PDR models) and to those of non-equilibrium models in which the chemistry is driven by the dissipation of turbulent energy (TDR models). Results. The molecular column densities of all the velocity components span up to two orders of magnitude. Those of CN, HCN, and HNC are linearly correlated with each other with mean ratios N(HCN)/N(HNC) = 4.8 $\\pm$ 1.3 and N(CN)/N(HNC) = 34 $\\pm$ 12, and more loosely correlated with those of HCO+, N(HNC)/N(HCO+) = 0.5 $\\pm$ 0.3, N(HCN)/N(HCO+) = 1.9 $\\pm$ 0.9, and N(CN)/N(HCO+) = 18 $\\pm$ 9. These ratios are similar to those inferred from observations of high Galactic latitude lines of sight, suggesting that the gas sampled by absorption lines in the Galactic plane has the same chemical properties as that in the Solar neighbourhood. The FWHM of the Gaussian velocity components span the range 0.3 to 3 km s-1 and those of the HCO+ lines are found to be 30% broader than those of CN-bearing molecules. The PDR models fail to reproduce simultaneously the observed abundances of the CN-bearing species and HCO+, even for high-density material (100 cm-3 < nH < 104 cm-3). The TDR models, in turn, are able to reproduce the observed abundances and abundance ratios of all the analysed molecules for the moderate gas densities (30 cm-3 < nH < 200 cm-3) and the turbulent energy observed in the diffuse interstellar medium. Conclusions. Intermittent turbulent dissipation appears to be a promising driver of the gas phase chemistry of the diffuse and translucent gas throughout the Galaxy. The details of the dissipation mechanisms still need to be investigated.", "prompt": "Aims. The comparative study of several molecular species at the origin of the gas phase chemistry in the diffuse interstellar medium (ISM) is a key input in unraveling the coupled chemical and dynamical evolution of the ISM. Methods. The lowest rotational lines of HCO+, HCN, HNC, and CN were observed at the IRAM-30m telescope in absorption against the \\lambda 3 mm and \\lambda 1.3 mm continuum emission of massive star-forming regions in the Galactic plane. The absorption lines probe the gas over kiloparsecs along these lines of sight. The excitation temperatures of HCO", "orig": "+ are inferred from the comparison of the absorptions in the two lowest transitions. The spectra of all molecular species on the same line of sight are decomposed into Gaussian velocity components. Most appear in all the spectra of a given line of sight. For each component, we derived the central opacity, the velocity dispersion, and computed the molecular column density. We compared our results to the predictions of UV-dominated chemical models of photodissociation regions (PDR models) and to those of non-equilibrium models in which the chemistry is driven by the dissipation of turbulent energy (TDR models). Results. The molecular column densities of all", "generated": "+\n\nContinuation:\n\nThe excitation temperatures of HCO+, HCN, and HNC were found to be relatively high, ranging from 5-10 K, indicating the presence of dense molecular gas in the vicinity of the massive star-forming regions. The CN molecule, on the other hand, exhibited a lower excitation temperature of around 3-5 K, suggesting a more diffuse distribution. The abundance ratios of these molecular species were also derived, revealing a significant enhancement of HCN and HNC in the densest regions, likely due to the presence of shock-induced chemistry. Furthermore, a correlation was observed between the CN abundance and"}
{"full_text": "In the late seventies, Clark [In Communication Systems and Random Process Theory (Proc. 2nd NATO Advanced Study Inst., Darlington, 1977) (1978) 721-734, Sijthoff & Noordhoff] pointed out that it would be natural for $\\pi_t$, the solution of the stochastic filtering problem, to depend continuously on the observed data $Y=\\{Y_s,s\\in[0,t]\\}$. Indeed, if the signal and the observation noise are independent one can show that, for any suitably chosen test function $f$, there exists a continuous map $\\theta^f_t$, defined on the space of continuous paths $C([0,t],\\mathbb{R}^d)$ endowed with the uniform convergence topology such that $\\pi_t(f)=\\theta^f_t(Y)$, almost surely; see, for example, Clark [In Communication Systems and Random Process Theory (Proc. 2nd NATO Advanced Study Inst., Darlington, 1977) (1978) 721-734, Sijthoff & Noordhoff], Clark and Crisan [Probab. Theory Related Fields 133 (2005) 43-56], Davis [Z. Wahrsch. Verw. Gebiete 54 (1980) 125-139], Davis [Teor. Veroyatn. Primen. 27 (1982) 160-167], Kushner [Stochastics 3 (1979) 75-83]. As shown by Davis and Spathopoulos [SIAM J. Control Optim. 25 (1987) 260-278], Davis [In Stochastic Systems: The Mathematics of Filtering and Identification and Applications, Proc. NATO Adv. Study Inst. Les Arcs, Savoie, France 1980 505-528], [In The Oxford Handbook of Nonlinear Filtering (2011) 403-424 Oxford Univ. Press], this type of robust representation is also possible when the signal and the observation noise are correlated, provided the observation process is scalar. For a general correlated noise and multidimensional observations such a representation does not exist. By using the theory of rough paths we provide a solution to this deficiency: the observation process $Y$ is \"lifted\" to the process $\\mathbf{Y}$ that consists of $Y$ and its corresponding L\\'{e}vy area process, and we show that there exists a continuous map $\\theta_t^f$, defined on a suitably chosen space of H\\\"{o}lder continuous paths such that $\\pi_t(f)=\\theta_t^f(\\mathbf{Y})$, almost surely.", "prompt": "In the late seventies, Clark [In Communication Systems and Random Process Theory (Proc. 2nd NATO Advanced Study Inst., Darlington, 1977) (1978) 721-734, Sijthoff & Noordhoff] pointed out that it would be natural for $\\pi_t$, the solution of the stochastic filtering problem, to depend continuously on the observed data $Y=\\{Y_s,s\\in[0,t]\\}$. Indeed, if the signal and the observation noise are independent one can show that, for any suitably chosen test function $f$, there exists a continuous map $\\theta^f", "orig": "_t$, defined on the space of continuous paths $C([0,t],\\mathbb{R}^d)$ endowed with the uniform convergence topology such that $\\pi_t(f)=\\theta^f_t(Y)$, almost surely; see, for example, Clark [In Communication Systems and Random Process Theory (Proc. 2nd NATO Advanced Study Inst., Darlington, 1977) (1978) 721-734, Sijthoff & Noordhoff], Clark and Crisan [Probab. Theory Related Fields 133 (2005) 43-56], Davis [Z. Wahrsch.", "generated": "$...\n\nThis insight of Clark's has far-reaching implications for the development of stochastic filtering theory. In particular, it allows for the application of functional analytic techniques to the study of the filtering problem, which has led to significant advances in the field. For instance, the continuity of $\\pi_t$ with respect to the observed data $Y$ enables the use of differential equations to describe the evolution of the filter over time. This, in turn, has facilitated the development of algorithms for solving the filtering problem numerically, which are essential for many practical applications.\n\nFurthermore, Clark's observation has also led to connections with other areas of mathematics, such"}
{"full_text": "A so-called smart material is a material that is the seat of one or more multiphysical coupling. One of the key points in the development of the constitutive laws of these materials, either at the local or at the global scale, is to formulate a free energy density (or enthalpy) from vectors, tensors, at a given order and for a class of given symmetry, depending on the symmetry classes of the crystal constituting the material or the symmetry of the representative volume element. This article takes as a support of study the stress and magnetization couple ($\\sigma$, m) involved in the phenomena of magnetoelastic coupling in a cubic symmetry medium. Several studies indeed show a non-monotonic sensitivity of the magnetic susceptibility and magnetostriction of certain soft magnetic materials under stress. Modeling such a phenomenon requires the introduction of a second order stress term in the Gibbs free energy density. A polynomial formulation in the two variables stress and magnetization is preferred over a tensorial formulation. For a given material symmetry class, this allows to express more easily the free energy density at any bi-degree in $\\sigma$ and m (i.e. at any constitutive tensors order for the so-called tensorial formulation). A rigorous and systematic method is essential to obtain the high-degree magneto-mechanical coupling terms and to build a free energy density function at any order which is invariant by the action of the cubic (octahedral) group. For that aim, theoretical and computer tools in Invariant Theory, that allow for the mathematical description of cubic nonlinear magneto-elasticity, are introduced. Minimal integrity bases of the invariant algebra for the pair (m, $\\sigma$), under the proper (orientation-preserving) and the full cubic groups, are then proposed. The minimal integrity basis for the proper cubic group is constituted of 60 invariants, while the minimal integrity basis for the full cubic group (the one of interest for magneto-elasticity) is made up of 30 invariants. These invariants are formulated in a (coordinate free) intrinsic manner, using a generalized cross product to write some of them. The counting of independent invariants of a given multi-degree in (m, $\\sigma$) is performed. It is shown accordingly that it is possible to list without error all the material parameters useful for the description of the coupled magnetoelastic behavior from the integrity basis. The technique is applied to derive general expressions $\\Psi$ $\\star$ ($\\sigma$, m) of the free energy density at the magnetic domains scale exhibiting cubic symmetry. The classic results for an isotropic medium are recovered.", "prompt": "A so-called smart material is a material that is the seat of one or more multiphysical coupling. One of the key points in the development of the constitutive laws of these materials, either at the local or at the global scale, is to formulate a free energy density (or enthalpy) from vectors, tensors, at a given order and for a class of given symmetry, depending on the symmetry classes of the crystal constituting the material or the symmetry of the representative volume element. This article takes as a support of study the stress and magnetization couple ($\\sigma$, m) involved in the phenomena of magnetoelastic coupling", "orig": "in a cubic symmetry medium. Several studies indeed show a non-monotonic sensitivity of the magnetic susceptibility and magnetostriction of certain soft magnetic materials under stress. Modeling such a phenomenon requires the introduction of a second order stress term in the Gibbs free energy density. A polynomial formulation in the two variables stress and magnetization is preferred over a tensorial formulation. For a given material symmetry class, this allows to express more easily the free energy density at any bi-degree in $\\sigma$ and m (i.e. at any constitutive tensors order for the so-called tensorial formulation). A rigorous and systematic method is essential to obtain the high-degree", "generated": ".\n\nThe magnetoelastic coupling phenomenon in these smart materials arises from the interaction between the mechanical stress and the magnetic field. When a magnetic field is applied to a magnetoelastic material, it can induce mechanical deformations, while conversely, mechanical stress can alter the magnetic properties of the material. To accurately describe this complex behavior, a comprehensive understanding of the coupling between stress and magnetization is essential.\n\nIn formulating the free energy density, it is crucial to consider the symmetry classes of the crystal constituting the material, as well as the symmetry of the representative volume element. This is because the symmetry properties of the material can significantly influence the"}
{"full_text": "In this paper, a few novel data hiding techniques are proposed. These techniques are improvements over the classical LSB data hiding technique and the Fibonacci LSB data-hiding technique proposed by Battisti et al. \\cite{r1}. The classical LSB technique is the simplest, but using this technique it is possible to embed only in first few bit-planes, since image quality becomes drastically distorted when embedding in higher bit-planes. Battisti et al. \\cite{r1} proposed an improvement over this by using Fibonacci decomposition technique and generating a different set of virtual bit-planes all together, thereby increasing the number of bit-planes. In this paper, first we mathematically model and generalize this particular approach of virtual bit-plane generation. Then we propose two novel embedding techniques, both of which are special-cases of our generalized model. The first embedding scheme is based on decomposition of a number (pixel-value) in sum of prime numbers, while the second one is based on decomposition in sum of natural numbers. Each of these particular representations generates a different set of (virtual) bit-planes altogether, suitable for embedding purposes. They not only allow one to embed secret message in higher bit-planes but also do it without much distortion, with a much better stego-image quality, in a reliable and secured manner, guaranteeing efficient retrieval of secret message. A comparative performance study between the classical Least Significant Bit (LSB) method, the data hiding technique using Fibonacci -p-Sequence decomposition and our proposed schemes has been done. Theoretical analysis indicates that image quality of the stego-image hidden by the technique using Fibonacci decomposition improves against simple LSB substitution method, while the same using the prime decomposition method improves drastically against that using Fibonacci decomposition technique, and finally the natural number decomposition method is a further improvement against that using prime decomposition technique. Also, optimality for the last technique is proved. For both of our data-hiding techniques, the experimental results show that, the stego-image is visually indistinguishable from the original cover image.", "prompt": "In this paper, a few novel data hiding techniques are proposed. These techniques are improvements over the classical LSB data hiding technique and the Fibonacci LSB data-hiding technique proposed by Battisti et al. \\cite{r1}. The classical LSB technique is the simplest, but using this technique it is possible to embed only in first few bit-planes, since image quality becomes drastically distorted when embedding in higher bit-planes. Battisti et al. \\cite{r1} proposed an improvement over this by using Fibonacci decomposition technique and generating a different set of virtual bit-planes all together, thereby increasing the number of bit-planes. In", "orig": "this paper, first we mathematically model and generalize this particular approach of virtual bit-plane generation. Then we propose two novel embedding techniques, both of which are special-cases of our generalized model. The first embedding scheme is based on decomposition of a number (pixel-value) in sum of prime numbers, while the second one is based on decomposition in sum of natural numbers. Each of these particular representations generates a different set of (virtual) bit-planes altogether, suitable for embedding purposes. They not only allow one to embed secret message in higher bit-planes but also do it without much distortion, with a much better stego-image quality", "generated": "our proposed techniques, we take it a step further by introducing a hybrid approach that combines the strengths of both the classical LSB and Fibonacci LSB techniques. Our first technique, which we call \"Bit-Plane Shifting,\" involves shifting the embedding process to higher bit-planes while still maintaining the image quality. This is achieved by using a novel bit-plane selection algorithm that dynamically selects the optimal bit-planes for embedding based on the image characteristics. Our second technique, dubbed \"Fibonacci-LSB Fusion,\" involves fusing the Fibonacci decomposition technique with the classical LSB technique to create a more robust and secure data hiding scheme. This fusion enables"}
{"full_text": "As a traditional tool of external assistance, crutches play an important role in society. They have a wide range of applications to help either the elderly and disabled to walk or to treat certain illnesses or for post-operative rehabilitation. But there are many different types of crutches, including shoulder crutches and elbow crutches. How to choose has become an issue that deserves to be debated. Because while crutches help people walk, they also have an impact on the body. Inappropriate choice of crutches or long-term misuse can lead to problems such as scoliosis. Previous studies were mainly experimental measurements or the construction of dynamic models to calculate the load on joints with crutches. These studies focus only on the level of the joints, ignoring the role that muscles play in this process. Although some also take into account the degree of muscle activation, there is still a lack of quantitative analysis. The traditional dynamic model can be used to calculate the load on each joint. However, due to the activation of the muscle, this situation only causes part of the load transmitted to the joint, and the work of the chair will compensate the other part of the load. Analysis at the muscle level allows a better understanding of the impact of crutches on the body. By comparing the levels of activation of the trunk muscles, it was found that the use of crutches for walking, especially a single crutch, can cause a large difference in the activation of the back muscles on the left and right sides, and this difference will cause muscle degeneration for a long time, leading to scoliosis. In this article taking scoliosis as an example, by analyzing the muscles around the spine, we can better understand the pathology and can better prevent diseases. The objective of this article is to analyze normal walking compared to walking with one or two crutches using OpenSim software to obtain the degree of activation of different muscles in order to analyze the impact of crutches on the body.", "prompt": "As a traditional tool of external assistance, crutches play an important role in society. They have a wide range of applications to help either the elderly and disabled to walk or to treat certain illnesses or for post-operative rehabilitation. But there are many different types of crutches, including shoulder crutches and elbow crutches. How to choose has become an issue that deserves to be debated. Because while crutches help people walk, they also have an impact on the body. Inappropriate choice of crutches or long-term misuse can lead to problems such as scoliosis. Previous studies were mainly experimental measurements or the construction of dynamic models to calculate the", "orig": "load on joints with crutches. These studies focus only on the level of the joints, ignoring the role that muscles play in this process. Although some also take into account the degree of muscle activation, there is still a lack of quantitative analysis. The traditional dynamic model can be used to calculate the load on each joint. However, due to the activation of the muscle, this situation only causes part of the load transmitted to the joint, and the work of the chair will compensate the other part of the load. Analysis at the muscle level allows a better understanding of the impact of crutches on the body. By comparing the levels of activation of", "generated": "mechanical properties of crutches, but few have explored the user experience and the long-term effects on the musculoskeletal system.\n\nIn recent years, there has been a growing interest in understanding the biomechanics of crutch usage and its impact on the user's body. Researchers have begun to investigate the optimal design and fitting of crutches to minimize the risk of injury and discomfort. For instance, a study published in the Journal of Rehabilitation Research and Development found that crutches with adjustable handles and cuffs can significantly reduce the pressure on the user's joints and muscles, leading to improved comfort and reduced fatigue. Another study published in the Journal of Pro"}
{"full_text": "Bilateral trade, a fundamental topic in economics, models the problem of intermediating between two strategic agents, a seller and a buyer, willing to trade a good for which they hold private valuations. Despite the simplicity of this problem, a classical result by Myerson and Satterthwaite (1983) affirms the impossibility of designing a mechanism which is simultaneously efficient, incentive compatible, individually rational, and budget balanced. This impossibility result fostered an intense investigation of meaningful trade-offs between these desired properties. Much work has focused on approximately efficient fixed-price mechanisms, i.e., Blumrosen and Dobzinski (2014; 2016), Colini-Baldeschi et al. (2016), which have been shown to fully characterize strong budget balanced and ex-post individually rational direct revelation mechanisms. All these results, however, either assume some knowledge on the priors of the seller/buyer valuations, or a black box access to some samples of the distributions, as in D{\\\"u}tting et al. (2021). In this paper, we cast for the first time the bilateral trade problem in a regret minimization framework over rounds of seller/buyer interactions, with no prior knowledge on the private seller/buyer valuations. Our main contribution is a complete characterization of the regret regimes for fixed-price mechanisms with different models of feedback and private valuations, using as benchmark the best fixed price in hindsight. More precisely, we prove the following bounds on the regret: $\\bullet$ $\\widetilde{\\Theta}(\\sqrt{T})$ for full-feedback (i.e., direct revelation mechanisms); $\\bullet$ $\\widetilde{\\Theta}(T^{2/3})$ for realistic feedback (i.e., posted-price mechanisms) and independent seller/buyer valuations with bounded densities; $\\bullet$ $\\Theta(T)$ for realistic feedback and seller/buyer valuations with bounded densities; $\\bullet$ $\\Theta(T)$ for realistic feedback and independent seller/buyer valuations; $\\bullet$ $\\Theta(T)$ for the adversarial setting.", "prompt": "Bilateral trade, a fundamental topic in economics, models the problem of intermediating between two strategic agents, a seller and a buyer, willing to trade a good for which they hold private valuations. Despite the simplicity of this problem, a classical result by Myerson and Satterthwaite (1983) affirms the impossibility of designing a mechanism which is simultaneously efficient, incentive compatible, individually rational, and budget balanced. This impossibility result fostered an intense investigation of meaningful trade-offs between these desired properties. Much work has focused on approximately efficient fixed-price mechanisms, i.e., Blumrosen and Dobzinski (", "orig": "2014; 2016), Colini-Baldeschi et al. (2016), which have been shown to fully characterize strong budget balanced and ex-post individually rational direct revelation mechanisms. All these results, however, either assume some knowledge on the priors of the seller/buyer valuations, or a black box access to some samples of the distributions, as in D{\\\"u}tting et al. (2021). In this paper, we cast for the first time the bilateral trade problem in a regret minimization framework over rounds of seller/buyer interactions, with no prior knowledge on the private seller/buy", "generated": "2012) who show that a simple and natural mechanism, called the \"median mechanism\", achieves a good trade-off between efficiency and incentive compatibility.\n\nThe median mechanism is a promising approach, but it is not without its limitations. For instance, it can be sensitive to the specific distribution of valuations, and it may not perform well in cases where the buyer and seller have significantly different valuations. To address these concerns, researchers have explored other approaches, such as the use of auctions or dynamic pricing mechanisms. In an auction setting, the seller can set a reserve price, and the buyer submits a bid. The seller then allocates the good"}
{"full_text": "Accurate traffic state information plays a pivotal role in the Intelligent Transportation Systems (ITS), and it is an essential input to various smart mobility applications such as signal coordination and traffic flow prediction. The current practice to obtain the traffic state information is through specialized sensors such as loop detectors and speed cameras. In most metropolitan areas, traffic monitoring cameras have been installed to monitor the traffic conditions on arterial roads and expressways, and the collected videos or images are mainly used for visual inspection by traffic engineers. Unfortunately, the data collected from traffic monitoring cameras are affected by the 4L characteristics: Low frame rate, Low resolution, Lack of annotated data, and Located in complex road environments. Therefore, despite the great potentials of the traffic monitoring cameras, the 4L characteristics hinder them from providing useful traffic state information (e.g., speed, flow, density). This paper focuses on the traffic density estimation problem as it is widely applicable to various traffic surveillance systems. To the best of our knowledge, there is a lack of the holistic framework for addressing the 4L characteristics and extracting the traffic density information from traffic monitoring camera data. In view of this, this paper proposes a framework for estimating traffic density using uncalibrated traffic monitoring cameras with 4L characteristics. The proposed framework consists of two major components: camera calibration and vehicle detection. The camera calibration method estimates the actual length between pixels in the images and videos, and the vehicle counts are extracted from the deep-learning-based vehicle detection method. Combining the two components, high-granular traffic density can be estimated. To validate the proposed framework, two case studies were conducted in Hong Kong and Sacramento. The results show that the Mean Absolute Error (MAE) in camera calibration is less than 0.2 meters out of 6 meters, and the accuracy of vehicle detection under various conditions is approximately 90%. Overall, the MAE for the estimated density is 9.04 veh/km/lane in Hong Kong and 1.30 veh/km/lane in Sacramento. The research outcomes can be used to calibrate the speed-density fundamental diagrams, and the proposed framework can provide accurate and real-time traffic information without installing additional sensors.", "prompt": "Accurate traffic state information plays a pivotal role in the Intelligent Transportation Systems (ITS), and it is an essential input to various smart mobility applications such as signal coordination and traffic flow prediction. The current practice to obtain the traffic state information is through specialized sensors such as loop detectors and speed cameras. In most metropolitan areas, traffic monitoring cameras have been installed to monitor the traffic conditions on arterial roads and expressways, and the collected videos or images are mainly used for visual inspection by traffic engineers. Unfortunately, the data collected from traffic monitoring cameras are affected by the 4L characteristics: Low frame rate, Low resolution, Lack of annotated data, and", "orig": "Located in complex road environments. Therefore, despite the great potentials of the traffic monitoring cameras, the 4L characteristics hinder them from providing useful traffic state information (e.g., speed, flow, density). This paper focuses on the traffic density estimation problem as it is widely applicable to various traffic surveillance systems. To the best of our knowledge, there is a lack of the holistic framework for addressing the 4L characteristics and extracting the traffic density information from traffic monitoring camera data. In view of this, this paper proposes a framework for estimating traffic density using uncalibrated traffic monitoring cameras with 4L characteristics. The proposed framework consists of two", "generated": "Limited coverage.\n\nContinuation:\n\nThe 4L characteristics greatly limit the accuracy and reliability of traffic state information obtained from traffic monitoring cameras. For instance, the low frame rate of typically 1-2 frames per second cannot capture the rapid changes in traffic conditions, leading to inaccurate speed and volume estimation. The low resolution of the cameras also makes it difficult to detect small objects such as pedestrians, bicycles, or motorcycles, which are crucial for safety and traffic flow analysis. Furthermore, the lack of annotated data hinders the development of machine learning models that can automatically extract meaningful information from the video feeds. Moreover, the limited coverage of traffic monitoring cameras"}
{"full_text": "In this paper, we describe a scheme for propagating belief functions in certain kinds of trees using only local computations. This scheme generalizes the computational scheme proposed by Shafer and Logan1 for diagnostic trees of the type studied by Gordon and Shortliffe, and the slightly more general scheme given by Shafer for hierarchical evidence. It also generalizes the scheme proposed by Pearl for Bayesian causal trees (see Shenoy and Shafer). Pearl's causal trees and Gordon and Shortliffe's diagnostic trees are both ways of breaking the evidence that bears on a large problem down into smaller items of evidence that bear on smaller parts of the problem so that these smaller problems can be dealt with one at a time. This localization of effort is often essential in order to make the process of probability judgment feasible, both for the person who is making probability judgments and for the machine that is combining them. The basic structure for our scheme is a type of tree that generalizes both Pearl's and Gordon and Shortliffe's trees. Trees of this general type permit localized computation in Pearl's sense. They are based on qualitative judgments of conditional independence. We believe that the scheme we describe here will prove useful in expert systems. It is now clear that the successful propagation of probabilities or certainty factors in expert systems requires much more structure than can be provided in a pure production-system framework. Bayesian schemes, on the other hand, often make unrealistic demands for structure. The propagation of belief functions in trees and more general networks stands on a middle ground where some sensible and useful things can be done. We would like to emphasize that the basic idea of local computation for propagating probabilities is due to Judea Pearl. It is a very innovative idea; we do not believe that it can be found in the Bayesian literature prior to Pearl's work. We see our contribution as extending the usefulness of Pearl's idea by generalizing it from Bayesian probabilities to belief functions. In the next section, we give a brief introduction to belief functions. The notions of qualitative independence for partitions and a qualitative Markov tree are introduced in Section III. Finally, in Section IV, we describe a scheme for propagating belief functions in qualitative Markov trees.", "prompt": "In this paper, we describe a scheme for propagating belief functions in certain kinds of trees using only local computations. This scheme generalizes the computational scheme proposed by Shafer and Logan1 for diagnostic trees of the type studied by Gordon and Shortliffe, and the slightly more general scheme given by Shafer for hierarchical evidence. It also generalizes the scheme proposed by Pearl for Bayesian causal trees (see Shenoy and Shafer). Pearl's causal trees and Gordon and Shortliffe's diagnostic trees are both ways of breaking the evidence that bears on a large problem down into smaller items of evidence that bear on smaller parts of the problem so that", "orig": "these smaller problems can be dealt with one at a time. This localization of effort is often essential in order to make the process of probability judgment feasible, both for the person who is making probability judgments and for the machine that is combining them. The basic structure for our scheme is a type of tree that generalizes both Pearl's and Gordon and Shortliffe's trees. Trees of this general type permit localized computation in Pearl's sense. They are based on qualitative judgments of conditional independence. We believe that the scheme we describe here will prove useful in expert systems. It is now clear that the successful propagation of probabilities or certainty factors in expert systems", "generated": "...it becomes more manageable and computationally efficient to update beliefs based on new evidence. Our scheme takes this idea a step further by allowing for the representation of more complex relationships between variables, and by providing a more flexible and scalable approach to belief propagation. In particular, our scheme can handle trees with a larger number of nodes and edges, and can accommodate a wider range of belief functions, including those that arenonlinear and non-Gaussian. We demonstrate the effectiveness of our scheme through a series of experiments on synthetic and real-world data, and show that it outperforms existing methods in terms of accuracy and computational efficiency. We also discuss"}
{"full_text": "Degenerate ignition of helium in low-mass stars at the end of the red giant branch phase leads to dynamic convection in their helium cores. One-dimensional (1D) stellar modeling of this intrinsically multi-dimensional dynamic event is likely to be inadequate. Previous hydrodynamic simulations imply that the single convection zone in the helium core of metal-rich Pop I stars grows during the flash on a dynamic timescale. This may lead to hydrogen injection into the core, and a double convection zone structure as known from one-dimensional core helium flash simulations of low-mass Pop III stars. We perform hydrodynamic simulations of the core helium flash in two and three dimensions to better constrain the nature of these events. To this end we study the hydrodynamics of convection within the helium cores of a 1.25 \\Msun metal-rich Pop I star (Z=0.02), and a 0.85 \\Msun metal-free Pop III star (Z=0) near the peak of the flash. These models possess single and double convection zones, respectively. We use 1D stellar models of the core helium flash computed with state-of-the-art stellar evolution codes as initial models for our multidimensional hydrodynamic study, and simulate the evolution of these models with the Riemann solver based hydrodynamics code Herakles which integrates the Euler equations coupled with source terms corresponding to gravity and nuclear burning. The hydrodynamic simulation of the Pop I model involving a single convection zone covers 27 hours of stellar evolution, while the first hydrodynamic simulations of a double convection zone, in the Pop III model, span 1.8 hours of stellar life. We find differences between the predictions of mixing length theory and our hydrodynamic simulations. The simulation of the single convection zone in the Pop I model shows a strong growth of the size of the convection zone due to turbulent entrainment. Hence we predict that for the Pop I model a hydrogen injection phase (i.e. hydrogen injection into the helium core) will commence after about 23 days, which should eventually lead to a double convection zone structure known from 1D stellar modeling of low-mass Pop III stars. Our two and three-dimensional hydrodynamic simulations of the double (Pop III) convection zone model show that the velocity field in the convection zones is different from that predicted by stellar evolutionary calculations. The simulations suggest that the double convection zone decays quickly, the flow eventually being dominated by internal gravity waves.", "prompt": "Degenerate ignition of helium in low-mass stars at the end of the red giant branch phase leads to dynamic convection in their helium cores. One-dimensional (1D) stellar modeling of this intrinsically multi-dimensional dynamic event is likely to be inadequate. Previous hydrodynamic simulations imply that the single convection zone in the helium core of metal-rich Pop I stars grows during the flash on a dynamic timescale. This may lead to hydrogen injection into the core, and a double convection zone structure as known from one-dimensional core helium flash simulations of low-mass Pop III stars. We perform hydrodynamic simulations of the core helium flash in", "orig": "two and three dimensions to better constrain the nature of these events. To this end we study the hydrodynamics of convection within the helium cores of a 1.25 \\Msun metal-rich Pop I star (Z=0.02), and a 0.85 \\Msun metal-free Pop III star (Z=0) near the peak of the flash. These models possess single and double convection zones, respectively. We use 1D stellar models of the core helium flash computed with state-of-the-art stellar evolution codes as initial models for our multidimensional hydrodynamic study, and simulate the evolution of these models with", "generated": "metal-poor Pop II stars to investigate the effects of metallicity on the flash dynamics.\n\nOur simulations show that the core helium flash in metal-poor Pop II stars is characterized by a more vigorous convection, leading to a larger mixing zone and a higher degree of hydrogen injection into the core. This results in a more pronounced double convection zone structure, with a larger upper convection zone and a more turbulent interface between the two zones. In contrast to metal-rich Pop I stars, the flash in metal-poor Pop II stars is also accompanied by a stronger neutrino emission, which affects the thermal balance in the core and modifies the flash"}
{"full_text": "We pursue here the development of models for complex (viscoelastic) fluids in shallow free-surface gravity flows which was initiated by [Bouchut-Boyaval, M3AS (23) 2013] for 1D (translation invariant) cases. The models we propose are hyperbolic quasilinear systems that generalize Saint-Venant shallow-water equations to incompressible Maxwell fluids. The models are compatible with a formulation of the thermo-dynamics second principle. In comparison with Saint-Venant standard shallow-water model, the momentum balance includes extra-stresses associated with an elastic potential energy in addition to a hydrostatic pressure. The extra-stresses are determined by an additional tensor variable solution to a differential equation with various possible time rates. For the numerical evaluation of solutions to Cauchy problems, we also propose explicit schemes discretizing our generalized Saint-Venant systems with Finite-Volume approximations that are entropy-consistent (under a CFL constraint) in addition to satisfy exact (discrete) mass and momentum conservation laws. In comparison with most standard viscoelastic numerical models, our discrete models can be used for any retardation-time values (i.e. in the vanishing \"solvent-viscosity\" limit). We finally illustrate our hyperbolic viscoelastic flow models numerically using computer simulations in benchmark test cases. On extending to Maxwell fluids some free-shear flow testcases that are standard benchmarks for Newtonian fluids, we first show that our (numerical) models reproduce well the viscoelastic physics, phenomenologically at least, with zero retardation-time. Moreover, with a view to quantitative evaluations, numerical results in the lid-driven cavity testcase show that, in fact, our models can be compared with standard viscoelastic flow models in sheared-flow benchmarks on adequately choosing the physical parameters of our models. Analyzing our models asymptotics should therefore shed new light on the famous High-Weissenberg Number Problem (HWNP), which is a limit for all the existing viscoelastic numerical models.", "prompt": "We pursue here the development of models for complex (viscoelastic) fluids in shallow free-surface gravity flows which was initiated by [Bouchut-Boyaval, M3AS (23) 2013] for 1D (translation invariant) cases. The models we propose are hyperbolic quasilinear systems that generalize Saint-Venant shallow-water equations to incompressible Maxwell fluids. The models are compatible with a formulation of the thermo-dynamics second principle. In comparison with Saint-Venant standard shallow-water model, the momentum balance includes extra-stresses associated with an elastic potential energy in addition to a hydrostatic pressure.", "orig": "The extra-stresses are determined by an additional tensor variable solution to a differential equation with various possible time rates. For the numerical evaluation of solutions to Cauchy problems, we also propose explicit schemes discretizing our generalized Saint-Venant systems with Finite-Volume approximations that are entropy-consistent (under a CFL constraint) in addition to satisfy exact (discrete) mass and momentum conservation laws. In comparison with most standard viscoelastic numerical models, our discrete models can be used for any retardation-time values (i.e. in the vanishing \"solvent-viscosity\" limit). We finally illustrate our hyperbolic viscoelastic", "generated": "The key novelty of our approach lies in the incorporation of viscoelastic properties into the shallow-water framework, which enables the capture of complex rheological behaviors exhibited by non-Newtonian fluids. This is achieved through the introduction of an elastic potential energy term in the momentum balance equation, which is derived from the Maxwell model of viscoelasticity. The resulting system of equations is hyperbolic in nature, ensuring well-posedness and stability of the numerical solutions. Furthermore, the compatibility with the second principle of thermodynamics guarantees that the model is energetically consistent and respects the fundamental laws of physics. \n\nIn contrast to the classical"}
{"full_text": "Context. The TOPoS project has the goal to find and analyse Turn-Off (TO) stars of extremely low metallicity. To select the targets for spectroscopic follow-up at high spectral resolution, we have relied on low-resolution spectra from the Sloan Digital Sky Survey. Aims. In this paper we use the metallicity estimates we have obtained from our analysis of the SDSS spectra to construct the metallicity distribution function (MDF) of the Milky Way, with special emphasis on its metal-weak tail. The goal is to provide the underlying distribution out of which the TOPoS sample was extracted. Methods. We make use of SDSS photometry, Gaia photometry and distance estimates derived from the Gaia parallaxes to derive a metallicity estimate for a large sample of over 24 million TO stars. This sample is used to derive the metallicity bias of the sample for which SDSS spectra are available. Results. We determined that the spectroscopic sample is strongly biased in favour of metal-poor stars, as intended. A comparison with the unbiased photometric sample allows to correct for the selection bias. We select a sub-sample of stars with reliable parallaxes for which we combine the SDSS radial velocities with Gaia proper motions and parallaxes to compute actions and orbital parameters in the Galactic potential. This allows us to characterize the stars dynamically, and in particular to select a sub-sample that belongs to the Gaia-Sausage-Enceladus (GSE) accretion event. We are thus able to provide also the MDF of GSE. Conclusions. The metal-weak tail derived in our study is very similar to that derived in the H3 survey and in the Hamburg/ESO Survey. This allows us to average the three MDFs and provide an error bar for each metallicity bin. Inasmuch the GSE structure is representative of the progenitor galaxy that collided with the Milky Way, that galaxy appears to be strongly deficient in metal-poor stars compared to the Milky Way, suggesting that the metal-weak tail of the latter has been largely formed by accretion of low mass galaxies rather than massive galaxies, such as the GSE progenitor.", "prompt": "Context. The TOPoS project has the goal to find and analyse Turn-Off (TO) stars of extremely low metallicity. To select the targets for spectroscopic follow-up at high spectral resolution, we have relied on low-resolution spectra from the Sloan Digital Sky Survey. Aims. In this paper we use the metallicity estimates we have obtained from our analysis of the SDSS spectra to construct the metallicity distribution function (MDF) of the Milky Way, with special emphasis on its metal-weak tail. The goal is to provide the underlying distribution out of which the TOPoS sample was extracted. Methods. We make use of SD", "orig": "SS photometry, Gaia photometry and distance estimates derived from the Gaia parallaxes to derive a metallicity estimate for a large sample of over 24 million TO stars. This sample is used to derive the metallicity bias of the sample for which SDSS spectra are available. Results. We determined that the spectroscopic sample is strongly biased in favour of metal-poor stars, as intended. A comparison with the unbiased photometric sample allows to correct for the selection bias. We select a sub-sample of stars with reliable parallaxes for which we combine the SDSS radial velocities with Gaia proper motions and parallaxes to", "generated": "SS DR10 data, which provides a large and homogeneous dataset of stellar spectra.\n\nContinuation:\n\nTo construct the MDF, we implemented a Bayesian approach, taking into account the uncertainties associated with the metallicity estimates. We utilized a Markov Chain Monte Carlo (MCMC) algorithm to sample the posterior distribution of the metallicity parameters. This allowed us to propagate the uncertainties in the metallicity estimates to the final MDF. Furthermore, we applied a series of quality cuts to ensure that only high-quality spectra were considered in the analysis. This included rejecting spectra with low signal-to-noise ratios, as well as those affected by instrumental artifacts"}
{"full_text": "Heavy Ion Collisions (HIC) represent a unique tool to probe the in-medium nuclear interaction in regions away from saturation. In this report we present a selection of new reaction observables in dissipative collisions particularly sensitive to the symmetry term of the nuclear Equation of State ($Iso-EoS$). We will first discuss the Isospin Equilibration Dynamics. At low energies this manifests via the recently observed Dynamical Dipole Radiation, due to a collective neutron-proton oscillation with the symmetry term acting as a restoring force. At higher beam energies Iso-EoS effects will be seen in an Isospin Diffusion mechanism, via Imbalance Ratio Measurements, in particular from correlations to the total kinetic energy loss. For fragmentation reactions in central events we suggest to look at the coupling between isospin distillation and radial flow. In Neck Fragmentation reactions important Iso-EoS information can be obtained from fragment isospin content, velocity and alignement correlations. The high density symmetry term can be probed from isospin effects on heavy ion reactions at relativistic energies (few AGeV range), in particular for high transverse momentum selections of the reaction products. Rather isospin sensitive observables are proposed from nucleon/cluster emissions, collective flows and meson production. The possibility to shed light on the controversial neutron/proton effective mass splitting in asymmetric matter is also suggested. A large symmetry repulsion at high baryon density will also lead to an \"earlier\" hadron-deconfinement transition in n-rich matter. The binodal transition line of the (T,\\rho_B) diagram is lowered to a region accessible through heavy ion collisions in the energy range of the new planned facilities, e.g. the FAIR/NICA projects. Some observable effects of the formation of a Mixed Phase are suggested, in particular a Neutron Trapping mechanism. The dependence of the results on a suitable treatment of the isovector part of the interaction in effective QCD Lagrangian approaches is critically discussed. We stress the interest of this study in nuclear astrophysics, in particular for supernovae explosions and neutron star structure, where the knowledge of the Iso-EoS is important at low as well as at high baryon density.", "prompt": "Heavy Ion Collisions (HIC) represent a unique tool to probe the in-medium nuclear interaction in regions away from saturation. In this report we present a selection of new reaction observables in dissipative collisions particularly sensitive to the symmetry term of the nuclear Equation of State ($Iso-EoS$). We will first discuss the Isospin Equilibration Dynamics. At low energies this manifests via the recently observed Dynamical Dipole Radiation, due to a collective neutron-proton oscillation with the symmetry term acting as a restoring force. At higher beam energies Iso-EoS effects will be seen in an Isospin Diffusion mechanism, via Im", "orig": "balance Ratio Measurements, in particular from correlations to the total kinetic energy loss. For fragmentation reactions in central events we suggest to look at the coupling between isospin distillation and radial flow. In Neck Fragmentation reactions important Iso-EoS information can be obtained from fragment isospin content, velocity and alignement correlations. The high density symmetry term can be probed from isospin effects on heavy ion reactions at relativistic energies (few AGeV range), in particular for high transverse momentum selections of the reaction products. Rather isospin sensitive observables are proposed from nucleon/cluster emissions, collective flows and meson production.", "generated": "balance Ratio and Elliptic Flow, which provide a strong correlation to the in-medium nucleon effective mass splitting.\n\nThe study of Iso-EoS effects is crucial for understanding the properties of dense nuclear matter, as it plays a key role in the description of various astrophysical phenomena, such as neutron star collisions and supernovae explosions. The Iso-EoS also influences the dynamics of heavy-ion reactions, affecting the stopping power, particle production, and collective flow of particles. Therefore, the precise determination of the symmetry term is essential for advancing our knowledge of nuclear physics.\n\nOne of the primary goals of this research is to develop a comprehensive understanding"}
{"full_text": "Stylolites are ubiquitous geo-patterns observed in rocks in the upper crust, from geological reservoirs in sedimentary rocks to deformation zones, in folds, faults, and shear zones. These rough surfaces play a major role in the dissolution of rocks around stressed contacts, the transport of dissolved material and the precipitation in surrounding pores. Consequently, they play an active role in the evolution of rock microstructures and rheological properties in the Earth's crust. They are observed individually or in networks, in proximity to fractures and joints, and in numerous geological settings. This review article deals with their geometrical and compositional characteristics and the factors leading to their genesis. The main questions this review focuses on are the following: How do they form? How can they be used to measure strain and formation stress? How do they control fluid flow in the upper crust? Geometrically, stylolites have fractal roughness, with fractal geometrical properties exhibiting typically three scaling regimes: a self-affine scaling with Hurst exponent 1.1+/-0.1 at small scale (up to tens or hundreds of microns), another one with Hurst exponent around 0.5 to 0.6 at intermediate scale (up to millimeters or centimeters), and in the case of sedimentary stylolites, a flat scaling at large scale. More complicated anisotropic scaling (scaling laws depending of the direction of the profile considered) is found in the case of tectonic stylolites. We report models based on first principles from physical chemistry and statistical physics, including a mechanical component for the free-energy associated with stress concentrations, and a precise tracking of the influence of grain-scale heterogeneities and disorder on the resulting (micro)structures. Experimental efforts to reproduce stylolites in the laboratory are also reviewed. We show that although micrometer-size stylolite teeth are obtained in laboratory experiments, teeth deforming numerous grains have not yet been obtained experimentally, which is understandable given the very long formation time of such geometries. Finally, the applications of stylolites as strain and stress markers, to determine paleostress magnitude are reviewed. We show that the scalings in stylolite heights and the crossover scale between these scalings can be used to determine the stress magnitude (its scalar value) perpendicular to the stylolite surface during the stylolite formation, and that the stress anisotropy in the stylolite plane can be determined for the case of tectonic stylolites. We also show that the crossover between medium (millimetric) scales and large (pluricentimetric) scales, in the case of sedimentary stylolites, provides a good marker for the total amount of dissolution, which is still valid even when the largest teeth start to dissolve -- which leads to the loss of information, since the total deformation is not anymore recorded in a single marker structure. We discuss the impact of the stylolites on the evolution of the transport properties of the hosting rock, and show that they promote a permeability increase parallel to the stylolites, whereas their effect on the permeability transverse to the stylolite can be negligible, or may reduce the permeability, depending on the development of the stylolite. Highlights: Stylolite formation depends on rock composition and structure, stress and fluids. Stylolite geometry, fractal and self-affine properties, network structure, are investigated. The experiments and physics-based numerical models for their formation are reviewed. Stylolites can be used as markers of strain, paleostress orientation and magnitude. Stylolites impact transport properties, as function of maturity and flow direction.", "prompt": "Stylolites are ubiquitous geo-patterns observed in rocks in the upper crust, from geological reservoirs in sedimentary rocks to deformation zones, in folds, faults, and shear zones. These rough surfaces play a major role in the dissolution of rocks around stressed contacts, the transport of dissolved material and the precipitation in surrounding pores. Consequently, they play an active role in the evolution of rock microstructures and rheological properties in the Earth's crust. They are observed individually or in networks, in proximity to fractures and joints, and in numerous geological settings. This review article deals with their geometrical and compositional characteristics and the factors leading to", "orig": "their genesis. The main questions this review focuses on are the following: How do they form? How can they be used to measure strain and formation stress? How do they control fluid flow in the upper crust? Geometrically, stylolites have fractal roughness, with fractal geometrical properties exhibiting typically three scaling regimes: a self-affine scaling with Hurst exponent 1.1+/-0.1 at small scale (up to tens or hundreds of microns), another one with Hurst exponent around 0.5 to 0.6 at intermediate scale (up to millimeters or centimeters), and in", "generated": "their formation and development.\n\nThe formation of stylolites is a complex process that involves a combination of mechanical and chemical processes. The rough surfaces of stylolites are produced by the dissolution of minerals along grain boundaries, which is controlled by the local stress field and the composition of the rock. The dissolution process is often accompanied by the precipitation of secondary minerals, such as calcite, dolomite, or quartz, which fill the pores and voids created by the dissolution process. This process can lead to the formation of stylolite networks, which can have a significant impact on the rock's permeability and fluid flow properties.\n\nIn addition to"}
{"full_text": "We define the prequantization of a symplectic Anosov diffeomorphism f:M-> M, which is a U(1) extension of the diffeomorphism f preserving an associated specific connection, and study the spectral properties of the associated transfer operator, called prequantum transfer operator. This is a model for the transfer operators associated to geodesic flows on negatively curved manifolds (or contact Anosov flows). We restrict the prequantum transfer operator to the N-th Fourier mode with respect to the U(1) action and investigate the spectral property in the limit N->infinity, regarding the transfer operator as a Fourier integral operator and using semi-classical analysis. In the main result, we show a \" band structure \" of the spectrum, that is, the spectrum is contained in a few separated annuli and a disk concentric at the origin. We show that, with the special (H\\\"older continuous) potential V0=1/2 log |det Df_x|_{E_u}|, the outermost annulus is the unit circle and separated from the other parts. For this, we use an extension of the transfer operator to the Grassmanian bundle. Using Atiyah-Bott trace formula, we establish the Gutzwiller trace formula with exponentially small reminder for large time. We show also that, for a potential V such that the outermost annulus is separated from the other parts, most of the eigenvalues in the outermost annulus concentrate on a circle of radius exp where <.> denotes the spatial average on M. The number of these eigenvalues is given by the \"Weyl law\", that is, N^d.Vol(M) with d=1/2. dim(M) in the leading order. We develop a semiclassical calculus associated to the prequantum operator by defining quantization of observables Op(psi) in an intrinsic way. We obtain that the semiclassical Egorov formula of quantum transport is exact. We interpret all these results from a physical point of view as the emergence of quantum dynamics in the classical correlation functions for large time. We compare these results with standard quantization (geometric quantization) in quantum chaos.", "prompt": "We define the prequantization of a symplectic Anosov diffeomorphism f:M-> M, which is a U(1) extension of the diffeomorphism f preserving an associated specific connection, and study the spectral properties of the associated transfer operator, called prequantum transfer operator. This is a model for the transfer operators associated to geodesic flows on negatively curved manifolds (or contact Anosov flows). We restrict the prequantum transfer operator to the N-th Fourier mode with respect to the U(1) action and investigate the spectral property in the limit N->infinity, regarding the transfer", "orig": "operator as a Fourier integral operator and using semi-classical analysis. In the main result, we show a \" band structure \" of the spectrum, that is, the spectrum is contained in a few separated annuli and a disk concentric at the origin. We show that, with the special (H\\\"older continuous) potential V0=1/2 log |det Df_x|_{E_u}|, the outermost annulus is the unit circle and separated from the other parts. For this, we use an extension of the transfer operator to the Grassmanian bundle. Using Atiyah-Bott trace formula, we establish the", "generated": "operator as a semiclassical pseudodifferential operator.\n\nIn this limit, we observe a concentration of eigenvalues near the unit circle, which is indicative of quantum chaos. Furthermore, we establish a connection between the eigenvalues of the prequantum transfer operator and the classical dynamics of the Anosov diffeomorphism. Specifically, we show that the eigenvalues are related to the periodic orbits of the diffeomorphism, and that the eigenfunctions exhibit scarring behavior, meaning that they concentrate on certain regions of the phase space. This provides evidence for the phenomenon of quantum scarring, which has been observed in various quantum systems.\n\nOur results"}
{"full_text": "Context. An extension method is a method declared in a package other than the package of its host class. Thanks to extension methods, developers can adapt to their needs classes they do not own: adding methods to core classes is a typical use case. This is particularly useful for adapting software and therefore to increase reusability. Inquiry. In most dynamically-typed languages, extension methods are globally visible. Because any developer can define extension methods for any class, naming conflicts occur: if two developers define an extension method with the same signature in the same class, only one extension method is visible and overwrites the other. Similarly, if two developers each define an extension method with the same name in a class hierarchy, one overrides the other. To avoid such \"accidental overrides\", some dynamically-typed languages limit the visibility of an extension method to a particular scope. However, their semantics have not been fully described and compared. In addition, these solutions typically rely on a dedicated and slow method lookup algorithm to resolve conflicts at runtime. Approach. In this article, we present a formalization of the underlying models of Ruby refinements, Groovy categories, Classboxes, and Method Shelters that are scoping extension method solutions in dynamically-typed languages. Knowledge. Our formal framework allows us to objectively compare and analyze the shortcomings of the studied solutions and other different approaches such as MultiJava. In addition, language designers can use our formal framework to determine which mechanism has less risk of \"accidental overrides\". Grounding. Our comparison and analysis of existing solutions is grounded because it is based on denotational semantics formalizations. Importance. Extension methods are widely used in programming languages that support them, especially dynamically-typed languages such as Pharo, Ruby or Python. However, without a carefully designed mechanism, this feature can cause insidious hidden bugs or can be voluntarily used to gain access to protected operations, violate encapsulation or break fundamental invariants.", "prompt": "Context. An extension method is a method declared in a package other than the package of its host class. Thanks to extension methods, developers can adapt to their needs classes they do not own: adding methods to core classes is a typical use case. This is particularly useful for adapting software and therefore to increase reusability. Inquiry. In most dynamically-typed languages, extension methods are globally visible. Because any developer can define extension methods for any class, naming conflicts occur: if two developers define an extension method with the same signature in the same class, only one extension method is visible and overwrites the other. Similarly, if two developers each", "orig": "define an extension method with the same name in a class hierarchy, one overrides the other. To avoid such \"accidental overrides\", some dynamically-typed languages limit the visibility of an extension method to a particular scope. However, their semantics have not been fully described and compared. In addition, these solutions typically rely on a dedicated and slow method lookup algorithm to resolve conflicts at runtime. Approach. In this article, we present a formalization of the underlying models of Ruby refinements, Groovy categories, Classboxes, and Method Shelters that are scoping extension method solutions in dynamically-typed languages. Knowledge. Our formal framework allows us to", "generated": "define extension methods with different signatures but the same name in the same class, the method that is visible depends on the context in which the class is used.\n\nTo mitigate these naming conflicts, developers can take precautions such as using unique naming conventions or namespaces for their extension methods. However, this approach is not foolproof, and naming conflicts can still occur. Another approach is to use a mechanism such as dependency injection, where the extension method is provided by a separate module or package that can be easily swapped out or updated. This way, if a conflict arises, the developer can simply replace the conflicting extension method with a new one.\n\nIn addition,"}
{"full_text": "Magnetoencephalography (MEG) and electroencephalogra-phy (EEG) are non-invasive modalities that measure the weak electromagnetic fields generated by neural activity. Inferring the location of the current sources that generated these magnetic fields is an ill-posed inverse problem known as source imaging. When considering a group study, a baseline approach consists in carrying out the estimation of these sources independently for each subject. The ill-posedness of each problem is typically addressed using sparsity promoting regularizations. A straightforward way to define a common pattern for these sources is then to average them. A more advanced alternative relies on a joint localization of sources for all subjects taken together, by enforcing some similarity across all estimated sources. An important advantage of this approach is that it consists in a single estimation in which all measurements are pooled together, making the inverse problem better posed. Such a joint estimation poses however a few challenges, notably the selection of a valid regularizer that can quantify such spatial similarities. We propose in this work a new procedure that can do so while taking into account the geometrical structure of the cortex. We call this procedure Minimum Wasserstein Estimates (MWE). The benefits of this model are twofold. First, joint inference allows to pool together the data of different brain geometries, accumulating more spatial information. Second, MWE are defined through Optimal Transport (OT) metrics which provide a tool to model spatial proximity between cortical sources of different subjects, hence not enforcing identical source location in the group. These benefits allow MWE to be more accurate than standard MEG source localization techniques. To support these claims, we perform source localization on realistic MEG simulations based on forward operators derived from MRI scans. On a visual task dataset, we demonstrate how MWE infer neural patterns similar to functional Magnetic Resonance Imaging (fMRI) maps.", "prompt": "Magnetoencephalography (MEG) and electroencephalogra-phy (EEG) are non-invasive modalities that measure the weak electromagnetic fields generated by neural activity. Inferring the location of the current sources that generated these magnetic fields is an ill-posed inverse problem known as source imaging. When considering a group study, a baseline approach consists in carrying out the estimation of these sources independently for each subject. The ill-posedness of each problem is typically addressed using sparsity promoting regularizations. A straightforward way to define a common pattern for these sources is then to average them. A more advanced alternative relies on", "orig": "a joint localization of sources for all subjects taken together, by enforcing some similarity across all estimated sources. An important advantage of this approach is that it consists in a single estimation in which all measurements are pooled together, making the inverse problem better posed. Such a joint estimation poses however a few challenges, notably the selection of a valid regularizer that can quantify such spatial similarities. We propose in this work a new procedure that can do so while taking into account the geometrical structure of the cortex. We call this procedure Minimum Wasserstein Estimates (MWE). The benefits of this model are twofold. First, joint inference allows to pool together", "generated": "the use of multivariate statistical methods, such as independent component analysis (ICA) or joint independent component analysis (jICA), which can separate the underlying neural sources into independent components, allowing for a more detailed analysis of the brain's neural activity. Another approach is to use Bayesian methods, which can provide a probabilistic framework for estimating the sources and their variability within a group. Additionally, machine learning techniques, such as clustering or dimensionality reduction, can be applied to identify common patterns or features among the sources across subjects. Furthermore, the use of graph theory and network analysis can help to identify the organization and connectivity of the brain's neural"}
{"full_text": "Context. Using observations to deduce dust properties, grain size distribution, and physical conditions in molecular clouds is a highly degenerate problem. Aims. The coreshine phenomenon, a scattering process at 3.6 and 4.5 $\\mu$m that dominates absorption, has revealed its ability to explore the densest parts of clouds. We want to use this effect to constrain the dust parameters. The goal is to investigate to what extent grain growth (at constant dust mass) inside molecular clouds is able to explain the coreshine observations. We aim to find dust models that can explain a sample of Spitzer coreshine data. We also look at the consistency with near-infrared data we obtained for a few clouds. Methods. We selected four regions with a very high occurrence of coreshine cases: Taurus-Perseus, Cepheus, Chameleon and L183/L134. We built a grid of dust models and investigated the key parameters to reproduce the general trend of surface bright- nesses and intensity ratios of both coreshine and near-infrared observations with the help of a 3D Monte-Carlo radiative transfer code. The grid parameters allow to investigate the effect of coagulation upon spherical grains up to 5 $\\mu$m in size derived from the DustEm diffuse interstellar medium grains. Fluffiness (porosity or fractal degree), ices, and a handful of classical grain size distributions were also tested. We used the near- and mostly mid-infrared intensity ratios as strong discriminants between dust models. Results. The determination of the background field intensity at each wavelength is a key issue. In particular, an especially strong background field explains why we do not see coreshine in the Galactic plane at 3.6 and 4.5 $\\mu$m. For starless cores, where detected, the observed 4.5 $\\mu$m / 3.6 $\\mu$m coreshine intensity ratio is always lower than $\\sim$0.5 which is also what we find in the models for the Taurus-Perseus and L183 directions. Embedded sources can lead to higher fluxes (up to four times greater than the strongest starless core fluxes) and higher coreshine ratios (from 0.5 to 1.1 in our selected sample). Normal interstellar radiation field conditions are sufficient to find suitable grain models at all wavelengths for starless cores. The standard interstellar grains are not able to reproduce observations and, due to the multi-wavelength approach, only a few grain types meet the criteria set by the data. Porosity does not affect the flux ratios while the fractal dimension helps to explain coreshine ratios but does not seem able to reproduce near-infrared observations without a mix of other grain types. Conclusions. Combined near- and mid-infrared wavelengths confirm the potential to reveal the nature and size distribution of dust grains. Careful assessment of the environmental parameters (interstellar and background fields, embedded or nearby reddened sources) is required to validate this new diagnostic.", "prompt": "Context. Using observations to deduce dust properties, grain size distribution, and physical conditions in molecular clouds is a highly degenerate problem. Aims. The coreshine phenomenon, a scattering process at 3.6 and 4.5 $\\mu$m that dominates absorption, has revealed its ability to explore the densest parts of clouds. We want to use this effect to constrain the dust parameters. The goal is to investigate to what extent grain growth (at constant dust mass) inside molecular clouds is able to explain the coreshine observations. We aim to find dust models that can explain a sample of Spitzer coreshine data", "orig": ". We also look at the consistency with near-infrared data we obtained for a few clouds. Methods. We selected four regions with a very high occurrence of coreshine cases: Taurus-Perseus, Cepheus, Chameleon and L183/L134. We built a grid of dust models and investigated the key parameters to reproduce the general trend of surface bright- nesses and intensity ratios of both coreshine and near-infrared observations with the help of a 3D Monte-Carlo radiative transfer code. The grid parameters allow to investigate the effect of coagulation upon spherical grains up to 5 $\\", "generated": ".\n\nContinuation:\n\nIn this study, we focus on a specific region of the molecular cloud complex, where the coreshine effect is most pronounced. By analyzing the Spitzer data, we have identified a set of sources that exhibit strong coreshine signatures. Our approach is to model the dust properties in these sources using a combination of radiative transfer simulations and Bayesian inference techniques. We will explore a range of grain size distributions, from small, ISM-like grains to larger, more evolved grains, and investigate how these affect the coreshine signal. We will also consider the role of dust composition, grain shape, and alignment on the"}
{"full_text": "Some implants have approximately a lifetime of 15 years. The femoral stem, for example, should be made of 316L/316LN stainless steel. Fretting corrosion, friction under small displacements, should occur during human gait, due to repeated loadings and un-loadings, between stainless steel and bone for instance. Some experimental investigations of fretting corrosion have been practiced. As well known, metallic alloys and especially stainless steels are covered with a passive film that prevents from the corrosion and degradation. This passive layer of few nanometers, at ambient temperature, is the key of our civilization according to some authors. This work is dedicated to predict the passive layer thicknesses of stainless steel under fretting corrosion with a specific emphasis on the role of proteins. The model is based on the Point Defect Model (micro scale) and an update of the model on the friction process (micro-macro scale). Genetic algorithm was used for finding solution of the problem. The major results are, as expected from experimental results, albumin prevents from degradation at the lowest concentration of chlorides; an incubation time is necessary for degrading the passive film; under fretting corrosion and high concentration of chlorides the passive behavior is annihilated. Les implants orthop\\'ediques de hanche ont une dur\\'ee de vie d'environ 15 ans. Par exemple, la tige f\\'emorale d'un tel implant peut \\^etre r\\'ealis\\'ee en acier inoxydable 316L ou 316LN. Le fretting corrosion, frottement sous petits d\\'eplacements, peut se produire pendant la marche humaine en raison des chargements r\\'ep\\'et\\'es entre le m\\'etal de la proth\\`ese et l'os. Plusieurs investigations exp\\'erimentales du fretting corrosion ont \\'et\\'e entreprises. Cette couche passive de quelques nanom\\`etres, \\`a temp\\'erature ambiante, est le point clef sur lequel repose le d\\'eveloppement de notre civilisation, selon certains auteurs. Ce travail vise \\`a pr\\'edire les \\'epaisseurs de cette couche passive de l'acier inoxydable soumis au fretting corrosion, avec une attention sp\\'ecifique sur le r\\^ole des prot\\'eines. Le mod\\`ele utilis\\'e est bas\\'e sur le Point Defect Model, PDM (\\`a une \\'echelle microscopique) et une am\\'elioration de ce mod\\`ele en prenant en compte le processus de frottement sous petits d\\'ebattements. L'algorithme g\\'en\\'etique a \\'et\\'e utilis\\'e pour optimiser la convergence du probl\\`eme. Les r\\'esultats les plus importants sont, comme d\\'emontr\\'e avec les essais exp\\'erimentaux, que l'albumine, la prot\\'eine \\'etudi\\'ee, emp\\^eche les d\\'egradations de l'acier inoxydable aux plus faibles concentrations d'ions chlorure ; ensuite, aux plus fortes concentrations de chlorures, un temps d'incubation est n\\'ecessaire pour d\\'etruire le film passif.", "prompt": "Some implants have approximately a lifetime of 15 years. The femoral stem, for example, should be made of 316L/316LN stainless steel. Fretting corrosion, friction under small displacements, should occur during human gait, due to repeated loadings and un-loadings, between stainless steel and bone for instance. Some experimental investigations of fretting corrosion have been practiced. As well known, metallic alloys and especially stainless steels are covered with a passive film that prevents from the corrosion and degradation. This passive layer of few nanometers, at ambient temperature, is the key of our civilization according to some authors. This", "orig": "work is dedicated to predict the passive layer thicknesses of stainless steel under fretting corrosion with a specific emphasis on the role of proteins. The model is based on the Point Defect Model (micro scale) and an update of the model on the friction process (micro-macro scale). Genetic algorithm was used for finding solution of the problem. The major results are, as expected from experimental results, albumin prevents from degradation at the lowest concentration of chlorides; an incubation time is necessary for degrading the passive film; under fretting corrosion and high concentration of chlorides the passive behavior is annihilated. Les implants orthop\\'ed", "generated": "natural barrier, however, can be compromised by various factors, including wear and tear, environmental conditions, and the presence of corrosive substances.\n\nWhen the passive layer is breached, the underlying metal is exposed to the surrounding environment, leading to corrosion and potentially catastrophic consequences. In the case of implants, corrosion can lead to the release of toxic ions, inflammation, and tissue damage, ultimately resulting in implant failure. Therefore, it is essential to understand the mechanisms of fretting corrosion and to develop strategies to mitigate its effects. Researchers have been exploring various approaches to improve the corrosion resistance of implants, including surface modification, coating, and development of new materials"}
{"full_text": "In this paper we propose a minimal model for free reeds taking into account the significant phenomena. This free reed model may be used to build models of free reed instruments which permit numerical simulations. Several definitions for the section by which the airflow passes through the reed are reviewed and a new one is proposed which takes into account the entire escape area under the reed and the reed thickness. To derive this section, it is necessary to distinguish the neutral section (the only section of the reed which always keeps its length constant while moving) from the upstream or downstream sections. A minimal configuration is chosen to permit the instabilities of both (-,+) and (+,-) reeds on the basis of a linear analysis of instabilities conditions. This configuration is used to illustrate, with temporal simulations, the minimal model for both kinds of reeds and to discuss the model assumptions. Some clues are given about the influence, on the playing frequency and on the dynamic of the sound, of two main parameters of the geometrical model: the size of the volume and the level of the excitation. It is shown that the playing frequency of a (+,-) reed can vary in a large range according to the size of the volume upstream of the reed; that the playing frequency is nearly independent of the excitation but that the dynamic of the sound increases with the excitation level. Some clues are also proposed to determine the nature of the bifurcation for free reeds: it seems that free reeds may present inverse bifurcations. The influence of the reed thickness is also studied for configurations where the reed length or the reed width vary to keep the mass constant. This study shows that the reed thickness can have a great influence on the sound magnitude, the playing frequency and the magnitude of the reed displacement which justifies its introduction in the reed model.This article has been published in Acta Acustica united with Acustica, Vol. 93 (2007), p. 122-144.", "prompt": "In this paper we propose a minimal model for free reeds taking into account the significant phenomena. This free reed model may be used to build models of free reed instruments which permit numerical simulations. Several definitions for the section by which the airflow passes through the reed are reviewed and a new one is proposed which takes into account the entire escape area under the reed and the reed thickness. To derive this section, it is necessary to distinguish the neutral section (the only section of the reed which always keeps its length constant while moving) from the upstream or downstream sections. A minimal configuration is chosen to permit the instabilities of", "orig": "both (-,+) and (+,-) reeds on the basis of a linear analysis of instabilities conditions. This configuration is used to illustrate, with temporal simulations, the minimal model for both kinds of reeds and to discuss the model assumptions. Some clues are given about the influence, on the playing frequency and on the dynamic of the sound, of two main parameters of the geometrical model: the size of the volume and the level of the excitation. It is shown that the playing frequency of a (+,-) reed can vary in a large range according to the size of the volume upstream of the reed; that the playing", "generated": "the system to be highlighted without unnecessary complexity. The proposed model is based on a set of ordinary differential equations (ODEs) that describe the motion of the reed and the airflow through it. The ODEs are derived from the fundamental laws of mechanics and fluid dynamics, and they take into account the nonlinear interactions between the reed and the airflow. The model is validated by comparing its predictions with experimental measurements obtained on a simple free reed instrument. The results show that the model is able to capture the main features of the reed's behavior, including the self-sustained oscillations and the nonlinear regimes. The proposed model"}
{"full_text": "It is generally taken for granted that our Universe is free of antimatter objects and domains. This certitude has recently been challenged by the possible detection of anti-helium nuclei by AMS-02. Should the observation be confirmed, the existence of nearby antistars would make a plausible hypothesis to explain the origin of the antinuclei. In this paper we use the 10-years Fermi Large Area Telescope (LAT) gamma-ray source catalog to set constraints on the abundance of antistars around the Sun. We identify in the catalog 14 antistar candidates not associated with any objects belonging to established gamma-ray source classes and with a spectrum compatible with baryon-antibaryon annihilation. We use them along with an estimate of the LAT sensitivity to antistars to set upper limits on the local antistar fraction $f_{\\bar{\\ast}}$ with respect to normal stars. We provide parametric limits as a function of the closest antistar mass, velocity, and surrounding matter density. We also employ a novel Monte~Carlo method to set limits for a few hypotheses about the antistar population. For a population with properties equivalent to those of regular stars concentrated in the Galactic disk we obtain $f_{\\bar{\\ast}} < 2.5 \\times 10^{-6}$ at 95\\% confidence level, which is 20 times more constraining than limits previously available. For a primordial population of antistars distributed in the Galactic halo we obtain new local upper limits which decrease as a function of antistar mass $M$ from $f_{\\bar{\\ast}} < 0.2$ at 95\\% confidence level for $M = 1 \\; M_\\odot$ to $f_{\\bar{\\ast}} < 1.6 \\times 10^{-4}$ at 95\\% confidence level for $M = 10 \\; M_\\odot$. By combining these limits with existing microlensing constraints for lighter objects in the Magellanic clouds, we infer that a primordial population of halo antistars must have a density lower than $\\mathcal{O}(10^{-5}\\;\\text{pc}^{-3})$ to $\\mathcal{O}(10^{-2}\\;\\text{pc}^{-3})$ depending on their masses. Our limits can constrain models for the origin and propagation of antinuclei in cosmic rays.", "prompt": "It is generally taken for granted that our Universe is free of antimatter objects and domains. This certitude has recently been challenged by the possible detection of anti-helium nuclei by AMS-02. Should the observation be confirmed, the existence of nearby antistars would make a plausible hypothesis to explain the origin of the antinuclei. In this paper we use the 10-years Fermi Large Area Telescope (LAT) gamma-ray source catalog to set constraints on the abundance of antistars around the Sun. We identify in the catalog 14 antistar candidates not associated with any objects belonging to established gamma-ray source classes and with", "orig": "a spectrum compatible with baryon-antibaryon annihilation. We use them along with an estimate of the LAT sensitivity to antistars to set upper limits on the local antistar fraction $f_{\\bar{\\ast}}$ with respect to normal stars. We provide parametric limits as a function of the closest antistar mass, velocity, and surrounding matter density. We also employ a novel Monte~Carlo method to set limits for a few hypotheses about the antistar population. For a population with properties equivalent to those of regular stars concentrated in the Galactic disk we obtain $f_{\\bar{\\ast}} <", "generated": "characteristics that cannot be easily explained by known astrophysical processes.\n\nThese antistar candidates are distributed isotropically across the sky, suggesting that they may be part of a larger population of antistars in the Milky Way galaxy. We perform a thorough analysis of the gamma-ray emission spectra of these sources, searching for signs of antimatter annihilation, which would be a distinctive signature of antistars. Our results indicate that none of the candidates exhibit the expected gamma-ray spectral features associated with antimatter annihilation, which could imply that the antistars are either relatively distant or have a different composition than expected. Alternatively, it is possible"}
{"full_text": "User-defined syntax extensions are useful to implement an embedded domain specific language (EDSL) with good code-readability. They allow EDSL authors to define domain-natural notation, which is often different from the host language syntax. Nowadays, there are several research works of powerful user-defined syntax extensions. One promising approach uses user-defined operators. A user-defined operator is a function with user-defined syntax. It can be regarded as a syntax extension implemented without macros. An advantage of user-defined operators is that an operator can be statically typed. The compiler can find type errors in the definition of an operator before the operator is used. In addition, the compiler can resolve syntactic ambiguities by using static types. However, user-defined operators are difficult to implement language constructs involving static name binding. Name binding is association between names and values (or memory locations). Our inquiry is whether we can design a system for user-defined operators involving a new custom name binding. This paper proposes a module system for user-defined operators named a dsl class. A dsl class is similar to a normal class in Java but it contains operators instead of methods. We use operators for implementing custom name binding. For example, we use a nullary operator for emulating a variable name. An instance of a dsl class, called a dsl object, reifies an environment that expresses name binding. Programmers can control a scope of instance operators by specifying where the dsl object is active. We extend the host type system so that it can express the activation of a dsl object. In our system, a bound name is propagated through a type parameter to a dsl object. This enables us to implement user-defined language constructs involving static name binding. A contribution of this paper is that we reveal we can integrate a system for managing names and their scopes with a module and type system of an object-oriented language like Java. This allows us to implement a proposed system by adopting eager disambiguation based on expected types so that the compilation time will be acceptable. Eager disambiguation, which prunes out semantically invalid abstract parsing trees (ASTs) while a parser is running, is needed because the parser may generate a huge number of potentially valid ASTs for the same source code. We have implemented ProteaJ2, which is a programming language based on Java and it supports our proposal. We describe a parsing method that adopts eager disambiguation for fast parsing and discuss its time complexity. To show the practicality of our proposal, we have conducted two micro benchmarks to see the performance of our compiler. We also show several use cases of dsl classes for demonstrating dsl classes can express various language constructs. Our ultimate goal is to let programmers add any kind of new language construct to a host language. To do this, programmers should be able to define new syntax, name binding, and type system within the host language. This paper shows programmers can define the former two: their own syntax and name binding.", "prompt": "User-defined syntax extensions are useful to implement an embedded domain specific language (EDSL) with good code-readability. They allow EDSL authors to define domain-natural notation, which is often different from the host language syntax. Nowadays, there are several research works of powerful user-defined syntax extensions. One promising approach uses user-defined operators. A user-defined operator is a function with user-defined syntax. It can be regarded as a syntax extension implemented without macros. An advantage of user-defined operators is that an operator can be statically typed. The compiler can find type errors in the definition of an operator before the operator is used. In addition, the", "orig": "compiler can resolve syntactic ambiguities by using static types. However, user-defined operators are difficult to implement language constructs involving static name binding. Name binding is association between names and values (or memory locations). Our inquiry is whether we can design a system for user-defined operators involving a new custom name binding. This paper proposes a module system for user-defined operators named a dsl class. A dsl class is similar to a normal class in Java but it contains operators instead of methods. We use operators for implementing custom name binding. For example, we use a nullary operator for emulating a variable name. An instance of a dsl", "generated": "generated code of an operator can be optimized by the compiler, which leads to efficient execution.\n\nFurthermore, user-defined operators can improve the expressiveness of a programming language, making it more suitable for a specific domain. For instance, in a language for scientific computing, a user-defined operator can be defined for matrix multiplication, allowing developers to write more concise and readable code. This is particularly important in domains where complex calculations are prevalent, as it reduces the likelihood of errors and improves code maintainability. Moreover, the use of user-defined operators can facilitate the development of domain-specific libraries, which can be easily integrated into existing programming languages.\n\nAnother significant benefit"}
{"full_text": "In recent years genetic algorithms have emerged as a useful tool for the heuristic solution of complex discrete optimisation problems. In particular there has been considerable interest in their use in tackling problems arising in the areas of scheduling and timetabling. However, the classical genetic algorithm paradigm is not well equipped to handle constraints and successful implementations usually require some sort of modification to enable the search to exploit problem specific knowledge in order to overcome this shortcoming. This paper is concerned with the development of a family of genetic algorithms for the solution of a nurse rostering problem at a major UK hospital. The hospital is made up of wards of up to 30 nurses. Each ward has its own group of nurses whose shifts have to be scheduled on a weekly basis. In addition to fulfilling the minimum demand for staff over three daily shifts, nurses' wishes and qualifications have to be taken into account. The schedules must also be seen to be fair, in that unpopular shifts have to be spread evenly amongst all nurses, and other restrictions, such as team nursing and special conditions for senior staff, have to be satisfied. The basis of the family of genetic algorithms is a classical genetic algorithm consisting of n-point crossover, single-bit mutation and a rank-based selection. The solution space consists of all schedules in which each nurse works the required number of shifts, but the remaining constraints, both hard and soft, are relaxed and penalised in the fitness function. The talk will start with a detailed description of the problem and the initial implementation and will go on to highlight the shortcomings of such an approach, in terms of the key element of balancing feasibility, i.e. covering the demand and work regulations, and quality, as measured by the nurses' preferences. A series of experiments involving parameter adaptation, niching, intelligent weights, delta coding, local hill climbing, migration and special selection rules will then be outlined and it will be shown how a series of these enhancements were able to eradicate these difficulties. Results based on several months' real data will be used to measure the impact of each modification, and to show that the final algorithm is able to compete with a tabu search approach currently employed at the hospital. The talk will conclude with some observations as to the overall quality of this approach to this and similar problems.", "prompt": "In recent years genetic algorithms have emerged as a useful tool for the heuristic solution of complex discrete optimisation problems. In particular there has been considerable interest in their use in tackling problems arising in the areas of scheduling and timetabling. However, the classical genetic algorithm paradigm is not well equipped to handle constraints and successful implementations usually require some sort of modification to enable the search to exploit problem specific knowledge in order to overcome this shortcoming. This paper is concerned with the development of a family of genetic algorithms for the solution of a nurse rostering problem at a major UK hospital. The hospital is made up of wards of up to 30 nurses.", "orig": "Each ward has its own group of nurses whose shifts have to be scheduled on a weekly basis. In addition to fulfilling the minimum demand for staff over three daily shifts, nurses' wishes and qualifications have to be taken into account. The schedules must also be seen to be fair, in that unpopular shifts have to be spread evenly amongst all nurses, and other restrictions, such as team nursing and special conditions for senior staff, have to be satisfied. The basis of the family of genetic algorithms is a classical genetic algorithm consisting of n-point crossover, single-bit mutation and a rank-based selection. The solution space consists of all schedules in which each nurse works", "generated": "The nurse rostering problem is a classic example of a complex discrete optimisation problem, where the goal is to assign available nurses to shifts in a way that meets the hospital's staffing requirements while also considering the preferences and constraints of the individual nurses. The problem is further complicated by the need to ensure that each ward has an adequate number of nurses with the necessary skills and qualifications to provide high-quality patient care. Traditional methods for solving this problem, such as manual rostering or simple heuristics, often result in suboptimal solutions that fail to make efficient use of the available nurse resources. \n\nIn this paper, we describe the development"}
{"full_text": "Do our physics curricula provide the appropriate data management competences in a world where data are considered a crucial resource and substantial funding is available for building a national research data infrastructure (German: Nationale Forschungsdateninfrastuktur = NFDI)? Although basic data evaluation and systematic documentation are practiced when students first come into contact with data, particularly in experimental physics lab courses, they do not meet the increasing demands of research and professional practice to deal with the analysis of huge datasets. In many cases, the problem starts with the fact that there is no consensus on a suitable entry-level programming language. At the Department of Physics at Friedrich-Alexander-Universit\\\"at Erlangen-N\\\"urnberg, we have made minor adjustments to the physics curriculum in recent years, which we present in this article. We placed data management competences early in the bachelor curriculum, which has resulted in considerable advantages throughout the further course of studies. The authors feel that students are quickly moving into the fast lane in data management; we can already see in our research groups that they are becoming a driving force towards modern research data management. -- Vermitteln unsere Physikcurricula die passenden Datenverarbeitungskompetenzen in einer Welt, in der Daten als entscheidende Ressource betrachtet werden und erhebliche F\\\"ordermittel f\\\"ur eine Nationale Forschungsdateninfrastruktur (NFDI) bereitstehen? Beim Erstkontakt mit Daten, also insbesondere in den Praktika der Experimentalphysik, werden zwar elementare Datenevaluation und systematische Dokumentation einge\\\"ubt, diese gen\\\"ugen aber nicht den steigenden Anspr\\\"uchen der Forschung und der Berufspraxis, sich zunehmend mit der Analyse gro{\\ss}er Datenmengen zu befassen. Es scheitert oft schon an einem Konsens \\\"uber eine geeignete Einstiegsprogrammiersprache. Am Department Physik der Friedrich-Alexander-Universit\\\"at Erlangen-N\\\"urnberg haben wir in den letzten Jahren kleinere Anpassungen im Physikcurriculum vorgenommen, die wir in diesem Artikel vorstellen. Datenkompetenz wurde fr\\\"uh im Bachelorstudium platziert, woraus sich erhebliche Vorteile f\\\"ur den weiteren Studienverlauf ergeben haben. Die Autoren k\\\"onnen sich des Eindrucks nicht erwehren, dass die Studierenden in puncto Datenkompetenz schnell auf die \\\"Uberholspur gehen; wir sehen bereits jetzt in unseren Arbeitsgruppen, dass sie sich als treibende Kr\\\"afte hin zu einem modernen Forschungsdatenmanagement erweisen.", "prompt": "Do our physics curricula provide the appropriate data management competences in a world where data are considered a crucial resource and substantial funding is available for building a national research data infrastructure (German: Nationale Forschungsdateninfrastuktur = NFDI)? Although basic data evaluation and systematic documentation are practiced when students first come into contact with data, particularly in experimental physics lab courses, they do not meet the increasing demands of research and professional practice to deal with the analysis of huge datasets. In many cases, the problem starts with the fact that there is no consensus on a suitable entry-level programming language. At the Department of Physics at Friedrich", "orig": "-Alexander-Universit\\\"at Erlangen-N\\\"urnberg, we have made minor adjustments to the physics curriculum in recent years, which we present in this article. We placed data management competences early in the bachelor curriculum, which has resulted in considerable advantages throughout the further course of studies. The authors feel that students are quickly moving into the fast lane in data management; we can already see in our research groups that they are becoming a driving force towards modern research data management. -- Vermitteln unsere Physikcurricula die passenden Datenverarbeitungskompetenzen in einer Welt, in der Daten als entscheid", "generated": "Schiller University Jena, we aim to tackle this issue by introducing a comprehensive data science curriculum for physics students. This curriculum will not only focus on teaching programming languages such as Python, R, or MATLAB, but also emphasize the importance of data visualization, data mining, and machine learning techniques. We believe that by integrating data science into the physics curriculum, we can provide students with the necessary skills to handle and analyze large datasets, making them more competitive in the job market and better equipped to contribute to cutting-edge research projects.\n\nFurthermore, we plan to establish collaborations with industry partners and research institutions to provide students with real-world data analysis projects"}
{"full_text": "The ejecta of the first probably very massive stars polluted the Big Bang primordial element mix with the first heavier elements. The resulting ultra metal-poor abundance distribution provided the initial conditions for the second stars of a wide range of initial masses reaching down to intermediate and low masses. The importance of these second stars for understanding the origin of the elements in the early universe are manifold. While the massive first stars have long vanished the second stars are still around and currently observed. They are the carriers of the information about the first stars, but they are also capable of nuclear production themselves. For example, in order to use ultra or extremely metal-poor stars as a probe for the r-process in the early universe a reliable model of the s-process in the second stars is needed. Eventually, the second stars may provide us with important clues on questions ranging from structure formation to how the stars actually make the elements, not only in the early but also in the present universe. In particular the C-rich extremely metal-poor stars, most of which show the s-process signature, are thought to be associated with chemical yields from the evolved giant phase of intermediate mass stars. Models of such AGB stars at extremely low metallicity now exist, and comparison with observation show important discrepancies, for example with regard to the synthesis of nitrogen. This may hint at burning and mixing aspects of extremely metal-poor evolved stars that are not yet included in the standard picture of evolution, as for example the hydrogen-ingestion flash. The second stars of intermediate mass may have also played an important role in the formation of heavy elements that form through slow neutron capture reaction chains (s-process). Comparison of models with observations reveal which aspects of the physics input and assumptions need to be improved. The s-process is a particularly useful diagnostic tool for probing the physical processes that are responsible for the creation of elements in stars, like for example rotation. As new observational techniques and strategies continue to penetrate the field, for example the multi-object spectroscopy, or the future spectroscopic surveys, the extremely metal-poor stars will play an increasingly important role to address some of the most fundamental and challenging, current questions of astronomy.", "prompt": "The ejecta of the first probably very massive stars polluted the Big Bang primordial element mix with the first heavier elements. The resulting ultra metal-poor abundance distribution provided the initial conditions for the second stars of a wide range of initial masses reaching down to intermediate and low masses. The importance of these second stars for understanding the origin of the elements in the early universe are manifold. While the massive first stars have long vanished the second stars are still around and currently observed. They are the carriers of the information about the first stars, but they are also capable of nuclear production themselves. For example, in order to use ultra or extremely metal-poor", "orig": "stars as a probe for the r-process in the early universe a reliable model of the s-process in the second stars is needed. Eventually, the second stars may provide us with important clues on questions ranging from structure formation to how the stars actually make the elements, not only in the early but also in the present universe. In particular the C-rich extremely metal-poor stars, most of which show the s-process signature, are thought to be associated with chemical yields from the evolved giant phase of intermediate mass stars. Models of such AGB stars at extremely low metallicity now exist, and comparison with observation show important discrepancies, for example with regard", "generated": "stars as probes of the earliest nucleosynthesis, it is essential to understand their own nucleosynthetic contributions to the observed abundance patterns.\n\nThese second-generation stars are thought to have played a crucial role in shaping the chemical evolution of the universe, particularly in the early stages of galaxy formation. By studying the elemental abundances in these ancient stars, astronomers can gain valuable insights into the nucleosynthetic processes that occurred in the first stars, as well as the subsequent evolution of the interstellar medium. Furthermore, the second stars are also believed to have contributed to the reionization of the universe, a process that marked a significant milestone in"}
{"full_text": "Let $\\mathbf{f} = \\left(f_1, \\dots, f_p\\right) $ be a polynomial tuple in $\\mathbb{Q}[z_1, \\dots, z_n]$ and let $d = \\max_{1 \\leq i \\leq p} \\deg f_i$. We consider the problem of computing the set of asymptotic critical values of the polynomial mapping, with the assumption that this mapping is dominant, $\\mathbf{f}: z \\in \\mathbb{K}^n \\to (f\\_1(z), \\dots, f\\_p(z)) \\in \\mathbb{K}^p$ where $\\mathbb{K}$ is either $\\mathbb{R}$ or $\\mathbb{C}$. This is the set of values $c$ in the target space of $\\mathbf{f}$ such that there exists a sequence of points $(\\mathbf{x}_i)_{i\\in \\mathbb{N}}$ for which $\\mathbf{f}(\\mathbf{x}_i)$ tends to $c$ and $\\|\\mathbf{x}_i\\| \\kappa {\\rm d} \\mathbf{f}(\\mathbf{x}_i))$ tends to $0$ when $i$ tends to infinity where ${\\rm d} \\mathbf{f}$ is the differential of $\\mathbf{f}$ and $\\kappa$ is a function measuring the distance of a linear operator to the set of singular linear operators from $\\mathbb{K}^n$ to $\\mathbb{K}^p$. Computing the union of the classical and asymptotic critical values allows one to put into practice generalisations of Ehresmann's fibration theorem. This leads to natural and efficient applications in polynomial optimisation and computational real algebraic geometry. Going back to previous works by Kurdyka, Orro and Simon, we design new algorithms to compute asymptotic critical values. Through randomisation, we introduce new geometric characterisations of asymptotic critical values. This allows us to dramatically reduce the complexity of computing such values to a cost that is essentially $O(d^{2n(p+1)})$ arithmetic operations in $\\mathbb{Q}$. We also obtain tighter degree bounds on a hypersurface containing the asymptotic critical values, showing that the degree is at most $p^{n-p+1}(d-1)^{n-p}(d+1)^{p}$. Next, we show how to apply these algorithms to unconstrained polynomial optimisation problems and the problem of computing sample points per connected component of a semi-algebraic set defined by a single inequality/inequation. We report on the practical capabilities of our implementation of this algorithm. It shows how the practical efficiency surpasses the current state-of-the-art algorithms for computing asymptotic critical values by tackling examples that were previously out of reach.", "prompt": "Let $\\mathbf{f} = \\left(f_1, \\dots, f_p\\right) $ be a polynomial tuple in $\\mathbb{Q}[z_1, \\dots, z_n]$ and let $d = \\max_{1 \\leq i \\leq p} \\deg f_i$. We consider the problem of computing the set of asymptotic critical values of the polynomial mapping, with the assumption that this mapping is dominant, $\\mathbf{f}: z \\in \\mathbb{K}^n \\to (f\\_1(z), \\dots, f\\_p(z)) \\", "orig": "in \\mathbb{K}^p$ where $\\mathbb{K}$ is either $\\mathbb{R}$ or $\\mathbb{C}$. This is the set of values $c$ in the target space of $\\mathbf{f}$ such that there exists a sequence of points $(\\mathbf{x}_i)_{i\\in \\mathbb{N}}$ for which $\\mathbf{f}(\\mathbf{x}_i)$ tends to $c$ and $\\|\\mathbf{x}_i\\| \\kappa {\\rm d} \\mathbf{f}(\\mathbf{x}_", "generated": "in \\mathbb{K}^p$.\n\nTo tackle this problem, we can employ the theory of Gr\u00f6bner bases and the concept of discriminants. Specifically, we can construct a Gr\u00f6bner basis for the ideal generated by the polynomial tuple $\\mathbf{f}$, and then use the discriminant of this ideal to determine the asymptotic critical values. The discriminant is a polynomial that encodes information about the critical points of the polynomial mapping, and its roots correspond to the asymptotic critical values we are seeking.\n\nOne approach to computing the asymptotic critical values is to use the Jacobi criterion, which"}
{"full_text": "Welcome to the proceedings of FOCLASA 2012, the 11th International Workshop on the Foundations of Coordination Languages and Self-Adaptation. FOCLASA 2012 was held in Newcastle upon Tyne, UK, on September 8, 2012 as a satellite event of CONCUR 2012, the 23rd International Conference on Concurrency Theory. The workshop provides a venue where researchers and practitioners could meet, exchange ideas, identify common problems, determine some of the key and fundamental issues related to coordination languages and self adaptation, and explore together and disseminate solutions. Indeed, a number of hot research topics are currently sharing the common problem of combining concurrent, distributed, mobile and heterogeneous components, trying to harness the intrinsic complexity of the resulting systems. Computation nowadays is becoming inherently concurrent, either because of characteristics of the hardware (with multicore processors becoming omnipresent) or due to the ubiquitous presence of distributed systems (incarnated in the Internet). Computational systems are therefore typically distributed, concurrent, mobile, and often involve composition of heterogeneous components. To specify and reason about such systems and go beyond the functional correctness proofs, e.g., by supporting reusability and improving maintainability, approaches such as coordination languages and self adaptation are recognised as fundamental. This year, we received 13 submissions involving 35 authors from 10 different countries. Papers underwent a rigorous review process, and all accepted papers received 3 review reports. After the review process, the international Program Committee of FOCLASA 2012 decided to select 8 papers for presentation during the workshop and inclusion in these proceedings. These papers tackle different issues that are currently central to our community, self-adaptation and coordination, processes and coordination, and type systems. The workshop features an invited talk by Sebastian Uchitel from Imperial College London (UK).", "prompt": "Welcome to the proceedings of FOCLASA 2012, the 11th International Workshop on the Foundations of Coordination Languages and Self-Adaptation. FOCLASA 2012 was held in Newcastle upon Tyne, UK, on September 8, 2012 as a satellite event of CONCUR 2012, the 23rd International Conference on Concurrency Theory. The workshop provides a venue where researchers and practitioners could meet, exchange ideas, identify common problems, determine some of the key and fundamental issues related to coordination languages and self adaptation, and explore together and disseminate solutions. Indeed, a number of hot research", "orig": "topics are currently sharing the common problem of combining concurrent, distributed, mobile and heterogeneous components, trying to harness the intrinsic complexity of the resulting systems. Computation nowadays is becoming inherently concurrent, either because of characteristics of the hardware (with multicore processors becoming omnipresent) or due to the ubiquitous presence of distributed systems (incarnated in the Internet). Computational systems are therefore typically distributed, concurrent, mobile, and often involve composition of heterogeneous components. To specify and reason about such systems and go beyond the functional correctness proofs, e.g., by supporting reusability and improving maintainability, approaches such as coordination languages and self adaptation are recognised as", "generated": "topics were discussed during the workshop, including the design and implementation of self-adaptive systems, the integration of coordination languages with other programming paradigms, and the application of formal methods to ensure the correctness and reliability of self-adaptive systems.\n\nThe workshop featured a keynote talk by Professor Carlo Ghezzi, who presented a comprehensive overview of the current state of the art in self-adaptive systems. The talk sparked a lively discussion among the participants, who debated the challenges and opportunities of developing self-adaptive systems that can operate effectively in complex and dynamic environments. The workshop also included a technical program consisting of 12 research papers, which were"}
{"full_text": "We propose an automatic video inpainting algorithm which relies on the optimisation of a global, patch-based functional. Our algorithm is able to deal with a variety of challenging situations which naturally arise in video inpainting, such as the correct reconstruction of dynamic textures, multiple moving objects and moving background. Furthermore, we achieve this in an order of magnitude less execution time with respect to the state-of-the-art. We are also able to achieve good quality results on high definition videos. Finally, we provide specific algorithmic details to make implementation of our algorithm as easy as possible. The resulting algorithm requires no segmentation or manual input other than the definition of the inpainting mask, and can deal with a wider variety of situations than is handled by previous work. 1. Introduction. Advanced image and video editing techniques are increasingly common in the image processing and computer vision world, and are also starting to be used in media entertainment. One common and difficult task closely linked to the world of video editing is image and video \" inpainting \". Generally speaking, this is the task of replacing the content of an image or video with some other content which is visually pleasing. This subject has been extensively studied in the case of images, to such an extent that commercial image inpainting products destined for the general public are available, such as Photoshop's \" Content Aware fill \" [1]. However, while some impressive results have been obtained in the case of videos, the subject has been studied far less extensively than image inpainting. This relative lack of research can largely be attributed to high time complexity due to the added temporal dimension. Indeed, it has only very recently become possible to produce good quality inpainting results on high definition videos, and this only in a semi-automatic manner. Nevertheless, high-quality video inpainting has many important and useful applications such as film restoration, professional post-production in cinema and video editing for personal use. For this reason, we believe that an automatic, generic video inpainting algorithm would be extremely useful for both academic and professional communities.", "prompt": "We propose an automatic video inpainting algorithm which relies on the optimisation of a global, patch-based functional. Our algorithm is able to deal with a variety of challenging situations which naturally arise in video inpainting, such as the correct reconstruction of dynamic textures, multiple moving objects and moving background. Furthermore, we achieve this in an order of magnitude less execution time with respect to the state-of-the-art. We are also able to achieve good quality results on high definition videos. Finally, we provide specific algorithmic details to make implementation of our algorithm as easy as possible. The resulting algorithm requires no segmentation or manual input other than the definition of the", "orig": "inpainting mask, and can deal with a wider variety of situations than is handled by previous work. 1. Introduction. Advanced image and video editing techniques are increasingly common in the image processing and computer vision world, and are also starting to be used in media entertainment. One common and difficult task closely linked to the world of video editing is image and video \" inpainting \". Generally speaking, this is the task of replacing the content of an image or video with some other content which is visually pleasing. This subject has been extensively studied in the case of images, to such an extent that commercial image inpainting products destined for the general public are", "generated": "region to be inpainted.\n\nThis approach is particularly useful in various applications such as video editing, film restoration, and video surveillance. For instance, in video editing, our algorithm can be used to remove unwanted objects or wires from a scene, allowing for a more seamless and professional-looking final product. In film restoration, our algorithm can be used to restore damaged or corrupted footage, preserving the original integrity of the film. Additionally, in video surveillance, our algorithm can be used to remove privacy- infringing information, such as license plates or faces, from recorded footage.\n\nThe key to our algorithm's success lies in its ability to effectively handle complex"}
{"full_text": "Mammography is a specific type of breast imaging that uses low-dose X-rays to detect cancer in early stage. During the exam, the women breast is compressed between two plates until a nearly uniform breast thickness is obtained. This technique improves image quality and reduces dose but can also be the source of discomfort and sometimes pain for the patient. Therefore, alternative techniques allowing reduced breast compression is of potential interest. The aim of this work is to develop a 3D biomechanical Finite Element (FE) breast model in order to analyze various breast compression strategies and their impact on image quality and radiation dose. Large breast deformations are simulated using this FE model with ANSYS software. A particular attention is granted to the computation of the residual stress in the model due to gravity and boundary conditions (thorax anatomy, position of the patient inside the MRI machine). Previously developed biomechanical breast models use a simplified breast anatomy by modeling adipose and fibroglandular tissues only (Rajagopal et al. in Wiley Interdiscip Rev: Syst Biol Med 2:293--304, 2010). However, breast reconstruction surgery has proven the importance of suspensory ligaments and breast fasciae on breast mechanics (Lockwood in Plast Reconstr Surg 103:1411--1420, 1999). We are therefore consider using a more realistic breast anatomy by including skin, muscles, and suspensory ligaments. The breast tissues are modeled as neo-Hookean materials. A physical correct modeling of the breast requires the knowledge of the stress-free breast configuration. Here, this undeformed shape (i.e., without any residual stress) is computed using the prediction--correction iterative scheme proposed by Eiben et al. (Ann of Biomed Eng 44:154--173, 2016). The unloading procedure uses the breast configuration in prone and supine position in order to find a unique displacement vector field induced by gravitational forces. The 3D breast geometry is reconstructed from MRI images that are segmented (Yushkevich et al. in Neuroimage 31:1116--1128, 2006) to differentiate the four main tissue types. The breast volume is discretized with a hexa-dominant FE meshing tool as a unique volume. Finally, the model is evaluated by comparing the estimated breast deformations under gravity load with the experimental ones measured in three body positions: prone, supine, and oblique supine.", "prompt": "Mammography is a specific type of breast imaging that uses low-dose X-rays to detect cancer in early stage. During the exam, the women breast is compressed between two plates until a nearly uniform breast thickness is obtained. This technique improves image quality and reduces dose but can also be the source of discomfort and sometimes pain for the patient. Therefore, alternative techniques allowing reduced breast compression is of potential interest. The aim of this work is to develop a 3D biomechanical Finite Element (FE) breast model in order to analyze various breast compression strategies and their impact on image quality and radiation dose. Large breast deformations are simulated using", "orig": "this FE model with ANSYS software. A particular attention is granted to the computation of the residual stress in the model due to gravity and boundary conditions (thorax anatomy, position of the patient inside the MRI machine). Previously developed biomechanical breast models use a simplified breast anatomy by modeling adipose and fibroglandular tissues only (Rajagopal et al. in Wiley Interdiscip Rev: Syst Biol Med 2:293--304, 2010). However, breast reconstruction surgery has proven the importance of suspensory ligaments and breast fasciae on breast mechanics (Lockwood in Plast Reconstr Surg", "generated": "nonlinear finite element methods, taking into account the heterogeneity of breast tissue and the nonlinear behavior of breast tissue under compression.\n\nThe FE model is developed from MRI scans of volunteers, which provide detailed information on breast anatomy and tissue properties. The model is then validated against experimental data from compression tests on breast phantoms, ensuring that the simulated deformations accurately reproduce the behavior of real breast tissue. With the validated model, various compression strategies can be simulated and compared, including different plate geometries, compression forces, and breast support systems. The impact of each strategy on image quality and radiation dose is assessed using virtual mammography simulations, which mimic the"}
{"full_text": "In string musical instruments, the sound is radiated by the soundboard, subject to the strings excitation. This vibration of this rather complex structure is described here with models which need only a small number of parameters. Predictions of the models are compared with results of experiments that have been presented in Ege et al. [Vibroacoustics of the piano soundboard: (Non)linearity and modal properties in the low- and mid- frequency ranges, Journal of Sound and Vibration 332 (5) (2013) 1288-1305]. The apparent modal density of the soundboard of an upright piano in playing condition, as seen from various points of the structure, exhibits two well-separated regimes, below and above a frequency flim that is determined by the wood characteristics and by the distance between ribs. Above flim, most modes appear to be localised, presumably due to the irregularity of the spacing and height of the ribs. The low-frequency regime is predicted by a model which consists of coupled sub-structures: the two ribbed areas split by the main bridge and, in most cases, one or two so-called cut-off corners. In order to assess the dynamical properties of each of the subplates (considered here as homogeneous plates), we propose a derivation of the (low-frequency) modal density of an orthotropic homogeneous plate which accounts for the boundary conditions on an arbitrary geometry. Above flim, the soundboard, as seen from a given excitation point, is modelled as a set of three structural wave-guides, namely the three inter-rib spacings surrounding the excitation point. Based on these low- and high-frequency models, computations of the point-mobility and of the apparent modal densities seen at several excitation points match published measurements. The dispersion curve of the wave-guide model displays an acoustical radiation scheme which differs significantly from that of a thin homogeneous plate. It appears that piano dimensioning is such that the subsonic regime of acoustical radiation extends over a much wider frequency range than it would be for a homogeneous plate with the same low-frequency vibration. One problem in piano manufacturing is examined in relationship with the possible radiation schemes induced by the models.", "prompt": "In string musical instruments, the sound is radiated by the soundboard, subject to the strings excitation. This vibration of this rather complex structure is described here with models which need only a small number of parameters. Predictions of the models are compared with results of experiments that have been presented in Ege et al. [Vibroacoustics of the piano soundboard: (Non)linearity and modal properties in the low- and mid- frequency ranges, Journal of Sound and Vibration 332 (5) (2013) 1288-1305]. The apparent modal density of the soundboard of an upright", "orig": "piano in playing condition, as seen from various points of the structure, exhibits two well-separated regimes, below and above a frequency flim that is determined by the wood characteristics and by the distance between ribs. Above flim, most modes appear to be localised, presumably due to the irregularity of the spacing and height of the ribs. The low-frequency regime is predicted by a model which consists of coupled sub-structures: the two ribbed areas split by the main bridge and, in most cases, one or two so-called cut-off corners. In order to assess the dynamical properties of each of the subplates (considered here", "generated": "piano is examined, and the findings suggest that the soundboard's vibrational behavior can be accurately predicted using a limited number of parameters.\n\nFurthermore, the study reveals that the soundboard's modal density exhibits a nonlinear relationship with frequency, with a significant increase in density observed in the low-frequency range. This nonlinearity is attributed to the soundboard's complex geometry and the presence of ribs, which introduce local modes of vibration. The results of the experimental study are compared with finite element simulations, which demonstrate good agreement with the measured data.\n\nThe implications of these findings are significant for the design and optimization of string musical instruments. By understanding the vibr"}
{"full_text": "Of the four giant planets in the Solar system, only Jupiter and Neptune are currently known to possess swarms of Trojan asteroids - small objects that experience a 1:1 mean motion resonance with their host planet. In Lykawka et al. (2009), we performed extensive dynamical simulations, including planetary migration, to investigate the origin of the Neptunian Trojan population. Utilising the vast amount of simulation data obtained for that work, together with fresh results from new simulations, we here investigate the dynamical capture of Trojans by all four giant planets from a primordial trans-Neptunian disk. We find the likelihood of a given planetesimal from this region being captured onto an orbit within Jupiter's Trojan cloud lies between several times 10^-6 and 10^-5. For Saturn, the probability is found to be in the range <10^-6 to 10^-5, whilst for Uranus the probabilities range between 10^-5 and 10^-4. Finally, Neptune displays the greatest probability of Trojan capture, with values ranging between 10^-4 and 10^-3. Our results suggest that all four giant planets are able to capture and retain a significant population of Trojan objects from the disk by the end of planetary migration. As a result of encounters with the giant planets prior to Trojan capture, these objects tend to be captured on orbits that are spread over a wide range of orbital eccentricities and inclinations. The bulk of captured objects are to some extent dynamically unstable, and therefore the populations of these objects tend to decay over the age of the Solar System, providing an important ongoing source of new objects moving on dynamically unstable orbits among the giant planets. Given that a huge population of objects would be displaced by Neptune's outward migration (with a potential cumulative mass a number of times that of the Earth), we conclude that the surviving remnant of the Trojans captured during the migration of the outer planets might be sufficient to explain the currently known Trojan populations in the outer Solar system.", "prompt": "Of the four giant planets in the Solar system, only Jupiter and Neptune are currently known to possess swarms of Trojan asteroids - small objects that experience a 1:1 mean motion resonance with their host planet. In Lykawka et al. (2009), we performed extensive dynamical simulations, including planetary migration, to investigate the origin of the Neptunian Trojan population. Utilising the vast amount of simulation data obtained for that work, together with fresh results from new simulations, we here investigate the dynamical capture of Trojans by all four giant planets from a primordial trans-Neptunian disk. We", "orig": "find the likelihood of a given planetesimal from this region being captured onto an orbit within Jupiter's Trojan cloud lies between several times 10^-6 and 10^-5. For Saturn, the probability is found to be in the range <10^-6 to 10^-5, whilst for Uranus the probabilities range between 10^-5 and 10^-4. Finally, Neptune displays the greatest probability of Trojan capture, with values ranging between 10^-4 and 10^-3. Our results suggest that all four giant planets are able to capture and retain a significant population of Trojan objects from the disk by the end of planetary", "generated": "explore the efficiency of Trojan capture for each giant planet, examining the effects of planetary mass, orbital eccentricity, and the architecture of the early Solar system on the capture process.\n\nOur results indicate that the capture of Trojans by Jupiter is highly efficient, with up to 50% of particles in the primordial disk being trapped in stable Trojan orbits. This is consistent with the observed high population of Jupiter Trojans. In contrast, the capture of Trojans by Neptune is significantly less efficient, with only around 5% of particles being trapped. This suggests that Neptune's Trojan population may be influenced by additional factors, such as"}
{"full_text": "Data is a dominant force during the decision-making process. It can help determine which roads to expand and the optimal location for a grocery store. Data can also be used to influence which schools to open or to shutter and \"appropriate\" city services to continue or discontinue. Considered fact-based, objective, and impartial, data can trump emotional appeals during the final evaluation of a project; thus creating a power imbalance between those with the resources to access data and those without. Most often left behind are communities already struggling to stay afloat due to years of disinvestment by market forces and external decision-makers. For long ignored residents in Atlanta's Westside neighborhoods, the burden of inaccessible data continuously thwarts their opportunity for mobility. However, with the advent of the internet and the global push for open data, access to information is no longer solely in the hands of those with power, influence and money. Online tools, like the Westside Communities Alliance (WCA) Data Dashboard, quickly disseminate data to those most impacted by \"data driven decision-making,\" thus creating the potential of a genuinely equitable society. Based out of the Georgia Institute of Technology, the WCA works to build and sustain relationships among constituencies located in West Atlanta with the goal to strengthen partnerships around issues of common concern. The creation of the Data Dashboard stemmed from a recognized community desire for more localized control and the need for improvements to the communities' overall prosperity. Development of the site progressed through significant engagement between the WCA, community groups, and local agencies. The Dashboard takes the vast abundance of data and synthesizes it into a format that is both visually and geographically user-friendly. Through different portals, users can access neighborhood-level data around demographics, housing, education, and history that is formatting in a way that is easily accessible and understandable. The site includes qualitative research that goes beyond data and stats to give a totality of the community. By allowing West Atlanta advocacy groups to easily retrieve data, the WCA Data Dashboard empowers residents, nonprofits, and neighborhood associations to be full participants in the decision-making process.", "prompt": "Data is a dominant force during the decision-making process. It can help determine which roads to expand and the optimal location for a grocery store. Data can also be used to influence which schools to open or to shutter and \"appropriate\" city services to continue or discontinue. Considered fact-based, objective, and impartial, data can trump emotional appeals during the final evaluation of a project; thus creating a power imbalance between those with the resources to access data and those without. Most often left behind are communities already struggling to stay afloat due to years of disinvestment by market forces and external decision-makers. For long ignored residents in Atlanta's West", "orig": "side neighborhoods, the burden of inaccessible data continuously thwarts their opportunity for mobility. However, with the advent of the internet and the global push for open data, access to information is no longer solely in the hands of those with power, influence and money. Online tools, like the Westside Communities Alliance (WCA) Data Dashboard, quickly disseminate data to those most impacted by \"data driven decision-making,\" thus creating the potential of a genuinely equitable society. Based out of the Georgia Institute of Technology, the WCA works to build and sustain relationships among constituencies located in West Atlanta with the goal to strengthen partnerships around issues of common concern", "generated": "End, this means that their voices are often drowned out by the loud whispers of data-driven policymakers.\n\nAs a result, the narrative of \"progress\" and \"development\" is crafted without their input, leaving them vulnerable to gentrification and displacement. The once-thriving neighborhoods, now tainted by neglect and decay, are prime targets for developers seeking to capitalize on the area's proximity to downtown. The promise of new jobs, increased property values, and a revitalized economy sounds enticing, but it comes at a steep cost. Long-time residents are priced out of their own homes, forced to navigate the complexities of Section 8 housing and"}
{"full_text": "Context: Actor-based programming languages offer many essential features for developing modern distributed reactive systems. These systems exploit the actor model's isolation property to fulfill their performance and scalability demands. Unfortunately, the reliance of the model on isolation as its most fundamental property requires programmers to express complex interaction patterns between their actors to be expressed manually in terms of complex combinations of messages sent between the isolated actors. Inquiry: In the last three decades, several language design proposals have been introduced to reduce the complexity that emerges from describing said interaction and coordination of actors. We argue that none of these proposals is satisfactory in order to express the many complex interaction patterns between actors found in modern reactive distributed systems. Approach: We describe seven smart home automation scenarios (in which an actor represents every smart home appliance) to motivate the support by actor languages for five radically different types of message synchronization patterns, which are lacking in modern distributed actor-based languages. Fortunately, these five types of synchronisation patterns have been studied extensively by the Complex Event Processing (CEP) community. Our paper describes how such CEP patterns are elegantly added to an actor-based programming language. Knowledge: Based on our findings, we propose an extension of the single-message matching paradigm of contemporary actor-based languages in order to support a multiple-message matching way of thinking in the same way as proposed by CEP languages. Our proposal thus enriches the actor-model by ways of declaratively describing complex message combinations to which an actor can respond. Grounding: We base the problem-statement of the paper on an online poll in the home automation community that has motivated the real need for the CEP-based synchronisation operators between actors proposed in the paper. Furthermore, we implemented a DSL -- called Sparrow -- that supports said operators and we argue quantitatively (in terms of LOC and in terms of a reduction of the concerns that have to be handled by programmers) that the DSL outperforms existing approaches. Importance: This work aims to provide a set of synchronization operators that help actor-based languages to handle the complex interaction required by modern reactive distributed systems. To the best of our knowledge, our proposal is the first one to add advanced CEP synchronization operators to the -- relatively simplistic single-message based matching -- mechanisms of most actor-based languages.", "prompt": "Context: Actor-based programming languages offer many essential features for developing modern distributed reactive systems. These systems exploit the actor model's isolation property to fulfill their performance and scalability demands. Unfortunately, the reliance of the model on isolation as its most fundamental property requires programmers to express complex interaction patterns between their actors to be expressed manually in terms of complex combinations of messages sent between the isolated actors. Inquiry: In the last three decades, several language design proposals have been introduced to reduce the complexity that emerges from describing said interaction and coordination of actors. We argue that none of these proposals is satisfactory in order to express the many complex interaction patterns between actors found in modern", "orig": "reactive distributed systems. Approach: We describe seven smart home automation scenarios (in which an actor represents every smart home appliance) to motivate the support by actor languages for five radically different types of message synchronization patterns, which are lacking in modern distributed actor-based languages. Fortunately, these five types of synchronisation patterns have been studied extensively by the Complex Event Processing (CEP) community. Our paper describes how such CEP patterns are elegantly added to an actor-based programming language. Knowledge: Based on our findings, we propose an extension of the single-message matching paradigm of contemporary actor-based languages in order to support a multiple-message matching way of thinking in", "generated": "distributed systems.\n\nContinuation: \n\nOne of the primary reasons for this dissatisfaction lies in the fact that these proposals focus primarily on the syntax and semantics of message passing, neglecting the more profound structural and organizational aspects of actor interactions. For instance, many languages provide constructs for specifying message handlers or-routing strategies, but fail to address the intricate dependencies and constraints that arise from the interactions of multiple actors. Furthermore, the majority of these approaches rely on explicit actor references or identifiers, which can lead to tight coupling and rigid system structures.\n\nIn addition, modern distributed systems often exhibit complex topological structures, with actors organized into hierarchical or heterogeneous clusters, which"}
{"full_text": "The software supply chain is becoming a widespread analogy to designate the series of steps taken to go from source code published by developers to executables running on the users? computers. A security vulnerability in any of these steps puts users at risk, and evidence shows that attacks on the supply chain are becoming more common. The consequences of an attack on the software supply chain can be tragic in a society that relies on many interconnected software systems, and this has led research interest as well as governmental incentives for supply chain security to rise. GNU Guix is a software deployment tool and software distribution that supports provenance tracking, reproducible builds, and reproducible software environments. Unlike many software distributions, it consists exclusively of source code: it provides a set of package definitions that describe how to build code from source. Together, these properties set it apart from many deployment tools that center on the distribution of binaries. This paper focuses on one research question: how can Guix and similar systems allow users to securely update their software? Guix source code is distributed using the Git version control system; updating Guix-installed software packages means, first, updating the local copy of the Guix source code. Prior work on secure software updates focuses on systems very different from Guix -- systems such as Debian, Fedora, or PyPI where updating consists in fetching metadata about the latest binary artifacts available -- and is largely inapplicable in the context of Guix. By contrast, the main threats for Guix are attacks on its source code repository, which could lead users to run inauthentic code or to downgrade their system. Deployment tools that more closely resemble Guix, from Nix to Portage, either lack secure update mechanisms or suffer from shortcomings. Our main contribution is a model and tool to authenticate new Git revisions. We further show how, building on Git semantics, we build protections against downgrade attacks and related threats. We explain implementation choices. This work has been deployed in production two years ago, giving us insight on its actual use at scale every day. The Git checkout authentication at its core is applicable beyond the specific use case of Guix, and we think it could benefit to developer teams that use Git. As attacks on the software supply chain appear, security research is now looking at every link of the supply chain. Secure updates are one important aspect of the supply chain, but this paper also looks at the broader context: how Guix models and implements the supply chain, from upstream source code to binaries running on computers. While much recent work focuses on attestation -- certifying each link of the supply chain -- Guix takes a more radical approach: enabling independent verification of each step, building on reproducible builds, \"bootstrappable\" builds, and provenance tracking. The big picture shows how Guix can be used as the foundation of secure software supply chains.", "prompt": "The software supply chain is becoming a widespread analogy to designate the series of steps taken to go from source code published by developers to executables running on the users? computers. A security vulnerability in any of these steps puts users at risk, and evidence shows that attacks on the supply chain are becoming more common. The consequences of an attack on the software supply chain can be tragic in a society that relies on many interconnected software systems, and this has led research interest as well as governmental incentives for supply chain security to rise. GNU Guix is a software deployment tool and software distribution that supports provenance tracking, reproducible builds, and reproducible software", "orig": "environments. Unlike many software distributions, it consists exclusively of source code: it provides a set of package definitions that describe how to build code from source. Together, these properties set it apart from many deployment tools that center on the distribution of binaries. This paper focuses on one research question: how can Guix and similar systems allow users to securely update their software? Guix source code is distributed using the Git version control system; updating Guix-installed software packages means, first, updating the local copy of the Guix source code. Prior work on secure software updates focuses on systems very different from Guix -- systems such as Debian, Fedora", "generated": ".\n\nAs the software supply chain continues to expand and become more complex, the need for robust security measures becomes increasingly urgent. Attackers are constantly finding new ways to exploit vulnerabilities in the supply chain, making it essential to develop innovative solutions to mitigate these risks. GNU Guix, with its focus on transparency and reproducibility, is a promising approach to addressing these concerns. By providing a clear record of the entire software development and deployment process, GNU Guix enables developers and users to track the origin and integrity of software components, thereby reducing the risk of tainted or malicious code.\n\nMoreover, reproducible builds and software ensure that the resulting executables"}
{"full_text": "Fermi can measure the spectral properties of gamma-ray bursts over a very large energy range and is opening a new window on the prompt emission of these energetic events. Localizations by the instruments on Fermi in combination with follow-up by Swift provide accurate positions for observations at longer wavelengths leading to the determination of redshifts, the true energy budget, host galaxy properties and facilitate comparison with pre-Fermi bursts. Multi-wavelength follow-up observations were performed on the afterglows of four bursts with high energy emission detected by Fermi/LAT : GRB090323, GRB090328, GRB090510 and GRB090902B. They were obtained in the optical/near-infrared bands with GROND mounted at the MPG/ESO 2.2m telescope and additionally of GRB090323 in the optical with the 2 m telescope in Tautenburg, Germany. Three of the events are classified as long bursts while GRB090510 is a well localized short GRB with GeV emission. In addition, host galaxies were detected for three of the four bursts. Spectroscopic follow-up was initiated with the VLT for GRB090328 and GRB090510. The afterglow observations in 7 bands are presented for all bursts and their host galaxies are investigated. Knowledge of the distance and the local dust extinction enables comparison of the afterglows of LAT-detected GRBs with the general sample. The spectroscopic redshifts of GRB090328 and GRB090510 were determined to be z=0.7354+/-0.0003 and z=0.903 +/- 0.001 and dust corrected star-formation rates of 4.8 Mdot yr^-1 and 0.60 M_dot yr^-1 were derived for their host galaxies, respectively. The afterglows of long bursts exhibit power-law decay indices alpha from less than 1 to ~2.3 and spectral indices (beta) values from 0.65 to ~1.2 which are fairly standard for GRB afterglows. Constraints are placed on the jet half opening angles of less than 2.1 deg to greater than 6.4 deg which allows limits to be placed on the beaming corrected energies. These range from less than 5x10^50 erg to the one of the highest values ever recorded, greater than 2.2x10^52 erg for GRB090902B, and are not consistent with a standard candle. The extremely energetic long Fermi bursts have optical afterglows which lie in the top half of the brightness distribution of all optical afterglows detected in the Swift era or even in the top 5% if incompleteness is considered. The properties of the host galaxies of these LAT detected bursts in terms of extinction, star formation rates and masses do not appear to differ from previous samples.", "prompt": "Fermi can measure the spectral properties of gamma-ray bursts over a very large energy range and is opening a new window on the prompt emission of these energetic events. Localizations by the instruments on Fermi in combination with follow-up by Swift provide accurate positions for observations at longer wavelengths leading to the determination of redshifts, the true energy budget, host galaxy properties and facilitate comparison with pre-Fermi bursts. Multi-wavelength follow-up observations were performed on the afterglows of four bursts with high energy emission detected by Fermi/LAT : GRB090323, GRB090328, GRB090510 and GRB090", "orig": "902B. They were obtained in the optical/near-infrared bands with GROND mounted at the MPG/ESO 2.2m telescope and additionally of GRB090323 in the optical with the 2 m telescope in Tautenburg, Germany. Three of the events are classified as long bursts while GRB090510 is a well localized short GRB with GeV emission. In addition, host galaxies were detected for three of the four bursts. Spectroscopic follow-up was initiated with the VLT for GRB090328 and GRB090510. The afterglow observations in 7 bands are presented for all", "generated": "902B.\n\nThese joint efforts have provided a wealth of new information about the physics of gamma-ray bursts. For instance, the high-energy emission detected by Fermi/LAT in GRB090510 revealed a delayed onset of GeV emission with respect to the MeV emission, which has significant implications for our understanding of the acceleration processes at play in these events. Furthermore, the combination of Fermi and Swift data allowed for a detailed study of the broadband spectral energy distribution of these bursts, providing valuable insights into the radiative mechanisms and particle acceleration processes. The accurate localizations provided by Fermi have also enabled the identification of the host galaxies of"}
{"full_text": "We study operator algebras arising from monomial ideals in the ring of polynomials in noncommuting variables, through the apparatus of subproduct systems and C*-correspondences. We provide a full comparison amongst the related operator algebras. For our analysis we isolate a partially defined dynamical system, to which we refer as the {\\em quantised dynamics} of the monomial ideal. In addition we revisit several previously considered constructions. These include Matsumoto's subshift C*-algebras, as well as the tensor and the Pimsner algebras associated with dynamical systems or graphs. We sort out the various relations by giving concrete conditions and counterexamples that orientate the operator algebras of our context. It appears that the boundary C*-algebras do not arise as the quotient with the compact operators unconditionally. We establish a dichotomy to this effect by examining the resulting tensor algebras. We identify their boundary representations, we analyse their C*-envelopes, and we give criteria for hyperrigidity. Moreover we completely classify them in terms of the data provided by the monomial ideals. For tensor algebras of C*-correspondences and bounded isomorphisms this is achieved up to the level of local conjugacy (in the sense of Davidson and Roydor) for the quantised dynamics. For tensor algebras of subproduct systems and algebraic isomorphisms this is achieved up to the level of equality of monomial ideals modulo permutations of the variables. In the process we accomplish more in different directions. Most notably we show that tensor algebras form a complete invariant for isomorphic (resp. similar) subproduct systems of homogeneous ideals up to isometric (resp. bounded) isomorphisms. The results on local conjugacy are obtained via an alternative proof of the breakthrough result of Davidson and Katsoulis on piecewise conjugate systems. For our purposes we use appropriate compressions of the Fock representation. We then apply this alternative proof locally for the partially defined quantised dynamics. In this way we avoid the topological graphs machinery and pave the way for further applications. These include operator algebras of dynamical systems over commuting contractions or over row commuting contractions.", "prompt": "We study operator algebras arising from monomial ideals in the ring of polynomials in noncommuting variables, through the apparatus of subproduct systems and C*-correspondences. We provide a full comparison amongst the related operator algebras. For our analysis we isolate a partially defined dynamical system, to which we refer as the {\\em quantised dynamics} of the monomial ideal. In addition we revisit several previously considered constructions. These include Matsumoto's subshift C*-algebras, as well as the tensor and the Pimsner algebras associated with dynamical systems or graphs. We sort out the", "orig": "various relations by giving concrete conditions and counterexamples that orientate the operator algebras of our context. It appears that the boundary C*-algebras do not arise as the quotient with the compact operators unconditionally. We establish a dichotomy to this effect by examining the resulting tensor algebras. We identify their boundary representations, we analyse their C*-envelopes, and we give criteria for hyperrigidity. Moreover we completely classify them in terms of the data provided by the monomial ideals. For tensor algebras of C*-correspondences and bounded isomorphisms this is achieved up to the level of", "generated": "relationships between these diverse operator algebras, highlighting the connections and disparities between them.\n\nOur investigation unveils a rich structure, wherein the quantised dynamics of the monomial ideal serves as a unifying thread. This dynamical system encapsulates the intricate patterns of noncommutativity present in the polynomial ring, and its study enables us to tease apart the distinct flavours of the various operator algebras. Through a series of precise characterisations, we elucidate the circumstances under which these algebras coincide, and those in which they diverge. The Pimsner algebra, for instance, is shown to arise as a quotient of the"}
{"full_text": "It is a classical result of Ginibre that the normalized bulk $k$-point correlation functions of a complex $n\\times n$ Gaussian matrix with independent entries of mean zero and unit variance are asymptotically given by the determinantal point process on $\\mathbb{C}$ with kernel $K_{\\infty}(z,w):=\\frac{1}{\\pi}e^{-|z|^2/2-|w|^2/2+z\\bar{w}}$ in the limit $n\\to\\infty$. In this paper, we show that this asymptotic law is universal among all random $n\\times n$ matrices $M_n$ whose entries are jointly independent, exponentially decaying, have independent real and imaginary parts and whose moments match that of the complex Gaussian ensemble to fourth order. Analogous results at the edge of the spectrum are also obtained. As an application, we extend a central limit theorem for the number of eigenvalues of complex Gaussian matrices in a small disk to these more general ensembles. These results are non-Hermitian analogues of some recent universality results for Hermitian Wigner matrices. However, a key new difficulty arises in the non-Hermitian case, due to the instability of the spectrum for such matrices. To resolve this issue, we the need to work with the log-determinants $\\log|\\det(M_n-z_0)|$ rather than with the Stieltjes transform $\\frac{1}{n}\\operatorname {tr}(M_n-z_0)^{-1}$, in order to exploit Girko's Hermitization method. Our main tools are a four moment theorem for these log-determinants, together with a strong concentration result for the log-determinants in the Gaussian case. The latter is established by studying the solutions of a certain nonlinear stochastic difference equation. With some extra consideration, we can extend our arguments to the real case, proving universality for correlation functions of real matrices which match the real Gaussian ensemble to the fourth order. As an application, we show that a real $n\\times n$ matrix whose entries are jointly independent, exponentially decaying and whose moments match the real Gaussian ensemble to fourth order has $\\sqrt{\\frac{2n}{\\pi}}+o(\\sqrt{n})$ real eigenvalues asymptotically almost surely.", "prompt": "It is a classical result of Ginibre that the normalized bulk $k$-point correlation functions of a complex $n\\times n$ Gaussian matrix with independent entries of mean zero and unit variance are asymptotically given by the determinantal point process on $\\mathbb{C}$ with kernel $K_{\\infty}(z,w):=\\frac{1}{\\pi}e^{-|z|^2/2-|w|^2/2+z\\bar{w}}$ in the limit $n\\to\\infty$. In this paper, we show that this asymptotic law is universal among all random $n\\t", "orig": "imes n$ matrices $M_n$ whose entries are jointly independent, exponentially decaying, have independent real and imaginary parts and whose moments match that of the complex Gaussian ensemble to fourth order. Analogous results at the edge of the spectrum are also obtained. As an application, we extend a central limit theorem for the number of eigenvalues of complex Gaussian matrices in a small disk to these more general ensembles. These results are non-Hermitian analogues of some recent universality results for Hermitian Wigner matrices. However, a key new difficulty arises in the non-Hermitian case, due to the instability of the", "generated": "imes n$ matrices with independent and identically distributed entries of mean zero and unit variance, subject to certain moment conditions.\n\nThis universality result has far-reaching implications for the study of random matrix theory and its applications to various fields, including statistics, physics, and engineering. For instance, it provides a theoretical foundation for the use of Gaussian matrices as a model for random matrix ensembles in various statistical analyses. Moreover, it sheds light on the universal behavior of eigenvalue distributions in complex systems, which is crucial for understanding many physical phenomena, such as the behavior of energies in quantum systems or the stability of complex networks.\n\nIn particular, our result"}
{"full_text": "Aims. The HIFI instrument onboard Herschel has allowed high spectral resolution and sensitive observations of ground-state transi- tions of three molecular ions: the methylidyne cation CH+, its isotopologue 13CH+, and sulfanylium SH+. Because of their unique chemical properties, a comparative analysis of these cations provides essential clues to the link between the chemistry and dynamics of the diffuse interstellar medium. Methods. The CH+, 13CH+, and SH+ lines are observed in absorption towards the distant high-mass star-forming regions (SFRs) DR21(OH), G34.3+0.1, W31C, W33A, W49N, and W51, and towards two sources close to the Galactic centre, SgrB2(N) and SgrA*+50. All sight lines sample the diffuse interstellar matter along pathlengths of several kiloparsecs across the Galactic Plane. In order to compare the velocity structure of each species, the observed line profiles were deconvolved from the hyperfine structure of the SH+ transition and the CH+, 13CH+, and SH+ spectra were independently decomposed into Gaussian velocity components. To analyse the chemical composition of the foreground gas, all spectra were divided, in a second step, into velocity intervals over which the CH+, 13CH+, and SH+ column densities and abundances were derived. Results. SH+ is detected along all observed lines of sight, with a velocity structure close to that of CH+ and 13CH+. The linewidth distributions of the CH+, SH+, and 13CH+ Gaussian components are found to be similar. These distributions have the same mean (<\\delta\\u{psion}> ~ 4.2 km s-1) and standard deviation (\\sigma(\\delta\\u{psion}) ~ 1.5 km s-1). This mean value is also close to that of the linewidth distribution of the CH+ visible transitions detected in the solar neighbourhood. We show that the lack of absorption components narrower than 2 km s-1 is not an artefact caused by noise: the CH+, 13CH+, and SH+ line profiles are therefore statistically broader than those of most species detected in absorption in diffuse interstellar gas (e. g. HCO+, CH, or CN). The SH+/CH+ column density ratio observed in the components located away from the Galactic centre spans two orders of magnitude and correlates with the CH+ abundance. Conversely, the ratio observed in the components close to the Galactic centre varies over less than one order of magnitude with no apparent correlation with the CH+ abundance. The observed dynamical and chemical properties of SH+ and CH+ are proposed to trace the ubiquitous process of turbulent dissipation, in shocks or shears, in the diffuse ISM and the specific environment of the Galactic centre regions.", "prompt": "Aims. The HIFI instrument onboard Herschel has allowed high spectral resolution and sensitive observations of ground-state transi- tions of three molecular ions: the methylidyne cation CH+, its isotopologue 13CH+, and sulfanylium SH+. Because of their unique chemical properties, a comparative analysis of these cations provides essential clues to the link between the chemistry and dynamics of the diffuse interstellar medium. Methods. The CH+, 13CH+, and SH+ lines are observed in absorption towards the distant high-mass star-forming regions (SFRs) DR21(OH), G34.3", "orig": "+0.1, W31C, W33A, W49N, and W51, and towards two sources close to the Galactic centre, SgrB2(N) and SgrA*+50. All sight lines sample the diffuse interstellar matter along pathlengths of several kiloparsecs across the Galactic Plane. In order to compare the velocity structure of each species, the observed line profiles were deconvolved from the hyperfine structure of the SH+ transition and the CH+, 13CH+, and SH+ spectra were independently decomposed into Gaussian velocity components. To analyse the chemical composition of the foreground gas", "generated": "+0.1, and W49N.\n\nThese observations have provided unprecedented insights into the physical and chemical conditions prevailing in the diffuse interstellar medium, allowing us to better understand the intricate processes governing the formation and evolution of these regions. The high spectral resolution afforded by HIFI enables us to resolve the narrow absorption features associated with the molecular ions, permitting a detailed analysis of their kinematics and spatial distribution. Furthermore, the simultaneous detection of multiple transitions of these cations has enabled us to constrain their column densities and excitation temperatures with high accuracy.\n\nIn this study, we present a comprehensive analysis of the CH+, 13CH+, and SH"}
{"full_text": "This version is the last version of our book project on Hamilton-Jacobi Equations and Control Problems with discontinuities. Compared to the third version (online in december 2022), we have improved Part V (Stratified solutions for state-constraints problems) and Part VI on the applications but also the stability results for stratified solutions; we have rewritten a large part of the introduction and added guidelines for the reader. As in the previous versions, we have incorporated new results and examples, changed some points-of-view, detailed some proofs and corrected several mistakes. Version 3 had 550 pages, this one 630.As the third version, it is composed of six parts: Part I is still a toolbox with key results which are used in all the other parts. The study of the simplest case, i.e. the case of a co-dimension 1 discontinuity, is now split in two parts: in Part II, we only consider control problems and the associated Bellman Equations are treated by using only the classical notion of viscosity solutions. In this part, the methods are a combinations of control and pdes techniques. On the contrary, Part III describes purely pdes approaches which are inspired by the literature on Hamilton Jacobi Equations on networks and which can handle the case of non-convex Hamiltonians. In this part, we present two notions of solutions, namely flux-limited and junction viscosity solutions, and we study in detail their properties by providing comparison and stability results. We also show that they are ``almost'' equivalent when both make sense, i.e. for quasi-convex Hamiltonians. Part IV concerns stratified problems in $\\R^N$, i.e. problems with discontinuities of any co-dimensions: the main change compared to the previous version is the introduction of a notion of ``weak'' stratified (sub)solution. In Part V, we address the case of stratified problems in bounded or unbounded domains with state-constraints, allowing very surprising applications as well as singular boundary conditions. Finally, in Part VI we describe some applications to KPP (Kolmogorov-Petrovsky-Piskunov) type problems and we discuss possible extensions to problems with jumps and to ``stratified networks''.Even if we consider this version as being the final one, all comments are welcome!", "prompt": "This version is the last version of our book project on Hamilton-Jacobi Equations and Control Problems with discontinuities. Compared to the third version (online in december 2022), we have improved Part V (Stratified solutions for state-constraints problems) and Part VI on the applications but also the stability results for stratified solutions; we have rewritten a large part of the introduction and added guidelines for the reader. As in the previous versions, we have incorporated new results and examples, changed some points-of-view, detailed some proofs and corrected several mistakes. Version 3 had 550 pages, this one 630.As the", "orig": "third version, it is composed of six parts: Part I is still a toolbox with key results which are used in all the other parts. The study of the simplest case, i.e. the case of a co-dimension 1 discontinuity, is now split in two parts: in Part II, we only consider control problems and the associated Bellman Equations are treated by using only the classical notion of viscosity solutions. In this part, the methods are a combinations of control and pdes techniques. On the contrary, Part III describes purely pdes approaches which are inspired by the literature on Hamilton Jacobi Equations on networks and which", "generated": "culmination of several years of rigorous research and dedication, we are proud to present this comprehensive and refined manuscript, which we believe will serve as a valuable resource for researchers, students, and professionals working in the field of control theory and Hamilton-Jacobi equations.\n\nIn this latest iteration, we have placed particular emphasis on expanding and clarifying the sections on stratified solutions, which we believe hold significant potential for advancing the understanding of state-constrained problems. The rewritten introduction provides a more nuanced and accessible overview of the subject matter, making it easier for new readers to navigate the complexities of the field. Additionally, the added guidelines for the reader will enable"}
{"full_text": "Background: Odd-odd nuclei, around doubly closed shells, have been extensively used to study proton-neutron interactions. However, the evolution of these interactions as a function of the binding energy, ultimately when nuclei become unbound, is poorly known. The $^{26}$F nucleus, composed of a deeply bound $\\pi0d\\_{5/2}$ proton and an unbound $\\nu0d\\_{3/2}$ neutron on top of an $^{24}$O core, is particularly adapted for this purpose. The coupling of this proton and neutron results in a $J^{\\pi} = 1^{+}\\_1 - 4^{+}\\_1$ multiplet, whose energies must be determined to study the influence of the proximity of the continuum on the corresponding proton-neutron interaction. The $J^{\\pi} = 1^{+}\\_1, 2^{+}\\_1,4^{+}\\_1$ bound states have been determined, and only a clear identification of the $J^{\\pi} =3^{+}\\_1$ is missing.Purpose: We wish to complete the study of the $J^{\\pi} = 1^{+}\\_1 - 4^{+}\\_1$ multiplet in $^{26}$F, by studying the energy and width of the $J^{\\pi} =3^{+}\\_1$ unbound state. The method was firstly validated by the study of unbound states in $^{25}$F, for which resonances were already observed in a previous experiment.Method: Radioactive beams of $^{26}$Ne and $^{27}$Ne, produced at about $440A$\\,MeV by the FRagment Separator at the GSI facility, were used to populate unbound states in $^{25}$F and $^{26}$F via one-proton knockout reactions on a CH$\\_2$ target, located at the object focal point of the R$^3$B/LAND setup. The detection of emitted $\\gamma$-rays and neutrons, added to the reconstruction of the momentum vector of the $A-1$ nuclei, allowed the determination of the energy of three unbound states in $^{25}$F and two in $^{26}$F. Results: Based on its width and decay properties, the first unbound state in $^{25}$F is proposed to be a $J^{\\pi} = 1/2^-$ arising from a $p\\_{1/2}$ proton-hole state. In $^{26}$F, the first resonance at 323(33)~keV is proposed to be the $J^{\\pi} =3^{+}\\_1$ member of the $J^{\\pi} = 1^{+}\\_1 - 4^{+}\\_1$ multiplet. Energies of observed states in $^{25,26}$F have been compared to calculations using the independent-particle shell model, a phenomenological shell-model, and the ab initio valence-space in-medium similarity renormalization group method.Conclusions: The deduced effective proton-neutron interaction is weakened by about 30-40\\% in comparison to the models, pointing to the need of implementing the role of the continuum in theoretical descriptions, or to a wrong determination of the atomic mass of $^{26}$F.", "prompt": "Background: Odd-odd nuclei, around doubly closed shells, have been extensively used to study proton-neutron interactions. However, the evolution of these interactions as a function of the binding energy, ultimately when nuclei become unbound, is poorly known. The $^{26}$F nucleus, composed of a deeply bound $\\pi0d\\_{5/2}$ proton and an unbound $\\nu0d\\_{3/2}$ neutron on top of an $^{24}$O core, is particularly adapted for this purpose. The coupling of this proton and neutron results in a $J^{\\pi} = 1^{+}\\", "orig": "_1 - 4^{+}\\_1$ multiplet, whose energies must be determined to study the influence of the proximity of the continuum on the corresponding proton-neutron interaction. The $J^{\\pi} = 1^{+}\\_1, 2^{+}\\_1,4^{+}\\_1$ bound states have been determined, and only a clear identification of the $J^{\\pi} =3^{+}\\_1$ is missing.Purpose: We wish to complete the study of the $J^{\\pi} = 1^{+}\\_1 - 4^{+}\\", "generated": "$\n\nContinuation: \n\nAs the binding energy of the nucleus decreases, the proton-neutron interactions are expected to change significantly, potentially leading to a modification of the nuclear structure. In the case of $^{26}$F, the deeply bound proton is expected to dominate the nuclear structure, while the unbound neutron will exhibit a more extended wave function. This unusual configuration allows for a unique investigation of the interplay between proton-neutron interactions and the nuclear binding energy.\n\nStudies of the $^{26}$F nucleus have been limited due to its short half-life and the difficulties in producing and identifying it experimentally. However, recent advances in radioactive"}
{"full_text": "The transition from the L to the T spectral type of brown dwarfs is marked by a very rapid transition phase, remarkable brightening in the J-band and a higher binary frequency. Despite being an active area of inquiry, this transition regime still remains one of the most poorly understood phases of brown dwarf evolution. We resolved the L dwarf 2MASS J03105986+1648155 for the first time into two almost equally bright components straddling the L/T transition. Since such a co-eval system with common age and composition provides crucial information of this special transition phase, we monitored the system over 3 years to derive first orbital parameters and dynamical mass estimates, as well as a spectral type determination. We obtained resolved high angular resolution, near-IR images with HST and the adaptive optics instrument NACO at the VLT including the laser guide star system PARSEC. Based on two epochs of astrometric data we derive a minimum semi-major axis of 5.2 +- 0.8 AU. The assumption of a face-on circular orbit yields an orbital period of 72 +- 4 years and a total system mass of 30-60 Mjup. This places the masses of the individual components of the system at the lower end of the mass regime of brown dwarfs. The achieved photometry allowed a first spectral type determination of L9 +- 1 for each component. In addition, this seems to be only the fifth resolved L/T transition binary with a flux reversal. While ultimate explanations for this effect are still owing, the 2MASS J03105986+1648155 system adds an important benchmark object for improving our understanding of this remarkable evolutionary phase of brown dwarfs. Additionally, the observational results of 2MASS J03105986+1648155 AB derived with the new PARSEC AO system at the VLT show the importance of this technical capability. The updated AO system allows us to significantly extend the sample of brown dwarfs observable with high-resolution from the ground and hence to reveal more of their physical properties.", "prompt": "The transition from the L to the T spectral type of brown dwarfs is marked by a very rapid transition phase, remarkable brightening in the J-band and a higher binary frequency. Despite being an active area of inquiry, this transition regime still remains one of the most poorly understood phases of brown dwarf evolution. We resolved the L dwarf 2MASS J03105986+1648155 for the first time into two almost equally bright components straddling the L/T transition. Since such a co-eval system with common age and composition provides crucial information of this special transition phase, we monitored the system over 3 years to derive first", "orig": "orbital parameters and dynamical mass estimates, as well as a spectral type determination. We obtained resolved high angular resolution, near-IR images with HST and the adaptive optics instrument NACO at the VLT including the laser guide star system PARSEC. Based on two epochs of astrometric data we derive a minimum semi-major axis of 5.2 +- 0.8 AU. The assumption of a face-on circular orbit yields an orbital period of 72 +- 4 years and a total system mass of 30-60 Mjup. This places the masses of the individual components of the system at the lower end of", "generated": "estimates of their orbital parameters and to constrain models of brown dwarf atmospheres.\n\nOur observations, carried out using a combination of spectroscopic and photometric techniques, revealed a fascinating view of the dynamics of this binary system. The two components, designated as 2MASS J03105986+1648155A and 2MASS J03105986+1648155B, exhibit a nearly equal mass ratio, with the primary component being slightly more massive than the secondary. The orbital period of the system was found to be approximately 3.5 years, with a semi-major axis of about 2.5 astronomical units. This"}
{"full_text": "Decay modes of excited nuclei are investigated in $^{78,82}$Kr + $^{40}$Ca reactions at 5.5 MeV/nucleon. Charged products were measured by means of the $4\\pi$ INDRA array. Kinetic-energy spectra and angular distributions of fragments with atomic number 3 $\\le Z \\le$ 28 indicate a high degree of relaxation and are compatible with a fission-like phenomenon. Persistence of structure effects is evidenced from elemental cross-sections ($\\sigma_{Z}$) as well as a strong odd-even-staggering (o-e-s) of the light-fragment yields. The magnitude of the staggering does not significantly depend on the neutron content of the emitting system. Fragment-particle coincidences suggest that the light partners in very asymmetric fission are emitted either cold or at excitation energies below the particle emission thresholds. The evaporation residue cross-section of the $^{78}$Kr + $^{40}$Ca reaction is slightly higher than the one measured in $^{82}$Kr + $^{40}$Ca reaction. The fission-like component is larger by $\\sim$ 25% for the reaction having the lowest neutron-to-proton ratio. These experimental features are confronted to the predictions of theoretical models. The Hauser-Feshbach approach including the emission of fragments up to $Z$ = 14 in their ground states as well as excited states does not account for the main features of $\\sigma_{Z}$. For both reactions, the transition-state formalism reasonably reproduces the $Z$-distribution of the fragments with charge 12 $\\le Z \\le$ 28. However, this model strongly overestimates the light-fragment cross-sections and does not explain the o-e-s of the yields for 6 $\\le Z \\le$ 10. The shape of the whole $Z$-distribution and the o-e-s of the light-fragment yields are satisfactorily reproduced within the dinuclear system framework which treats the competition between evaporation, fusion-fission and quasifission processes. The model suggests that heavy fragments come mainly from quasifission while light fragments are predominantly populated by fusion. An underestimation of the cross sections for 16 $\\le Z \\le$ 22 could signal a mechanism in addition to the capture process.", "prompt": "Decay modes of excited nuclei are investigated in $^{78,82}$Kr + $^{40}$Ca reactions at 5.5 MeV/nucleon. Charged products were measured by means of the $4\\pi$ INDRA array. Kinetic-energy spectra and angular distributions of fragments with atomic number 3 $\\le Z \\le$ 28 indicate a high degree of relaxation and are compatible with a fission-like phenomenon. Persistence of structure effects is evidenced from elemental cross-sections ($\\sigma_{Z}$) as well as a strong odd-even-staggering (o-e-s) of the light-fragment", "orig": "yields. The magnitude of the staggering does not significantly depend on the neutron content of the emitting system. Fragment-particle coincidences suggest that the light partners in very asymmetric fission are emitted either cold or at excitation energies below the particle emission thresholds. The evaporation residue cross-section of the $^{78}$Kr + $^{40}$Ca reaction is slightly higher than the one measured in $^{82}$Kr + $^{40}$Ca reaction. The fission-like component is larger by $\\sim$ 25% for the reaction having the lowest neutron-to-proton ratio. These experimental features are confronted to the predictions of theoretical models", "generated": ".\n\nThe observed o-e-s pattern suggests that the nuclear structure plays a significant role in the decay process, even at such high excitation energies. This is in contrast to previous studies that reported a washing out of structure effects at similar energies. The present results imply that the reaction mechanism involves a complex interplay between collective and single-particle degrees of freedom, leading to a preservation of shell effects in the fragmentation process.\n\nFurthermore, the analysis of the kinetic-energy spectra reveals a significant contribution from symmetric fission, accompanied by a broad bump in the fragmentation mass distribution. This bump is likely associated with the formation of heavy fragments with A \u223c"}
{"full_text": "Programming languages and platforms improve over time, sometimes resulting in new language features that offer many benefits. However, despite these benefits, developers may not always be willing to adopt them in their projects for various reasons. In this paper, we describe an empirical study where we assess the adoption of a particular new language feature. Studying how developers use (or do not use) new language features is important in programming language research and engineering because it gives designers insight into the usability of the language to create meaning programs in that language. This knowledge, in turn, can drive future innovations in the area. Here, we explore Java 8 default methods, which allow interfaces to contain (instance) method implementations. Default methods can ease interface evolution, make certain ubiquitous design patterns redundant, and improve both modularity and maintainability. A focus of this work is to discover, through a scientific approach and a novel technique, situations where developers found these constructs useful and where they did not, and the reasons for each. Although several studies center around assessing new language features, to the best of our knowledge, this kind of construct has not been previously considered. Despite their benefits, we found that developers did not adopt default methods in all situations. Our study consisted of submitting pull requests introducing the language feature to 19 real-world, open source Java projects without altering original program semantics. This novel assessment technique is proactive in that the adoption was driven by an automatic refactoring approach rather than waiting for developers to discover and integrate the feature themselves. In this way, we set forth best practices and patterns of using the language feature effectively earlier rather than later and are able to possibly guide (near) future language evolution. We foresee this technique to be useful in assessing other new language features, design patterns, and other programming idioms.", "prompt": "Programming languages and platforms improve over time, sometimes resulting in new language features that offer many benefits. However, despite these benefits, developers may not always be willing to adopt them in their projects for various reasons. In this paper, we describe an empirical study where we assess the adoption of a particular new language feature. Studying how developers use (or do not use) new language features is important in programming language research and engineering because it gives designers insight into the usability of the language to create meaning programs in that language. This knowledge, in turn, can drive future innovations in the area. Here, we explore Java 8 default methods, which", "orig": "allow interfaces to contain (instance) method implementations. Default methods can ease interface evolution, make certain ubiquitous design patterns redundant, and improve both modularity and maintainability. A focus of this work is to discover, through a scientific approach and a novel technique, situations where developers found these constructs useful and where they did not, and the reasons for each. Although several studies center around assessing new language features, to the best of our knowledge, this kind of construct has not been previously considered. Despite their benefits, we found that developers did not adopt default methods in all situations. Our study consisted of submitting pull requests introducing the language feature to 19", "generated": "have been introduced to facilitate the evolution of interfaces and provide a more flexible way of adding new functionality to existing APIs. Despite their potential benefits, default methods have not been widely adopted by developers, and their usage remains limited. Our study aims to investigate the reasons behind this phenomenon, examining both quantitative and qualitative data from open-source projects and surveys of professional developers. We analyzed over 10,000 Java projects on GitHub, tracking the adoption rate of default methods over time, and conducted in-depth interviews with 20 experienced Java developers to gather their opinions and experiences with this feature. Our results show that while default methods offer significant advantages in terms of"}
{"full_text": "Technologies such as aerial photogrammetry allow production of 3D topographic data including complex environments such as urban areas. Therefore, it is possible to create High Resolution (HR) Digital Elevation Models (DEM) incorporating thin above ground elements influencing overland flow paths. Even though this category of big data has a high level of accuracy, there are still errors in measurements and hypothesis under DEM elaboration. Moreover, operators look for optimizing spatial discretization resolution in order to improve flood models computation time. Errors in measurement, errors in DEM generation, and operator choices for inclusion of this data within 2D hydraulic model, might influence results of flood models simulations. These errors and hypothesis may influence significantly flood modelling results variability. The purpose of this study is to investigate uncertainties related to (i) the own error of high resolution topographic data, and (ii) the modeller choices when including topographic data in hydraulic codes. The aim is to perform a Global Sensitivity Analysis (GSA) which goes through a Monte-Carlo uncertainty propagation, to quantify impact of uncertainties, followed by a Sobol' indices computation, to rank influence of identified parameters on result variability. A process using a coupling of an environment for parametric computation (Prom{\\'e}th{\\'e}e) and a code relying on 2D shallow water equations (FullSWOF 2D) has been developed (P-FS tool). The study has been performed over the lower part of the Var river valley using the estimated hydrograph of 1994 flood event. HR topographic data has been made available for the study area, which is 17.5 km 2 , by Nice municipality. Three uncertain parameters were studied: the measurement error (var. E), the level of details of above-ground element representation in DEM (buildings, sidewalks, etc.) (var. S), and the spatial discretization resolution (grid cell size for regular mesh) (var. R). Parameter var. E follows a probability density function, whereas parameters var. S and var. R. are discrete operator choices. Combining these parameters, a database of 2, 000 simulations has been produced using P-FS tool implemented on a high performance computing structure. In our study case, the output of interest is the maximal", "prompt": "Technologies such as aerial photogrammetry allow production of 3D topographic data including complex environments such as urban areas. Therefore, it is possible to create High Resolution (HR) Digital Elevation Models (DEM) incorporating thin above ground elements influencing overland flow paths. Even though this category of big data has a high level of accuracy, there are still errors in measurements and hypothesis under DEM elaboration. Moreover, operators look for optimizing spatial discretization resolution in order to improve flood models computation time. Errors in measurement, errors in DEM generation, and operator choices for inclusion of this data within 2D hydraulic model, might influence results", "orig": "of flood models simulations. These errors and hypothesis may influence significantly flood modelling results variability. The purpose of this study is to investigate uncertainties related to (i) the own error of high resolution topographic data, and (ii) the modeller choices when including topographic data in hydraulic codes. The aim is to perform a Global Sensitivity Analysis (GSA) which goes through a Monte-Carlo uncertainty propagation, to quantify impact of uncertainties, followed by a Sobol' indices computation, to rank influence of identified parameters on result variability. A process using a coupling of an environment for parametric computation (Prom{\\'e}th{\\'", "generated": ".\n\nThe significance of accurate DEMs in flood modeling cannot be overstated, as they play a critical role in simulating the flow of water through complex urban environments. However, the errors mentioned above can lead to discrepancies in the results, which in turn can impact the reliability of flood risk assessments and mitigation strategies. For instance, inaccurate DEMs can result in incorrect identification of flood-prone areas, leading to inadequate flood protection measures. Furthermore, the inclusion of thin above-ground elements, such as buildings and bridges, in DEMs can significantly affect the simulation of overland flow paths, making it essential to ensure the accuracy of these elements.\n\nTo"}
