{"full_text": "When can we compute the diameter of a graph in quasi linear time? We address this question for the class of {\\em split graphs}, that we observe to be the hardest instances for deciding whether the diameter is at most two. We stress that although the diameter of a non-complete split graph can only be either $2$ or $3$, under the Strong Exponential-Time Hypothesis (SETH) we cannot compute the diameter of an $n$-vertex $m$-edge split graph in less than quadratic time -- in the size $n+m$ of the input. Therefore it is worth to study the complexity of diameter computation on {\\em subclasses} of split graphs, in order to better understand the complexity border. Specifically, we consider the split graphs with bounded {\\em clique-interval number} and their complements, with the former being a natural variation of the concept of interval number for split graphs that we introduce in this paper. We first discuss the relations between the clique-interval number and other graph invariants such as the classic interval number of graphs, the treewidth, the {\\em VC-dimension} and the {\\em stabbing number} of a related hypergraph. Then, in part based on these above relations, we almost completely settle the complexity of diameter computation on these subclasses of split graphs: - For the $k$-clique-interval split graphs, we can compute their diameter in truly subquadratic time if $k={\\cal O}(1)$, and even in quasi linear time if $k=o(\\log{n})$ and in addition a corresponding ordering of the vertices in the clique is given. However, under SETH this cannot be done in truly subquadratic time for any $k = \\omega(\\log{n})$. - For the {\\em complements} of $k$-clique-interval split graphs, we can compute their diameter in truly subquadratic time if $k={\\cal O}(1)$, and even in time ${\\cal O}(km)$ if a corresponding ordering of the vertices in the stable set is given. Again this latter result is optimal under SETH up to polylogarithmic factors. Our findings raise the question whether a $k$-clique interval ordering can always be computed in quasi linear time. We prove that it is the case for $k=1$ and for some subclasses such as bounded-treewidth split graphs, threshold graphs and comparability split graphs. Finally, we prove that some important subclasses of split graphs -- including the ones mentioned above -- have a bounded clique-interval number.", "prompt": "When can we compute the diameter of a graph in quasi linear time? We address this question for the class of {\\em split graphs}, that we observe to be the hardest instances for deciding whether the diameter is at most two. We stress that although the diameter of a non-complete split graph can only be either $2", "orig": "$ or $3$, under the Strong Exponential-Time Hypothesis (SETH) we cannot compute the diameter of an $n$-vertex $m$-edge split graph in less than quadratic time -- in the size $n+m$ of the input. Therefore it is worth to study the complexity of diameter", "generated": "When can we compute the diameter of a graph in quasi linear time? We address this question for the class of {\\em split graphs}, that we observe to be the hardest instances for deciding whether the diameter is at most two. We stress that although the diameter of a non-complete split graph can only be either $2"}
{"full_text": "Simplified silica (Zeosil 1165 MP) and SBR (140k carrying silanol end-groups) nanocomposites have been formulated by mixing of a reduced number of ingredients with respect to industrial applications. The thermo-mechanical history of the samples during the mixing process was monitored and adjusted to identical final temperatures. The filler structure on large scales up to micrometers was studied by transmission electron microscopy (TEM) and very small-angle X-ray scattering (SAXS). A complete quantitative model extending from the primary silica nanoparticle (of radius \\approx 10 nm), to nanoparticles aggregates, up to micrometer-sized branches with typical lateral dimension of 150 nm is proposed. Image analysis of the TEM-pictures yields the fraction of zones of pure polymer, which extend between the branches of a large-scale filler network. This network is compatible with a fractal of average dimension 2.4 as measured by scattering. On smaller length scales, inside the branches, small silica aggregates are present. Their average radius has been deduced from a Kratky analysis, and it ranges between 35 and 40 nm for all silica fractions investigated here (\\phi_si = 8-21% vol.). A central piece of our analysis is the description of the interaggregate interaction by a simulated structure factor for polydisperse spheres representing aggregates. A polydispersity of 30% in aggregate size is assumed, and interactions between these aggregates are described with a hard core repulsive potential. The same distribution in size is used to evaluate the polydisperse form factor. Comparison with the experimental intensity leads to the determination of the average aggregate compacity (assumed identical for all aggregates in the distribution, between 31% and 38% depending on \\phi_si), and thus aggregation number (ca. 45, with a large spread). Because of the effect of aggregate compacity and of pure polymer zones, the volume fraction of aggregates is higher in the branches than \\phi_si. The repulsion between aggregates has a strong effect on the apparent isothermal compressibility: it leads to a characteristic low-q depression, which cannot be interpreted as aggregate mass decrease in our data. In addition, the reinforcement effect of these silica structures in the SBR-matrix is characterized with oscillatory shear and described with a model based on the same aggregate compacity. Finally, our results show that it is possible to analyze the complex structure of interacting aggregates in nanocomposites of industrial origin in a self-consistent and quantitative manner.", "prompt": "Simplified silica (Zeosil 1165 MP) and SBR (140k carrying silanol end-groups) nanocomposites have been formulated by mixing of a reduced number of ingredients with respect to industrial applications. The thermo-mechanical history of the samples during the mixing process was monitored and adjusted to identical", "orig": "final temperatures. The filler structure on large scales up to micrometers was studied by transmission electron microscopy (TEM) and very small-angle X-ray scattering (SAXS). A complete quantitative model extending from the primary silica nanoparticle (of radius \\approx 10 nm), to nanoparticles aggregates, up to micrometer-sized", "generated": "Simplified silica (Zeosil 1165 MP) and SBR (140k carrying silanol end-groups) nanocomposites have been formulated by mixing of a reduced number of ingredients with respect to industrial applications. The thermo-mechanical history of the samples during the mixing process was monitored and adjusted to identical"}
{"full_text": "Collisions of actinide nuclei form, during very short times of few zs ($10^{-21}$ s), the heaviest ensembles of interacting nucleons available on Earth. Such collisions are used to produce super-strong electric fields by the huge number of interacting protons to test spontaneous positron-electron pair emission (vacuum decay) predicted by the quantum electrodynamics (QED) theory. Multi-nucleon transfer in actinide collisions could also be used as an alternative way to fusion in order to produce neutron-rich heavy and superheavy elements thanks to inverse quasifission mechanisms. Actinide collisions are studied in a dynamical quantum microscopic approach. The three-dimensional time-dependent Hartree-Fock (TDHF) code {\\textsc{tdhf3d}} is used with a full Skyrme energy density functional to investigate the time evolution of expectation values of one-body operators, such as fragment position and particle number. This code is also used to compute the dispersion of the particle numbers (e.g., widths of fragment mass and charge distributions) from TDHF transfer probabilities, on the one hand, and using the Balian-Veneroni variational principle, on the other hand. A first application to test QED is discussed. Collision times in $^{238}$U+$^{238}$U are computed to determine the optimum energy for the observation of the vacuum decay. It is shown that the initial orientation strongly affects the collision times and reaction mechanism. The highest collision times predicted by TDHF in this reaction are of the order of $\\sim4$ zs at a center of mass energy of 1200 MeV. According to modern calculations based on the Dirac equation, the collision times at $E_{cm}>1$ GeV are sufficient to allow spontaneous electron-positron pair emission from QED vacuum decay, in case of bare uranium ion collision. A second application of actinide collisions to produce neutron-rich transfermiums is discussed. A new inverse quasifission mechanism associated to a specific orientation of the nuclei is proposed to produce transfermium nuclei ($Z>100$) in the collision of prolate deformed actinides such as $^{232}$Th+$^{250}$Cf. The collision of the tip of one nucleus with the side of the other results in a nucleon flux toward the latter. The probability distributions for transfermium production in such a collision are computed. The produced nuclei are more neutron-rich than those formed in fusion reactions, thus, leading to more stable isotopes closer to the predicted superheavy island of stability. In addition to mass and charge dispersion, the Balian-Veneroni variational principle is used to compute correlations between $Z$ and $N$ distributions, which are zero in standard TDHF calculations.", "prompt": "Collisions of actinide nuclei form, during very short times of few zs ($10^{-21}$ s), the heaviest ensembles of interacting nucleons available on Earth. Such collisions are used to produce super-strong electric fields by the huge number of interacting protons to test spontaneous positron-electron pair emission", "orig": "(vacuum decay) predicted by the quantum electrodynamics (QED) theory. Multi-nucleon transfer in actinide collisions could also be used as an alternative way to fusion in order to produce neutron-rich heavy and superheavy elements thanks to inverse quasifission mechanisms. Actinide collisions are studied", "generated": "Collisions of actinide nuclei form, during very short times of few zs ($10^{-21}$ s), the heaviest ensembles of interacting nucleons available on Earth. Such collisions are used to produce super-strong electric fields by the huge number of interacting protons to test spontaneous positron-electron pair emission"}
{"full_text": "Mapping forest aboveground biomass (AGB) has become an important task, particularly for the reporting of carbon stocks and changes. AGB can be mapped using synthetic aperture radar data (SAR) or passive optical data. However, these data are insensitive to high AGB levels (\\textgreater{}150 Mg/ha, and \\textgreater{}300 Mg/ha for P-band), which are commonly found in tropical forests. Studies have mapped the rough variations in AGB by combining optical and environmental data at regional and global scales. Nevertheless, these maps cannot represent local variations in AGB in tropical forests. In this paper, we hypothesize that the problem of misrepresenting local variations in AGB and AGB estimation with good precision occurs because of both methodological limits (signal saturation or dilution bias) and a lack of adequate calibration data in this range of AGB values. We test this hypothesis by developing a calibrated regression model to predict variations in high AGB values (mean \\textgreater{}300 Mg/ha) in French Guiana by a methodological approach for spatial extrapolation with data from the optical geoscience laser altimeter system (GLAS), forest inventories, radar, optics, and environmental variables for spatial inter-and extrapolation. Given their higher point count, GLAS data allow a wider coverage of AGB values. We find that the metrics from GLAS footprints are correlated with field AGB estimations (R 2 =0.54, RMSE=48.3 Mg/ha) with no bias for high values. First, predictive models, including remote-sensing, environmental variables and spatial correlation functions, allow us to obtain \"wall-to-wall\" AGB maps over French Guiana with an RMSE for the in situ AGB estimates of ~51 Mg/ha and R${}^2$=0.48 at a 1-km grid size. We conclude that a calibrated regression model based on GLAS with dependent environmental data can produce good AGB predictions even for high AGB values if the calibration data fit the AGB range. We also demonstrate that small temporal and spatial mismatches between field data and GLAS footprints are not a problem for regional and global calibrated regression models because field data aim to predict large and deep tendencies in AGB variations from environmental gradients and do not aim to represent high but stochastic and temporally limited variations from forest dynamics. Thus, we advocate including a greater variety of data, even if less precise and shifted, to better represent high AGB values in global models and to improve the fitting of these models for high values.", "prompt": "Mapping forest aboveground biomass (AGB) has become an important task, particularly for the reporting of carbon stocks and changes. AGB can be mapped using synthetic aperture radar data (SAR) or passive optical data. However, these data are insensitive to high AGB levels (\\textgreater{}150 Mg/ha", "orig": ", and \\textgreater{}300 Mg/ha for P-band), which are commonly found in tropical forests. Studies have mapped the rough variations in AGB by combining optical and environmental data at regional and global scales. Nevertheless, these maps cannot represent local variations in AGB in tropical forests. In this paper, we hypo", "generated": "Mapping forest aboveground biomass (AGB) has become an important task, particularly for the reporting of carbon stocks and changes. AGB can be mapped using synthetic aperture radar data (SAR) or passive optical data. However, these data are insensitive to high AGB levels (>150 Mg/ha). As a"}
{"full_text": "In robotic radiation therapy, high-energy photon beams from different directions are directed at a target within the patient. Target motion can be tracked by robotic ultrasound and then compensated by synchronous beam motion. However, moving the beams may result in beams passing through the ultrasound transducer or the robot carrying it. While this can be avoided by pausing the beam delivery, the treatment time would increase. Typically, the beams are delivered in an order which minimizes the robot motion and thereby the overall treatment time. However, this order can be changed, i.e., instead of pausing beams, other feasible beam could be delivered. We address this problem of dynamically ordering the beams by applying a model checking paradigm to select feasible beams. Since breathing patterns are complex and change rapidly, any offline model would be too imprecise. Thus, model checking must be conducted online, predicting the patient's current breathing pattern for a short amount of time and checking which beams can be delivered safely. Monitoring the treatment delivery online provides the option to reschedule beams dynamically in order to avoid pausing and hence to reduce treatment time. While human breathing patterns are complex and may change rapidly, we need a model which can be verified quickly and use approximation by a superposition of sine curves. Further, we simplify the 3D breathing motion into separate 1D models. We compensate the simplification by adding noise inside the model itself. In turn, we synchronize between the multiple models representing the different spatial directions, the treatment simulation, and corresponding verification queries. Our preliminary results show a 16.02 % to 37.21 % mean improvement on the idle time compared to a static beam schedule, depending on an additional safety margin. Note that an additional safety margin around the ultrasound robot can decrease idle times but also compromises plan quality by limiting the range of available beam directions. In contrast, the approach using online model checking maintains the plan quality. Further, we compare to a naive machine learning approach that does not achieve its goals while being harder to reason about.", "prompt": "In robotic radiation therapy, high-energy photon beams from different directions are directed at a target within the patient. Target motion can be tracked by robotic ultrasound and then compensated by synchronous beam motion. However, moving the beams may result in beams passing through the ultrasound transducer or the robot carrying it. While this can be avoided", "orig": "by pausing the beam delivery, the treatment time would increase. Typically, the beams are delivered in an order which minimizes the robot motion and thereby the overall treatment time. However, this order can be changed, i.e., instead of pausing beams, other feasible beam could be delivered. We address this problem of", "generated": "In robotic radiation therapy, high-energy photon beams from different directions are directed at a target within the patient. Target motion can be tracked by robotic ultrasound and then compensated by synchronous beam motion. However, moving the beams may result in beams passing through the ultrasound transducer or the robot carrying it. While this can be avoided"}
{"full_text": "CANVAS caused by RFC1 biallelic expansions is a major cause of inherited sensory neuronopathy. Detection of RFC1 expansion is challenging and CANVAS can be associated with atypical features. We clinically and genetically characterized 50 patients, selected based on the presence of sensory neuronopathy confirmed by EMG. We screened RFC1 expansion by PCR, repeat-primed PCR, and Southern blotting of long-range PCR products, a newly developed method. Neuropathological characterization was performed on the brain and spinal cord of one patient. Most patients (88%) carried a biallelic (AAGGG)n expansion in RFC1. In addition to the core CANVAS phenotype (sensory neuronopathy, cerebellar syndrome, and vestibular impairment), we observed chronic cough (97%), oculomotor signs (85%), motor neuron involvement (55%), dysautonomia (50%), and parkinsonism (10%). Motor neuron involvement was found for 24 of 38 patients (63.1%). First motor neuron signs, such as brisk reflexes, extensor plantar responses, and/or spasticity, were present in 29% of patients, second motor neuron signs, such as fasciculations, wasting, weakness, or a neurogenic pattern on EMG in 18%, and both in 16%. Mixed motor and sensory neuronopathy was observed in 19% of patients. Among six non-RFC1 patients, one carried a heterozygous AAGGG expansion and a pathogenic variant in GRM1. Neuropathological examination of one RFC1 patient with an enriched phenotype, including parkinsonism, dysautonomia, and cognitive decline, showed posterior column and lumbar posterior root atrophy. Degeneration of the vestibulospinal and spinocerebellar tracts was mild. We observed marked astrocytic gliosis and axonal swelling of the synapse between first and second motor neurons in the anterior horn at the lumbar level. The cerebellum showed mild depletion of Purkinje cells, with empty baskets, torpedoes, and astrogliosis characterized by a disorganization of the Bergmann's radial glia. We found neuronal loss in the vagal nucleus. The pars compacta of the substantia nigra was depleted, with widespread Lewy bodies in the locus coeruleus, substantia nigra, hippocampus, entorhinal cortex, and amygdala. We propose new guidelines for the screening of RFC1 expansion, considering different expansion motifs. Here, we developed a new method to more easily detect pathogenic RFC1 expansions. We report frequent motor neuron involvement and different neuronopathy subtypes. Parkinsonism was more prevalent in this cohort than in the general population, 10% versus the expected 1% (p < .001). We describe, for the first time, the spinal cord pathology in CANVAS, showing the alteration of posterior columns and roots, astrocytic gliosis and axonal swelling, suggesting motor neuron synaptic dysfunction.", "prompt": "CANVAS caused by RFC1 biallelic expansions is a major cause of inherited sensory neuronopathy. Detection of RFC1 expansion is challenging and CANVAS can be associated with atypical features. We clinically and genetically characterized 50 patients, selected based on the presence of sensory neuronopathy confirmed by EMG. We", "orig": "screened RFC1 expansion by PCR, repeat-primed PCR, and Southern blotting of long-range PCR products, a newly developed method. Neuropathological characterization was performed on the brain and spinal cord of one patient. Most patients (88%) carried a biallelic (AAGGG)n expansion in RFC", "generated": "CANVAS caused by RFC1 biallelic expansions is a major cause of inherited sensory neuronopathy. Detection of RFC1 expansion is challenging and CANVAS can be associated with atypical features. We clinically and genetically characterized 50 patients, selected based on the presence of sensory neuronopathy confirmed by EMG. We"}
{"full_text": "(l) I have enough evidence to render the sentence S probable. (la) So, relative to what I know, it is rational of me to believe S. (2) Now that I have more evidence, S may no longer be probable. (2a) So now, relative to what I know, it is not rational of me to believe S. These seem a perfectly ordinary, common sense, pair of situations. Generally and vaguely, I take them to embody what I shall call probabilistic inference. This form of inference is clearly non-monotonic. Relatively few people have taken this form of inference, based on high probability, to serve as a foundation for non-monotonic logic or for a logical or defeasible inference. There are exceptions: Jane Nutter [16] thinks that sometimes probability has something to do with non-monotonic reasoning. Judea Pearl [ 17] has recently been exploring the possibility. There are any number of people whom one might call probability enthusiasts who feel that probability provides all the answers by itself, with no need of help from logic. Cheeseman [1], Henrion [5] and others think it useful to look at a distribution of probabilities over a whole algebra of statements, to update that distribution in the light of new evidence, and to use the latest updated distribution of probability over the algebra as a basis for planning and decision making. A slightly weaker form of this approach is captured by Nilsson [15], where one assumes certain probabilities for certain statements, and infers the probabilities, or constraints on the probabilities of other statement. None of this corresponds to what I call probabilistic inference. All of the inference that is taking place, either in Bayesian updating, or in probabilistic logic, is strictly deductive. Deductive inference, particularly that concerned with the distribution of classical probabilities or chances, is of great importance. But this is not to say that there is no important role for what earlier logicians have called \"ampliative\" or \"inductive\" or \"scientific\" inference, in which the conclusion goes beyond the premises, asserts more than do the premises. This depends on what David Israel [6] has called \"real rules of inference\". It is characteristic of any such logic or inference procedure that it can go wrong: that statements accepted at one point may be rejected at a later point. Research underlying the results reported here has been partially supported by the Signals Warfare Center of the United States Army.", "prompt": "(l) I have enough evidence to render the sentence S probable. (la) So, relative to what I know, it is rational of me to believe S. (2) Now that I have more evidence, S may no longer be probable. (2a) So now, relative to what I know,", "orig": "it is not rational of me to believe S. These seem a perfectly ordinary, common sense, pair of situations. Generally and vaguely, I take them to embody what I shall call probabilistic inference. This form of inference is clearly non-monotonic. Relatively few people have taken this form of inference, based on", "generated": "(l) I have enough evidence to render the sentence S probable. (la) So, relative to what I know, it is rational of me to believe S. (2) Now that I have more evidence, S may no longer be probable. (2a) So now, relative to what I know,"}
{"full_text": "Let $G$ be a simple, undirected, finite graph with vertex set $V(G)$ and edge set $E(G)$. A $k$-dimensional box is a Cartesian product of closed intervals $[a_1,b_1]\\times [a_2,b_2]\\times...\\times [a_k,b_k]$. The {\\it boxicity} of $G$, $\\boxi(G)$ is the minimum integer $k$ such that $G$ can be represented as the intersection graph of $k$-dimensional boxes, i.e. each vertex is mapped to a $k$-dimensional box and two vertices are adjacent in $G$ if and only if their corresponding boxes intersect. Let $\\poset=(S,P)$ be a poset where $S$ is the ground set and $P$ is a reflexive, anti-symmetric and transitive binary relation on $S$. The dimension of $\\poset$, $\\dim(\\poset)$ is the minimum integer $t$ such that $P$ can be expressed as the intersection of $t$ total orders. Let $G_\\poset$ be the \\emph{underlying comparability graph} of $\\poset$, i.e. $S$ is the vertex set and two vertices are adjacent if and only if they are comparable in $\\poset$. It is a well-known fact that posets with the same underlying comparability graph have the same dimension. The first result of this paper links the dimension of a poset to the boxicity of its underlying comparability graph. In particular, we show that for any poset $\\poset$, $\\boxi(G_\\poset)/(\\chi(G_\\poset)-1) \\le \\dim(\\poset)\\le 2\\boxi(G_\\poset)$, where $\\chi(G_\\poset)$ is the chromatic number of $G_\\poset$ and $\\chi(G_\\poset)\\ne1$. It immediately follows that if $\\poset$ is a height-2 poset, then $\\boxi(G_\\poset)\\le \\dim(\\poset)\\le 2\\boxi(G_\\poset)$ since the underlying comparability graph of a height-2 poset is a bipartite graph. The second result of the paper relates the boxicity of a graph $G$ with a natural partial order associated with the \\emph{extended double cover} of $G$, denoted as $G_c$: Note that $G_c$ is a bipartite graph with partite sets $A$ and $B$ which are copies of $V(G)$ such that corresponding to every $u\\in V(G)$, there are two vertices $u_A\\in A$ and $u_B\\in B$ and $\\{u_A,v_B\\}$ is an edge in $G_c$ if and only if either $u=v$ or $u$ is adjacent to $v$ in $G$. Let $\\poset_c$ be the natural height-2 poset associated with $G_c$ by making $A$ the set of minimal elements and $B$ the set of maximal elements. We show that $\\frac{\\boxi(G)}{2} \\le \\dim(\\poset_c) \\le 2\\boxi(G)+4$. These results have some immediate and significant consequences. The upper bound $\\dim(\\poset)\\le 2\\boxi(G_\\poset)$ allows us to derive hitherto unknown upper bounds for poset dimension such as $\\dim(\\poset)\\le 2\\tw(G_\\poset)+4$, since boxicity of any graph is known to be at most its $\\tw+2$. In the other direction, using the already known bounds for partial order dimension we get the following: (1) The boxicity of any graph with maximum degree $\\Delta$ is $O(\\Delta\\log^2\\Delta)$ which is an improvement over the best known upper bound of $\\Delta^2+2$. (2) There exist graphs with boxicity $\\Omega(\\Delta\\log\\Delta)$. This disproves a conjecture that the boxicity of a graph is $O(\\Delta)$. (3) There exists no polynomial-time algorithm to approximate the boxicity of a bipartite graph on $n$ vertices with a factor of $O(n^{0.5-\\epsilon})$ for any $\\epsilon>0$, unless $NP=ZPP$.", "prompt": "Let $G$ be a simple, undirected, finite graph with vertex set $V(G)$ and edge set $E(G)$. A $k$-dimensional box is a Cartesian product of closed intervals $[a_1,b_1]\\times [a_2,b_2]\\times...\\", "orig": "times [a_k,b_k]$. The {\\it boxicity} of $G$, $\\boxi(G)$ is the minimum integer $k$ such that $G$ can be represented as the intersection graph of $k$-dimensional boxes, i.e. each vertex is mapped to a $k$-", "generated": "A $k$-dimensional box is a Cartesian product of closed intervals $[a_1,b_1]\\times [a_2,b_2]\\times...\\times [a_k,b_k$ where $a_i,b_i \\in \\mathbb{R}$ for each $1\\leq"}
{"full_text": "The aim of this work is to study the asymptotic behavior of a structure made of plates of thickness $2\\delta$ when $\\delta\\to 0$. This study is carried on within the frame of linear elasticity by using the unfolding method. It is based on several decompositions of the structure displacements and on the passing to the limit in fixed domains. We begin with studying the displacements of a plate. We show that any displacement is the sum of an elementary displacement concerning the normal lines on the middle surface of the plate and a residual displacement linked to these normal lines deformations. An elementary displacement is linear with respect to the variable $x$3. It is written $U(^x)+R(^x)\\land x3e3$ where U is a displacement of the mid-surface of the plate. We show a priori estimates and convergence results when $\\delta \\to 0$. We characterize the limits of the unfolded displacements of a plate as well as the limits of the unfolded of the strained tensor. Then we extend these results to the structures made of plates. We show that any displacement of a structure is the sum of an elementary displacement of each plate and of a residual displacement. The elementary displacements of the structure (e.d.p.s.) coincide with elementary rods displacements in the junctions. Any e.d.p.s. is given by two functions belonging to $H1(S;R3)$ where S is the skeleton of the structure (the plates mid-surfaces set). One of these functions : U is the skeleton displacement. We show that U is the sum of an extensional displacement and of an inextensional one. The first one characterizes the membrane displacements and the second one is a rigid displacement in the direction of the plates and it characterizes the plates flexion. Eventually we pass to the limit as $\\delta \\to 0$ in the linearized elasticity system, on the one hand we obtain a variational problem that is satisfied by the limit extensional displacement, and on the other hand, a variational problem satisfied by the limit of inextensional displacements.", "prompt": "The aim of this work is to study the asymptotic behavior of a structure made of plates of thickness $2\\delta$ when $\\delta\\to 0$. This study is carried on within the frame of linear elasticity by using the unfolding method. It is based on several decompositions of the structure displacements and", "orig": "on the passing to the limit in fixed domains. We begin with studying the displacements of a plate. We show that any displacement is the sum of an elementary displacement concerning the normal lines on the middle surface of the plate and a residual displacement linked to these normal lines deformations. An elementary displacement is linear with respect to", "generated": "The aim of this work is to study the asymptotic behavior of a structure made of plates of thickness $2\\delta$ when $\\delta\\to 0$. This study is carried on within the frame of linear elasticity by using the unfolding method. It is based on several decompositions of the structure displacements and"}
{"full_text": "Single electron sources enable electron quantum optics experiments where single electrons emitted in a ballistic electronic interferometer plays the role of a single photons emitted in an optical medium in Quantum Optics. A qualitative step has been made with the recent generation of single charge levitons obtained by applying Lorentzian voltage pulse on the contact of the quantum conductor. Simple to realize and operate, the source emits electrons in the form of striking minimal excitation states called levitons. We review the striking properties of levitons and their possible applications in quantum physics to electron interferometry and entanglement. W Schematic generation of time resolved single charges called levitons using Lorentzian voltage pulses applied on a contact. A Quantum Point Contact is used to partition the levitons for further analysis. Injecting levitons on opposite contacts with a delay $\\\\tau$ enables to probe electronic like Hong Ou Mandel correlations. Copyright line will be provided by the publisher 1 Single electron sources In this introduction, we will distinguish single charge sources from coherent single electrons sources. The former have been developed for quantum metrology where the goal is to transfer an integer charge at high frequency f through a conductor with good accuracy to realize a quantized current source whose current I = ef shows metrological accuracy. The latter, the coherent single electrons source, aims at emitting (injecting) a single electron whose wave-function is well defined and controlled to realize further single electron coherent manipulation via quantum gates. The gates are provided by electronic beam-splitters made with Quantum Point Contacts or provided by electronic Mach-Zehnder and Fabry-Prot interferometers. Here it is important that the injected single electron is the only excitation created in the conductor. The frequency f of injection is not chosen to have a large current, as current accuracy is not the goal, but only to get sufficient statistics on the electron transfer events to extract physical information. 1.1 single charge sources for current standards The first manipulation of single charges trace back to the early 90's where physicists took advantage of charge quan-tization of a submicronic metallic island nearly isolated from leads by tunnel barriers. The finite energy E C = e 2 /2C to charge the small capacitor C with a single charge being larger than temperature (typically one kelvin for", "prompt": "Single electron sources enable electron quantum optics experiments where single electrons emitted in a ballistic electronic interferometer plays the role of a single photons emitted in an optical medium in Quantum Optics. A qualitative step has been made with the recent generation of single charge levitons obtained by applying Lorentzian voltage pulse on the contact", "orig": "of the quantum conductor. Simple to realize and operate, the source emits electrons in the form of striking minimal excitation states called levitons. We review the striking properties of levitons and their possible applications in quantum physics to electron interferometry and entanglement. W Schematic generation of time resolved single charges called", "generated": "Single electron sources enable electron quantum optics experiments where single electrons emitted in a ballistic electronic interferometer plays the role of a single photons emitted in an optical medium in Quantum Optics. A qualitative step has been made with the recent generation of single charge levitons obtained by applying Lorentzian voltage pulse on the contact"}
{"full_text": "As the nearest known AGB star (d=64pc) and one of the brightest (mK-2), L2 Pup is a particularly interesting benchmark object to monitor the final stages of stellar evolution. We report new lucky imaging observations of this star with the VLT/NACO adaptive optics system in twelve narrow band filters covering the 1.0-4.0 microns wavelength range. These diffraction limited images reveal an extended circumstellar dust lane in front of the star, that exhibits a high opacity in the J band and becomes translucent in the H and K bands. In the L band, extended thermal emission from the dust is detected. We reproduce these observations using Monte-Carlo radiative transfer modeling of a dust disk with the RADMC-3D code. We also present new interferometric observations with the VLTI/VINCI and MIDI instruments. We measure in the K band an upper limit to the limb-darkened angular diameter of theta_LD = 17.9 +/- 1.6 mas, converting to a maximum linear radius of R = 123 +/- 14 Rsun. Considering the geometry of the extended K band emission in the NACO images, this upper limit is probably close to the actual angular diameter of the star. The position of L2 Pup in the Herzsprung-Russell diagram indicates that this star has a mass around 2 Msun and is probably experiencing an early stage of the asymptotic giant branch. We do not detect any stellar companion of L2 Pup in our adaptive optics and interferometric observations, and we attribute its apparent astrometric wobble in the Hipparcos data to variable lighting effects on its circumstellar material. We however do not exclude the presence of a binary companion, as the large loop structure extending to more than 10 AU to the North-East of the disk in our L band images may be the result of interaction between the stellar wind of L2 Pup and a hidden secondary object. The geometric configuration that we propose, with a large dust disk seen almost edge-on, appears particularly favorable to test and develop our understanding of the formation of bipolar nebulae.", "prompt": "As the nearest known AGB star (d=64pc) and one of the brightest (mK-2), L2 Pup is a particularly interesting benchmark object to monitor the final stages of stellar evolution. We report new lucky imaging observations of this star with the VLT/NACO adaptive optics system in", "orig": "twelve narrow band filters covering the 1.0-4.0 microns wavelength range. These diffraction limited images reveal an extended circumstellar dust lane in front of the star, that exhibits a high opacity in the J band and becomes translucent in the H and K bands. In the L band, extended thermal emission", "generated": "As the nearest known AGB star (d=64pc) and one of the brightest (mK-2), L2 Pup is a particularly interesting benchmark object to monitor the final stages of stellar evolution. We report new lucky imaging observations of this star with the VLT/NACO adaptive optics system in"}
{"full_text": "We describe astrophysical processes around a black hole keeping primarily the physics of accretion in mind. In Section 1, we briefly discuss the formation, evolution and detection of black holes. We also discuss the difference of flow properties around a black hole and a Newtonian star. In Section 2, we present past and present developments in the study of spherically accreting flows. We study the properties of Bondi flow with and without radiative transfer. In the presence of significant angular momentum, which is especially true in a binary system, matter will be accreted as a thin Keplerian disk. In Section 3, we discuss a large number of models of these disks including the more popular standard disk model. We present magnetized disk models as well. Since the angular momentum is high in these systems, rotational motion is the most dominant component compared to the radial or the vertical velocity components. In Section 4, we study thick disk models which are of low angular momentum but still have no significant radial motion. The accretion rates could be very high causing the flow to become radiation dominated and the disk to be geometrically thick. For low accretion rates, ion pressure supported disks are formed. In Section 5, we extensively discuss the properties of transonic flows which has with sub-Keplerian angular momentum. In the absence of shock discontinuities, these sub-Keplerian flows are basically advecting, similar to Bondi flows, close to the black holes, though far away they match Keplerian or sub-Keplerian disks. In presence of shocks, the post-shock flow becomes rotation dominated similar to thick disks. In Section 6, we present results of important numerical simulations of accretion flows. Significant results from the studies of evolution of viscous transonic flows are reported. In Section 7, we discuss some observational evidences of the black hole accretion. We also present a detailed model of a generalized accretion disk and present its spectra and compare with observations. In Section 8, we summarize the review and make concluding remarks.", "prompt": "We describe astrophysical processes around a black hole keeping primarily the physics of accretion in mind. In Section 1, we briefly discuss the formation, evolution and detection of black holes. We also discuss the difference of flow properties around a black hole and a Newtonian star. In Section 2, we", "orig": "present past and present developments in the study of spherically accreting flows. We study the properties of Bondi flow with and without radiative transfer. In the presence of significant angular momentum, which is especially true in a binary system, matter will be accreted as a thin Keplerian disk. In Section", "generated": "We describe astrophysical processes around a black hole keeping primarily the physics of accretion in mind. In Section 1, we briefly discuss the formation, evolution and detection of black holes. We also discuss the difference of flow properties around a black hole and a Newtonian star. In Section 2, we"}
{"full_text": "Ranking schemes drive many real-world decisions, like, where to study, whom to hire, what to buy, etc. Many of these decisions often come with high consequences. For example, a university can be deemed less prestigious if not featured in a top-k list, and consumers might not even explore products that do not get recommended to buyers. At the heart of most of these decisions are opaque ranking schemes, which dictate the ordering of data entities, but their internal logic is inaccessible or proprietary. Drawing inferences about the ranking differences is like a guessing game to the stakeholders, like, the rankees (i.e., the entities who are ranked, like product companies) and the decision-makers (i.e., who use the rankings, like buyers). In this paper, we aim to enable transparency in ranking interpretation by using algorithmic rankers that learn from available data and by enabling human reasoning about the learned ranking differences using explainable AI (XAI) methods. To realize this aim, we leverage the exploration-explanation paradigm of human-data interaction to let human stakeholders explore subsets and groupings of complex multi-attribute ranking data using visual explanations of model fit and attribute influence on rankings. We realize this explanation paradigm for transparent ranking interpretation in TRIVEA, a visual analytic system that is fueled by: i) visualizations of model fit derived from algorithmic rankers that learn the associations between attributes and rankings from available data and ii) visual explanations derived from XAI methods that help abstract important patterns, like, the relative influence of attributes in different ranking ranges. Using TRIVEA, end users not trained in data science have the agency to transparently reason about the global and local behavior of the rankings without the need to open black-box ranking models and develop confidence in the resulting attribute-based inferences. We demonstrate the efficacy of TRIVEA using multiple usage scenarios and subjective feedback from researchers with diverse domain expertise. Keywords: Visual Analytics, Learning-to-Rank, Explainable ML, Ranking", "prompt": "Ranking schemes drive many real-world decisions, like, where to study, whom to hire, what to buy, etc. Many of these decisions often come with high consequences. For example, a university can be deemed less prestigious if not featured in a top-k list, and consumers might not even explore products that do", "orig": "not get recommended to buyers. At the heart of most of these decisions are opaque ranking schemes, which dictate the ordering of data entities, but their internal logic is inaccessible or proprietary. Drawing inferences about the ranking differences is like a guessing game to the stakeholders, like, the rankees (i.e., the entities who", "generated": "Ranking schemes drive many real-world decisions, like, where to study, whom to hire, what to buy, etc. Many of these decisions often come with high consequences. For example, a university can be deemed less prestigious if not featured in a top-k list, and consumers might not even explore products that don"}
{"full_text": "We consider a class of mean field games in which the agents interact through both their states and controls, and we focus on situations in which a generic agent tries to adjust her speed (control) to an average speed (the average is made in a neighborhood in the state space). In such cases, the monotonicity assumptions that are frequently made in the theory of mean field games do not hold, and uniqueness cannot be expected in general. Such model lead to systems of forward-backward nonlinear nonlocal parabolic equations; the latter are supplemented with various kinds of boundary conditions, in particular Neumann-like boundary conditions stemming from reflection conditions on the underlying controled stochastic processes. The present work deals with numerical approximations of the above mentioned systems. After describing the finite difference scheme, we propose an iterative method for solving the systems of nonlinear equations that arise in the discrete setting; it combines a continuation method, Newton iterations and inner loops of a bigradient like solver. The numerical method is used for simulating two examples. We also make experiments on the behaviour of the iterative algorithm when the parameters of the model vary. The theory of mean field games, (MFGs for short), aims at studying deterministic or stochastic differential games (Nash equilibria) as the number of agents tends to infinity. It supposes that the rational agents are indistinguishable and individually have a negligible influence on the game, and that each individual strategy is influenced by some averages of quantities depending on the states (or the controls as in the present work) of the other agents. MFGs have been introduced in the pioneering works of J-M. Lasry and P-L. Lions [17, 18, 19]. Independently and at approximately the same time, the notion of mean field games arose in the engineering literature, see the works of M.Y. Huang, P.E. Caines and R.Malham{\\'e} [14, 15]. The present work deals with numerical approximations of mean field games in which the agents interact through both their states and controls; it follows a more theoretical work by the second author, [16], which is devoted to the mathematical analysis of the related systems of nonlocal partial differential equations. There is not much literature on MFGs in which the agents also interact through their controls, see [13, 12, 8, 10, 7, 16]. To stress the fact that the latter situation is considered, we will sometimes use the terminology mean field games of control and the acronym MFGC.", "prompt": "We consider a class of mean field games in which the agents interact through both their states and controls, and we focus on situations in which a generic agent tries to adjust her speed (control) to an average speed (the average is made in a neighborhood in the state space). In such cases, the monotonicity", "orig": "assumptions that are frequently made in the theory of mean field games do not hold, and uniqueness cannot be expected in general. Such model lead to systems of forward-backward nonlinear nonlocal parabolic equations; the latter are supplemented with various kinds of boundary conditions, in particular Neumann-like boundary conditions stemming from reflection conditions on the", "generated": "We consider a class of mean field games in which the agents interact through both their states and controls, and we focus on situations in which a generic agent tries to adjust her speed (control) to an average speed (the average is made in a neighborhood in the state space). In such cases, the monotonicity"}
{"full_text": "Angiogenesis, the formation of new vessels, is one of the key mechanisms in tumor development and an appealing target for therapy. Non-invasive, high-resolution, high sensitivity, quantitative 3D imaging techniques are required to correctly depict tumor heterogeneous vasculature over time. Ultrafast Doppler was recently introduced and provides an unprecedented combination of resolution, penetration depth and sensitivity without requiring any contrast agents. The technique was further extended to 3D with Ultrafast Doppler Tomography (UFD-T). In this work, UFD-T was applied to the monitoring of tumor angiogenesis in vivo providing structural and functional information at different stages of development. UFD-T volume renderings showed that our murine model's vasculature stems from pre-existing vessels and sprouts to perfuse the whole volume as the tumor grows until a critical size is reached. Then, as the network becomes insufficient, the tumor core is no longer irrigated because the vasculature is mainly concentrated in the periphery. In addition to spatial distribution and growth patterns, UFD-T allowed a quantitative analysis of vessel size and length, revealing that the diameter-distribution of vessels remained relatively constant throughout tumor growth. The network is dominated by small vessels at all stages of tumor development with more than 74% of the vessels less than 200 $\\mu$m in diameter. This study also showed that cumulative vessel length is more closely related to tumor radius than volume, indicating that the vascularization becomes insufficient when a critical mass is reached. UFD-T was also compared with dynamic contrast-enhanced ultrasound (DCE-US) and shown to provide complementary information regarding the link between structure and perfusion. In conclusion, UFD-T is capable of an in vivo quantitative assessment of the development of tumor vasculature (vessels with blood speed >1mm/s (sensitivity limit) assessed with a resolution limit of 80 $\\mu$m) in 3D. The technique has very interesting potential as a tool for treatment monitoring, response assessment and treatment planning for optimal drug efficiency.", "prompt": "Angiogenesis, the formation of new vessels, is one of the key mechanisms in tumor development and an appealing target for therapy. Non-invasive, high-resolution, high sensitivity, quantitative 3D imaging techniques are required to correctly depict tumor heterogeneous vasculature over time. Ultrafast Doppler was recently introduced and", "orig": "provides an unprecedented combination of resolution, penetration depth and sensitivity without requiring any contrast agents. The technique was further extended to 3D with Ultrafast Doppler Tomography (UFD-T). In this work, UFD-T was applied to the monitoring of tumor angiogenesis in vivo providing structural and functional information at different stages", "generated": "...has shown great potential in providing real-time imaging of blood flow within the microvasculature of tumors. However, a more detailed understanding of tumor angiogenesis at the micro-scale is necessary for targeted therapy. In this regard, techniques that can image the formation and remodeling of new vessels within tumors are highly valuable."}
{"full_text": "This article is devoted to the study of the following semilinear equation with measure data which originates in the gravitational Maxwell gauged $O(3)$ sigma model, $$-\\Delta u + A_0(\\prod^k_{j=1}|x-p_j|^{2n_j} )^{-a} \\frac{e^u}{(1+e^u)^{1+a}} = 4\\pi\\sum_{j=1}^k n_j\\delta_{p_j} - 4\\pi\\sum^l_{j=1}m_j\\delta_{q_j} \\quad{\\rm in}\\;\\; \\mathbb{R}^2.\\qquad(E)$$ In this equation the $\\{\\delta_{p_j}\\}_{j=1}^k$ (resp. $\\{\\delta_{q_j}\\}_{j=1}^l$ ) are Dirac masses concentrated at the points $\\{p_j\\}_{j=1}^k$, (resp. $\\{q_j\\}_{j=1}^l$), $n_j$ and $m_j$ are positive integers, and $a$ is a nonnegative real number. We set $ N=\\sum^k_{j=1}n_j $ and $M= \\sum^l_{j=1}m_j$. In previous works \\cite{C,Y2}, some qualitative properties of solutions of $(E)$ with $a=0$ have been established. Our aim in this article is to study the more general case where $a>0$. The additional difficulties of this case come from the fact that the nonlinearity is no longer monotone and the data are signed measures. As a consequence we cannot anymore construct directly the solutions by the monotonicity method combined with the supersolutions and subsolutions technique. Instead we develop a new and self-contained approach which enables us to emphasize the role played by the gravitation in the gauged $O(3)$ sigma model. Without the gravitational term, i.e. if $a=0$, problem $(E)$ has a layer's structure of solutions $\\{u_\\beta\\}_{\\beta\\in(-2(N-M),\\, -2]}$, where $u_\\beta$ is the unique non-topological solution such that $u_{\\beta}=\\beta\\ln |x|+O(1)$ for $-2(N-M)<\\beta<-2$ and $u_{-2}=-2\\ln |x|-2\\ln\\ln |x|+O(1)$ at infinity respectively. On the contrary, when $a>0$, the set of solutions to problem $(E)$ has a much richer structure: besides the topological solutions, there exists a sequence of non-topological solutions in type I, i.e. such that $u $ tends to $-\\infty$ at infinity, and of non-topological solutions of type II, which tend to $\\infty$ at infinity. The existence of these types of solutions depends on the values of the parameters $N,\\, M,\\, \\beta$ and on the gravitational interaction associated to $a$.", "prompt": "This article is devoted to the study of the following semilinear equation with measure data which originates in the gravitational Maxwell gauged $O(3)$ sigma model, $$-\\Delta u + A_0(\\prod^k_{j=1}|x-p_j|^{2n_j} )^{-a}", "orig": "\\frac{e^u}{(1+e^u)^{1+a}} = 4\\pi\\sum_{j=1}^k n_j\\delta_{p_j} - 4\\pi\\sum^l_{j=1}m_j\\delta_{q_j} \\quad", "generated": "This article is devoted to the study of the following semilinear equation with measure data which originates in the gravitational Maxwell gauged $O(3)$ sigma model, $$-\\Delta u + A_0(\\prod^k_{j=1}|x-p_j|^{2n_j} )^{-a}"}
{"full_text": "Voting is a general method for aggregating the preferences of multiple agents. Each agent ranks all the possible alternatives, and based on this, an aggregate ranking of the alternatives (or at least a winning alternative) is produced. However, when there are many alternatives, it is impractical to simply ask agents to report their complete preferences. Rather, the agents preferences, or at least the relevant parts thereof, need to be elicited. This is done by asking the agents a (hopefully small) number of simple queries about their preferences, such as comparison queries, which ask an agent to compare two of the alternatives. Prior work on preference elicitation in voting has focused on the case of unrestricted preferences. It has been shown that in this setting, it is sometimes necessary to ask each agent (almost) as many queries as would be required to determine an arbitrary ranking of the alternatives. In contrast, in this paper, we focus on single-peaked preferences. We show that such preferences can be elicited using only a linear number of comparison queries, if either the order with respect to which preferences are single-peaked is known, or at least one other agents complete preferences are known. We show that using a sublinear number of queries does not suffice. We also consider the case of cardinally single-peaked preferences. For this case, we show that if the alternatives cardinal positions are known, then an agents preferences can be elicited using only a logarithmic number of queries; however, we also show that if the cardinal positions are not known, then a sublinear number of queries does not suffice. We present experimental results for all elicitation algorithms. We also consider the problem of only eliciting enough information to determine the aggregate ranking, and show that even for this more modest objective, a sublinear number of queries per agent does not suffice for known ordinal or unknown cardinal positions. Finally, we discuss whether and how these techniques can be applied when preferences are almost single-peaked.", "prompt": "Voting is a general method for aggregating the preferences of multiple agents. Each agent ranks all the possible alternatives, and based on this, an aggregate ranking of the alternatives (or at least a winning alternative) is produced. However, when there are many alternatives, it is impractical to simply ask agents to report", "orig": "their complete preferences. Rather, the agents preferences, or at least the relevant parts thereof, need to be elicited. This is done by asking the agents a (hopefully small) number of simple queries about their preferences, such as comparison queries, which ask an agent to compare two of the alternatives. Prior work on preference", "generated": "Voting is a general method for aggregating the preferences of multiple agents. Each agent ranks all the possible alternatives, and based on this, an aggregate ranking of the alternatives (or at least a winning alternative) is produced. However, when there are many alternatives, it is impractical to simply ask agents to report"}
{"full_text": "Context: An island parser reads an input text and builds the parse (or abstract syntax) tree of only the programming constructs of interest in the text. These constructs are called islands and the rest of the text is called water, which the parser ignores and skips over. Since an island parser does not have to parse all the details of the input, it is often easy to develop but still useful enough for a number of software engineering tools. When a parser generator is used, the developer can implement an island parser by just describing a small number of grammar rules, for example, in Parsing Expression Grammar (PEG). Inquiry: In practice, however, the grammar rules are often complicated since the developer must define the water inside the island; otherwise, the island parsing will not reduce the total number of grammar rules. When describing the grammar rules for such water, the developer must consider other rules and enumerate a set of symbols, which we call alternative symbols. Due to this difficulty, island parsing seems to be not widely used today despite its usefulness in many applications. Approach: This paper proposes the lake symbols for addressing this difficulty in developing an island parser. It also presents an extension to PEG for supporting the lake symbols. The lake symbols automate the enumeration of the alternative symbols for the water inside an island. The paper proposes an algorithm for translating the extended PEG to the normal PEG, which can be given to an existing parser generator based on PEG. Knowledge: The user can use lake symbols to define water without specifying each alternative symbol. Our algorithms can calculate all alternative symbols for a lake symbol, based on where the lake symbol is used in the grammar. Grounding: We implemented a parser generator accepting our extended PEG and implemented 36 island parsers for Java and 20 island parsers for Python. Our experiments show that the lake symbols reduce 42 % of grammar rules for Java and 89 % of rules for Python on average, excluding the case where islands are expressions. Importance: This work eases the use of island parsing. Lake symbols enable the user to define the water inside the island simpler than before. Defining water inside the island is essential to apply island parsing for practical programming languages.", "prompt": "Context: An island parser reads an input text and builds the parse (or abstract syntax) tree of only the programming constructs of interest in the text. These constructs are called islands and the rest of the text is called water, which the parser ignores and skips over. Since an island parser does not have to parse all", "orig": "the details of the input, it is often easy to develop but still useful enough for a number of software engineering tools. When a parser generator is used, the developer can implement an island parser by just describing a small number of grammar rules, for example, in Parsing Expression Grammar (PEG). Inquiry: In practice, however", "generated": "An island parser reads an input text and builds the parse (or abstract syntax) tree of only the programming constructs of interest in the text. These constructs are called islands and the rest of the text is called water, which the parser ignores and skips over. Since an island parser does not have to parse all of the"}
{"full_text": "Objective: Realistic tissue-mimicking phantoms are essential for the development, the investigation and the calibration of medical imaging techniques and protocols. Because it requires taking both mechanical and imaging properties into account, the development of robust, calibrated phantoms is a major challenge in elastography. Soft polyvinyl chloride gels in a liquid plasticizer (plastisol or PVCP) have been proposed as soft tissue-mimicking phantoms (TMP) for elasticity imaging. PVCP phantoms are relatively low-cost and can be easily stored over long time periods without any specific requirements. In this work, the preparation of a PVCP gel phantom for both MR and ultrasoundelastography is proposed and its acoustic, NMR and mechanical properties are studied.Material and methods: The acoustic and magnetic resonance imaging properties of PVCP are measured for different mass ratios between ultrasound speckle particles and PVCP solution, and between resin and plasticizer. The linear mechanical properties of plastisol samples are then investigated over time using not only indentation tests, but also MR and ultrasound-elastography clinical protocols. These properties are compared to typical values reported for biological soft tissues and to the values found in the literature for PVCP gels.Results and conclusions: After a period of two weeks, the mechanical properties of the plastisol samples measured with indentation testing are stable for at least the following 4 weeks (end of follow-up period 43 days after gelation-fusion). Neither the mechanical nor the NMR properties of plastisol gels were found to be affected by the addition of cellulose as acoustic speckle. Mechanical properties of the proposed gels were successfully characterized by clinical, commercially-available MR Elastography and sonoelastography protocols. PVCP with a mass ratio of ultrasound speckle particles of 0.6% to 0.8% and a mass ratio between resin and plasticizer between 50 and 70% appears as a good TMP candidate that can be used with both MR and ultrasound-based elastography methods.", "prompt": "Objective: Realistic tissue-mimicking phantoms are essential for the development, the investigation and the calibration of medical imaging techniques and protocols. Because it requires taking both mechanical and imaging properties into account, the development of robust, calibrated phantoms is a major challenge in elastography. Soft polyvinyl chloride", "orig": "gels in a liquid plasticizer (plastisol or PVCP) have been proposed as soft tissue-mimicking phantoms (TMP) for elasticity imaging. PVCP phantoms are relatively low-cost and can be easily stored over long time periods without any specific requirements. In this work, the preparation of", "generated": "Objective: Realistic tissue-mimicking phantoms are essential for the development, the investigation and the calibration of medical imaging techniques and protocols. Because it requires taking both mechanical and imaging properties into account, the development of robust, calibrated phantoms is a major challenge in elastography. Soft polyvinyl chloride"}
{"full_text": "An expression of the piano soundboard mechanical mobility (in the direction normal to the soundboard) depending on a small number of parameters and valid up to several kHz is given in this communication. Up to 1.1 kHz, our experimental and numerical investigations confirm previous results showing that the soundboard behaves like a homogeneous plate with isotropic properties and clamped boundary conditions. Therefore, according to the Skudrzyk mean-value theorem (Skudrzyk 1980), only the mass of the structure M, the modal density n(f), and the mean loss factor eta(f), are needed to express the average driving point mobility. Moreover, the expression of the envelope - resonances and antiresonances - of the mobility can be derived, according to (Langley 1994). We measured the modal loss factor and the modal density of the soundboard of an upright piano in playing condition, in an anechoic environment. The measurements could be done up to 2.5 kHz, with a novel high-resolution modal analysis technique (see the ICA companion-paper, Ege and Boutillon (2010)). Above 1.1 kHz, the change in the observed modal density together with numerical simulations confirm Berthaut's finding that the waves in the soundboard are confined between adjacent ribs (Berthaut et al. 2003). Extending the Skudrzyk and Langley approaches, we synthesize the mechanical mobility at the bridge up to 2.5 kHz. The validity of the computation for an extended spectral domain is discussed. It is also shown that the evolution of the modal density with frequency is consistent with the rise of mobility (fall of impedance) in this frequency range and that both are due to the inter-rib effect appearing when the half-wavelength becomes equal to the rib spacing. Results match previous observations by Wogram (1980), Conklin (1996), Giordano (1998), Nakamura (1983) and could be used for numerical simulations for example. This approach avoids the detailed description of the soundboard, based on a very high number of parameters. However, it can be used to predict the changes of the driving point mobility, and possibly of the sound radiation in the treble range, resulting from structural modifications.", "prompt": "An expression of the piano soundboard mechanical mobility (in the direction normal to the soundboard) depending on a small number of parameters and valid up to several kHz is given in this communication. Up to 1.1 kHz, our experimental and numerical investigations confirm previous results showing that the soundboard behaves like a homogeneous", "orig": "plate with isotropic properties and clamped boundary conditions. Therefore, according to the Skudrzyk mean-value theorem (Skudrzyk 1980), only the mass of the structure M, the modal density n(f), and the mean loss factor eta(f), are needed to express the average driving point mobility.", "generated": "An expression of the piano soundboard mechanical mobility (in the direction normal to the soundboard) depending on a small number of parameters and valid up to several kHz is given in this communication. Up to 1.1 kHz, our experimental and numerical investigations confirm previous results showing that the soundboard behaves like a homogeneous"}
{"full_text": "The Gaussian graphical model, a popular paradigm for studying relationship among variables in a wide range of applications, has attracted great attention in recent years. This paper considers a fundamental question: When is it possible to estimate low-dimensional parameters at parametric square-root rate in a large Gaussian graphical model? A novel regression approach is proposed to obtain asymptotically efficient estimation of each entry of a precision matrix under a sparseness condition relative to the sample size. When the precision matrix is not sufficiently sparse, or equivalently the sample size is not sufficiently large, a lower bound is established to show that it is no longer possible to achieve the parametric rate in the estimation of each entry. This lower bound result, which provides an answer to the delicate sample size question, is established with a novel construction of a subset of sparse precision matrices in an application of Le Cam's lemma. Moreover, the proposed estimator is proven to have optimal convergence rate when the parametric rate cannot be achieved, under a minimal sample requirement. The proposed estimator is applied to test the presence of an edge in the Gaussian graphical model or to recover the support of the entire model, to obtain adaptive rate-optimal estimation of the entire precision matrix as measured by the matrix $\\ell_q$ operator norm and to make inference in latent variables in the graphical model. All of this is achieved under a sparsity condition on the precision matrix and a side condition on the range of its spectrum. This significantly relaxes the commonly imposed uniform signal strength condition on the precision matrix, irrepresentability condition on the Hessian tensor operator of the covariance matrix or the $\\ell_1$ constraint on the precision matrix. Numerical results confirm our theoretical findings. The ROC curve of the proposed algorithm, Asymptotic Normal Thresholding (ANT), for support recovery significantly outperforms that of the popular GLasso algorithm.", "prompt": "The Gaussian graphical model, a popular paradigm for studying relationship among variables in a wide range of applications, has attracted great attention in recent years. This paper considers a fundamental question: When is it possible to estimate low-dimensional parameters at parametric square-root rate in a large Gaussian graphical model? A novel regression approach is proposed", "orig": "to obtain asymptotically efficient estimation of each entry of a precision matrix under a sparseness condition relative to the sample size. When the precision matrix is not sufficiently sparse, or equivalently the sample size is not sufficiently large, a lower bound is established to show that it is no longer possible to achieve the parametric", "generated": "The Gaussian graphical model, a popular paradigm for studying relationship among variables in a wide range of applications, has attracted great attention in recent years. This paper considers a fundamental question: When is it possible to estimate low-dimensional parameters at parametric square-root rate in a large Gaussian graphical model? A novel regression approach is proposed"}
{"full_text": "In this article we address the first part of the programme presented in \\cite{Teleman_arXiv_III}, \\S 2; we construct the local $K$- theory level of the index formula. Our construction is sufficiently general to encompass the algebra of pseudo-differential operators of order zero on smooth manifolds, elliptic pseudo-differential operators of order zero, their abstract symbol (see Introduction \\S 2.) and their local $K$- theory analytical and topological index classes, see \\cite{Teleman_arXiv_III}, \\S 5, Definition 5 and 6. Our definitions are sufficiently general to apply to exact sequences of singular integral operators, which are of interest in the case of the index theorem on Lipschitz and quasi-conformal manifolds, see \\cite{Teleman_IHES}, \\cite{Teleman_Acta}, \\cite{Donaldson_Sullivan}, \\cite{Connes_Sullivan_Teleman}. In this article we introduce localised algebras (Definition 3) $\\mathit{A}$ and in \\S 6 we define their local algebraic $K$-theory. A localised algebra $\\mathit{A}$ is an algebra in which a decreasing filtration by vector sub-spaces $\\mathit{A}_{\\mu}$ is introduced. The filtration $\\mathit{A}_{\\mu}$ induces a filtration on the space of matrices $\\mathbb{M}(\\mathit{A}_{\\mu})$. Although we define solely $K^{loc}_{\\ast}(\\mathit{A})$ for $\\ast= 0, \\; 1$, we expect our construction could be extended in higher degrees. We stress that our construction of $K^{loc}_{0}(\\mathit{A})$ uses exclusively idempotent matrices and that the use of finite projective modules is totally avoided. (Idempotent matrices, rather than projective modules, contain less arbitrariness in the description of the $K_{0}$ classes and allow a better filtration control). The group $K^{loc}_{0}(\\mathit{A})$ is by definition the quotient space of the space of the Grothendieck completion of the space of idempotent matrices through three equivalence relations: -i) stabilisation $\\sim_{s}$, -2) local conjugation $\\sim_{l}$, {\\em and} -3) projective limit with respect to the filtration. By definition, the $K_{1}^{loc} (\\mathit{A})$ is the projective limit of the local $K_{1}(\\mathit{A}_{\\mu})$ groups. The group $K_{1}(\\mathit{A}_{\\mu})$ is by definition the quotient of $\\mathbb{GL}(\\mathit{A}_{\\mu})$ modulo the equivalence relation generated by: -1) stabilisation $\\sim_{s}$, --2) local conjugation $\\sim_{l}$ and -3) $\\sim_{\\mathbb{O}(\\mathit{A}_{\\mu})}$, where $\\mathbb{O}(\\mathit{A}_{\\mu})$ is the sub-module generated by elements of the form $ u \\oplus u^{-1} $, for any $u \\in \\mathbb{GL}(\\mathit{A}_{\\mu})$. The class of any invertible element $u$ modulo conjugation (inner auto-morphisms) we call the Jordan canonical form of $u$. The local conjugation preserves the local Jordan canonical form of invertible elements. The equivalence relation $\\sim_{\\mathbb{O}(\\mathit{A}_{\\mu})}$ insures existence of opposite elements in $K_{1}(\\mathit{A}_{\\mu})$ and $K_{1}^{loc}(\\mathit{A})$. Our definition of $K^{loc}_{1}(\\mathit{A})$ does not use the commutator sub-group $[\\mathbb{GL}(\\mathit{A}), \\mathbb{GL}(\\mathit{A})]$ nor elementary matrices in its construction. We define short exact sequences of localised algebras. To get the corresponding (open) six terms exact sequence (Theorem 51) one has to take the tensor product of the expected six terms exact sequence by $\\mathbb{Z}[\\frac{1}{2}]$. We expect the factor $\\otimes_{\\mathbb{Z}[\\frac{1}{2}}]$ to have important consequences. Our work shows that the basic structure of $K_{1}$ resides in the {\\em additive} sub-group generated by elements of the form $u \\oplus u^{-1}$, $u \\in \\mathbb{GL}(\\mathit{A})$, rather than in the {\\em multiplicativ} commutator sub-group $[\\mathbb{GL}(\\mathit{A}), \\mathbb{GL}(\\mathit{A})]$. Even into the case of trivially filtered algebras, $\\mathit{A}_{\\mu} = \\mathit{A}$, for all $\\mu \\in \\mathbb{N}$, the introduced group $K^{loc}_{1}(\\mathit{A})$ should provide more information than the classical group $K_{1}(\\mathit{A})$.", "prompt": "In this article we address the first part of the programme presented in \\cite{Teleman_arXiv_III}, \\S 2; we construct the local $K$- theory level of the index formula. Our construction is sufficiently general to encompass the algebra of pseudo-differential operators of order zero on smooth", "orig": "manifolds, elliptic pseudo-differential operators of order zero, their abstract symbol (see Introduction \\S 2.) and their local $K$- theory analytical and topological index classes, see \\cite{Teleman_arXiv_III}, \\S 5, Definition 5 and 6.", "generated": "In this article we address the first part of the programme presented in \\cite{Teleman_arXiv_III}, \\S 2; we construct the local $K$-theory level of the index formula. Our construction is sufficiently general to encompass the algebra of pseudo-differential operators of order zero on smooth"}
{"full_text": "We introduce the dependent doors problem as an abstraction for situations in which one must perform a sequence of possibly dependent decisions, without receiving feedback information on the effectiveness of previously made actions. Informally, the problem considers a set of $d$ doors that are initially closed, and the aim is to open all of them as fast as possible. To open a door, the algorithm knocks on it and it might open or not according to some probability distribution. This distribution may depend on which other doors are currently open, as well as on which other doors were open during each of the previous knocks on that door. The algorithm aims to minimize the expected time until all doors open. Crucially, it must act at any time without knowing whether or which other doors have already opened. In this work, we focus on scenarios where dependencies between doors are both positively correlated and acyclic.The fundamental distribution of a door describes the probability it opens in the best of conditions (with respect to other doors being open or closed). We show that if in two configurations of $d$ doors corresponding doors share the same fundamental distribution, then these configurations have the same optimal running time up to a universal constant, no matter what are the dependencies between doors and what are the distributions. We also identify algorithms that are optimal up to a universal constant factor. For the case in which all doors share the same fundamental distribution we additionally provide a simpler algorithm, and a formula to calculate its running time. We furthermore analyse the price of lacking feedback for several configurations governed by standard fundamental distributions. In particular, we show that the price is logarithmic in $d$ for memoryless doors, but can potentially grow to be linear in $d$ for other distributions.We then turn our attention to investigate precise bounds. Even for the case of two doors, identifying the optimal sequence is an intriguing combinatorial question. Here, we study the case of two cascading memoryless doors. That is, the first door opens on each knock independently with probability $p\\_1$. The second door can only open if the first door is open, in which case it will open on each knock independently with probability $p\\_2$. We solve this problem almost completely by identifying algorithms that are optimal up to an additive term of 1.", "prompt": "We introduce the dependent doors problem as an abstraction for situations in which one must perform a sequence of possibly dependent decisions, without receiving feedback information on the effectiveness of previously made actions. Informally, the problem considers a set of $d$ doors that are initially closed, and the aim is to open all of them as", "orig": "fast as possible. To open a door, the algorithm knocks on it and it might open or not according to some probability distribution. This distribution may depend on which other doors are currently open, as well as on which other doors were open during each of the previous knocks on that door. The algorithm aims to minimize the expected", "generated": "We introduce the dependent doors problem as an abstraction for situations in which one must perform a sequence of possibly dependent decisions, without receiving feedback information on the effectiveness of previously made actions. Informally, the problem considers a set of $d$ doors that are initially closed, and the aim is to open all of them and"}
{"full_text": "The celebrated H\\\"{o}rmander condition is a sufficient (and nearly necessary) condition for a second-order linear Kolmogorov partial differential equation (PDE) with smooth coefficients to be hypoelliptic. As a consequence, the solutions of Kolmogorov PDEs are smooth at all positive times if the coefficients of the PDE are smooth and satisfy H\\\"{o}rmander's condition even if the initial function is only continuous but not differentiable. First-order linear Kolmogorov PDEs with smooth coefficients do not have this smoothing effect but at least preserve regularity in the sense that solutions are smooth if their initial functions are smooth. In this article, we consider the intermediate regime of nonhypoelliptic second-order Kolmogorov PDEs with smooth coefficients. The main observation of this article is that there exist counterexamples to regularity preservation in that case. More precisely, we give an example of a second-order linear Kolmogorov PDE with globally bounded and smooth coefficients and a smooth initial function with compact support such that the unique globally bounded viscosity solution of the PDE is not even locally H\\\"{o}lder continuous. From the perspective of probability theory, the existence of this example PDE has the consequence that there exists a stochastic differential equation (SDE) with globally bounded and smooth coefficients and a smooth function with compact support which is mapped by the corresponding transition semigroup to a function which is not locally H\\\"{o}lder continuous. In other words, degenerate noise can have a roughening effect. A further implication of this loss of regularity phenomenon is that numerical approximations may converge without any arbitrarily small polynomial rate of convergence to the true solution of the SDE. More precisely, we prove for an example SDE with globally bounded and smooth coefficients that the standard Euler approximations converge to the exact solution of the SDE in the strong and numerically weak sense, but at a rate that is slower then any power law.", "prompt": "The celebrated H\\\"{o}rmander condition is a sufficient (and nearly necessary) condition for a second-order linear Kolmogorov partial differential equation (PDE) with smooth coefficients to be hypoelliptic. As a consequence, the solutions of Kolmogorov PDEs are", "orig": "smooth at all positive times if the coefficients of the PDE are smooth and satisfy H\\\"{o}rmander's condition even if the initial function is only continuous but not differentiable. First-order linear Kolmogorov PDEs with smooth coefficients do not have this smoothing effect but at least preserve regular", "generated": "The celebrated H\u00f6rmander condition is a sufficient (and nearly necessary) condition for a second-order linear Kolmogorov partial differential equation (PDE) with smooth coefficients to be hypoelliptic. As a consequence, the solutions of Kolmogorov PDEs are characterized by the fact"}
{"full_text": "Introduction: Nottingham grading system is a major prognostic factor for invasive breast carcinoma (IBC). Its determination requires the evaluation of the mitotic score (MS) which is subject to low intra- and inter-observer reproducibility. The MS shall be performed in the most proliferative area of the tumor, which determination is hard but critical. Artificial intelligence based tools could help pathologists to detect mitosis on whole slide images (WSI). Objective: The aim of this study was to evaluate the contribution of a mitosis detection algorithms pecifically developed to assist the pathologist during the evaluation of the MS on WSI. Methods: Algorithmic mitosis detection is a two-step process: first the algorithm detects candidate objects resembling mitosis, then the selection is refined by a classifier. The densest mitoticregions are shown to the pathologist, then he can establish the MS with algorithm results. For this study, three expert pathologists have determined a consensual ground truth for MS on fifty WSI of IBC. Those slides were also submitted to two readers pathologists who evaluated the MS of each slide twice, with and without the assistance of the algorithm, with a four week wash-out period. Interobserver reproducibility was measured by evaluating the scores obtained with, and without assistance between two readers pathologists and was also measured between each reader pathologist and the expert ground truth to determine the accuracy of the established score. Results:Baseline linearly weighted Cohen's Kappa for interobserver agreement of MS between two readers pathologists was 0.482. Using the algorithm generated mitotic detection in WSI, the agreement score increased to 0.672. Baseline linearly weighted Cohen's Kappa for interobserver agreement of MS between each reader pathologist and expert consensus was 0.378 and 0.457 for pathologist 1 and 2 respectively. Using the algorithm generated mitoticdetection in WSI, the agreement score increased respectively to 0.629 and 0.726. Conclusion:The use of the developed algorithm constitutes a viable approach to assist the pathologist for the evaluation of the MS of IBC on WSI. Its use makes it possible to improve interobserver reproducibility between pathologists and the accuracy of the score established by expert consensus. The use of such a tool constitutes a new approach in the evaluation of the mitoticscore which could lead to an evolution of practices.", "prompt": "Introduction: Nottingham grading system is a major prognostic factor for invasive breast carcinoma (IBC). Its determination requires the evaluation of the mitotic score (MS) which is subject to low intra- and inter-observer reproducibility. The MS shall be performed in the most proliferative area of the tumor, which", "orig": "determination is hard but critical. Artificial intelligence based tools could help pathologists to detect mitosis on whole slide images (WSI). Objective: The aim of this study was to evaluate the contribution of a mitosis detection algorithms pecifically developed to assist the pathologist during the evaluation of the MS on WSI. Methods", "generated": "Nottingham grading system is a major prognostic factor for invasive breast carcinoma (IBC). Its determination requires the evaluation of the mitotic score (MS) which is subject to low intra- and inter-observer reproducibility. The MS shall be performed in the most proliferative area of the tumor, which is"}
{"full_text": "One of the major terminological forces driving ICT integration in research today is that of \"big data.\" While the phrase sounds inclusive and integrative, \"big data\" approaches are highly selective, excluding input that cannot be effectively structured, represented, or digitised. Data of this complex sort is precisely the kind that human activity produces, but the technological imperative to enhance signal through the reduction of noise does not accommodate this richness. Data and the computational approaches that facilitate \"big data\" have acquired a perceived objectivity that belies their curated, malleable, reactive, and performative nature. In an input environment where anything can \"be data\" once it is entered into the system as \"data,\" data cleaning and processing, together with the metadata and information architectures that structure and facilitate our cultural archives acquire a capacity to delimit what data are. This engenders a process of simplification that has major implications for the potential for future innovation within research environments that depend on rich material yet are increasingly mediated by digital technologies. This paper presents the preliminary findings of the European-funded KPLEX (Knowledge Complexity) project which investigates the delimiting effect digital mediation and datafication has on rich, complex cultural data. The paper presents a systematic review of existing implicit definitions of data, elaborating on the implications of these definitions and highlighting the ways in which metadata and computational technologies can restrict the interpretative potential of data. It sheds light on the gap between analogue or augmented digital practices and fully computational ones, and the strategies researchers have developed to deal with this gap. The paper proposes a reconceptualisation of data as it is functionally employed within digitally-mediated research so as to incorporate and acknowledge the richness and complexity of our source materials.", "prompt": "One of the major terminological forces driving ICT integration in research today is that of \"big data.\" While the phrase sounds inclusive and integrative, \"big data\" approaches are highly selective, excluding input that cannot be effectively structured, represented, or digitised. Data of this complex sort is precisely the kind that human", "orig": "activity produces, but the technological imperative to enhance signal through the reduction of noise does not accommodate this richness. Data and the computational approaches that facilitate \"big data\" have acquired a perceived objectivity that belies their curated, malleable, reactive, and performative nature. In an input environment where anything can \"be", "generated": "...nists and social scientists often work with. As researchers in these fields strive to comprehend intricate human experiences and the social structures that shape them, they must collect and analyze vast, messy datasets. The very selectivity of \"big data\" approaches can be seen as antithetical to the complexity of the human experience"}
{"full_text": "The ability to compose parts to form a more complex whole, and to analyze a whole as a combination of elements, is desirable across disciplines. This workshop bring together researchers applying compositional approaches to physics, NLP, cognitive science, and game theory. Within NLP, a long-standing aim is to represent how words can combine to form phrases and sentences. Within the framework of distributional semantics, words are represented as vectors in vector spaces. The categorical model of Coecke et al. [2010], inspired by quantum protocols, has provided a convincing account of compositionality in vector space models of NLP. There is furthermore a history of vector space models in cognitive science. Theories of categorization such as those developed by Nosofsky [1986] and Smith et al. [1988] utilise notions of distance between feature vectors. More recently G\\\"ardenfors [2004, 2014] has developed a model of concepts in which conceptual spaces provide geometric structures, and information is represented by points, vectors and regions in vector spaces. The same compositional approach has been applied to this formalism, giving conceptual spaces theory a richer model of compositionality than previously [Bolt et al., 2018]. Compositional approaches have also been applied in the study of strategic games and Nash equilibria. In contrast to classical game theory, where games are studied monolithically as one global object, compositional game theory works bottom-up by building large and complex games from smaller components. Such an approach is inherently difficult since the interaction between games has to be considered. Research into categorical compositional methods for this field have recently begun [Ghani et al., 2018]. Moreover, the interaction between the three disciplines of cognitive science, linguistics and game theory is a fertile ground for research. Game theory in cognitive science is a well-established area [Camerer, 2011]. Similarly game theoretic approaches have been applied in linguistics [J\\\"ager, 2008]. Lastly, the study of linguistics and cognitive science is intimately intertwined [Smolensky and Legendre, 2006, Jackendoff, 2007]. Physics supplies compositional approaches via vector spaces and categorical quantum theory, allowing the interplay between the three disciplines to be examined.", "prompt": "The ability to compose parts to form a more complex whole, and to analyze a whole as a combination of elements, is desirable across disciplines. This workshop bring together researchers applying compositional approaches to physics, NLP, cognitive science, and game theory. Within NLP, a long-standing aim is to represent how words", "orig": "can combine to form phrases and sentences. Within the framework of distributional semantics, words are represented as vectors in vector spaces. The categorical model of Coecke et al. [2010], inspired by quantum protocols, has provided a convincing account of compositionality in vector space models of NLP. There is furthermore a", "generated": "...combinations in language create meaning and context. This includes the composition of simple words to form complex sentences, as well as the representation of sentences as a combination of constituent elements like phrases and clauses. Compositional models of language have the potential to improve the understanding and generation of language, enabling systems to capture nuances"}
{"full_text": "Consider a class of decomposable combinatorial structures, using different types of atoms $\\Atoms = \\{\\At_1,\\ldots ,\\At_{|{\\Atoms}|}\\}$. We address the random generation of such structures with respect to a size $n$ and a targeted distribution in $k$ of its \\emph{distinguished} atoms. We consider two variations on this problem. In the first alternative, the targeted distribution is given by $k$ real numbers $\\TargFreq_1, \\ldots, \\TargFreq_k$ such that $0 < \\TargFreq_i < 1$ for all $i$ and $\\TargFreq_1+\\cdots+\\TargFreq_k \\leq 1$. We aim to generate random structures among the whole set of structures of a given size $n$, in such a way that the {\\em expected} frequency of any distinguished atom $\\At_i$ equals $\\TargFreq_i$. We address this problem by weighting the atoms with a $k$-tuple $\\Weights$ of real-valued weights, inducing a weighted distribution over the set of structures of size $n$. We first adapt the classical recursive random generation scheme into an algorithm taking $\\bigO{n^{1+o(1)}+mn\\log{n}}$ arithmetic operations to draw $m$ structures from the $\\Weights$-weighted distribution. Secondly, we address the analytical computation of weights such that the targeted frequencies are achieved asymptotically, i. e. for large values of $n$. We derive systems of functional equations whose resolution gives an explicit relationship between $\\Weights$ and $\\TargFreq_1, \\ldots, \\TargFreq_k$. Lastly, we give an algorithm in $\\bigO{k n^4}$ for the inverse problem, {\\it i.e.} computing the frequencies associated with a given $k$-tuple $\\Weights$ of weights, and an optimized version in $\\bigO{k n^2}$ in the case of context-free languages. This allows for a heuristic resolution of the weights/frequencies relationship suitable for complex specifications. In the second alternative, the targeted distribution is given by a $k$ natural numbers $n_1, \\ldots, n_k$ such that $n_1+\\cdots+n_k+r=n$ where $r \\geq 0$ is the number of undistinguished atoms. The structures must be generated uniformly among the set of structures of size $n$ that contain {\\em exactly} $n_i$ atoms $\\At_i$ ($1 \\leq i \\leq k$). We give a $\\bigO{r^2\\prod_{i=1}^k n_i^2 +m n k \\log n}$ algorithm for generating $m$ structures, which simplifies into a $\\bigO{r\\prod_{i=1}^k n_i +m n}$ for regular specifications.", "prompt": "Consider a class of decomposable combinatorial structures, using different types of atoms $\\Atoms = \\{\\At_1,\\ldots,\\At_{|{\\Atoms}|}\\}$. We address the random generation of such structures with respect to a size $n$ and a targeted distribution in $k$ of", "orig": "its \\emph{distinguished} atoms. We consider two variations on this problem. In the first alternative, the targeted distribution is given by $k$ real numbers $\\TargFreq_1, \\ldots, \\TargFreq_k$ such that $0 < \\TargFreq_i < 1", "generated": "The random generation of combinatorial structures with respect to a size $n$ and a targeted distribution in $k$ of the atoms $\\Atoms = \\{\\At_1,\\ldots,\\At_{|{\\Atoms}|}\\}$.\n\nConsider a class of decomposable combinatorial structures, using different types of"}
{"full_text": "Non-target screening consists in searching a sample for all present substances, suspected or unknown, with very little prior knowledge about the sample. This approach has been introduced more than a decade ago in the field of water analysis, together with dedicated compound identification tools, but is still very scarce for indoor and atmospheric trace gas measurements, despite the clear need for a better understanding of the atmospheric trace gas composition.For a systematic detection of emerging trace gases in the atmosphere, a new and powerful analytical method is gas chromatography (GC) of preconcentrated samples, followed by electron ionisation, high resolution mass spectrometry (EI-HRMS). In this work, we present data analysis tools to enable automated fragment formula annotation for unknown compounds measured by GC-EI-HRMS. Based on co-eluting mass/charge fragments, we developed an innovative data analysis method to reliably reconstruct the chemical formulae of the fragments, using efficient combinatorics and graph theory. The method does not require the presence of the molecular ion, which is absent in ~40% of EI spectra. Our method has been trained and validated on \\textgreater50 halocarbons and hydrocarbons, with 3 to 20 atoms and molar masses of 30 to 330 g mol-1, measured with a mass resolution of approx.~3500. For 90% of the compounds, more than 90% of the annotated fragment formulae are correct. Cases of wrong identification can be attributed to the scarcity of detected fragments per compound or the lack of isotopic constraint (no minor isotopocule detected).Our method enables to reconstruct most probable chemical formulae independently from spectral databases. Therefore, it demonstrates the suitability of EI-HRMS data for non-target analysis and paves the way for the identification of substances for which no EI mass spectrum is registered in databases. We illustrate the performances of our method for atmospheric trace gases and suggest that it may be well suited for many other types of samples. The L-GPL licenced Python code is released under the name ALPINAC for ALgorithmic Process for Identification of Non-targeted Atmospheric Compounds.", "prompt": "Non-target screening consists in searching a sample for all present substances, suspected or unknown, with very little prior knowledge about the sample. This approach has been introduced more than a decade ago in the field of water analysis, together with dedicated compound identification tools, but is still very scarce for indoor and atmospheric trace gas measurements,", "orig": "despite the clear need for a better understanding of the atmospheric trace gas composition.For a systematic detection of emerging trace gases in the atmosphere, a new and powerful analytical method is gas chromatography (GC) of preconcentrated samples, followed by electron ionisation, high resolution mass spectrometry (EI-HRMS).", "generated": "Non-target screening consists in searching a sample for all present substances, suspected or unknown, with very little prior knowledge about the sample. This approach has been introduced more than a decade ago in the field of water analysis, together with dedicated compound identification tools, but is still very scarce for indoor and atmospheric trace gas measurements."}
{"full_text": "Synthesis is a particularly challenging problem for concurrent programs. At the same time it is a very promising approach, since concurrent programs are difficult to get right, or to analyze with traditional verification techniques. This paper gives an introduction to distributed synthesis in the setting of Mazurkiewicz traces, and its applications to decentralized runtime monitoring. 1 Context Modern computing systems are increasingly distributed and heterogeneous. Software needs to be able to exploit these advances, providing means for applications to be more performant. Traditional concurrent programming paradigms, as in Java, are based on threads, shared-memory, and locking mechanisms that guard access to common data. More recent paradigms like the reactive programming model of Erlang [4] and Scala [35,36] replace shared memory by asynchronous message passing, where sending a message is non-blocking. In all these concurrent frameworks, writing reliable software is a serious challenge. Programmers tend to think about code mostly in a sequential way, and it is hard to grasp all possible schedulings of events in a concurrent execution. For similar reasons, verification and analysis of concurrent programs is a difficult task. Testing, which is still the main method for error detection in software, has low coverage for concurrent programs. The reason is that bugs in such programs are difficult to reproduce: they may happen under very specific thread schedules and the likelihood of taking such corner-case schedules is very low. Automated verification, such as model-checking and other traditional exploration techniques, can handle very limited instances of concurrent programs, mostly because of the very large number of possible states and of possible interleavings of executions. Formal analysis of programs requires as a prerequisite a clean mathematical model for programs. Verification of sequential programs starts usually with an abstraction step -- reducing the value domains of variables to finite domains, viewing conditional branching as non-determinism, etc. Another major simplification consists in disallowing recursion. This leads to a very robust computational model, namely finite-state automata and regular languages. Regular languages of words (and trees) are particularly well understood notions. The deep connections between logic and automata revealed by the foundational work of B\\\"uchi, Rabin, and others, are the main ingredients in automata-based verification .", "prompt": "Synthesis is a particularly challenging problem for concurrent programs. At the same time it is a very promising approach, since concurrent programs are difficult to get right, or to analyze with traditional verification techniques. This paper gives an introduction to distributed synthesis in the setting of Mazurkiewicz traces, and its applications to decentralized", "orig": "runtime monitoring. 1 Context Modern computing systems are increasingly distributed and heterogeneous. Software needs to be able to exploit these advances, providing means for applications to be more performant. Traditional concurrent programming paradigms, as in Java, are based on threads, shared-memory, and locking mechanisms that guard access to common data.", "generated": "Synthesis is a particularly challenging problem for concurrent programs. At the same time it is a very promising approach, since concurrent programs are difficult to get right, or to analyze with traditional verification techniques. This paper gives an introduction to distributed synthesis in the setting of Mazurkiewicz traces, and its applications to decentralized"}
{"full_text": "Let $(\\{X_i(t)\\}_{i\\in \\mathbb{Z}^d})_{t\\geq 0}$ be the system of interacting diffusions on $[0,\\infty)$ defined by the following collection of coupled stochastic differential equations: \\begin{eqnarray}dX_i(t)=\\sum\\limits_{j\\in \\mathbb{Z}^d}a(i,j)[X_j(t)-X_i(t)] dt+\\sqrt{bX_i(t)^2} dW_i(t), \\eqntext{i\\in \\mathbb{Z}^d,t\\geq 0.}\\end{eqnarray} Here, $a(\\cdot,\\cdot)$ is an irreducible random walk transition kernel on $\\mathbb{Z}^d\\times \\mathbb{Z}^d$, $b\\in (0,\\infty)$ is a diffusion parameter, and $(\\{W_i(t)\\}_{i\\in \\mathbb{Z}^d})_{t\\geq 0}$ is a collection of independent standard Brownian motions on $\\mathbb{R}$. The initial condition is chosen such that $\\{X_i(0)\\}_{i\\in \\mathbb{Z}^d}$ is a shift-invariant and shift-ergodic random field on $[0,\\infty)$ with mean $\\Theta\\in (0,\\infty)$ (the evolution preserves the mean). We show that the long-time behavior of this system is the result of a delicate interplay between $a(\\cdot,\\cdot)$ and $b$, in contrast to systems where the diffusion function is subquadratic. In particular, let $\\hat{a}(i,j)={1/2}[a(i,j)+a(j,i)]$, $i,j\\in \\mathbb{Z}^d$, denote the symmetrized transition kernel. We show that: (A) If $\\hat{a}(\\cdot,\\cdot)$ is recurrent, then for any $b>0$ the system locally dies out. (B) If $\\hat{a}(\\cdot,\\cdot)$ is transient, then there exist $b_*\\geq b_2>0$ such that: (B1)d The system converges to an equilibrium $\\nu_{\\Theta}$ (with mean $\\Theta$) if $0<b<b_*$. (B2) The system locally dies out if $b>b_*$. (B3) $\\nu_{\\Theta}$ has a finite 2nd moment if and only if $0<b<b_2$. (B4) The 2nd moment diverges exponentially fast if and only if $b>b_2$. The equilibrium $\\nu_{\\Theta}$ is shown to be associated and mixing for all $0<b<b_*$. We argue in favor of the conjecture that $b_*>b_2$. We further conjecture that the system locally dies out at $b=b_*$. For the case where $a(\\cdot,\\cdot)$ is symmetric and transient we further show that: (C) There exists a sequence $b_2\\geq b_3\\geq b_4\\geq ... >0$ such that: (C1) $\\nu_{\\Theta}$ has a finite $m$th moment if and only if $0<b<b_m$. (C2) The $m$th moment diverges exponentially fast if and only if $b>b_m$. (C3) $b_2\\leq (m-1)b_m<2$. uad(C4) $\\lim_{m\\to\\infty}(m-1)b_m=c=\\sup_{m\\geq 2}(m-1)b_m$. The proof of these results is based on self-duality and on a representation formula through which the moments of the components are related to exponential moments of the collision local time of random walks. Via large deviation theory, the latter lead to variational expressions for $b_*$ and the $b_m$'s, from which sharp bounds are deduced. The critical value $b_*$ arises from a stochastic representation of the Palm distribution of the system. The special case where $a(\\cdot,\\cdot)$ is simple random walk is commonly referred to as the parabolic Anderson model with Brownian noise. This case was studied in the memoir by Carmona and Molchanov [Parabolic Anderson Problem and Intermittency (1994) Amer. Math. Soc., Providence, RI], where part of our results were already established.", "prompt": "Let $(\\{X_i(t)\\}_{i\\in \\mathbb{Z}^d})_{t\\geq 0}$ be the system of interacting diffusions on $[0,\\infty)$ defined by the following collection of coupled stochastic differential equations: \\begin{eqnarray}d", "orig": "X_i(t)=\\sum\\limits_{j\\in \\mathbb{Z}^d}a(i,j)[X_j(t)-X_i(t)] dt+\\sqrt{bX_i(t)^2} dW_i(t), \\eqntext{i\\in \\mathbb{Z}^d", "generated": "$(\\{X_i(t)\\}_{i\\in \\mathbb{Z}^d})_{t\\geq 0}$ be the system of interacting diffusions on $[0,\\infty)$ defined by the following collection of coupled stochastic differential equations: \n\\begin{eqnarray}\nd"}
{"full_text": "AT 2000ch is a highly variable massive star and supernova imposter in NGC 3432 first detected in 2000. It is similar and often compared to SN 2009ip, and it is therefore expected to undergo a core-collapse supernova -- a SN imposter of similar brightness -- in the near future. We characterize the long-term variability of AT 2000ch in the radio and optical regimes with archival data reaching back to the year 1984. We use these newly reduced observations in addition to observations in the literature to restrict the mass-loss rates of AT 2000ch at multiple epochs based on different approaches, and to infer the general properties of its circumstellar nebula with respect to the detected radio brightness. We extend the known optical light curve of AT 2000ch up to the beginning of 2022 by performing point spread function photometry on archival data from the Palomar Transient Factory and the Zwicky Transient Facility. We reduced archival radio continuum observations obtained with the Very Large Array using standard calibration and imaging methods and complemented these with pre-reduced \\changes observations as well as observations obtained with the Westerbork Synthesis Radio Telescope and LOw Frequency ARray. For the analysis of AT 2000ch, we consider the optical light curve and color evolution, its radio continuum brightness at different frequencies and times, and the corresponding spectral indices. We estimated mass-loss rates and optical depths based on radio continuum brightnesses and Ha fluxes. We report two newly detected outbursts of AT 2000ch similar to those found in the 2000s and 13 re-brightening events, of which at least four are not conclusively detected because of insufficient sampling of the light curve. The dates of all outbursts and significant, well-sampled re-brightening events are consistent with a period of $\\sim 201 \\pm 12\\,$days over a total time-span of two decades. Such a behavior has never been found for any SN imposter, especially not for candidate SN~2009ip analogs. During 2010 to 2012 and 2014 to 2018, we only have a few detections, which is insufficient to come to any conclusion as to a possible less eruptive phase of the transient. We find steady dimming after the most recent re-brightening events and possible evidence of porosity in the circumstellar envelope, suggesting AT~2000ch may currently be in transition to a state of relative calm. We identified a second, unrelated source at a projected distance of $\\sim 23\\,$pc ($\\sim0.5^{\\prime\\prime}$) that has contaminated the optical measurements of AT~2000ch at its minimum luminosity over the last two decades probably on a $5\\%-10\\,\\%$ level, but this does not affect our overall findings and is negligible during re-brightening. We are able to restrict the mass-loss rate of AT~2000ch to range between several $10^{-6}\\,\\textrm{M}_{\\odot}/\\textrm{yr}$ and several $10^{-5}\\,\\textrm{M}_{\\odot}/\\textrm{yr}$. The fresh ejecta appear to be optically thick to radio continuum emission at least within the first $\\sim 25\\,$days after significant re-brightening.", "prompt": "AT 2000ch is a highly variable massive star and supernova imposter in NGC 3432 first detected in 2000. It is similar and often compared to SN 2009ip, and it is therefore expected to undergo a core-collapse supernova -- a SN imposter of similar brightness -- in", "orig": "the near future. We characterize the long-term variability of AT 2000ch in the radio and optical regimes with archival data reaching back to the year 1984. We use these newly reduced observations in addition to observations in the literature to restrict the mass-loss rates of AT 2000ch at multiple epochs based on", "generated": "AT 2000ch is a highly variable massive star and supernova imposter in NGC 3432 first detected in 2000. It is similar and often compared to SN 2009ip, and it is therefore expected to undergo a core-collapse supernova -- a SN imposter of similar brightness --"}
{"full_text": "Background Weight loss trajectories after bariatric surgery vary widely between individuals, and predicting weight loss before the operation remains challenging. We aimed to develop a model using machine learning to provide individual preoperative prediction of 5-year weight loss trajectories after surgery. Methods In this multinational retrospective observational study we enrolled adult participants (aged $\\ge$18 years) from ten prospective cohorts (including ABOS [NCT01129297], BAREVAL [NCT02310178], the Swedish Obese Subjects study, and a large cohort from the Dutch Obesity Clinic [Nederlandse Obesitas Kliniek]) and two randomised trials (SleevePass [NCT00793143] and SM-BOSS [NCT00356213]) in Europe, the Americas, and Asia, with a 5 year followup after Roux-en-Y gastric bypass, sleeve gastrectomy, or gastric band. Patients with a previous history of bariatric surgery or large delays between scheduled and actual visits were excluded. The training cohort comprised patients from two centres in France (ABOS and BAREVAL). The primary outcome was BMI at 5 years. A model was developed using least absolute shrinkage and selection operator to select variables and the classification and regression trees algorithm to build interpretable regression trees. The performances of the model were assessed through the median absolute deviation (MAD) and root mean squared error (RMSE) of BMI. Findings10 231 patients from 12 centres in ten countries were included in the analysis, corresponding to 30 602 patient-years. Among participants in all 12 cohorts, 7701 (75$\\bullet$3%) were female, 2530 (24$\\bullet$7%) were male. Among 434 baseline attributes available in the training cohort, seven variables were selected: height, weight, intervention type, age, diabetes status, diabetes duration, and smoking status. At 5 years, across external testing cohorts the overall mean MAD BMI was 2$\\bullet$8 kg/m${}^2$ (95% CI 2$\\bullet$6-3$\\bullet$0) and mean RMSE BMI was 4$\\bullet$7 kg/m${}^2$ (4$\\bullet$4-5$\\bullet$0), and the mean difference between predicted and observed BMI was-0$\\bullet$3 kg/m${}^2$ (SD 4$\\bullet$7). This model is incorporated in an easy to use and interpretable web-based prediction tool to help inform clinical decision before surgery. InterpretationWe developed a machine learning-based model, which is internationally validated, for predicting individual 5-year weight loss trajectories after three common bariatric interventions.", "prompt": "Background Weight loss trajectories after bariatric surgery vary widely between individuals, and predicting weight loss before the operation remains challenging. We aimed to develop a model using machine learning to provide individual preoperative prediction of 5-year weight loss trajectories after surgery. Methods In this multinational retrospective observational study we enrolled adult participants (aged $\\", "orig": "ge$18 years) from ten prospective cohorts (including ABOS [NCT01129297], BAREVAL [NCT02310178], the Swedish Obese Subjects study, and a large cohort from the Dutch Obesity Clinic [Nederlandse Obesitas Kliniek]) and two randomised trials (S", "generated": "Background Weight loss trajectories after bariatric surgery vary widely between individuals, and predicting weight loss before the operation remains challenging. We aimed to develop a model using machine learning to provide individual preoperative prediction of 5-year weight loss trajectories after surgery. \n\nMethods In this multinational retrospective observational study we enrolled adult participants (aged"}
{"full_text": "The practice of unit testing enables programmers to obtain automated feedback on whether a currently edited program is consistent with the expectations specified in test cases. Feedback is most valuable when it happens immediately, as defects can be corrected instantly before they become harder to fix. With growing and longer running test suites, however, feedback is obtained less frequently and lags behind program changes. The objective of test prioritization is to rank tests so that defects, if present, are found as early as possible or with the least costs. While there are numerous static approaches that output a ranking of tests solely based on the current version of a program, we focus on change-based test prioritization, which recommends tests that likely fail in response to the most recent program change. The canonical approach relies on coverage data and prioritizes tests that cover the changed region, but obtaining and updating coverage data is costly. More recently, information retrieval techniques that exploit overlapping vocabulary between change and tests have proven to be powerful, yet lightweight. In this work, we demonstrate the capabilities of information retrieval for prioritizing tests in dynamic programming languages using Python as example. We discuss and measure previously understudied variation points, including how contextual information around a program change can be used, and design alternatives to the widespread \\emph{TF-IDF} retrieval model tailored to retrieving failing tests. To obtain program changes with associated test failures, we designed a tool that generates a large set of faulty changes from version history along with their test results. Using this data set, we compared existing and new lexical prioritization strategies using four open-source Python projects, showing large improvements over untreated and random test orders and results consistent with related work in statically typed languages. We conclude that lightweight IR-based prioritization strategies are effective tools to predict failing tests in the absence of coverage data or when static analysis is intractable like in dynamic languages. This knowledge can benefit both individual programmers that rely on fast feedback, as well as operators of continuous integration infrastructure, where resources can be freed sooner by detecting defects earlier in the build cycle.", "prompt": "The practice of unit testing enables programmers to obtain automated feedback on whether a currently edited program is consistent with the expectations specified in test cases. Feedback is most valuable when it happens immediately, as defects can be corrected instantly before they become harder to fix. With growing and longer running test suites, however, feedback is obtained less", "orig": "frequently and lags behind program changes. The objective of test prioritization is to rank tests so that defects, if present, are found as early as possible or with the least costs. While there are numerous static approaches that output a ranking of tests solely based on the current version of a program, we focus on change-based", "generated": "The practice of unit testing enables programmers to obtain automated feedback on whether a currently edited program is consistent with the expectations specified in test cases. Feedback is most valuable when it happens immediately, as defects can be corrected instantly before they become harder to fix. With growing and longer running test suites, however, feedback is obtained less"}
{"full_text": "We consider the identification of heterogeneous linear elastic moduli in the context of time-harmonic elastodynamics. This inverse problem is formulated as the minimization of the modified error in constitutive equation (MECE), an energy-based cost functional defined as an weighted additive combination $\\mathcal{E}+\\kappa\\mathcal{D}$ of the error in constitutive equation (ECE) $\\mathcal{E}$, expressed using an energy seminorm, and a quadratic error term $\\mathcal{D}$ incorporating the kinematical measurements. MECE-based identification are known from existing computational evidence to enjoy attractive properties such as improved convexity, robustness to resonant frequencies, and tolerance to incompletely specified boundary conditions (BCs). The main goal of this work is to develop theoretical foundations, in a continuous setting, allowing to explain and justify some of the aforementioned beneficial properties, in particular addressing the general case where BCs may be underspecified. A specific feature of MECE formulations is that forward and adjoint solutions are governed by a fully coupled system, whose mathematical properties play a fundamental role in the qualitative and computational aspects of MECE minimization. We prove that this system has a unique and stable solution at any frequency, provided data is abundant enough (in a sense made precise therein) to at least compensate for any missing information on BCs. As a result, our formulation leads in such situations to a well-defined solution even though the relevant forward problem is not \\emph{a priori} clearly defined. This result has practical implications such as applicability of MECE to partial interior data (with important practical applications including ultrasound elastography), convergence of finite element discretizations and differentiability of the reduced MECE functional. In addition, we establish that usual least squares and pure ECE formulations are limiting cases of MECE formulations for small and large values of $\\kappa$, respectively. For the latter case, which corresponds to exact enforcement of kinematic data, we furthermore show that the reduced MECE Hessian is asymptotically positive for any parameter perturbation supported on the measurement region, thereby corroborating existing computational evidence on convexity improvement brought by MECE functionals. Finally, numerical studies that support and illustrate our theoretical findings, including a parameter reconstruction example using interior data, are presented.", "prompt": "We consider the identification of heterogeneous linear elastic moduli in the context of time-harmonic elastodynamics. This inverse problem is formulated as the minimization of the modified error in constitutive equation (MECE), an energy-based cost functional defined as an weighted additive combination $\\mathcal{E}+\\kappa\\", "orig": "mathcal{D}$ of the error in constitutive equation (ECE) $\\mathcal{E}$, expressed using an energy seminorm, and a quadratic error term $\\mathcal{D}$ incorporating the kinematical measurements. MECE-based identification are known from existing computational evidence to enjoy attractive properties such", "generated": "We consider the identification of heterogeneous linear elastic moduli in the context of time-harmonic elastodynamics. This inverse problem is formulated as the minimization of the modified error in constitutive equation (MECE), an energy-based cost functional defined as an weighted additive combination $\\mathcal{E}+\\kappa\\"}
{"full_text": "Various programming tools, languages, and environments give programmers the impression of changing a program while it is running. This experience of liveness has been discussed for over two decades and a broad spectrum of research on this topic exists. Amongst others, this work has been carried out in the communities around three major ideas which incorporate liveness as an important aspect: live programming, exploratory programming, and live coding. While there have been publications on the focus of each particular community, the overall spectrum of liveness across these three communities has not been investigated yet. Thus, we want to delineate the variety of research on liveness. At the same time, we want to investigate overlaps and differences in the values and contributions between the three communities. Therefore, we conducted a literature study with a sample of 212 publications on the terms retrieved from three major indexing services. On this sample, we conducted a thematic analysis regarding the following aspects: motivation for liveness, application domains, intended outcomes of running a system, and types of contributions. We also gathered bibliographic information such as related keywords and prominent publications. Besides other characteristics the results show that the field of exploratory programming is mostly about technical designs and empirical studies on tools for general-purpose programming. In contrast, publications on live coding have the most variety in their motivations and methodologies with a majority being empirical studies with users. As expected, most publications on live coding are applied to performance art. Finally, research on live programming is mostly motivated by making programming more accessible and easier to understand, evaluating their tool designs through empirical studies with users. In delineating the spectrum of work on liveness, we hope to make the individual communities more aware of the work of the others. Further, by giving an overview of the values and methods of the individual communities, we hope to provide researchers new to the field of liveness with an initial overview.", "prompt": "Various programming tools, languages, and environments give programmers the impression of changing a program while it is running. This experience of liveness has been discussed for over two decades and a broad spectrum of research on this topic exists. Amongst others, this work has been carried out in the communities around three major ideas which incorporate", "orig": "liveness as an important aspect: live programming, exploratory programming, and live coding. While there have been publications on the focus of each particular community, the overall spectrum of liveness across these three communities has not been investigated yet. Thus, we want to delineate the variety of research on liveness. At the", "generated": "Various programming tools, languages, and environments give programmers the impression of changing a program while it is running. This experience of liveness has been discussed for over two decades and a broad spectrum of research on this topic exists. Amongst others, this work has been carried out in the communities around three major ideas which incorporate"}
{"full_text": "Background: Recent studies reported postural balance disorders in patients and soccer players with groin pain (GP) compared to controls. Since postural balance asymmetry identified after an initial injury contributes for subsequent injuries, identification of this asymmetry in soccer players with GP may highlight the risk of sustaining subsequent noncontact lower extremity musculoskeletal injuries in these players. Therefore, the aims of this study were to (i) examine static and dynamic unipedal postural balance asymmetry in soccer players with GP compared to healthy ones, and (ii) quantify the risk of subsequent noncontact lower extremity injuries in these players. Research question: Do soccer players with GP exhibit higher static and dynamic unipedal postural balance asymmetry, and higher risk of sustaining subsequent injuries compared to controls. Methods: In this prospective case control study, 27 soccer players with non-time loss GP (GP group: GPG), and 27 healthy ones (control group: CG) were enrolled. Static and dynamic unipedal postural balance asymmetry were evaluated with a force platform using symmetry index (SI), and Y-balance test (Y-BT), respectively. Additionally, subsequent noncontact lower extremity musculoskeletal injuries were tracked for 10 months. Results: The GPG revealed higher (p < 0.01) SI in eyes closed condition, higher (p < 0.001) side-to-side asymmetry in anterior, posteromedial and posterolateral reach distances and in composite Y-BT score compared to CG. They showed lower (p < 0.001) composite score for injured limb and higher (p < 0.001) side-to-side asymmetry in posteromedial reach distance compared to the cutoff values of 89.6 % and 4 cm, respectively. Moreover, GPG exhibited higher odds (OR= 7.48; 95 % CI = 2.15, 26.00; p < 0.01) of sustaining subsequent injuries compared to CG. Significance: The Y-BT should be instituted into existing pre-participation physical examinations to screen for soccer players with non-time loss GP at an elevated risk of sustaining subsequent injuries. This could help coaches and clinicians make valid return to play decisions.", "prompt": "Background: Recent studies reported postural balance disorders in patients and soccer players with groin pain (GP) compared to controls. Since postural balance asymmetry identified after an initial injury contributes for subsequent injuries, identification of this asymmetry in soccer players with GP may highlight the risk of sustaining subsequent noncontact lower extremity mus", "orig": "culoskeletal injuries in these players. Therefore, the aims of this study were to (i) examine static and dynamic unipedal postural balance asymmetry in soccer players with GP compared to healthy ones, and (ii) quantify the risk of subsequent noncontact lower extremity injuries in these players. Research question: Do", "generated": "Recent studies reported postural balance disorders in patients and soccer players with groin pain (GP) compared to controls. Since postural balance asymmetry identified after an initial injury contributes for subsequent injuries, identification of this asymmetry in soccer players with GP may highlight the risk of sustaining subsequent noncontact lower extremity muscle injuries."}
{"full_text": "The type problem is the problem of deciding, for a simply connected Riemann surface, whether it is conformally equivalent to the complex plane or to the unit dic in the complex plane. We report on Teichm{\\\"u}ller's results on the type problem from his two papers Eine Anwendung quasikonformer Abbildungen auf das Typenproblem (An application of quasiconformal map-pings to the type problem) (1937) and Untersuchungen{\\\"u}ber kon-forme und quasikonforme Abbildungen (Investigations on con-formal and quasiconformal mappings) (1938). They concern simply connected Riemann surfaces defined as branched covers of the sphere. At the same time, we review the theory of line complexes, a combinatorial device used by Teichm{\\\"u}ller and others to encode branched coverings of the sphere. In the first paper, Teichm{\\\"u}ller proves that any two simply connected Riemann surfaces which are branched coverings of the Riemann sphere with finitely many branch values and which have the same line complex are quasiconformally equivalent. For this purpose, he introduces a technique for piecing together quasi-conformal mappings. He also obtains a result on the extension of smooth orientation-preserving diffeomorphisms of the circle to quasiconformal mappings of the disc which are conformal at the boundary. In the second paper, using line complexes, Teichm{\\\"u}ller gives a type criterion for a simply-connected surface which is a branched covering of the sphere, in terms of an adequately defined measure of ramification, defined by a limiting process. The result says that if the surface is ''sufficiently ramified'' (in a sense to be made precise), then it is hyperbolic. In the same paper, Te-ichm{\\\"u}ller answers by the negative a conjecture made by Nevan-linna which states a criterion for parabolicity in terms of the value of a (different) measure of ramification, defined by a limiting process. Teichm{\\\"u}ller's results in his first paper are used in the proof of the results of the second one.The final version of this paper will appear in Vol. VII of the Handbook of Teichm{\\\"u}ller theory (European Mathematical Society Publishing House, 2020).", "prompt": "The type problem is the problem of deciding, for a simply connected Riemann surface, whether it is conformally equivalent to the complex plane or to the unit dic in the complex plane. We report on Teichm{\\\"u}ller's results on the type problem from his two papers Eine Anwendung quas", "orig": "ikonformer Abbildungen auf das Typenproblem (An application of quasiconformal map-pings to the type problem) (1937) and Untersuchungen{\\\"u}ber kon-forme und quasikonforme Abbildungen (Investigations on con-formal and quasiconformal", "generated": "The type problem is the problem of deciding, for a simply connected Riemann surface, whether it is conformally equivalent to the complex plane or to the unit disc in the complex plane. We report on Teichm\u00fcller's results on the type problem from his two papers Eine Anwendung quasikonformer"}
{"full_text": "The absorption of free linear chains in a polymer brush was studied with respect to chain size $L$ and compatibility $\\chi$ with the brush by means of Monte Carlo (MC) simulations and Density Functional Theory (DFT) / Self-Consistent Field Theory (SCFT) at both moderate, $\\sigma_g = 0.25$, and high, $\\sigma_g = 1.00$, grafting densities using a bead-spring model. Different concentrations of the free chains $0.0625 \\le \\phi_o \\le 0.375$ are examined. Contrary to the case of $\\chi = 0$ when all species are almost completely ejected by the polymer brush irrespective of their length $L$, for $\\chi < 0$ we find that the degree of absorption (absorbed amount) $\\Gamma(L)$ undergoes a sharp crossover from weak to strong ($\\approx 100%$) absorption, discriminating between oligomers, $1\\le L\\le 8$, and longer chains. For a moderately dense brush, $\\sigma_g = 0.25$, the longer species, $L > 8$, populate predominantly the deep inner part of the brush whereas in a dense brush $\\sigma_g = 1.00$ they penetrate into the \"fluffy\" tail of the dense brush only. Gyration radius $R_g$ and end-to-end distance $R_e$ of absorbed chains thereby scale with length $L$ as free polymers in the bulk. Using both MC and DFT/SCFT methods for brushes of different chain length $32 \\le N \\le 256$, we demonstrate the existence of unique {\\em critical} value of compatibility $\\chi = \\chi^{c}<0$. For $\\chi^{c}(\\phi_o)$ the energy of free chains attains the {\\em same} value, irrespective of length $L$ whereas the entropy of free chain displays a pronounced minimum. At $\\chi^{c}$ all density profiles of absorbing chains with different $L$ intersect at the same distance from the grafting plane. The penetration/expulsion kinetics of free chains into the polymer brush after an instantaneous change in their compatibility $\\chi$ displays a rather rich behavior. We find three distinct regimes of penetration kinetics of free chains regarding the length $L$: I ($1\\le L\\le 8$), II ($8 \\le L \\le N$), and III ($L > N$), in which the time of absorption $\\tau$ grows with $L$ at a different rate. During the initial stages of penetration into the brush one observes a power-law increase of $\\Gamma \\propto t^\\alpha$ with power $\\alpha \\propto -\\ln \\phi_o$ whereby penetration of the free chains into the brush gets {\\em slower} as their concentration rises.", "prompt": "The absorption of free linear chains in a polymer brush was studied with respect to chain size $L$ and compatibility $\\chi$ with the brush by means of Monte Carlo (MC) simulations and Density Functional Theory (DFT) / Self-Consistent Field Theory (SCFT) at both moderate, $\\sigma_g", "orig": "= 0.25$, and high, $\\sigma_g = 1.00$, grafting densities using a bead-spring model. Different concentrations of the free chains $0.0625 \\le \\phi_o \\le 0.375$ are examined. Contrary to the case of $\\chi =", "generated": "The absorption of free linear chains in a polymer brush was studied with respect to chain size $L$ and compatibility $\\chi$ with the brush by means of Monte Carlo (MC) simulations and Density Functional Theory (DFT) / Self-Consistent Field Theory (SCFT) at both moderate, $\\sigma_g"}
{"full_text": "The symmetry algebras of certain families of quantum spin chains are considered in detail. The simplest examples possess m states per site (m\\geq2), with nearest-neighbor interactions with U(m) symmetry, under which the sites transform alternately along the chain in the fundamental m and its conjugate representation \\bar{m}. We find that these spin chains, even with {\\em arbitrary} coefficients of these interactions, have a symmetry algebra A_m much larger than U(m), which implies that the energy eigenstates fall into sectors that for open chains (i.e., free boundary conditions) can be labeled by j=0, 1, >..., L, for the 2L-site chain, such that the degeneracies of all eigenvalues in the jth sector are generically the same and increase rapidly with j. For large j, these degeneracies are much larger than those that would be expected from the U(m) symmetry alone. The enlarged symmetry algebra A_m(2L) consists of operators that commute in this space of states with the Temperley-Lieb algebra that is generated by the set of nearest-neighbor interaction terms; A_m(2L) is not a Yangian. There are similar results for supersymmetric chains with gl(m+n|n) symmetry of nearest-neighbor interactions, and a richer representation structure for closed chains (i.e., periodic boundary conditions). The symmetries also apply to the loop models that can be obtained from the spin chains in a spacetime or transfer matrix picture. In the loop language, the symmetries arise because the loops cannot cross. We further define tensor products of representations (for the open chains) by joining chains end to end. The fusion rules for decomposing the tensor product of representations labeled j_1 and j_2 take the same form as the Clebsch-Gordan series for SU(2). This and other structures turn the symmetry algebra \\cA_m into a ribbon Hopf algebra, and we show that this is ``Morita equivalent'' to the quantum group U_q(sl_2) for m=q+q^{-1}. The open-chain results are extended to the cases |m|< 2 for which the algebras are no longer semisimple; these possess continuum limits that are critical (conformal) field theories, or massive perturbations thereof. Such models, for open and closed boundary conditions, arise in connection with disordered fermions, percolation, and polymers (self-avoiding walks), and certain non-linear sigma models, all in two dimensions. A product operation is defined in a related way for the Temperley-Lieb representations also, and the fusion rules for this are related to those for A_m or U_q(sl_2) representations; this is useful for the continuum limits also, as we discuss in a companion paper.", "prompt": "The symmetry algebras of certain families of quantum spin chains are considered in detail. The simplest examples possess m states per site (m\\geq2), with nearest-neighbor interactions with U(m) symmetry, under which the sites transform alternately along the chain in the fundamental m and its conjugate representation \\", "orig": "bar{m}. We find that these spin chains, even with {\\em arbitrary} coefficients of these interactions, have a symmetry algebra A_m much larger than U(m), which implies that the energy eigenstates fall into sectors that for open chains (i.e., free boundary conditions) can be labeled by j=0,", "generated": "The symmetry algebras of certain families of quantum spin chains are considered in detail. The simplest examples possess m states per site (m\u22652), with nearest-neighbor interactions with U(m) symmetry, under which the sites transform alternately along the chain in the fundamental m and its conjugate representation. \n\nIn"}
{"full_text": "Numerical schemes for the solution of the Euler equations have recently been developed, which involve the discretisation of the internal energy equation, with corrective terms to ensure the correct capture of shocks, and, more generally, the consistency in the Lax-Wendroff sense. These schemes may be staggered or colocated, using either struc-tured meshes or general simplicial or tetrahedral/hexahedral meshes. The time discretization is performed by fractional-step algorithms; these may be either based on semi-implicit pressure correction techniques or segregated in such a way that only explicit steps are involved (referred to hereafter as \"explicit\" variants). In order to ensure the positivity of the density, the internal energy and the pressure, the discrete convection operators for the mass and internal energy balance equations are carefully designed; they use an upwind technique with respect to the material velocity only. The construction of the fluxes thus does not need any Rie-mann or approximate Riemann solver, and yields easily implementable algorithms. The stability is obtained without restriction on the time step for the pressure correction scheme and under a CFL-like condition for explicit variants: preservation of the integral of the total energy over the computational domain, and positivity of the density and the internal energy. The semi-implicit first-order upwind scheme satisfies a local discrete entropy inequality. If a MUSCL-like scheme is used in order to limit the scheme diffusion, then a weaker property holds: the entropy inequality is satisfied up to a remainder term which is shown to tend to zero with the space and time steps, if the discrete solution is controlled in L $\\infty$ and BV norms. The explicit upwind variant also satisfies such a weaker property, at the price of an estimate for the velocity which could be derived from the introduction of a new stabilization term in the momentum balance. Still for the explicit scheme, with the above-mentioned MUSCL-like scheme, the same result only holds if the ratio of the time to the space step tends to zero.", "prompt": "Numerical schemes for the solution of the Euler equations have recently been developed, which involve the discretisation of the internal energy equation, with corrective terms to ensure the correct capture of shocks, and, more generally, the consistency in the Lax-Wendroff sense. These schemes may be staggered or colocated", "orig": ", using either struc-tured meshes or general simplicial or tetrahedral/hexahedral meshes. The time discretization is performed by fractional-step algorithms; these may be either based on semi-implicit pressure correction techniques or segregated in such a way that only explicit steps are involved (referred to hereafter", "generated": "Numerical schemes for the solution of the Euler equations have recently been developed, which involve the discretisation of the internal energy equation, with corrective terms to ensure the correct capture of shocks, and, more generally, the consistency in the Lax-Wendroff sense. These schemes may be staggered or colocated"}
{"full_text": "How to efficiently and reliably spread information in a system is one of the most fundamental problems in distributed computing. Recently, inspired by biological scenarios, several works focused on identifying the minimal communication resources necessary to spread information under faulty conditions. Here we study the self-stabilizing bit-dissemination problem, introduced by Boczkowski, Korman, and Natale in [SODA 2017]. The problem considers a fully-connected network of n agents, with a binary world of opinions, one of which is called correct. At any given time, each agent holds an opinion bit as its public output. The population contains a source agent which knows which opinion is correct. This agent adopts the correct opinion and remains with it throughout the execution. We consider the basic PULL model of communication, in which each agent observes relatively few randomly chosen agents in each round. The goal of the non-source agents is to quickly converge on the correct opinion, despite having an arbitrary initial configuration, i.e., in a self-stabilizing manner. Once the population converges on the correct opinion, it should remain with it forever. Motivated by biological scenarios in which animals observe and react to the behavior of others, we focus on the extremely constrained model of passive communication, which assumes that when observing another agent the only information that can be extracted is the opinion bit of that agent. We prove that this problem can be solved in a poly-logarithmic in n number of rounds with high probability, while sampling a logarithmic number of agents at each round. Previous works solved this problem faster and using fewer samples, but they did that by decoupling the messages sent by agents from their output opinion, and hence do not fit the framework of passive communication. Moreover, these works use complex recursive algorithms with refined clocks that are unlikely to be used by biological entities. In contrast, our proposed algorithm has a natural appeal as it is based on letting agents estimate the current tendency direction of the dynamics, and then adapt to the emerging trend.", "prompt": "How to efficiently and reliably spread information in a system is one of the most fundamental problems in distributed computing. Recently, inspired by biological scenarios, several works focused on identifying the minimal communication resources necessary to spread information under faulty conditions. Here we study the self-stabilizing bit-dissemination problem, introduced by Boc", "orig": "zkowski, Korman, and Natale in [SODA 2017]. The problem considers a fully-connected network of n agents, with a binary world of opinions, one of which is called correct. At any given time, each agent holds an opinion bit as its public output. The population contains a", "generated": "How to efficiently and reliably spread information in a system is one of the most fundamental problems in distributed computing. Recently, inspired by biological scenarios, several works focused on identifying the minimal communication resources necessary to spread information under faulty conditions. Here we study the self-stabilizing bit-dissemination problem, introduced by Busnel"}
{"full_text": "The impact of outliers and anomalies on model estimation and data processing is of paramount importance, as evidenced by the extensive body of research spanning various fields over several decades: thousands of research papers have been published on the subject. As a consequence, numerous reviews, surveys, and textbooks have sought to summarize the existing literature, encompassing a wide range of methods from both the statistical and data mining communities. While these endeavors to organize and summarize the research are invaluable, they face inherent challenges due to the pervasive nature of outliers and anomalies in all data-intensive applications, irrespective of the specific application field or scientific discipline. As a result, the resulting collection of papers remains voluminous and somewhat heterogeneous. To address the need for knowledge organization in this domain, this paper implements the first systematic meta-survey of general surveys and reviews on outlier and anomaly detection. Employing a classical systematic survey approach, the study collects nearly 500 papers using two specialized scientific search engines. From this comprehensive collection, a subset of 56 papers that claim to be general surveys on outlier detection is selected using a snowball search technique to enhance field coverage. A meticulous quality assessment phase further refines the selection to a subset of 25 high-quality general surveys. Using this curated collection, the paper investigates the evolution of the outlier detection field over a 20-year period, revealing emerging themes and methods. Furthermore, an analysis of the surveys sheds light on the survey writing practices adopted by scholars from different communities who have contributed to this field. Finally, the paper delves into several topics where consensus has emerged from the literature. These include taxonomies of outlier types, challenges posed by high-dimensional data, the importance of anomaly scores, the impact of learning conditions, difficulties in benchmarking, and the significance of neural networks. Non-consensual aspects are also discussed, particularly the distinction between local and global outliers and the challenges in organizing detection methods into meaningful taxonomies.", "prompt": "The impact of outliers and anomalies on model estimation and data processing is of paramount importance, as evidenced by the extensive body of research spanning various fields over several decades: thousands of research papers have been published on the subject. As a consequence, numerous reviews, surveys, and textbooks have sought to summarize the existing literature, encompass", "orig": "ing a wide range of methods from both the statistical and data mining communities. While these endeavors to organize and summarize the research are invaluable, they face inherent challenges due to the pervasive nature of outliers and anomalies in all data-intensive applications, irrespective of the specific application field or scientific discipline. As a result, the resulting collection of", "generated": "The impact of outliers and anomalies on model estimation and data processing is of paramount importance, as evidenced by the extensive body of research spanning various fields over several decades: thousands of research papers have been published on the subject. As a consequence, numerous reviews, surveys, and textbooks have sought to summarize the existing literature, encompass"}
{"full_text": "In this paper we introduce the word {\\em fresco} to denote a monogenic geometric (a,b)-module. This \"basic object\" (generalized Brieskorn module with one generator) corresponds to the formal germ of the minimal filtered (regular) differential equation. Such an equation is satisfied by a relative de Rham cohomology class at a critical value of a holomorphic function on a smooth complex manifold. In [B.09] the first structure theorems are proved. Then in [B.10] we introduced the notion of {\\em theme} which corresponds in the \\ $[\\lambda]-$primitive case to frescos having a unique Jordan-H{\\\"o}lder sequence (a unique Jordan block for the monodromy). Themes correspond to asymptotic expansion of a given vanishing period, so to an image of a fresco in the module of asymptotic expansions. For a fixed relative de Rham cohomology class (for instance given by a smooth differential form $d-$closed and $df-$closed) each choice of a vanishing cycle in the spectral eigenspace of the monodromy for the eigenvalue \\ $exp(2i\\pi.\\lambda)$ \\ produces a \\ $[\\lambda]-$primitive theme, which is a quotient of the fresco associated to the given relative de Rham class itself. \\\\ We show that for any fresco there exists an {\\em unique} Jordan-H{\\\"o}lder sequence, called the {\\em principal J-H. sequence}, with corresponding quotients giving the opposite of the roots of the Bernstein polynomial in increasing order. We study the semi-simple part of a given fresco and we characterize the semi-simplicity of a fresco by the fact for any given order on the roots of its Bernstein polynomial we may find a J-H. sequence making them appear with this order. Then we construct a numerical invariant, called the \\ $\\beta-$invariant, and we show that it produces numerical criteria in order to give a necessary and sufficient condition on a fresco to be semi-simple. We show that these numerical invariants define a natural algebraic stratification on the set of isomorphism classes of fresco with given fundamental invariants (or equivalently with given roots of the Bernstein polynomial).", "prompt": "In this paper we introduce the word {\\em fresco} to denote a monogenic geometric (a,b)-module. This \"basic object\" (generalized Brieskorn module with one generator) corresponds to the formal germ of the minimal filtered (regular) differential equation. Such an equation is satisfied by a", "orig": "relative de Rham cohomology class at a critical value of a holomorphic function on a smooth complex manifold. In [B.09] the first structure theorems are proved. Then in [B.10] we introduced the notion of {\\em theme} which corresponds in the \\ $[\\lambda]-$primitive", "generated": "In this paper we introduce the word {\\em fresco} to denote a monogenic geometric (a,b)-module. This \"basic object\" (generalized Brieskorn module with one generator) corresponds to the formal germ of the minimal filtered (regular) differential equation. Such an equation is satisfied by a"}
{"full_text": "Graphs and various graph-like combinatorial structures, such as preorders and hypergraphs, are ubiquitous in programming. This paper focuses on representing graphs in a purely functional programming language like Haskell. There are several existing approaches; one of the most recently developed ones is the \"algebraic graphs\" approach (2017). It uses an algebraic data type to represent graphs and has attracted users, including from industry, due to its emphasis on equational reasoning and making a common class of bugs impossible by eliminating internal invariants. The previous formulation of algebraic graphs did not support edge labels, which was a serious practical limitation. In this paper, we redesign the main algebraic data type and remove this limitation. We follow a fairly standard approach of parameterising a data structure with a semiring of edge labels. The new formulation is both more general and simpler: the two operations for composing graphs used in the previous work can now be obtained from a single operation by fixing the semiring parameter to zero and one, respectively. By instantiating the new data type with different semirings, and working out laws for interpreting the resulting expression trees, we discover an unusual algebraic structure, which we call \"united monoids\", that is, a pair of monoids whose unit elements coincide. We believe that it is worth studying united monoids in their full generality, going beyond the graphs which prompted their discovery. To that end, we characterise united monoids with a minimal set of axioms, prove a few basic theorems, and discuss several notable examples. We validate the presented approach by implementing it in the open-source *algebraic-graphs* library. Our theoretical contributions are supported by proofs that are included in the paper and have also been machine-checked in Agda. By extending algebraic graphs with support for edge labels, we make them suitable for a much larger class of possible applications. By studying united monoids, we provide a theoretical foundation for further research in this area.", "prompt": "Graphs and various graph-like combinatorial structures, such as preorders and hypergraphs, are ubiquitous in programming. This paper focuses on representing graphs in a purely functional programming language like Haskell. There are several existing approaches; one of the most recently developed ones is the \"algebraic graphs\" approach (201", "orig": "7). It uses an algebraic data type to represent graphs and has attracted users, including from industry, due to its emphasis on equational reasoning and making a common class of bugs impossible by eliminating internal invariants. The previous formulation of algebraic graphs did not support edge labels, which was a serious practical limitation. In", "generated": "Graphs and various graph-like combinatorial structures, such as preorders and hypergraphs, are ubiquitous in programming. This paper focuses on representing graphs in a purely functional programming language like Haskell. There are several existing approaches; one of the most recently developed ones is the \"algebraic graphs\" approach (201"}
{"full_text": "The purpose of this study is to optimize the selection of prophylactic cardioverter defibrillator implantation candidates. Currently, the main criterion for implantation is a low Left Ventricular Ejection Fraction (LVEF) whose specificity is relatively poor. We designed two classifiers aimed to predict, from long term ECG recordings (Holter), whether a low-LVEF patient is likely or not to undergo ventricular arrhythmia in the next six months. One classifier is a single hidden layer neural network whose variables are the most relevant features extracted from Holter recordings, and the other classifier has a structure that capitalizes on the physiological decomposition of the arrhythmogenic factors into three disjoint groups: the myocardial substrate, the triggers and the autonomic nervous system (ANS). In this ad hoc network, the features were assigned to each group; one neural network classifier per group was designed and its complexity was optimized. The outputs of the classifiers were fed to a single neuron that provided the required probability estimate. The latter was thresholded for final discrimination A dataset composed of 186 pre-implantation 30-mn Holter recordings of patients equipped with an implantable cardioverter defibrillator (ICD) in primary prevention was used in order to design and test this classifier. 44 out of 186 patients underwent at least one treated ventricular arrhythmia during the six-month follow-up period. Performances of the designed classifier were evaluated using a cross-test strategy that consists in splitting the database into several combinations of a training set and a test set. The average arrhythmia prediction performances of the ad-hoc classifier are NPV = 77% $\\pm$ 13% and PPV = 31% $\\pm$ 19% (Negative Predictive Value $\\pm$ std, Positive Predictive Value $\\pm$ std). According to our study, improving prophylactic ICD-implantation candidate selection by automatic classification from ECG features may be possible, but the availability of a sizable dataset appears to be essential to decrease the number of False Negatives.", "prompt": "The purpose of this study is to optimize the selection of prophylactic cardioverter defibrillator implantation candidates. Currently, the main criterion for implantation is a low Left Ventricular Ejection Fraction (LVEF) whose specificity is relatively poor. We designed two classifiers aimed to predict, from long term E", "orig": "CG recordings (Holter), whether a low-LVEF patient is likely or not to undergo ventricular arrhythmia in the next six months. One classifier is a single hidden layer neural network whose variables are the most relevant features extracted from Holter recordings, and the other classifier has a structure that capitalizes on the", "generated": "The purpose of this study is to optimize the selection of prophylactic cardioverter defibrillator implantation candidates. Currently, the main criterion for implantation is a low Left Ventricular Ejection Fraction (LVEF) whose specificity is relatively poor. We designed two classifiers aimed to predict, from long term electro"}
{"full_text": "Aims: The large abundances of CH$^+$ in the diffuse interstellar medium (ISM) are a long standing issue of our understanding of the thermodynamical and chemical states of the gas. We investigate, here, the formation of CH+ in turbulent and multiphase environments, where the heating of the gas is almost solely driven by the photoelectric effect. Methods: The diffuse ISM is simulated using the magnetohydrodynamic (MHD) code RAMSES which self-consistently computes the dynamical and thermal evolution of the gas along with the time-dependent evolutions of the abundances of H$^+$, H, and H$_2$. The rest of the chemistry, including the abundance of CH$^+$, is computed in post-processing, at equilibrium, under the constraint of out-ofequilibrium of H$^+$, H, and H$_2$. The comparison with the observations is performed taking into account an often neglected, yet paramount, piece of information, namely the length of the intercepted diffuse matter along the observed lines of sight. Results: The quasi totality of the mass of CH$^+$ originates from the unstable gas, in environments where the kinetic temperature is larger than 600 K, the density ranges between 0.6 and 10 cm$^{-3}$, the electronic fraction ranges between 3 x 10$^{-4}$ and 6 x 10$^{-3}$, and the molecular fraction is smaller than 0.4. Its formation is driven by warm and out-of-equilibrium H$_2$ initially formed in the cold neutral medium (CNM) and injected in more diffuse environments and even the warm neutral medium (WNM) through a combination of advection and thermal instability. The simulation which displays the tightest agreement with the HI-to-H$_2$ transition and the thermal pressure distribution observed in the Solar Neighborhood is found to naturally reproduce the observed abundances of CH$^+$, the dispersion of observations, the probability of occurrence of most of the lines of sight, the fraction of non-detections of CH$^+$, and the distribution of its line profiles. The amount of CH$^+$ and the statistical properties of the simulated lines of sight are set by the fraction of unstable gas rich in H$_2$ which is controlled, on Galactic scales, by the mean density of the diffuse ISM (or, equivalently, its total mass), the amplitude of the mean UV radiation field, and the strength of the turbulent forcing. Conclusions: This work offers a new and natural solution to an 80 years old chemical riddle. The almost ubiquitous presence of CH$^+$ in the diffuse ISM likely results from the exchanges of matter between the CNM and the WNM induced by the combination of turbulent advection and thermal instability, without the need to invoke ambipolar diffusion or regions of intermittent turbulent dissipation. Through two phase turbulent mixing, CH$^+$ might thus be a tracer of the H$_2$ mass loss rate of CNM clouds.", "prompt": "Aims: The large abundances of CH$^+$ in the diffuse interstellar medium (ISM) are a long standing issue of our understanding of the thermodynamical and chemical states of the gas. We investigate, here, the formation of CH+ in turbulent and multiphase environments, where the heating of", "orig": "the gas is almost solely driven by the photoelectric effect. Methods: The diffuse ISM is simulated using the magnetohydrodynamic (MHD) code RAMSES which self-consistently computes the dynamical and thermal evolution of the gas along with the time-dependent evolutions of the abundances of H$^+$", "generated": "Aims: The large abundances of CH$^+$ in the diffuse interstellar medium (ISM) are a long standing issue of our understanding of the thermodynamical and chemical states of the gas. We investigate, here, the formation of CH+ in turbulent and multiphase environments, where the heating of"}
{"full_text": "We build in this paper a numerical solution procedure to compute the flow induced by a spherical flame expanding from a point source at a constant expansion velocity, with an instantaneous chemical reaction. The solution is supposed to be self-similar and the flow is split in three zones: an inner zone composed of burnt gases at rest, an intermediate zone where the solution is regular and the initial atmosphere composed of fresh gases at rest. The intermediate zone is bounded by the reactive shock (inner side) and the so-called precursor shock (outer side), for which Rankine-Hugoniot conditions are written; the solution in this zone is governed by two ordinary differential equations which are solved numerically. We show that, for any admissible precursor shock speed, the construction combining this numerical resolution with the exploitation of jump conditions is unique, and yields decreasing pressure, density and velocity profiles in the intermediate zone. In addition, the reactive shock speed is larger than the velocity on the outer side of the shock, which is consistent with the fact that the difference of these two quantities is the so-called flame velocity, i.e. the (relative) velocity at which the chemical reaction progresses in the fresh gases. Finally, we also observe numerically that the function giving the flame velocity as a function of the precursor shock speed is increasing; this allows to embed the resolution in a Newton-like procedure to compute the flow for a given flame speed (instead of for a given precursor shock speed). The resulting numerical algorithm is applied to stoichiometric hydrogen-air mixtures. Key words. spherical flames, reactive Euler equations, Riemann problems burnt zone (constant state) intermediate zone (regular solution) unburnt zone (constant initial state) W b W 2 W 1 W 0 reactive shock, r = $\\sigma$ r t. precursor shock, r = $\\sigma$ p t. W = ($\\rho$, u, p): local fluid state. Fig. 1.1. Structure of the solution. 1. Problem position. We address the flame propagation in a reactive infinite atmosphere of initial constant composition. The ignition is supposed to occur at a single point (chosen to be the origin of R 3) and the flow is supposed to satisfy a spherical symmetry property: the density $\\rho$, the pressure p, the internal energy e and the entropy s only depend on the distance r to the origin and the velocity reads u = ur/r, where r stands for the position vector. The flame is supposed to be infinitely thin and to move at a constant speed. The flow is governed by the Euler equations, and we seek a solution with the following structure:", "prompt": "We build in this paper a numerical solution procedure to compute the flow induced by a spherical flame expanding from a point source at a constant expansion velocity, with an instantaneous chemical reaction. The solution is supposed to be self-similar and the flow is split in three zones: an inner zone composed of burnt gases at rest,", "orig": "an intermediate zone where the solution is regular and the initial atmosphere composed of fresh gases at rest. The intermediate zone is bounded by the reactive shock (inner side) and the so-called precursor shock (outer side), for which Rankine-Hugoniot conditions are written; the solution in this zone is governed by two ordinary", "generated": "We build in this paper a numerical solution procedure to compute the flow induced by a spherical flame expanding from a point source at a constant expansion velocity, with an instantaneous chemical reaction. The solution is supposed to be self-similar and the flow is split in three zones: an inner zone composed of burnt gases at rest,"}
{"full_text": "Shock modelling predicts an electron density enhancement within the magnetic precursor of C-shocks. Previous observations of SiO, H13CO+, HN13C and H13CN toward the young L1448-mm outflow showed an over-excitation of the ion fluid that was attributed to an electron density enhancement in the precursor. We re-visit this interpretation and test if it still holds when we consider different source morphologies and kinetic temperatures for the observed molecules, and also give some insight on the spatial extent of the electron density enhancement around L1448-mm. We estimate the opacities of H13CO+ and HN13C by observing the J=3\\to2 lines of rarer isotopologues to confirm that the emission is optically thin. To model the excitation of the molecules, we use the large velocity gradient (LVG) approximation with updated collisional coefficients to i) re- analyse the observations toward the positions where the over-excitation of H13CO+ has previously been observed [i.e. toward L1448- mm at offsets (0,0) and (0,-10)], and ii) to investigate if the electron density enhancement is still required for the cases of extended and compact emission, and for kinetic temperatures of up to 400 K. We also report several lines of SiO, HN13C and H13CO+ toward new positions around this outflow, to investigate the spatial extent of the over-excitation of the ions in L1448-mm. From the isotopologue observations, we find that the emission of H13CO+ and HN13C from the precursor is optically thin if this emission is extended. Using the new collisional coefficients, an electron density enhancement is still needed to explain the excitation of H13CO+ for extended emission and for gas temperatures of\\le 400 K toward L1448-mm (0,-10), and possibly also toward L1448-mm (0,0). For compact emission the data cannot be fitted. We do not find any evidence for the over-excitation of the ion fluid toward the newly observed positions around L1448-mm. The observed line emission of SiO, H13CO+ and HN13C toward L1448-mm (0,0) and (0,-10) is consistent with an electron density enhancement in the precursor component, if this emission is spatially extended. This is also true for the case of high gas temperatures (\\le400 K) toward the (0,-10) offset. The electron density enhancement seems to be restricted to the southern, redshifted lobe of the L1448-mm outflow. Interferometric images of the line emission of these molecules are needed to confirm the spatial extent of the over-excitation of the ions and thus, of the electron density enhancement in the magnetic precursor of L1448-mm.", "prompt": "Shock modelling predicts an electron density enhancement within the magnetic precursor of C-shocks. Previous observations of SiO, H13CO+, HN13C and H13CN toward the young L1448-mm outflow showed an over-excitation of the ion fluid that was attributed to an electron density enhancement in the precursor", "orig": ". We re-visit this interpretation and test if it still holds when we consider different source morphologies and kinetic temperatures for the observed molecules, and also give some insight on the spatial extent of the electron density enhancement around L1448-mm. We estimate the opacities of H13CO+ and HN13C", "generated": "Shock modelling predicts an electron density enhancement within the magnetic precursor of C-shocks. Previous observations of SiO, H13CO+, HN13C and H13CN toward the young L1448-mm outflow showed an over-excitation of the ion fluid that was attributed to an electron density enhancement in the precursor"}
{"full_text": "Increasingly sophisticated function development is taking place with the aim of developing efficient, safe and increasingly Automated Driving Functions. This development is possible with the use of diverse data from sources such as Navigation Systems, eHorizon, on-board sensor data, Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (V2V) communication. Increasing challenges arise with the dependency on large amounts of real-time data coming from off-board sources. At the core of addressing these challenges lies the concept of a Digital Dependability Identity (DDI) of a component or system. DDIs are modular, composable, and executable components in the field, facilitating: $\\bullet$ efficient synthesis of component and system dependability information, $\\bullet$ effective evaluation of information for safe and secure composition of highly distributed and autonomous Cyber Physical Systems. In AVL's Connected Powertrain (TM), Automated Driving Functions are tailored to Powertrain Control Strategies that predictively increase energy efficiency according to the powertrain type and its component efficiencies. Simultaneously, the burden on the driver is reduced by optimizing the vehicle velocity, whilst minimizing any journey time penalty.In this work, the development of dependable Automated Driving Functions is exemplified by the Traffic Light Assistant, an adaptive strategy that utilizes predictions of preceding traffic, upcoming road curvature, inclination, speed limits, and especially traffic light signal phase and timing information to increase the energy efficiency in an urban traffic environment. A key aspect of this development is the possibility for seamless and simultaneous development; from office simulation to human-in-the-loop and to real-time tests that include vehicle and powertrain hardware. Driver's acceptance and comfort is rated in an advanced diver simulator mounted on a hexapod, capable of emulating longitudinal and lateral acceleration of a real vehicle. Test results from real-time function validation on a Powertrain Testbed are shown, including real traffic light signal phasing information and traffic flow representation on Graz city roads.", "prompt": "Increasingly sophisticated function development is taking place with the aim of developing efficient, safe and increasingly Automated Driving Functions. This development is possible with the use of diverse data from sources such as Navigation Systems, eHorizon, on-board sensor data, Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (", "orig": "V2V) communication. Increasing challenges arise with the dependency on large amounts of real-time data coming from off-board sources. At the core of addressing these challenges lies the concept of a Digital Dependability Identity (DDI) of a component or system. DDIs are modular, composable, and executable components in the", "generated": "Increasingly sophisticated function development is taking place with the aim of developing efficient, safe and increasingly Automated Driving Functions. This development is possible with the use of diverse data from sources such as Navigation Systems, eHorizon, on-board sensor data, Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle ("}
{"full_text": "Monte Carlo (MC) sampling is a popular method for estimating the statistics (e.g. expectation and variance) of a random variable. Its slow convergence has led to the emergence of advanced techniques to reduce the variance of the MC estimator for the outputs of computationally expensive solvers. The control variates (CV) method corrects the MC estimator with a term derived from auxiliary random variables that are highly correlated with the original random variable. These auxiliary variables may come from surrogate models. Such a surrogate-based CV strategy is extended here to the multilevel Monte Carlo (MLMC) framework, which relies on a sequence of levels corresponding to numerical simulators with increasing accuracy and computational cost. MLMC combines output samples obtained across levels, into a telescopic sum of differences between MC estimators for successive fidelities. In this paper, we introduce three multilevel variance reduction strategies that rely on surrogate-based CV and MLMC. MLCV is presented as an extension of CV where the correction terms devised from surrogate models for simulators of different levels add up. MLMC-CV improves the MLMC estimator by using a CV based on a surrogate of the correction term at each level. Further variance reduction is achieved by using the surrogate-based CVs of all the levels in the MLMC-MLCV strategy. Alternative solutions that reduce the subset of surrogates used for the multilevel estimation are also introduced. The proposed methods are tested on a test case from the literature consisting of a spectral discretization of an uncertain 1D heat equation, where the statistic of interest is the expected value of the integrated temperature along the domain at a given time. The results are assessed in terms of the accuracy and computational cost of the multilevel estimators, depending on whether the construction of the surrogates, and the associated computational cost, precede the evaluation of the estimator. It was shown that when the lower fidelity outputs are strongly correlated with the high-fidelity outputs, a significant variance reduction is obtained when using surrogate models for the coarser levels only. It was also shown that taking advantage of pre-existing surrogate models proves to be an even more efficient strategy.", "prompt": "Monte Carlo (MC) sampling is a popular method for estimating the statistics (e.g. expectation and variance) of a random variable. Its slow convergence has led to the emergence of advanced techniques to reduce the variance of the MC estimator for the outputs of computationally expensive solvers. The control variates (CV", "orig": ") method corrects the MC estimator with a term derived from auxiliary random variables that are highly correlated with the original random variable. These auxiliary variables may come from surrogate models. Such a surrogate-based CV strategy is extended here to the multilevel Monte Carlo (MLMC) framework, which relies on a sequence of levels corresponding", "generated": "Monte Carlo (MC) sampling is a popular method for estimating the statistics (e.g. expectation and variance) of a random variable. Its slow convergence has led to the emergence of advanced techniques to reduce the variance of the MC estimator for the outputs of computationally expensive solvers. The control variates (CV"}
{"full_text": "Scaling temporal dynamics in functional MRI (fMRI) signals have been evidenced for a decade as intrinsic characteristics of ongoing brain activity (Zarahn et al., 1997). Recently, scaling properties were shown to fluctuate across brain networks and to be modulated between rest and task (He, 2011): notably, Hurst exponent, quantifying long memory, decreases under task in activating and deactivating brain regions. In most cases, such results were obtained: First, from univariate (voxelwise or regionwise) analysis, hence focusing on specific cognitive systems such as Resting-State Networks (RSNs) and raising the issue of the specificity of this scale-free dynamics modulation in RSNs. Second, using analysis tools designed to measure a single scaling exponent related to the second order statistics of the data, thus relying on models that either implicitly or explicitly assume Gaussianity and (asymptotic) self-similarity, while fMRI signals may significantly depart from those either of those two assumptions (Ciuciu et al., 2008; Wink et al., 2008). To address these issues, the present contribution elaborates on the analysis of the scaling properties of fMRI temporal dynamics by proposing two significant variations. First, scaling properties are technically investigated using the recently introduced Wavelet Leader-based Multifractal formalism (WLMF; Wendt et al., 2007). This measures a collection of scaling exponents, thus enables a richer and more versatile description of scale invariance (beyond correlation and Gaussianity), referred to as multifractality. Also, it benefits from improved estimation performance compared to tools previously used in the literature. Second, scaling properties are investigated in both RSN and non-RSN structures (e.g., artifacts), at a broader spatial scale than the voxel one, using a multivariate approach, namely the Multi-Subject Dictionary Learning (MSDL) algorithm (Varoquaux et al., 2011) that produces a set of spatial components that appear more sparse than their Independent Component Analysis (ICA) counterpart. These tools are combined and applied to a fMRI dataset comprising 12 subjects with resting-state and activation runs (Sadaghiani et al., 2009). Results stemming from those analysis confirm the already reported task-related decrease of long memory in functional networks, but also show that it occurs in artifacts, thus making this feature not specific to functional networks. Further, results indicate that most fMRI signals appear multifractal at rest except in non-cortical regions. Task-related modulation of multifractality appears only significant in functional networks and thus can be considered as the key property disentangling functional networks from artifacts. These finding are discussed in the light of the recent literature reporting scaling dynamics of EEG microstate sequences at rest and addressing non-stationarity issues in temporally independent fMRI modes.", "prompt": "Scaling temporal dynamics in functional MRI (fMRI) signals have been evidenced for a decade as intrinsic characteristics of ongoing brain activity (Zarahn et al., 1997). Recently, scaling properties were shown to fluctuate across brain networks and to be modulated between rest and task (He, 2011):", "orig": "notably, Hurst exponent, quantifying long memory, decreases under task in activating and deactivating brain regions. In most cases, such results were obtained: First, from univariate (voxelwise or regionwise) analysis, hence focusing on specific cognitive systems such as Resting-State Networks (RSNs) and", "generated": "Scaling temporal dynamics in functional MRI (fMRI) signals have been evidenced for a decade as intrinsic characteristics of ongoing brain activity (Zarahn et al., 1997). Recently, scaling properties were shown to fluctuate across brain networks and to be modulated between rest and task (He, 2011)."}
{"full_text": "The problem of finding a reduced dimensionality representation of categorical variables while preserving their most relevant characteristics is fundamental for the analysis of complex data. Specifically, given a co-occurrence matrix of two variables, one often seeks a compact representation of one variable which preserves information about the other variable. We have recently introduced ``Sufficient Dimensionality Reduction' [GT-2003], a method that extracts continuous reduced dimensional features whose measurements (i.e., expectation values) capture maximal mutual information among the variables. However, such measurements often capture information that is irrelevant for a given task. Widely known examples are illumination conditions, which are irrelevant as features for face recognition, writing style which is irrelevant as a feature for content classification, and intonation which is irrelevant as a feature for speech recognition. Such irrelevance cannot be deduced apriori, since it depends on the details of the task, and is thus inherently ill defined in the purely unsupervised case. Separating relevant from irrelevant features can be achieved using additional side data that contains such irrelevant structures. This approach was taken in [CT-2002], extending the information bottleneck method, which uses clustering to compress the data. Here we use this side-information framework to identify features whose measurements are maximally informative for the original data set, but carry as little information as possible on a side data set. In statistical terms this can be understood as extracting statistics which are maximally sufficient for the original dataset, while simultaneously maximally ancillary for the side dataset. We formulate this tradeoff as a constrained optimization problem and characterize its solutions. We then derive a gradient descent algorithm for this problem, which is based on the Generalized Iterative Scaling method for finding maximum entropy distributions. The method is demonstrated on synthetic data, as well as on real face recognition datasets, and is shown to outperform standard methods such as oriented PCA.", "prompt": "The problem of finding a reduced dimensionality representation of categorical variables while preserving their most relevant characteristics is fundamental for the analysis of complex data. Specifically, given a co-occurrence matrix of two variables, one often seeks a compact representation of one variable which preserves information about the other variable. We have recently introduced ``S", "orig": "ufficient Dimensionality Reduction' [GT-2003], a method that extracts continuous reduced dimensional features whose measurements (i.e., expectation values) capture maximal mutual information among the variables. However, such measurements often capture information that is irrelevant for a given task. Widely known examples are illumination conditions, which are irrelevant as features", "generated": "The problem of finding a reduced dimensionality representation of categorical variables while preserving their most relevant characteristics is fundamental for the analysis of complex data. Specifically, given a co-occurrence matrix of two variables, one often seeks a compact representation of one variable which preserves information about the other variable. We have recently introduced ``S"}
{"full_text": "In 1893, the \"physicist-engineer\" Andr\\'e Blondel invents the oscilloscope for displaying voltage and current variables. With this powerful means of investigation, he first studies the phenomena of the arc then used for the coastal and urban lighting and then, the singing arc used as a transmitter of radio waves in wireless telegraphy. In 1905, he highlights a new type of non-sinusoidal oscillations in the singing arc. Twenty years later, Balthasar van der Pol will recognize that such oscillations were in fact \"relaxation oscillations\". To explain this phenomenon, he uses a representation in the phase plane and shows that its evolution takes the form of small cycles. During World War I the triode gradually replaces the singing arc in transmission systems. At the end of the war, using analogy, Blondel transposes to the triode most of the results he had obtained for the singing arc. In April 1919, he publishes a long memoir in which he introduces the terminology \"self-sustained oscillations\" and proposes to illustrate this concept starting from the example of the Tantalus cup which is then picked up by Van der Pol and Philippe Le Corbeiller. He then provides the definition of a self sustained system which is quite similar to that given later by Aleksandr Andronov and Van der Pol. To study the stability of oscillations sustained by the triode and by the singing arc he uses, this time, a representation in the complex plane and he expresses the amplitude in polar coordinates. He then justifies the maintaining of oscillations by the existence cycles which nearly present all the features of Poincar\\'e's limit cycles. Finally, in November 1919, Blondel performs, a year before Van der Pol, the setting in equation of the triode oscillations. In March 1926, Blondel establishes the differential equation characterizing the oscillations of the singing arc, completely similar to that obtained concomitantly by Van der Pol for the triode. Thus, throughout his career, Blondel, has made fundamental and relatively unknown contributions to the development of the theory of nonlinear oscillations. The purpose of this article is to analyze his main work in this area and to measure their importance or influence by placing them in the perspective of the development of this theory.", "prompt": "In 1893, the \"physicist-engineer\" Andr\\'e Blondel invents the oscilloscope for displaying voltage and current variables. With this powerful means of investigation, he first studies the phenomena of the arc then used for the coastal and urban lighting and then, the singing arc used as a", "orig": "transmitter of radio waves in wireless telegraphy. In 1905, he highlights a new type of non-sinusoidal oscillations in the singing arc. Twenty years later, Balthasar van der Pol will recognize that such oscillations were in fact \"relaxation oscillations\". To explain this phenomenon, he", "generated": "...source of light for the first electric lamps. His innovative device, the oscilloscope, allowed him to visualize and analyze the electrical signals in real-time, a major breakthrough in the field of electrical engineering. By studying the characteristics of the arc, Blondel was able to gain a deeper understanding of the underlying principles"}
{"full_text": "For many decades, ultrahigh energy charged particles of unknown origin that can be observed from the ground have been a puzzle for particle physicists and astrophysicists. As an attempt to discriminate among several possible production scenarios, astrophysicists try to test the statistical isotropy of the directions of arrival of these cosmic rays. At the highest energies, they are supposed to point toward their sources with good accuracy. However, the observations are so rare that testing the distribution of such samples of directional data on the sphere is nontrivial. In this paper, we choose a nonparametric framework that makes weak hypotheses on the alternative distributions and allows in turn to detect various and possibly unexpected forms of anisotropy. We explore two particular procedures. Both are derived from fitting the empirical distribution with wavelet expansions of densities. We use the wavelet frame introduced by [SIAM J. Math. Anal. 38 (2006b) 574-594 (electronic)], the so-called needlets. The expansions are truncated at scale indices no larger than some ${J^{\\star}}$, and the $L^p$ distances between those estimates and the null density are computed. One family of tests (called Multiple) is based on the idea of testing the distance from the null for each choice of $J=1,\\ldots,{J^{\\star}}$, whereas the so-called PlugIn approach is based on the single full ${J^{\\star}}$ expansion, but with thresholded wavelet coefficients. We describe the practical implementation of these two procedures and compare them to other methods in the literature. As alternatives to isotropy, we consider both very simple toy models and more realistic nonisotropic models based on Physics-inspired simulations. The Monte Carlo study shows good performance of the Multiple test, even at moderate sample size, for a wide sample of alternative hypotheses and for different choices of the parameter ${J^{\\star}}$. On the 69 most energetic events published by the Pierre Auger Collaboration, the needlet-based procedures suggest statistical evidence for anisotropy. Using several values for the parameters of the methods, our procedures yield $p$-values below 1%, but with uncontrolled multiplicity issues. The flexibility of this method and the possibility to modify it to take into account a large variety of extensions of the problem make it an interesting option for future investigation of the origin of ultrahigh energy cosmic rays.", "prompt": "For many decades, ultrahigh energy charged particles of unknown origin that can be observed from the ground have been a puzzle for particle physicists and astrophysicists. As an attempt to discriminate among several possible production scenarios, astrophysicists try to test the statistical isotropy of the directions of arrival of", "orig": "these cosmic rays. At the highest energies, they are supposed to point toward their sources with good accuracy. However, the observations are so rare that testing the distribution of such samples of directional data on the sphere is nontrivial. In this paper, we choose a nonparametric framework that makes weak hypotheses on the alternative", "generated": "For many decades, ultrahigh energy charged particles of unknown origin that can be observed from the ground have been a puzzle for particle physicists and astrophysicists. As an attempt to discriminate among several possible production scenarios, astrophysicists try to test the statistical isotropy of the directions of arrival of"}
{"full_text": "This short book is the result of various master and summer school courses I have taught. The objective is to introduce the readers to mathematical control theory, both in finite and infinite dimension. In the finite-dimensional context, we consider controlled ordinary differential equations (ODEs); in this context, existence and uniqueness issues are easily resolved thanks to the Picard-Lindel\\''of (Cauchy-Lipschitz) theorem. In infinite dimension, in view of dealing with controlled partial differential equations (PDEs), the concept of well-posed system is much more difficult and requires to develop a bunch of functional analysis tools, in particular semigroup theory -- and this, just for the setting in which the control system is written and makes sense. This is why I have splitted the book into two parts, the first being devoted to finite-dimensional control systems, and the second to infinite-dimensional ones. In spite of this splitting, it may be nice to learn basics of control theory for finite-dimensional linear autonomous control systems (e.g., the Kalman condition) and then to see in the second part how some results are extended to infinite dimension, where matrices are replaced by operators, and exponentials of matrices are replaced by semigroups. For instance, the reader will see how the Gramian controllability condition is expressed in infinite dimension, and leads to the celebrated Hilbert Uniqueness Method (HUM). Except the very last section, in the second part I have only considered linear autonomous control systems (the theory is already quite complicated), providing anyway several references to other textbooks for the several techniques existing to treat some particular classes of nonlinear PDEs. In contrast, in the first part on finite-dimensional control theory, there are much less difficulties to treat general nonlinear control systems, and I give here some general results on controllability, optimal control and stabilization. Of course, whether in finite or infinite dimension, there exist much finer results and methods in the literature, established however for specific classes of control systems. Here, my objective is to provide the reader with an introduction to control theory and to the main tools allowing to treat general control systems. I hope this will serve as motivation to go deeper into the theory or numerical aspects that are not covered here.", "prompt": "This short book is the result of various master and summer school courses I have taught. The objective is to introduce the readers to mathematical control theory, both in finite and infinite dimension. In the finite-dimensional context, we consider controlled ordinary differential equations (ODEs); in this context, existence and uniqueness issues are easily resolved", "orig": "thanks to the Picard-Lindel\\''of (Cauchy-Lipschitz) theorem. In infinite dimension, in view of dealing with controlled partial differential equations (PDEs), the concept of well-posed system is much more difficult and requires to develop a bunch of functional analysis tools, in particular", "generated": "This short book is the result of various master and summer school courses I have taught. The objective is to introduce the readers to mathematical control theory, both in finite and infinite dimension. In the finite-dimensional context, we consider controlled ordinary differential equations (ODEs); in this context, existence and uniqueness issues are easily resolved"}
{"full_text": "We present Spitzer IRS mid-infrared (5-35 micron) spectra of a complete flux-limited sample (> 3 mJy at 8 micron) of young stellar object (YSO) candidates selected on the basis of their infrared colors in the Serpens Molecular Cloud. Spectra of 147 sources are presented and classified. Background stars (with slope consistent with a reddened stellar spectrum and silicate features in absorption), galaxies (with redshifted PAH features) and a planetary nebula (with high ionization lines) amount to 22% of contamination in this sample, leaving 115 true YSOs. Sources with rising spectra and ice absorption features, classified as embedded Stage I protostars, amount to 18% of the sample. The remaining 82% (94) of the disk sources are analyzed in terms of spectral energy distribution shapes, PAHs and silicate features. The presence, strength and shape of these silicate features are used to infer disk properties for these systems. About 8% of the disks have 30/13 micron flux ratios consistent with cold disks with inner holes or gaps, and 3% of the disks show PAH emission. Comparison with models indicates that dust grains in the surface of these disks have sizes of at least a few \\mu\\m. The 20 micron silicate feature is sometimes seen in absence of the 10 micron feature, which may be indicative of very small holes in these disks. No significant difference is found in the distribution of silicate feature shapes and strengths between sources in clusters and in the field. Moreover, the results in Serpens are compared with other well-studied samples: the c2d IRS sample distributed over 5 clouds and a large sample of disks in the Taurus star-forming region. The remarkably similar distributions of silicate feature characteristics in samples with different environment and median ages - if significant - imply that the dust population in the disk surface results from an equilibrium between dust growth and destructive collision processes that are maintained over a few million years for any YSO population irrespective of environment.", "prompt": "We present Spitzer IRS mid-infrared (5-35 micron) spectra of a complete flux-limited sample (> 3 mJy at 8 micron) of young stellar object (YSO) candidates selected on the basis of their infrared colors in the Serpens Molecular Cloud. Spectra of 147", "orig": "sources are presented and classified. Background stars (with slope consistent with a reddened stellar spectrum and silicate features in absorption), galaxies (with redshifted PAH features) and a planetary nebula (with high ionization lines) amount to 22% of contamination in this sample, leaving 115 true YSO", "generated": "We present Spitzer IRS mid-infrared (5-35 micron) spectra of a complete flux-limited sample (> 3 mJy at 8 micron) of young stellar object (YSO) candidates selected on the basis of their infrared colors in the Serpens Molecular Cloud. Spectra of 147"}
{"full_text": "In the frame of the test of NISP instrument for ESA Euclid mission, the question was raised to perform a metrology measurement of different components during the thermal vacuum test of NISP instrument. NISP will be tested at Laboratoire d'Astrophysique de Marseille (LAM) in ERIOS chamber under vacuum and thermal conditions in order to qualify the instrument in its operating environment and to perform the final acceptance test before delivery to the payload. One of the main objectives of the test campaign will be the measurement of the focus position of NISP image plane with respect to the EUCLID object plane. To simulate the EUCLID object plane, a telescope simulator with a very well know focal distance will be installed in front of NISP into ERIOS chamber. We need to measure at cold and vacuum the position of reflectors installed on NISP instrument and the telescope simulator. From these measurements, we will provide at operational temperature the measurement of references frames set on the telescope simulator and NISP, the knowledge of the coordinates of the object point source provided by the telescope simulator and the measurement of the angle between the telescope simulator optical axis and NISP optical axis. In this context, we have developed a metrology method based on the use of a laser tracker to measure the position of the reflectors inside ERIOS. The laser tracker is installed outside the vacuum chamber and measure through a curved window reflectors put inside the chamber either at ambient pressure or vacuum pressure. Several tests campaigns have been done at LAM to demonstrate the measurement performance with this configuration. Using a well know reflectors configuration, we show that it is possible to correct the laser tracker measurement from the window disturbances and from the vacuum impact. A corrective term is applied to the data and allows retrieving the real coordinates of the reflectors with a bias lower than 30$\\mu$m, which is lower than the laser tracker measurement uncertainties estimated at 60$\\mu$m. No additional error term of the laser tracker measurement is observed when using the laser tracker with the curved window and in vacuum, comparing with a classical use of the laser tracker. With these test campaign, we have been able to demonstrate the possibility to use a laser tracker to measure in real time during a vacuum thermal test the position of different mechanical parts into a vacuum chamber with an accuracy better than 60$\\mu$m.", "prompt": "In the frame of the test of NISP instrument for ESA Euclid mission, the question was raised to perform a metrology measurement of different components during the thermal vacuum test of NISP instrument. NISP will be tested at Laboratoire d'Astrophysique de Marseille (LAM) in ERIOS", "orig": "chamber under vacuum and thermal conditions in order to qualify the instrument in its operating environment and to perform the final acceptance test before delivery to the payload. One of the main objectives of the test campaign will be the measurement of the focus position of NISP image plane with respect to the EUCLID object plane. To simulate the", "generated": "In the frame of the test of NISP instrument for ESA Euclid mission, the question was raised to perform a metrology measurement of different components during the thermal vacuum test of NISP instrument. NISP will be tested at Laboratoire d'Astrophysique de Marseille (LAM) in ERIOS"}
{"full_text": "Reaction-diffusion analytical modeling of predator-prey systems has shown that specialist natural enemies can slow, stop and even reverse pest invasions, assuming that the prey population displays a strong Allee effect in its growth. Few additional analytical results have been obtained for other spatially distributed predator-prey systems, as traveling waves of non-monotonous systems are notoriously difficult to obtain. Traveling waves have indeed recently been shown to exist in predator-prey systems, but the direction of the wave, an essential item of information in the context of the control of biological invasions, is generally unknown. Preliminary numerical explorations have hinted that control by generalist predators might be possible for prey populations displaying logistic growth. We aimed to formalize the conditions in which spatial biological control can be achieved by generalists, through an analytical approach based on reaction-diffusion equations. The population of the focal prey - the invader - is assumed to grow according to a logistic function. The predator has a type II functional response and is present everywhere in the domain, at its carrying capacity, on alternative hosts. Control, defined as the invader becoming extinct in the domain, may result from spatially independent demographic dynamics or from a spatial extinction wave. Using comparison principles, we obtain sufficient conditions for control and for invasion, based on scalar bistable partial differential equations (PDEs). The searching efficiency and functional response plateau of the predator are identified as the main parameters defining the parameter space for prey extinction and invasion. Numerical explorations are carried out in the region of those control parameters space between the super-and subso-lutions, in which no conclusion about controllability can be drawn on the basis of analytical solutions. The ability of generalist predators to control prey populations with logistic growth lies in the bis-table dynamics of the coupled system, rather than in the bistability of prey-only dynamics as observed for specialist predators attacking prey populations displaying Allee effects. The consideration of space in predator-prey systems involving generalist predators with a parabolic functional response is crucial. Analysis of the ordinary differential equations (ODEs) system identifies parameter regions with monostable (extinction) and bistable (extinction or invasion) dynamics. By contrast, analysis of the associated PDE system distinguishes different and additional regions of invasion and extinction. Depending on the relative positions of these different zones, four patterns of spatial dynamics can be identified : traveling waves of extinction and invasion, pulse waves of extinction and heterogeneous stationary positive solutions of the Turing type. As a consequence, prey control is predicted to be possible when space is considered in additional situations other than those identified without considering space. The reverse situation is also possible. None of these considerations apply to spatial predator-prey systems with specialist natural enemies.", "prompt": "Reaction-diffusion analytical modeling of predator-prey systems has shown that specialist natural enemies can slow, stop and even reverse pest invasions, assuming that the prey population displays a strong Allee effect in its growth. Few additional analytical results have been obtained for other spatially distributed predator-prey systems, as traveling waves", "orig": "of non-monotonous systems are notoriously difficult to obtain. Traveling waves have indeed recently been shown to exist in predator-prey systems, but the direction of the wave, an essential item of information in the context of the control of biological invasions, is generally unknown. Preliminary numerical explorations have hinted that control", "generated": "Reaction-diffusion analytical modeling of predator-prey systems has shown that specialist natural enemies can slow, stop and even reverse pest invasions, assuming that the prey population displays a strong Allee effect in its growth. Few additional analytical results have been obtained for other spatially distributed predator-prey systems, as traveling wave"}
{"full_text": "Ion accelerators have been used by material scientists for decades to investigate radiation damage formation in nuclear materials and thus to emulate neutron-induced changes. The versatility of conditions in terms of particle energy, dose rate, fluence, etc., is a key asset of ion beams allowing for fully instrumented analytical studies. In addition, very short irradiation times and handling of non-radioactive samples dramatically curtail the global cost and duration as compared to in-reactor testing. Coupling of two or more beams, use of heated/cooled sample holders, and implementation of in situ characterization and microscopy pave the way to real time observation of microstructural and property evolution in various extreme radiation conditions more closely mimicking the nuclear environments. For these reasons, multiple ion beam facilities have been commissioned worldwide. In France, under the auspices of the Universit{\\'e} Paris-Saclay, the JANNuS platform for 'Joint Accelerators for Nanosciences and Nuclear Simulation' comprises five ion implanter and electrostatic accelerators with complementary performances. At CSNSM (CNRS \\& Univ Paris-Sud, Orsay), a 200 kV Transmission Electron Microscope is coupled to an accelerator and an implanter for in situ observation of microstructure modifications induced by ion beams in a material, making important contribution to the understanding of physical phenomena at the nanoscale. At CEA Paris-Saclay, the unique triple beam facility in Europe allows the simultaneous irradiation with heavy ions (like Fe, W) for nuclear recoil damage and implantation of a large array of ions including gasses for well-controlled modelling-oriented experiments. Several classes of materials are of interest for the nuclear industry ranging from metals and alloys, to oxides or glasses and carbides. This paper gives selected examples that illustrate the use of JANNuS ion beams in investigating the radiation resistance of structural materials for today's and tomorrow's nuclear reactors.", "prompt": "Ion accelerators have been used by material scientists for decades to investigate radiation damage formation in nuclear materials and thus to emulate neutron-induced changes. The versatility of conditions in terms of particle energy, dose rate, fluence, etc., is a key asset of ion beams allowing for fully instrumented analytical studies. In addition,", "orig": "very short irradiation times and handling of non-radioactive samples dramatically curtail the global cost and duration as compared to in-reactor testing. Coupling of two or more beams, use of heated/cooled sample holders, and implementation of in situ characterization and microscopy pave the way to real time observation of microstructural and property", "generated": "Ion accelerators have been used by material scientists for decades to investigate radiation damage formation in nuclear materials and thus to emulate neutron-induced changes. The versatility of conditions in terms of particle energy, dose rate, fluence, etc., is a key asset of ion beams allowing for fully instrumented analytical studies. In addition,"}
{"full_text": "Lean processes focus on doing only necessery things in an efficient way. Artificial intelligence and Machine Learning offer new opportunities to optimizing processes. The presented approach demonstrates an improvement of the test process by using Machine Learning as a support tool for test management. The scope is the semi-automation of the selection of regression tests. The proposed lean testing process uses Machine Learning as a supporting machine, while keeping the human test manager in charge of the adequate test case selection. 1 Introduction Many established long running projects and programs are execute regression tests during the release tests. The regression tests are the part of the release test to ensure that functionality from past releases still works fine in the new release. In many projects, a significant part of these regression tests are not automated and therefore executed manually. Manual tests are expensive and time intensive [1], which is why often only a relevant subset of all possible regression tests are executed in order to safe time and money. Depending on the software process, different approaches can be used to identify the right set of regression tests. The source code file level is a frequent entry point for this identification [2]. Advanced approaches combine different file level methods [3]. To handle black-box tests, methods like [4] or [5] can be used for test case prioritiza-tion. To decide which tests can be skipped, a relevance ranking of the tests in a regression test suite is needed. Based on the relevance a test is in or out of the regression test set for a specific release. This decision is a task of the test manager supported by experts. The task can be time-consuming in case of big (often a 4-to 5-digit number) regression test suites because the selection is specific to each release. Trends are going to continuous prioritization [6], which this work wants to support with the presented ML based approach for black box regression test case prioritization. Any regression test selection is made upon release specific changes. Changes can be new or deleted code based on refactoring or implementation of new features. But also changes on externals systems which are connected by interfaces have to be considered", "prompt": "Lean processes focus on doing only necessery things in an efficient way. Artificial intelligence and Machine Learning offer new opportunities to optimizing processes. The presented approach demonstrates an improvement of the test process by using Machine Learning as a support tool for test management. The scope is the semi-automation of the selection of regression tests. The", "orig": "proposed lean testing process uses Machine Learning as a supporting machine, while keeping the human test manager in charge of the adequate test case selection. 1 Introduction Many established long running projects and programs are execute regression tests during the release tests. The regression tests are the part of the release test to ensure that functionality from past releases still", "generated": "Lean processes focus on doing only necessary things in an efficient way. Artificial intelligence and Machine Learning offer new opportunities to optimize processes. The presented approach demonstrates an improvement of the test process by using Machine Learning as a support tool for test management. The scope is the semi-automation of the selection of regression tests. \n\nRegression"}
{"full_text": "We have shown, in a series of articles, that a classical description of a large number of economic agents can be replaced by a statistical fields formalism. To better understand the accumulation and allocation of capital among different sectors, the present paper applies this statistical fields description to a large number of heterogeneous agents divided into two groups. The first group is composed of a large number of firms in different sectors that collectively own the entire physical capital. The second group, investors, holds the entire financial capital and allocates it between firms across sectors according to investment preferences, expected returns, and stock prices variations on financial markets. In return, firms pay dividends to their investors. Financial capital is thus a function of dividends and stock valuations, whereas physical capital is a function of the total capital allocated by the financial sector. Whereas our previous work focused on the background fields that describe potential long-term equilibria, here we compute the transition functions of individual agents and study their probabilistic dynamics in the background field, as a function of their initial state. We show that capital accumulation depends on various factors. The probability associated with each firm's trajectories is the result of several contradictory effects: the firm tends to shift towards sectors with the greatest long-term return, but must take into account the impact of its shift on its attractiveness for investors throughout its trajectory. Since this trajectory depends largely on the average capital of transition sectors, a firm's attractiveness during its relocation depends on the relative level of capital in those sectors. Thus, an under-capitalized firm reaching a high-capital sector will experience a loss of attractiveness, and subsequently, in investors. Moreover, the firm must also consider the effects of competition in the intermediate sectors. An under-capitalized firm will tend to be ousted out towards sectors with lower average capital, while an over-capitalized firm will tend to shift towards higher averagecapital sectors. For investors, capital allocation depends on their short and long-term returns. These returns are not independent: in the short-term, returns are composed of both the firm's dividends and the increase in its stock prices. In the long-term, returns are based on the firm's growth expectations, but also, indirectly, on expectations of higher stock prices. Investors' capital allocation directly depends on the volatility of stock prices and {\\ldots}rms'dividends. Investors will tend to reallocate their capital to maximize their short and long-term returns. The higher their level of capital, the stronger the reallocation will be.", "prompt": "We have shown, in a series of articles, that a classical description of a large number of economic agents can be replaced by a statistical fields formalism. To better understand the accumulation and allocation of capital among different sectors, the present paper applies this statistical fields description to a large number of heterogeneous agents divided into two groups", "orig": ". The first group is composed of a large number of firms in different sectors that collectively own the entire physical capital. The second group, investors, holds the entire financial capital and allocates it between firms across sectors according to investment preferences, expected returns, and stock prices variations on financial markets. In return, firms pay dividends", "generated": "We have shown, in a series of articles, that a classical description of a large number of economic agents can be replaced by a statistical fields formalism. To better understand the accumulation and allocation of capital among different sectors, the present paper applies this statistical fields description to a large number of heterogeneous agents divided into two groups"}
{"full_text": "In this study, we investigate porosity evolution through the sedimentary input section of the North Sumatra Subduction zone by quantifying interstitial porosity, bound water content and fracture porosity based on IODP Expedition 362 data and post-cruise chemical analyses. During IODP Expedition 362, total porosity of the sedimentary section entering the North Sumatra subduction zone was measured. This total porosity is derived from the total water content of core samples thus including pore water and water bound to hydrous minerals like smectite. Clay mineral composition varies over the sedimentary section and is mainly kaolinite/illite in the Nicobar Fan units and smectite/illite in the prefan pelagic unit below. The prefan pelagic unit shows anomalously high total porosity values and is stratigraphically correlated to a high amplitude negative polarity (HANP) seismic reflector located landward. This HANP reflector has been previously interpreted as a porous fluid-rich layer where the d{\\'e}collement may develop along parts of the margin as a consequence of pore pressure buildup. We estimate clay bound water content from Cation Exchange Capacity (CEC) which gives information about the smectite/illite composition and soluble chloride content data. Interstitial porosity corresponds to onboard total porosity corrected from clay bound water and is more relevant in terms of sediment compaction state and fluid flow properties. Interstitial porosity versus vertical effective stress curve shows no evidence of undercompaction and suggests that the input section 2 has been experiencing normal consolidation due to high sediment accumulation rate. The porosity anomaly observed in the prefan pelagic unit results from the local occurrence of water-bearing minerals like smectite rather than excess pore pressure, which might, however, buildup more landward in the basin. We also estimate fracture porosity using a resistivity model for shales used in previous works based on wireline resistivity log and show that fracture porosity yields 4-6% in damaged parts of the sedimentary section investigated.", "prompt": "In this study, we investigate porosity evolution through the sedimentary input section of the North Sumatra Subduction zone by quantifying interstitial porosity, bound water content and fracture porosity based on IODP Expedition 362 data and post-cruise chemical analyses. During IODP Expedition 362, total", "orig": "porosity of the sedimentary section entering the North Sumatra subduction zone was measured. This total porosity is derived from the total water content of core samples thus including pore water and water bound to hydrous minerals like smectite. Clay mineral composition varies over the sedimentary section and is mainly kaolinite", "generated": "In this study, we investigate porosity evolution through the sedimentary input section of the North Sumatra Subduction zone by quantifying interstitial porosity, bound water content and fracture porosity based on IODP Expedition 362 data and post-cruise chemical analyses. During IODP Expedition 362, total"}
{"full_text": "Due to increasing numbers of product piracy worldwide, a cost-effective method for verifying the origin of a product is to be developed. For this purpose, a certificate of authenticity can be created using precisely measurable, unique properties of special physical objects that are difficult to reconstruct. In the context of the present work, this is a counterfeit-proof label composed of randomly distributed gold nanospheres or rods in a semi-transparent material. The characteristic positioning of the label's elements can be precisely measured using a smartphone's camera and additional technologies. This can create an offline usable verification method for the general public without the need for an existing network connection. The present work provides a first part of the proof of concept that such a system and especially the associated algorithmic computation method can be implemented and efficiently used in a mobile application. In addition, a method suitable in practice for transmitting and securing the required information is determined in each case. Furthermore, the results of the validation of counterfeit-proof product labels are analyzed in detail and existing weaknesses are pointed out. -- Auf Grund weltweit steigender Zahlen der Produktpiraterie soll ein kosteng\\\"unstiges Verfahren zur Verifizierung der Herkunft eines Produktes entwickelt werden. Daf\\\"ur l\\\"asst sich durch exakt messbare, einzigartige, jedoch schwer rekonstruierbare Eigenschaften spezieller physischer Objekte ein Echtheitszertifikat kreieren. Dieses ist im Kontext der vorliegenden Arbeit ein f\\\"alschungssicheres Label, das sich in einem semi-transparenten Material aus zuf\\\"allig verteilten Goldnanok\\\"ugelchen oder -st\\\"abchen zusammensetzt. Die charakteristischen Positionierungen der Elemente des Labels lassen sich mit der Kamera eines Smartphones und zus\\\"atzlichen Technologien pr\\\"azise messen. Dadurch kann f\\\"ur die breite Bev\\\"olkerung ohne die Notwendigkeit einer bestehenden Netzwerkverbindung ein offline verwendbares Verifikationsverfahren erschaffen werden. Die vorliegende Arbeit liefert einen ersten Teil des Machbarkeitsnachweises, dass ein derartiges System und insbesondere das damit einhergehende algorithmische Berechnungsverfahren in einer mobilen Applikation implementier -- und effizient einsetzbar ist. Zudem wird je eine in der Praxis geeignete Methode zur \\\"Ubermittlung und Sicherung der ben\\\"otigten Informationen eruiert. Des Weiteren werden die Resultate der Validierung von f\\\"alschungssicheren Produktlabeln ausf\\\"uhrlich analysiert und vorhandene Schw\\\"achen aufgezeigt.", "prompt": "Due to increasing numbers of product piracy worldwide, a cost-effective method for verifying the origin of a product is to be developed. For this purpose, a certificate of authenticity can be created using precisely measurable, unique properties of special physical objects that are difficult to reconstruct. In the context of the present work, this is a", "orig": "counterfeit-proof label composed of randomly distributed gold nanospheres or rods in a semi-transparent material. The characteristic positioning of the label's elements can be precisely measured using a smartphone's camera and additional technologies. This can create an offline usable verification method for the general public without the need for an existing network connection. The present work", "generated": "Due to increasing numbers of product piracy worldwide, a cost-effective method for verifying the origin of a product is to be developed. For this purpose, a certificate of authenticity can be created using precisely measurable, unique properties of special physical objects that are difficult to reconstruct. In the context of the present work, this is a"}
{"full_text": "Context: Domain-specific languages (DSLs) enable domain experts to specify tasks and problems themselves, while enabling static analysis to elucidate issues in the modelled domain early. Although language workbenches have simplified the design of DSLs and extensions to general purpose languages, static analyses must still be implemented manually. Inquiry: Moreover, static analyses, e.g., complexity metrics, dependency analysis, and declaration-use analysis, are usually domain-dependent and cannot be easily reused. Therefore, transferring existing static analyses to another DSL incurs a huge implementation overhead. However, this overhead is not always intrinsically necessary: in many cases, while the concepts of the DSL on which a static analysis is performed are domain-specific, the underlying algorithm employed in the analysis is actually domain-independent and thus can be reused in principle, depending on how it is specified. While current approaches either implement static analyses internally or with an external Visitor, the implementation is tied to the language's grammar and cannot be reused easily. Thus far, a commonly used approach that achieves reusable static analysis relies on the transformation into an intermediate representation upon which the analysis is performed. This, however, entails a considerable additional implementation effort. Approach: To remedy this, it has been proposed to map the necessary domain-specific concepts to the algorithm's domain-independent data structures, yet without a practical implementation and the demonstration of reuse. Thus, to make static analysis reusable again, we employ relational Reference Attribute Grammars (RAGs) by creating such a mapping to a domain-independent overlay structure using higher-order attributes. Knowledge: We describe how static analysis can be specified on analysis-specific data structures, how relational RAGs can help with the specification, and how a mapping from the domain-specific language can be performed. Furthermore, we demonstrate how a static analysis for a DSL can be externalized and reused in another general purpose language. Grounding: The approach was evaluated using the RAG system JastAdd. To illustrate reusability, we implemented two analyses with two addressed languages each: a cycle detection analysis used in a small state machine DSL and for detecting circular dependencies in Java types and packages, and an analysis of variable shadowing, applied to both Java and the Modelica modelling language. Thereby, we demonstrate the reuse of two analysis algorithms in three completely different domains. Additionally, we use the cycle detection analysis to evaluate the efficiency by comparing our external analysis to an internal reference implementation analysing all Java programs in the Qualitas Corpus and thereby are able to show that an externalized analysis incurs only minimal overhead. Importance: We make static analysis reusable, again, showing the practicality and efficiency of externalizing static analysis for both DSLs and general purpose languages using relational RAGs.", "prompt": "Context: Domain-specific languages (DSLs) enable domain experts to specify tasks and problems themselves, while enabling static analysis to elucidate issues in the modelled domain early. Although language workbenches have simplified the design of DSLs and extensions to general purpose languages, static analyses must still be implemented manually. Inquiry:", "orig": "Moreover, static analyses, e.g., complexity metrics, dependency analysis, and declaration-use analysis, are usually domain-dependent and cannot be easily reused. Therefore, transferring existing static analyses to another DSL incurs a huge implementation overhead. However, this overhead is not always intrinsically necessary: in many cases, while the concepts", "generated": "Domain-specific languages (DSLs) enable domain experts to specify tasks and problems themselves, while enabling static analysis to elucidate issues in the modelled domain early. Although language workbenches have simplified the design of DSLs and extensions to general-purpose languages, static analyses must still be implemented manually.\n\nAs a result,"}
{"full_text": "The Lasso has attracted the attention of many authors these last years. While many efforts have been made to prove that the Lasso behaves like a variable selection procedure at the price of strong (though unavoidable) assumptions on the geometric structure of these variables, much less attention has been paid to the analysis of the performance of the Lasso as a regularization algorithm. Our first purpose here is to provide a conceptually very simple result in this direction. We shall prove that, provided that the regularization parameter is properly chosen, the Lasso works almost as well as the deterministic Lasso. This result does not require any assumption at all, neither on the structure of the variables nor on the regression function. Our second purpose is to introduce a new estimator particularly adapted to deal with infinite countable dictionaries. This estimator is constructed as an l0-penalized estimator among a sequence of Lasso estimators associated to a dyadic sequence of growing truncated dictionaries. The selection procedure automatically chooses the best level of truncation of the dictionary so as to make the best tradeoff between approximation, l1-regularization and sparsity. From a theoretical point of view, we shall provide an oracle inequality satisfied by this selected Lasso estimator. The oracle inequalities established for the Lasso and the selected Lasso estimators shall enable us to derive rates of convergence on a wide class of functions, showing that these estimators perform at least as well as greedy algorithms. Besides, we shall prove that the rates of convergence achieved by the selected Lasso estimator are optimal in the orthonormal case by bounding from below the minimax risk on some Besov bodies. Finally, some theoretical results about the performance of the Lasso for infinite uncountable dictionaries will be studied in the specific framework of neural networks. All the oracle inequalities presented in this paper are obtained via the application of a single general theorem of model selection among a collection of nonlinear models which is a direct consequence of the Gaussian concentration inequality. The key idea that enables us to apply this general theorem is to see l1-regularization as a model selection procedure among l1-balls.", "prompt": "The Lasso has attracted the attention of many authors these last years. While many efforts have been made to prove that the Lasso behaves like a variable selection procedure at the price of strong (though unavoidable) assumptions on the geometric structure of these variables, much less attention has been paid to the analysis of the performance of", "orig": "the Lasso as a regularization algorithm. Our first purpose here is to provide a conceptually very simple result in this direction. We shall prove that, provided that the regularization parameter is properly chosen, the Lasso works almost as well as the deterministic Lasso. This result does not require any assumption at all, neither on", "generated": "The Lasso has attracted the attention of many authors these last years. While many efforts have been made to prove that the Lasso behaves like a variable selection procedure at the price of strong (though unavoidable) assumptions on the geometric structure of these variables, much less attention has been paid to the analysis of the performance of"}
{"full_text": "In planetary fluid cores, the density depends on temperature and chemical composition, which diffuse at very different rates. This leads to various instabilities, bearing the name of double-diffusive convection. We investigate rotating double-diffusive convection (RDDC) in fluid spheres. We use the Boussinesq approximation with homogeneous internal thermal and compositional source terms. We focus on the finger regime, in which the thermal gradient is stabilising whereas the compositional one is destabilising. First, we perform a global linear stability analysis in spheres. The critical Rayleigh numbers drastically drop for stably stratified fluids, yielding large-scale convective motions where local analyses predict stability. We evidence the inviscid nature of this large-scale double-diffusive instability, enabling the determination of the marginal stability curve at realistic planetary regimes. In particular, we show that in stably stratified spheres, the Rayleigh numbers $Ra$ at the onset evolve like $Ra \\sim Ek^{-1}$, where $Ek$ is the Ekman number. This differs from rotating convection in unstably stratified spheres, for which $Ra \\sim Ek^{-4/3}$. The domain of existence of inviscid convection thus increases as $Ek^{-1/3}$. Second, we perform nonlinear simulations. We find a transition between two regimes of RDDC, controlled by the strength of the stratification. Furthermore, far from the RDDC onset, we find a dominating equatorially anti-symmetric, large-scale zonal flow slightly above the associated linear onset. Unexpectedly, a purely linear mechanism can explain this phenomenon, even far from the instability onset, yielding a symmetry breaking of the nonlinear flow at saturation. For even stronger stable stratification, the flow becomes mainly equatorially-symmetric and intense zonal jets develop. Finally, we apply our results to the early Earth core. Double diffusion can reduce the critical Rayleigh number by four decades for realistic core conditions. We suggest that the early Earth core was prone to turbulent RDDC, with large-scale zonal flows.", "prompt": "In planetary fluid cores, the density depends on temperature and chemical composition, which diffuse at very different rates. This leads to various instabilities, bearing the name of double-diffusive convection. We investigate rotating double-diffusive convection (RDDC) in fluid spheres. We use the Boussinesq", "orig": "approximation with homogeneous internal thermal and compositional source terms. We focus on the finger regime, in which the thermal gradient is stabilising whereas the compositional one is destabilising. First, we perform a global linear stability analysis in spheres. The critical Rayleigh numbers drastically drop for stably stratified fluids, yielding large-scale", "generated": "In planetary fluid cores, the density depends on temperature and chemical composition, which diffuse at very different rates. This leads to various instabilities, bearing the name of double-diffusive convection. We investigate rotating double-diffusive convection (RDDC) in fluid spheres. We use the Boussinesq"}
{"full_text": "This paper presents Skini, a programming methodology and an execution environment for interactive structured music. With this system, the composer programs his scores in the HipHop.js synchronous reactive language. They are then executed, or played, in live concerts, in interaction with the audience. The system aims at helping composers to find a good balance between the determinism of the compositions and the nondeterminism of the interactions with the public. Each execution of a Skini score yields to a different but aesthetically consistent interpretation. This work raises many questions in the musical fields. How to combine composition and interaction? How to control the musical style when the audience influences what is to play next? What are the possible connections with generative music? These are important questions for the Skini system but they are out of the scope of this paper that focuses exclusively on the computer science aspects of the system. From that perspective, the main questions are how to program the scores and in which language? General purpose languages are inappropriate because their elementary constructs (i.e., variables, functions, loops, etc.) do not match the constructions needed to express music and musical constraints. We show that synchronous programming languages are a much better fit because they rely on temporal constructs that can be directly used to represent musical scores and because their malleability enables composers to experiment easily with artistic variations of their initial scores. The paper mostly focuses on scores programming. It exposes the process a composer should follow from his very first musical intuitions up to the generation of a musical artifact. The paper presents some excerpts of the programming of a classical music composition that it then precisely relates to an actual recording. Examples of techno music and jazz are also presented, with audio artifact, to demonstrate the versatility of the system. Finally, brief presentations of past live concerts are presented as an evidence of viability of the system.", "prompt": "This paper presents Skini, a programming methodology and an execution environment for interactive structured music. With this system, the composer programs his scores in the HipHop.js synchronous reactive language. They are then executed, or played, in live concerts, in interaction with the audience. The system aims at helping composers to find a", "orig": "good balance between the determinism of the compositions and the nondeterminism of the interactions with the public. Each execution of a Skini score yields to a different but aesthetically consistent interpretation. This work raises many questions in the musical fields. How to combine composition and interaction? How to control the musical style when the", "generated": "This paper presents Skini, a programming methodology and an execution environment for interactive structured music. With this system, the composer programs his scores in the HipHop.js synchronous reactive language. They are then executed, or played, in live concerts, in interaction with the audience. The system aims at helping composers to find a"}
{"full_text": "Graph Neural Networks share with Logic Programming several key relational inference mechanisms. The datasets on which they are trained and evaluated can be seen as database facts containing ground terms. This makes possible modeling their inference mechanisms with equivalent logic programs, to better understand not just how they propagate information between the entities involved in the machine learning process but also to infer limits on what can be learned from a given dataset and how well that might generalize to unseen test data. This leads us to the key idea of this paper: modeling with the help of a logic program the information flows involved in learning to infer from the link structure of a graph and the information content of its nodes properties of new nodes, given their known connections to nodes with possibly similar properties. The problem is known as graph node property prediction and our approach will consist in emulating with help of a Prolog program the key information propagation steps of a Graph Neural Network's training and inference stages. We test our a approach on the ogbn-arxiv node property inference benchmark. To infer class labels for nodes representing papers in a citation network, we distill the dependency trees of the text associated to each node into directed acyclic graphs that we encode as ground Prolog terms. Together with the set of their references to other papers, they become facts in a database on which we reason with help of a Prolog program that mimics the information propagation in graph neural networks predicting node properties. In the process, we invent ground term similarity relations that help infer labels in the test set by propagating node properties from similar nodes in the training set and we evaluate their effectiveness in comparison with that of the graph's link structure. Finally, we implement explanation generators that unveil performance upper bounds inherent to the dataset. As a practical outcome, we obtain a logic program, that, when seen as machine learning algorithm, performs close to the state of the art on the node property prediction benchmark.", "prompt": "Graph Neural Networks share with Logic Programming several key relational inference mechanisms. The datasets on which they are trained and evaluated can be seen as database facts containing ground terms. This makes possible modeling their inference mechanisms with equivalent logic programs, to better understand not just how they propagate information between the entities involved in the machine learning process but", "orig": "also to infer limits on what can be learned from a given dataset and how well that might generalize to unseen test data. This leads us to the key idea of this paper: modeling with the help of a logic program the information flows involved in learning to infer from the link structure of a graph and the information content of its", "generated": "Graph Neural Networks share with Logic Programming several key relational inference mechanisms. The datasets on which they are trained and evaluated can be seen as database facts containing ground terms. This makes possible modeling their inference mechanisms with equivalent logic programs, to better understand not just how they propagate information between the entities involved in the machine learning process but"}
{"full_text": "Context: Software development tools that interact with running programs such as debuggers, profilers, and dynamic analysis frameworks are presumed to demand difficult tradeoffs among implementation complexity (cost), functionality, usability, and performance. Among the many consequences, tools are often delivered late (if ever), have limited functionality, require non-standard configurations, and impose serious performance costs on running programs. Inquiry: Can flexible tool support become a practical, first class, intrinsic requirement for a modern highperformance programming language implementation framework? Approach: We extended the Truffle Language Implementation Framework, which together with the GraalVM execution environment makes possible very high performance language implementations. Truffle's new Instrumentation Framework is language-agnostic and designed to derive high performance from the same technologies as do language implementations. Truffle Instrumentation includes: (1) low overhead capture of execution events by dynamically adding \"wrapper\" nodes to executing ASTs; (2) extensions to the Language Implementation Framework that allow per-language specialization, primarily for visual display of values and names, among others; and (3) versatile APIs and support services for implementing many kinds of tools without VM modification. Knowledge: It is now possible for a client in a production environment to insert (dynamically, with thread safety) an instrumentation probe that captures and reports abstractly specified execution events. A probe in fully optimized code imposes very low overhead until actually used to access (or modify) execution state. Event capture has enabled construction of numerous GraalVM services and tools that work for all implemented languages, either singly or in combination. Instrumentation has also proved valuable for implementing some traditionally tricky language features, as well as some GraalVM services such as placing bounds on resources consumed by running programs. Grounding: Tools for debugging (via multiple clients), profiling, statement counting, dynamic analysis, and others are now part of GraalVM or are in active development. Third parties have also used Truffle Instrumentation for innovative tool implementations. Importance: Experience with Truffle Instrumentation validates the notion that addressing developer tools support as a forethought can change expectations about the availability of practical, efficient tools for high-performance languages. Tool development becomes a natural part of language implementation, requiring little additional effort and offering the advantage of early and continuous availability.", "prompt": "Context: Software development tools that interact with running programs such as debuggers, profilers, and dynamic analysis frameworks are presumed to demand difficult tradeoffs among implementation complexity (cost), functionality, usability, and performance. Among the many consequences, tools are often delivered late (if ever), have limited functionality, require non-standard", "orig": "configurations, and impose serious performance costs on running programs. Inquiry: Can flexible tool support become a practical, first class, intrinsic requirement for a modern highperformance programming language implementation framework? Approach: We extended the Truffle Language Implementation Framework, which together with the GraalVM execution environment makes possible very high performance language implementations.", "generated": "Software development tools that interact with running programs such as debuggers, profilers, and dynamic analysis frameworks are presumed to demand difficult tradeoffs among implementation complexity (cost), functionality, usability, and performance. Among the many consequences, tools are often delivered late (if ever), have limited functionality, require non-standard interfaces,"}
{"full_text": "The analysis of practical probabilistic models on the computer demands a convenient representation for the available knowledge and an efficient algorithm to perform inference. An appealing representation is the influence diagram, a network that makes explicit the random variables in a model and their probabilistic dependencies. Recent advances have developed solution procedures based on the influence diagram. In this paper, we examine the fundamental properties that underlie those techniques, and the information about the probabilistic structure that is available in the influence diagram representation. The influence diagram is a convenient representation for computer processing while also being clear and non-mathematical. It displays probabilistic dependence precisely, in a way that is intuitive for decision makers and experts to understand and communicate. As a result, the same influence diagram can be used to build, assess and analyze a model, facilitating changes in the formulation and feedback from sensitivity analysis. The goal in this paper is to determine arbitrary conditional probability distributions from a given probabilistic model. Given qualitative information about the dependence of the random variables in the model we can, for a specific conditional expression, specify precisely what quantitative information we need to be able to determine the desired conditional probability distribution. It is also shown how we can find that probability distribution by performing operations locally, that is, over subspaces of the joint distribution. In this way, we can exploit the conditional independence present in the model to avoid having to construct or manipulate the full joint distribution. These results are extended to include maximal processing when the information available is incomplete, and optimal decision making in an uncertain environment. Influence diagrams as a computer-aided modeling tool were developed by Miller, Merkofer, and Howard [5] and extended by Howard and Matheson [2]. Good descriptions of how to use them in modeling are in Owen [7] and Howard and Matheson [2]. The notion of solving a decision problem through influence diagrams was examined by Olmsted [6] and such an algorithm was developed by Shachter [8]. The latter paper also shows how influence diagrams can be used to perform a variety of sensitivity analyses. This paper extends those results by developing a theory of the properties of the diagram that are used by the algorithm, and the information needed to solve arbitrary probability inference problems. Section 2 develops the notation and the framework for the paper and the relationship between influence diagrams and joint probability distributions. The general probabilistic inference problem is posed in Section 3. In Section 4 the transformations on the diagram are developed and then put together into a solution procedure in Section 5. In Section 6, this procedure is used to calculate the information requirement to solve an inference problem and the maximal processing that can be performed with incomplete information. Section 7 contains a summary of results.", "prompt": "The analysis of practical probabilistic models on the computer demands a convenient representation for the available knowledge and an efficient algorithm to perform inference. An appealing representation is the influence diagram, a network that makes explicit the random variables in a model and their probabilistic dependencies. Recent advances have developed solution procedures based on the influence diagram.", "orig": "In this paper, we examine the fundamental properties that underlie those techniques, and the information about the probabilistic structure that is available in the influence diagram representation. The influence diagram is a convenient representation for computer processing while also being clear and non-mathematical. It displays probabilistic dependence precisely, in a way that", "generated": "The analysis of practical probabilistic models on the computer demands a convenient representation for the available knowledge and an efficient algorithm to perform inference. An appealing representation is the influence diagram, a network that makes explicit the random variables in a model and their probabilistic dependencies. Recent advances have developed solution procedures based on the influence diagram."}
{"full_text": "A highly sheared dense aqueous suspension of granular cornstarch particles displays rich nonlinear rheology. We had previously demonstrated the growth and onset of interfacial instabilities when shear-thinning cornstarch suspensions were displaced by a Newtonian fluid, and had suggested methods to maximise displacement efficiency [Palak, R. Sathayanath, S. K. Kalpathy and R. Bandyopadhyay, Colloids Surf. A Physicochem. Eng. Asp., 629 (2021) 127405]. In the present work, we explore the miscible displacement of a dense aqueous cornstarch suspension in its discontinuous shear-thickening regime in a quasi-two-dimensional radial Hele-Shaw cell. We systematically study the growth kinetics of the inner interface between water and the cornstarch suspension, and also of the outer interface between the suspension and air. In addition to the growth of interfacial instabilities at the inner interface, we observe a transient withdrawal of the suspension and the formation of fingering instabilities at the outer interface. We demonstrate that these `reverse fingering' instabilities are extremely sensitive to the injection flow rate of water, the gap of the Hele-Shaw cell and the concentration of the displaced cornstarch suspension, {and emerge irrespective of immiscibility between the fluid pair. We believe that as the cornstarch suspension dilates due to the high shear rate imposed by the displacing fluid, the outer suspension-air interface responds with a restoring force, resulting in the penetration of air into the suspension and the formation of reverse fingers. We note that the growth of reverse fingers significantly reduces the displacement efficiency of the suspension. Finally, we demonstrate a correlation in the growth of inner and outer interfacial patterns by computing the velocity with which stresses propagate in the confined dense suspension. Our findings are useful in understanding the flow of granular materials through constrained geometries and can be extended to study stress propagation in shear-thickening materials due to a sudden imposition of high shear rate, such as in impact behaviour.", "prompt": "A highly sheared dense aqueous suspension of granular cornstarch particles displays rich nonlinear rheology. We had previously demonstrated the growth and onset of interfacial instabilities when shear-thinning cornstarch suspensions were displaced by a Newtonian fluid, and had suggested methods to maximise displacement efficiency [Pal", "orig": "ak, R. Sathayanath, S. K. Kalpathy and R. Bandyopadhyay, Colloids Surf. A Physicochem. Eng. Asp., 629 (2021) 127405]. In the present work, we explore the miscible displacement of a dense", "generated": "A highly sheared dense aqueous suspension of granular cornstarch particles displays rich nonlinear rheology. We had previously demonstrated the growth and onset of interfacial instabilities when shear-thinning cornstarch suspensions were displaced by a Newtonian fluid, and had suggested methods to maximise displacement efficiency [Pal"}
{"full_text": "Solar flare prediction is a central problem in space weather forecasting and has captivated the attention of a wide spectrum of researchers due to recent advances in both remote sensing as well as machine learning and deep learning approaches. The experimental findings based on both machine and deep learning models reveal significant performance improvements for task specific datasets. Along with building models, the practice of deploying such models to production environments under operational settings is a more complex and often time-consuming process which is often not addressed directly in research settings. We present a set of new heuristic approaches to train and deploy an operational solar flare prediction system for $\\geq$M1.0-class flares with two prediction modes: full-disk and active region-based. In full-disk mode, predictions are performed on full-disk line-of-sight magnetograms using deep learning models whereas in active region-based models, predictions are issued for each active region individually using multivariate time series data instances. The outputs from individual active region forecasts and full-disk predictors are combined to a final full-disk prediction result with a meta-model. We utilized an equal weighted average ensemble of two base learners' flare probabilities as our baseline meta learner and improved the capabilities of our two base learners by training a logistic regression model. The major findings of this study are: (i) We successfully coupled two heterogeneous flare prediction models trained with different datasets and model architecture to predict a full-disk flare probability for next 24 hours, (ii) Our proposed ensembling model, i.e., logistic regression, improves on the predictive performance of two base learners and the baseline meta learner measured in terms of two widely used metrics True Skill Statistic (TSS) and Heidke Skill core (HSS), and (iii) Our result analysis suggests that the logistic regression-based ensemble (Meta-FP) improves on the full-disk model (base learner) by $\\sim9\\%$ in terms TSS and $\\sim10\\%$ in terms of HSS. Similarly, it improves on the AR-based model (base learner) by $\\sim17\\%$ and $\\sim20\\%$ in terms of TSS and HSS respectively. Finally, when compared to the baseline meta model, it improves on TSS by $\\sim10\\%$ and HSS by $\\sim15\\%$.", "prompt": "Solar flare prediction is a central problem in space weather forecasting and has captivated the attention of a wide spectrum of researchers due to recent advances in both remote sensing as well as machine learning and deep learning approaches. The experimental findings based on both machine and deep learning models reveal significant performance improvements for task specific datasets. Along with", "orig": "building models, the practice of deploying such models to production environments under operational settings is a more complex and often time-consuming process which is often not addressed directly in research settings. We present a set of new heuristic approaches to train and deploy an operational solar flare prediction system for $\\geq$M1.0-class flares", "generated": "Solar flare prediction is a central problem in space weather forecasting and has captivated the attention of a wide spectrum of researchers due to recent advances in both remote sensing as well as machine learning and deep learning approaches. The experimental findings based on both machine and deep learning models reveal significant performance improvements for task specific datasets. Along with"}
{"full_text": "A large family of \"standard\" coboundary Hopf algebras is investigated. The existence of a universal R-matrix is demonstrated for the case when the parameters are in general position. Algebraic surfaces in parameter space are characterized by the appearance of certain ideals; in this case the universal R-matrix exists on the associated algebraic quotient. In special cases the quotient is a \"standard\" quantum group; all familiar quantum groups including twisted ones are obtained in this way. In other special cases one finds new types of coboundary bi-algebras. The \"standard\" universal R-matrix is shown to be the unique solution of a very simple, linear recursion relation. The classical limit is obtained in the case of quantized Kac-Moody algebras of finite and affine type. Returning to the general case, we study deformations of the standard R-matrix and the associated Hopf algebras. A preliminary investigation of the first order deformations uncovers a class of deformations that incompasses the quantization of all Kac-Moody algebras of finite and affine type. The corresponding exact deformations are described as generalized twists, $ R_\\epsilon = (F^t)^{-1}RF$, where $R$ is the standard R-matrix and the cocycle $F$ (a power series in the deformation parameter $\\epsilon$) is the solution of a linear recursion relation of the same type as that which determines $R$. Included here is the universal R-matrix for the elliptic quantum groups associated with $sl(n)$, a big surprise! Specializing again, to the case of quantized Kac-Moody algebras, and taking the classical limit of these esoteric quantum groups, one re-discovers all the trigonometric and elliptic r-matrices of Belavin and Drinfeld. The formulas obtained here are easier to use than the original ones, and the structure of the space of classical r-matrices is more transparent. The r-matrices obtained here are more general in that they are defined on the full Kac-Moody algebras, the central extensions of the loop groups.", "prompt": "A large family of \"standard\" coboundary Hopf algebras is investigated. The existence of a universal R-matrix is demonstrated for the case when the parameters are in general position. Algebraic surfaces in parameter space are characterized by the appearance of certain ideals; in this case the universal R-matrix exists on", "orig": "the associated algebraic quotient. In special cases the quotient is a \"standard\" quantum group; all familiar quantum groups including twisted ones are obtained in this way. In other special cases one finds new types of coboundary bi-algebras. The \"standard\" universal R-matrix is shown to be the unique solution of", "generated": "A large family of \"standard\" coboundary Hopf algebras is investigated. The existence of a universal R-matrix is demonstrated for the case when the parameters are in general position. Algebraic surfaces in parameter space are characterized by the appearance of certain ideals; in this case the universal R-matrix exists only"}
{"full_text": "Microarray analysis to monitor expression activities in thousands of genes simultaneously has become routine in biomedical research during the past decade. A tremendous amount of expression profiles are generated and stored in the public domain and information integration by meta-analysis to detect differentially expressed (DE) genes has become popular to obtain increased statistical power and validated findings. Methods that aggregate transformed $p$-value evidence have been widely used in genomic settings, among which Fisher's and Stouffer's methods are the most popular ones. In practice, raw data and $p$-values of DE evidence are often not available in genomic studies that are to be combined. Instead, only the detected DE gene lists under a certain $p$-value threshold (e.g., DE genes with $p$-value${}<0.001$) are reported in journal publications. The truncated $p$-value information makes the aforementioned meta-analysis methods inapplicable and researchers are forced to apply a less efficient vote counting method or na\\\"{i}vely drop the studies with incomplete information. The purpose of this paper is to develop effective meta-analysis methods for such situations with partially censored $p$-values. We developed and compared three imputation methods - mean imputation, single random imputation and multiple imputation - for a general class of evidence aggregation methods of which Fisher's and Stouffer's methods are special examples. The null distribution of each method was analytically derived and subsequent inference and genomic analysis frameworks were established. Simulations were performed to investigate the type I error, power and the control of false discovery rate (FDR) for (correlated) gene expression data. The proposed methods were applied to several genomic applications in colorectal cancer, pain and liquid association analysis of major depressive disorder (MDD). The results showed that imputation methods outperformed existing na\\\"{i}ve approaches. Mean imputation and multiple imputation methods performed the best and are recommended for future applications.", "prompt": "Microarray analysis to monitor expression activities in thousands of genes simultaneously has become routine in biomedical research during the past decade. A tremendous amount of expression profiles are generated and stored in the public domain and information integration by meta-analysis to detect differentially expressed (DE) genes has become popular to obtain increased statistical power and validated findings", "orig": ". Methods that aggregate transformed $p$-value evidence have been widely used in genomic settings, among which Fisher's and Stouffer's methods are the most popular ones. In practice, raw data and $p$-values of DE evidence are often not available in genomic studies that are to be combined. Instead,", "generated": "Microarray analysis to monitor expression activities in thousands of genes simultaneously has become routine in biomedical research during the past decade. A tremendous amount of expression profiles are generated and stored in the public domain and information integration by meta-analysis to detect differentially expressed (DE) genes has become popular to obtain increased statistical power and validated findings"}
{"full_text": "The equioscillation theorem interleaves the Haar condition, the existence and uniqueness and strong uniqueness of the optimal Chebyshev approximation and its characterization by the equioscillation condition in a way that cannot extend to multivariate approximation: Rice~[\\emph{Transaction of the AMS}, 1963] says ''A form of alternation is still present for functions of several variables. However, there is apparently no simple method of distinguishing between the alternation of a best approximation and the alternation of other approximating functions. This is due to the fact that there is no natural ordering of the critical points.'' In addition, in the context of multivariate approximation the Haar condition is typically not satisfied and strong uniqueness may hold or not. The present paper proposes an multivariate equioscillation theorem, which includes such a simple alternation condition on error extrema, existence and a sufficient condition for strong uniqueness. To this end, the relationship between the properties interleaved in the univariate equioscillation theorem is clarified: first, a weak Haar condition is proposed, which simply implies existence. Second, the equioscillation condition is shown to be equivalent to the optimality condition of convex optimization, hence characterizing optimality independently from uniqueness. It is reformulated as the synchronized oscillations between the error extrema and the components of a related Haar matrix kernel vector, in a way that applies to multivariate approximation. Third, an additional requirement on the involved Haar matrix and its kernel vector, called strong equioscillation, is proved to be sufficient for the strong uniqueness of the solution. These three disconnected conditions give rise to a multivariate equioscillation theorem, where existence, characterization by equioscillation and strong uniqueness are separated, without involving the too restrictive Haar condition. Remarkably, relying on optimality condition of convex optimization gives rise to a quite simple proof. Instances of multivariate problems with strongly unique, non-strong but unique and non-unique solutions are presented to illustrate the scope of the theorem.", "prompt": "The equioscillation theorem interleaves the Haar condition, the existence and uniqueness and strong uniqueness of the optimal Chebyshev approximation and its characterization by the equioscillation condition in a way that cannot extend to multivariate approximation: Rice~[\\emph{Transaction of the AMS}, 1963", "orig": "] says ''A form of alternation is still present for functions of several variables. However, there is apparently no simple method of distinguishing between the alternation of a best approximation and the alternation of other approximating functions. This is due to the fact that there is no natural ordering of the critical points.'' In addition", "generated": "The equioscillation theorem interleaves the Haar condition, the existence and uniqueness and strong uniqueness of the optimal Chebyshev approximation and its characterization by the equioscillation condition in a way that cannot extend to multivariate approximation: Rice~[\\emph{Transaction of the AMS}, 1963"}
{"full_text": "The fundamental plane of early-type galaxies is a rather tight three-parameter correlation discovered more than twenty years ago. It has resisted a both global and precise physical interpretation despite a consequent number of works, observational, theoretical or using numerical simulations. It appears that its precise properties depend on the population of galaxies in study. Instead of selecting a priori these populations, we propose to objectively construct homologous populations from multivariate analyses. We have undertaken multivariate cluster and cladistic analyses of a sample of 56 low-redshift galaxy clusters containing 699 early-type galaxies, using four parameters: effective radius, velocity dispersion, surface brightness averaged over effective radius, and Mg2 index. All our analyses are consistent with seven groups that define separate regions on the global fundamental plane, not across its thickness. In fact, each group shows its own fundamental plane, which is more loosely defined for less diversified groups. We conclude that the global fundamental plane is not a bent surface, but made of a collection of several groups characterizing several fundamental planes with different thicknesses and orientations in the parameter space. Our diversification scenario probably indicates that the level of diversity is linked to the number and the nature of transforming events and that the fundamental plane is the result of several transforming events. We also show that our classification, not the fundamental planes, is universal within our redshift range (0.007 - 0.053). We find that the three groups with the thinnest fundamental planes presumably formed through dissipative (wet) mergers. In one of them, this(ese) merger(s) must have been quite ancient because of the relatively low metallicity of its galaxies, Two of these groups have subsequently undergone dry mergers to increase their masses. In the k-space, the third one clearly occupies the region where bulges (of lenticular or spiral galaxies) lie and might also have formed through minor mergers and accretions. The two least diversified groups probably did not form by major mergers and must have been strongly affected by interactions, some of the gas in the objects of one of these groups having possibly been swept out. The interpretation, based on specific assembly histories of galaxies of our seven groups, shows that they are truly homologous. They were obtained directly from several observables, thus independently of any a priori classification. The diversification scenario relating these groups does not depend on models or numerical simulations, but is objectively provided by the cladistic analysis. Consequently, our classification is more easily compared to models and numerical simulations, and our work can be readily repeated with additional observables.", "prompt": "The fundamental plane of early-type galaxies is a rather tight three-parameter correlation discovered more than twenty years ago. It has resisted a both global and precise physical interpretation despite a consequent number of works, observational, theoretical or using numerical simulations. It appears that its precise properties depend on the population of galaxies in study. Instead", "orig": "of selecting a priori these populations, we propose to objectively construct homologous populations from multivariate analyses. We have undertaken multivariate cluster and cladistic analyses of a sample of 56 low-redshift galaxy clusters containing 699 early-type galaxies, using four parameters: effective radius, velocity dispersion, surface brightness averaged over", "generated": "The fundamental plane of early-type galaxies is a rather tight three-parameter correlation discovered more than twenty years ago. It has resisted a both global and precise physical interpretation despite a consequent number of works, observational, theoretical or using numerical simulations. It appears that its precise properties depend on the population of galaxies in study. Instead"}
{"full_text": "Object-oriented languages often use virtual machines (VMs) that provide mechanisms such as just-in-time (JIT) compilation and garbage collection (GC). These VM components are typically implemented in a separate layer, isolating them from the application. While this approach brings the software engineering benefits of clear separation and decoupling, it introduces barriers for both understanding VM behavior and evolving the VM implementation. For example, the GC and JIT compiler are typically fixed at VM build time, limiting arbitrary adaptation at run time. Furthermore, because of this separation, the implementation of the VM cannot typically be inspected and debugged in the same way as application code, enshrining a distinction in easy-to-work-with application and hard-to-work-with VM code. These characteristics pose a barrier for application developers to understand the engine on top of which their own code runs, and fosters a knowledge gap that prevents application developers to change the VM. We propose Live Metacircular Runtimes (LMRs) to overcome this problem. LMRs are language runtime systems that seamlessly integrate the VM into the application in live programming environments. Unlike classic metacircular approaches, we propose to completely remove the separation between application and VM. By systematically applying object-oriented design to VM components, we can build live runtime systems that are small and flexible enough, where VM engineers can benefit of live programming features such as short feedback loops, and application developers with fewer VM expertise can benefit of the stronger causal connections between their programs and the VM implementation. To evaluate our proposal, we implemented Bee/LMR, a live VM for a Smalltalk-derivative environment in 22057 lines of code. We analyze case studies on tuning the garbage collector, avoiding recompilations by the just-in-time compiler, and adding support to optimize code with vector instructions to demonstrate the trade-offs of extending exploratory programming to VM development in the context of an industrial application used in production. Based on the case studies, we illustrate how our approach facilitates the daily development work of a small team of application developers. Our approach enables VM developers to gain access to live programming tools traditionally reserved for application developers, while application developers can interact with the VM and modify it using the high-level tools they use every day. Both application and VM developers can seamlessly inspect, debug, understand, and modify the different parts of the VM with shorter feedback loops and higher-level tools.", "prompt": "Object-oriented languages often use virtual machines (VMs) that provide mechanisms such as just-in-time (JIT) compilation and garbage collection (GC). These VM components are typically implemented in a separate layer, isolating them from the application. While this approach brings the software engineering benefits of clear separation and decoupling", "orig": ", it introduces barriers for both understanding VM behavior and evolving the VM implementation. For example, the GC and JIT compiler are typically fixed at VM build time, limiting arbitrary adaptation at run time. Furthermore, because of this separation, the implementation of the VM cannot typically be inspected and debugged in the same way as application code", "generated": "Object-oriented languages often use virtual machines (VMs) that provide mechanisms such as just-in-time (JIT) compilation and garbage collection (GC). These VM components are typically implemented in a separate layer, isolating them from the application. While this approach brings the software engineering benefits of clear separation and decoupling"}
{"full_text": "Context: Model checking has become a key tool for gaining confidence in correctness of multi-threaded programs. Unit tests and functional tests do not suffice because of race conditions that are not discovered by those tests. This problem is addressed by model checking tools. A simple model checker is useful for detecting race conditions prior to production. Inquiry: Current model checkers hardwire the behavior of common thread operations, and do not recognize application-dependent thread paradigms or functions using simpler primitive operations. This introduces additional operations, causing current model checkers to be excessively slow. In addition, there is no mechanism to model the semantics of the actual thread wakeup policies implemented in the underlying thread library or operating system. Eliminating these constraints can make model checkers faster. Approach: McMini is an **extensible** model checker based on DPOR (Dynamic Partial Order Reduction). A mechanism was invented to declare to McMini new, primitive thread operations, typically in 100~lines or less of C~code. The mechanism was extended to also allow a user of McMini to declare alternative thread wakeup policies, including spurious wakeups from condition variables. Knowledge: In McMini, the user defines new thread operations. The user optimizes these operations by declaring to the DPOR algorithm information that reduces the number of thread schedules to be searched. One declares: (i) under what conditions an operation is enabled; (ii) which thread operations are independent of each other; and (iii) when two operations can be considered as co-enabled. An optional wakeup policy is implemented by defining when a wait operation (on a semaphore, condition variable, etc.) is enabled. A new enqueue thread operation is described, allowing a user to declare alternative wakeup policies. Grounding: McMini was first confirmed to operate correctly and efficiently as a traditional, but extensible model checker for mutex, semaphore, condition variable, and reader-writer lock. McMini's extensibility was then tested on novel primitive operations, representing other useful paradigms for multithreaded operations. An example is readers-and-two-writers. The speed of model checking was found to be five times faster and more, as compared to traditional implementations on top of condition variables. Alternative wakeup policies (e.g., FIFO, LIFO, arbitrary, etc.) were then tested using an enqueue operation. Finally, spurious wakeups were tested with a program that exposes a bug **only** in the presence of a spurious wakeup. Importance: Many applications employ functions for multithreaded paradigms that go beyond the traditional mutex, semaphore, and condition variables. They are defined on top of basic operations. The ability to directly define new primitives for these paradigms makes model checkers run faster by searching fewer thread schedules. The ability to model particular thread wakeup policies, including spurious wakeup for condition variables, is also important. Note that POSIX leaves undefined the wakeup policies of `pthread_mutex_lock`, `sem_wait`, and `pthread_cond_wait`. The POSIX thread implementation then chooses a particular policy (e.g., FIFO, arbitrary), which can be directly modeled by McMini.", "prompt": "Context: Model checking has become a key tool for gaining confidence in correctness of multi-threaded programs. Unit tests and functional tests do not suffice because of race conditions that are not discovered by those tests. This problem is addressed by model checking tools. A simple model checker is useful for detecting race conditions prior to production.", "orig": "Inquiry: Current model checkers hardwire the behavior of common thread operations, and do not recognize application-dependent thread paradigms or functions using simpler primitive operations. This introduces additional operations, causing current model checkers to be excessively slow. In addition, there is no mechanism to model the semantics of the actual thread wakeup policies", "generated": "Model checking has become a key tool for gaining confidence in correctness of multi-threaded programs. Unit tests and functional tests do not suffice because of race conditions that are not discovered by those tests. This problem is addressed by model checking tools. A simple model checker is useful for detecting race conditions prior to production.\n\nA model"}
{"full_text": "Several multiagent reinforcement learning (MARL) algorithms have been proposed to optimize agents decisions. Due to the complexity of the problem, the majority of the previously developed MARL algorithms assumed agents either had some knowledge of the underlying game (such as Nash equilibria) and/or observed other agents actions and the rewards they received. We introduce a new MARL algorithm called the Weighted Policy Learner (WPL), which allows agents to reach a Nash Equilibrium (NE) in benchmark 2-player-2-action games with minimum knowledge. Using WPL, the only feedback an agent needs is its own local reward (the agent does not observe other agents actions or rewards). Furthermore, WPL does not assume that agents know the underlying game or the corresponding Nash Equilibrium a priori. We experimentally show that our algorithm converges in benchmark two-player-two-action games. We also show that our algorithm converges in the challenging Shapleys game where previous MARL algorithms failed to converge without knowing the underlying game or the NE. Furthermore, we show that WPL outperforms the state-of-the-art algorithms in a more realistic setting of 100 agents interacting and learning concurrently. An important aspect of understanding the behavior of a MARL algorithm is analyzing the dynamics of the algorithm: how the policies of multiple learning agents evolve over time as agents interact with one another. Such an analysis not only verifies whether agents using a given MARL algorithm will eventually converge, but also reveals the behavior of the MARL algorithm prior to convergence. We analyze our algorithm in two-player-two-action games and show that symbolically proving WPLs convergence is difficult, because of the non-linear nature of WPLs dynamics, unlike previous MARL algorithms that had either linear or piece-wise-linear dynamics. Instead, we numerically solve WPLs dynamics differential equations and compare the solution to the dynamics of previous MARL algorithms.", "prompt": "Several multiagent reinforcement learning (MARL) algorithms have been proposed to optimize agents decisions. Due to the complexity of the problem, the majority of the previously developed MARL algorithms assumed agents either had some knowledge of the underlying game (such as Nash equilibria) and/or observed other agents actions and the rewards they", "orig": "received. We introduce a new MARL algorithm called the Weighted Policy Learner (WPL), which allows agents to reach a Nash Equilibrium (NE) in benchmark 2-player-2-action games with minimum knowledge. Using WPL, the only feedback an agent needs is its own local reward (the agent does", "generated": "Several multiagent reinforcement learning (MARL) algorithms have been proposed to optimize agents' decisions. Due to the complexity of the problem, the majority of the previously developed MARL algorithms assumed agents either had some knowledge of the underlying game (such as Nash equilibria) and/or observed other agents' actions and the"}
{"full_text": "The finance industry is producing an increasing amount of datasets that investment professionals can consider to be influential on the price of financial assets. These datasets were initially mainly limited to exchange data, namely price, capitalization and volume. Their coverage has now considerably expanded to include, for example, macroeconomic data, supply and demand of commodities, balance sheet data and more recently extra-financial data such as ESG scores. This broadening of the factors retained as influential constitutes a serious challenge for statistical modeling. Indeed, the instability of the correlations between these factors makes it practically impossible to identify the joint laws needed to construct scenarios. Fortunately, spectacular advances in Deep Learning field in recent years have given rise to GANs. GANs are a type of generative machine learning models that produce new data samples with the same characteristics as a training data distribution in an unsupervised way, avoiding data assumptions and human induced biases. In this work, we are exploring the use of GANs for synthetic financial scenarios generation. This pilot study is the result of a collaboration between Fujitsu and Advestis and it will be followed by a thorough exploration of the use cases that can benefit from the proposed solution. We propose a GANs-based algorithm that allows the replication of multivariate data representing several properties (including, but not limited to, price, market capitalization, ESG score, controversy score,. . .) of a set of stocks. This approach differs from examples in the financial literature, which are mainly focused on the reproduction of temporal asset price scenarios. We also propose several metrics to evaluate the quality of the data generated by the GANs. This approach is well fit for the generation of scenarios, the time direction simply arising as a subsequent (eventually conditioned) generation of data points drawn from the learned distribution. Our method will allow to simulate high dimensional scenarios (compared to $\\lesssim10$ features currently employed in most recent use cases) where network complexity is reduced thanks to a wisely performed feature engineering and selection. Complete results will be presented in a forthcoming study.", "prompt": "The finance industry is producing an increasing amount of datasets that investment professionals can consider to be influential on the price of financial assets. These datasets were initially mainly limited to exchange data, namely price, capitalization and volume. Their coverage has now considerably expanded to include, for example, macroeconomic data, supply and demand of", "orig": "commodities, balance sheet data and more recently extra-financial data such as ESG scores. This broadening of the factors retained as influential constitutes a serious challenge for statistical modeling. Indeed, the instability of the correlations between these factors makes it practically impossible to identify the joint laws needed to construct scenarios. Fortunately, spectacular advances in", "generated": "The finance industry is producing an increasing amount of datasets that investment professionals can consider to be influential on the price of financial assets. These datasets were initially mainly limited to exchange data, namely price, capitalization and volume. Their coverage has now considerably expanded to include, for example, macroeconomic data, supply and demand of"}
{"full_text": "In recent years, heterogeneous computing has emerged as the vital way to increase computers? performance and energy efficiency by combining diverse hardware devices, such as Graphics Processing Units (GPUs) and Field Programmable Gate Arrays (FPGAs). The rationale behind this trend is that different parts of an application can be offloaded from the main CPU to diverse devices, which can efficiently execute these parts as co-processors. FPGAs are a subset of the most widely used co-processors, typically used for accelerating specific workloads due to their flexible hardware and energy-efficient characteristics. These characteristics have made them prevalent in a broad spectrum of computing systems ranging from low-power embedded systems to high-end data centers and cloud infrastructures. However, these hardware characteristics come at the cost of programmability. Developers who create their applications using high-level programming languages (e.g., Java, Python, etc.) are required to familiarize with a hardware description language (e.g., VHDL, Verilog) or recently heterogeneous programming models (e.g., OpenCL, HLS) in order to exploit the co-processors? capacity and tune the performance of their applications. Currently, the above-mentioned heterogeneous programming models support exclusively the compilation from compiled languages, such as C and C++. Thus, the transparent integration of heterogeneous co-processors to the software ecosystem of managed programming languages (e.g. Java, Python) is not seamless. In this paper we rethink the engineering trade-offs that we encountered, in terms of transparency and compilation overheads, while integrating FPGAs into high-level managed programming languages. We present a novel approach that enables runtime code specialization techniques for seamless and high-performance execution of Java programs on FPGAs. The proposed solution is prototyped in the context of the Java programming language and TornadoVM; an open-source programming framework for Java execution on heterogeneous hardware. Finally, we evaluate the proposed solution for FPGA execution against both sequential and multi-threaded Java implementations showcasing up to 224x and 19.8x performance speedups, respectively, and up to 13.82x compared to TornadoVM running on an Intel integrated GPU. We also provide a break-down analysis of the proposed compiler optimizations for FPGA execution, as a means to project their impact on the applications? characteristics.", "prompt": "In recent years, heterogeneous computing has emerged as the vital way to increase computers? performance and energy efficiency by combining diverse hardware devices, such as Graphics Processing Units (GPUs) and Field Programmable Gate Arrays (FPGAs). The rationale behind this trend is that different parts of an application can be offloaded from", "orig": "the main CPU to diverse devices, which can efficiently execute these parts as co-processors. FPGAs are a subset of the most widely used co-processors, typically used for accelerating specific workloads due to their flexible hardware and energy-efficient characteristics. These characteristics have made them prevalent in a broad spectrum of computing systems ranging", "generated": "In recent years, heterogeneous computing has emerged as the vital way to increase computers' performance and energy efficiency by combining diverse hardware devices, such as Graphics Processing Units (GPUs) and Field Programmable Gate Arrays (FPGAs). The rationale behind this trend is that different parts of an application can be offloaded from"}
{"full_text": "We consider the problem of deterministic load balancing of tokens in the discrete model. A set of $n$ processors is connected into a $d$-regular undirected network. In every time step, each processor exchanges some of its tokens with each of its neighbors in the network. The goal is to minimize the discrepancy between the number of tokens on the most-loaded and the least-loaded processor as quickly as possible. Rabani et al. (1998) present a general technique for the analysis of a wide class of discrete load balancing algorithms. Their approach is to characterize the deviation between the actual loads of a discrete balancing algorithm with the distribution generated by a related Markov chain. The Markov chain can also be regarded as the underlying model of a continuous diffusion algorithm. Rabani et al. showed that after time $T = O(\\log (Kn)/\\mu)$, any algorithm of their class achieves a discrepancy of $O(d\\log n/\\mu)$, where $\\mu$ is the spectral gap of the transition matrix of the graph, and $K$ is the initial load discrepancy in the system. In this work we identify some natural additional conditions on deterministic balancing algorithms, resulting in a class of algorithms reaching a smaller discrepancy. This class contains well-known algorithms, eg., the Rotor-Router. Specifically, we introduce the notion of cumulatively fair load-balancing algorithms where in any interval of consecutive time steps, the total number of tokens sent out over an edge by a node is the same (up to constants) for all adjacent edges. We prove that algorithms which are cumulatively fair and where every node retains a sufficient part of its load in each step, achieve a discrepancy of $O(\\min\\{d\\sqrt{\\log n/\\mu},d\\sqrt{n}\\})$ in time $O(T)$. We also show that in general neither of these assumptions may be omitted without increasing discrepancy. We then show by a combinatorial potential reduction argument that any cumulatively fair scheme satisfying some additional assumptions achieves a discrepancy of $O(d)$ almost as quickly as the continuous diffusion process. This positive result applies to some of the simplest and most natural discrete load balancing schemes.", "prompt": "We consider the problem of deterministic load balancing of tokens in the discrete model. A set of $n$ processors is connected into a $d$-regular undirected network. In every time step, each processor exchanges some of its tokens with each of its neighbors in the network. The goal is to minimize the discrepancy", "orig": "between the number of tokens on the most-loaded and the least-loaded processor as quickly as possible. Rabani et al. (1998) present a general technique for the analysis of a wide class of discrete load balancing algorithms. Their approach is to characterize the deviation between the actual loads of a discrete balancing algorithm with the distribution", "generated": "We consider the problem of deterministic load balancing of tokens in the discrete model. A set of $n$ processors is connected into a $d$-regular undirected network. In every time step, each processor exchanges some of its tokens with each of its neighbors in the network. The goal is to minimize the discrepancy"}
{"full_text": "Low image contrast is a major limitation in transmission electron microscopy, since samples with low atomic number only weakly phase-modulate the illuminating electron beam, and beam-induced sample damage limits the usable electron dose. The contrast can be increased by converting the electron beam's phase modulation into amplitude modulation using a phase plate, a device that applies a $\\pi/2$ radian phase shift to part of the electron beam after it has passed through the sample. Previous phase plate designs rely on material placed in or near the electron beam to provide this phase shift. This results in image aberrations, an inconsistent time-varying phase shift, and resolution loss when the electron beam charges, damages, or is scattered from the material. In this thesis, I present the theory, design, and implementation of the laser phase plate, which instead uses a focused continuous-wave laser beam to phase shift the electron beam. A near-concentric Fabry-P\\'{e}rot optical cavity focuses and resonantly enhances the power of the laser beam in order to achieve the high intensity required to provide the phase shift. We demonstrate that the cavity can surpass this requirement and generate a record-high continuous-wave laser intensity of $590 \\, \\mathrm{GW}/\\mathrm{cm}^{-2}$. By integrating the cavity into a transmission electron microscope, we show that the ponderomotive potential of the laser beam applies a spatially selective phase shift to the electron beam. This enables us to make the first experimental observation of the relativistic reversal of the ponderomotive potential. We then theoretically analyze the properties of the contrast transfer function generated by the laser phase plate. We experimentally determine that resolution loss caused by thermal magnetic field noise emanating from electrically conductive materials in the cavity can be eliminated by designing the cavity with a sufficiently large electron beam aperture. Finally, we show that the laser phase plate provides a stable $\\pi/2$ phase shift and concomitant contrast enhancement when imaging frozen hydrated biological macromolecules. We use these images to successfully determine the structure of the molecules. This demonstrates the laser phase plate as the first stable and lossless phase plate for transmission electron microscopy", "prompt": "Low image contrast is a major limitation in transmission electron microscopy, since samples with low atomic number only weakly phase-modulate the illuminating electron beam, and beam-induced sample damage limits the usable electron dose. The contrast can be increased by converting the electron beam's phase modulation into amplitude modulation using a phase plate, a", "orig": "device that applies a $\\pi/2$ radian phase shift to part of the electron beam after it has passed through the sample. Previous phase plate designs rely on material placed in or near the electron beam to provide this phase shift. This results in image aberrations, an inconsistent time-varying phase shift, and", "generated": "Low image contrast is a major limitation in transmission electron microscopy, since samples with low atomic number only weakly phase-modulate the illuminating electron beam, and beam-induced sample damage limits the usable electron dose. The contrast can be increased by converting the electron beam's phase modulation into amplitude modulation using a phase plate, and"}
{"full_text": "Structure discovery in graphical models is the determination of the topology of a graph that encodes conditional independence properties of the joint distribution of all variables in the model. For some class of probability distributions, an edge between two variables is present if and only if the corresponding entry in the precision matrix is non-zero. For a finite sample estimate of the precision matrix, entries close to zero may be due to low sample effects, or due to an actual association between variables; these two cases are not readily distinguishable. %Fisher provided a hypothesis test based on a parametric approximation to the distribution of an entry in the precision matrix of a Gaussian distribution, but this may not provide valid upper bounds on $p$-values for non-Gaussian distributions. Many related works on this topic consider potentially restrictive distributional or sparsity assumptions that may not apply to a data sample of interest, and direct estimation of the uncertainty of an estimate of the precision matrix for general distributions remains challenging. Consequently, we make use of results for $U$-statistics and apply them to the covariance matrix. By probabilistically bounding the distortion of the covariance matrix, we can apply Weyl's theorem to bound the distortion of the precision matrix, yielding a conservative, but sound test threshold for a much wider class of distributions than considered in previous works. The resulting test enables one to answer with statistical significance whether an edge is present in the graph, and convergence results are known for a wide range of distributions. The computational complexities is linear in the sample size enabling the application of the test to large data samples for which computation time becomes a limiting factor. We experimentally validate the correctness and scalability of the test on multivariate distributions for which the distributional assumptions of competing tests result in underestimates of the false positive ratio. By contrast, the proposed test remains sound, promising to be a useful tool for hypothesis testing for diverse real-world problems.", "prompt": "Structure discovery in graphical models is the determination of the topology of a graph that encodes conditional independence properties of the joint distribution of all variables in the model. For some class of probability distributions, an edge between two variables is present if and only if the corresponding entry in the precision matrix is non-zero. For a finite", "orig": "sample estimate of the precision matrix, entries close to zero may be due to low sample effects, or due to an actual association between variables; these two cases are not readily distinguishable. %Fisher provided a hypothesis test based on a parametric approximation to the distribution of an entry in the precision matrix of a Gaussian distribution", "generated": "Structure discovery in graphical models is the determination of the topology of a graph that encodes conditional independence properties of the joint distribution of all variables in the model. For some class of probability distributions, an edge between two variables is present if and only if the corresponding entry in the precision matrix is non-zero. For a finite"}
{"full_text": "Technological progress in material synthesis, as well as artificial realization of condensed matter scenarios via ultra-cold atomic gases in optical lattices or epitaxial growth of thin films, is opening the gate to investigate a plethora of unprecedented strongly correlated electron systems. In a large subclass thereof, a metallic state of layered electrons undergoes an ordering transition below some temperature into unconventional states of matter driven by electronic correlations, such as magnetism, superconductivity, or other Fermi surface instabilities. While this type of phenomena has been a well-established direction of research in condensed matter for decades, the variety of today's accessible scenarios pose fundamental new challenges to describe them. A core complication is the multi-orbital nature of the low-energy electronic structure of these systems, such as the multi-d orbital nature of electrons in iron pnictides and transition-metal oxides in general, but also electronic states of matter on lattices with multiple sites per unit cell such as the honeycomb or kagome lattice. In this review, we propagate the functional renormalization group (FRG) as a suited approach to investigate multi-orbital Fermi surface instabilities. The primary goal of the review is to describe the FRG in explicit detail and render it accessible to everyone both at a technical and intuitive level. Summarizing recent progress in the field of multi-orbital Fermi surface instabilities, we illustrate how the unbiased fashion by which the FRG treats all kinds of ordering tendencies guarantees an adequate description of electronic phase diagrams and often allows to obtain parameter trends of sufficient accuracy to make qualitative predictions for experiments. This review includes detailed and illustrative illustrations of magnetism and, in particular, superconductivity for the iron pnictides from the viewpoint of FRG. Furthermore, it discusses candidate scenarios for topological bulk singlet superconductivity and exotic particle-hole condensates on hexagonal lattices such as sodium-doped cobaltates, graphene doped to van Hove Filling, and the kagome Hubbard model. In total, the FRG promises to be one of the most versatile and revealing numerical approaches to address unconventional Fermi surface instabilities in future fields of condensed matter research.", "prompt": "Technological progress in material synthesis, as well as artificial realization of condensed matter scenarios via ultra-cold atomic gases in optical lattices or epitaxial growth of thin films, is opening the gate to investigate a plethora of unprecedented strongly correlated electron systems. In a large subclass thereof, a metallic state of layered electrons", "orig": "undergoes an ordering transition below some temperature into unconventional states of matter driven by electronic correlations, such as magnetism, superconductivity, or other Fermi surface instabilities. While this type of phenomena has been a well-established direction of research in condensed matter for decades, the variety of today's accessible scenarios pose fundamental new", "generated": "Technological progress in material synthesis, as well as artificial realization of condensed matter scenarios via ultra-cold atomic gases in optical lattices or epitaxial growth of thin films, is opening the gate to investigate a plethora of unprecedented strongly correlated electron systems. In a large subclass thereof, a metallic state of layered electronic"}
{"full_text": "A standing challenge in undergraduate Computer Science curricula is the teaching and learning of computer programming. Through this paper which is an essay about programming, we aim to contribute to the plethora of existing pedagogies, approaches and philosophies, by discussing a specific feature of our approach in teaching principled programming to undergraduate students, in their first semester of studies, namely the utilization of pictures, both text-based and raster-based graphics. Although the given course has evolved substantially over the thirty years of its delivery regarding the programming languages (Miranda, C, C++, Java) and paradigms (functional, imperative, object-oriented, combination of procedural and object-oriented) used, the discussed visual feature has been maintained and steadily strengthened. We list abstraction, problem decomposition and synthesis, information hiding, reusability, modularity and extensibility as key principles of problem solving and algorithmic thinking. These principles are closely aligned with the advocated computational thinking techniques of problem decomposition, pattern recognition, pattern generalization and algorithm design. We aim for our students to familiarize themselves with all the above principles through practical problem solving. Our ongoing inquiry has been whether the problem domain of pictures is contributing valuably towards this aim. Moreover, an added-value is that students get a glimpse of computational complexity in a visual, empirical way. The presented work is not related to visual programming, since the students write their programs textually and not graphically; it's the output of their programs which is in visual form. Our approach though is loosely related to the classical paradigm of turtle graphics. However, our focus is Computer Science majors, who should be able to design and build turtles and other objects and not just use them. Indeed, the programming principles course helps them to do both and also to appreciate the multitude of algorithmic ways for producing the same visual output. Currently the given programming principles are approached both from a procedural, process-based and an object-oriented, concept-based perspective and the course uses the Java language. Through the presented example problems, we aim to show the appropriateness of the visual domain of pictures for supporting the learning of principled programming. The problem domain of pictures is abundantly rich with potential examples to draw from. Moreover, as reported in the literature, female students may show higher interest towards visual problem domains in programming classes, in relation to other problem domains. We plan to investigate this conjecture in the context of our broader aim to encourage more females to follow university studies in computer science; in this paper only a cursory finding is presented, that bears some relation to what is reported in the literature.", "prompt": "A standing challenge in undergraduate Computer Science curricula is the teaching and learning of computer programming. Through this paper which is an essay about programming, we aim to contribute to the plethora of existing pedagogies, approaches and philosophies, by discussing a specific feature of our approach in teaching principled programming to undergraduate students,", "orig": "in their first semester of studies, namely the utilization of pictures, both text-based and raster-based graphics. Although the given course has evolved substantially over the thirty years of its delivery regarding the programming languages (Miranda, C, C++, Java) and paradigms (functional, imperative, object-oriented, combination of procedural", "generated": "A standing challenge in undergraduate Computer Science curricula is the teaching and learning of computer programming. Through this paper which is an essay about programming, we aim to contribute to the plethora of existing pedagogies, approaches and philosophies, by discussing a specific feature of our approach in teaching principled programming to undergraduate students,"}
{"full_text": "Shadow libraries, also known as ''pirate libraries'', are online collections of copyrighted publications that have been made available for free without the permission of the copyright holders. They have gradually become key players of scientific knowledge dissemination, despite their illegality in most countries of the world. Many publishers and scientist-editors decry such libraries for their copyright infringement and loss of publication usage information, while some scholars and institutions support them, sometimes in a roundabout way, for their role in reducing inequalities of access to knowledge, particularly in low-income countries. Although there is a wealth of literature on shadow libraries, none of this have focused on its potential role in knowledge dissemination, through the open access movement. Here we analyze how shadow libraries can affect researchers' citation practices, highlighting some counter-intuitive findings about their impact on the Open Access Citation Advantage (OACA). Based on a large randomized sample, this study first shows that OA publications, including those in fully OA journals, receive more citations than their subscription-based counterparts do. However, the OACA has slightly decreased over the seven last years. The introduction of a distinction between those accessible or not via the Scihub platform among subscription-based suggest that the generalization of its use cancels the positive effect of OA publishing. The results show that publications in fully OA journals are victims of the success of Sci-hub. Thus, paradoxically, although Sci-hub may seem to facilitate access to scientific knowledge, it negatively affects the OA movement as a whole, by reducing the comparative advantage of OA publications in terms of visibility for researchers. The democratization of the use of Sci-hub may therefore lead to a vicious cycle, hindering efforts to develop full OA strategies without proposing a credible and sustainable alternative model for the dissemination of scientific knowledge.", "prompt": "Shadow libraries, also known as ''pirate libraries'', are online collections of copyrighted publications that have been made available for free without the permission of the copyright holders. They have gradually become key players of scientific knowledge dissemination, despite their illegality in most countries of the world. Many publishers and scientist-editors decry such", "orig": "libraries for their copyright infringement and loss of publication usage information, while some scholars and institutions support them, sometimes in a roundabout way, for their role in reducing inequalities of access to knowledge, particularly in low-income countries. Although there is a wealth of literature on shadow libraries, none of this have focused on its potential role", "generated": "Shadow libraries, also known as \"pirate libraries,\" are online collections of copyrighted publications that have been made available for free without the permission of the copyright holders. They have gradually become key players of scientific knowledge dissemination, despite their illegality in most countries of the world. Many publishers and scientist-editors decry such"}
{"full_text": "Model checking verifies that a model of a system satisfies a given property, and otherwise produces a counter-example explaining the violation. The verified properties are formally expressed in temporal logics. Some temporal logics, such as CTL, are branching: they allow to express facts about the whole computation tree of the model, rather than on each single linear computation. This branching aspect is even more critical when dealing with multi-modal logics, i.e. logics expressing facts about systems with several transition relations. A prominent example is CTLK, a logic that reasons about temporal and epistemic properties of multi-agent systems. In general, model checkers produce linear counter-examples for failed properties, composed of a single computation path of the model. But some branching properties are only poorly and partially explained by a linear counter-example. This paper proposes richer counter-example structures called tree-like annotated counter-examples (TLACEs), for properties in Action-Restricted CTL (ARCTL), an extension of CTL quantifying paths restricted in terms of actions labeling transitions of the model. These counter-examples have a branching structure that supports more complete description of property violations. Elements of these counter-examples are annotated with parts of the property to give a better understanding of their structure. Visualization and browsing of these richer counter-examples become a critical issue, as the number of branches and states can grow exponentially for deeply-nested properties. This paper formally defines the structure of TLACEs, characterizes adequate counter-examples w.r.t. models and failed properties, and gives a generation algorithm for ARCTL properties. It also illustrates the approach with examples in CTLK, using a reduction of CTLK to ARCTL. The proposed approach has been implemented, first by extending the NuSMV model checker to generate and export branching counter-examples, secondly by providing an interactive graphical interface to visualize and browse them.", "prompt": "Model checking verifies that a model of a system satisfies a given property, and otherwise produces a counter-example explaining the violation. The verified properties are formally expressed in temporal logics. Some temporal logics, such as CTL, are branching: they allow to express facts about the whole computation tree of the model, rather than", "orig": "on each single linear computation. This branching aspect is even more critical when dealing with multi-modal logics, i.e. logics expressing facts about systems with several transition relations. A prominent example is CTLK, a logic that reasons about temporal and epistemic properties of multi-agent systems. In general, model checkers", "generated": "Model checking verifies that a model of a system satisfies a given property, and otherwise produces a counter-example explaining the violation. The verified properties are formally expressed in temporal logics. Some temporal logics, such as CTL, are branching: they allow to express facts about the whole computation tree of the model, rather than"}
{"full_text": "On the basis of general relativity and quantum statistics, it was shown (Neslu\\v{s}an L.: 2009, Phys. Rev. D 80, 024015, arxiv:0808.3484) that the equation of state (ES) of extremely hot Fermi-Dirac gas in the surface layer of an ultra-relativistic compact object converges to the same form as the relativistic equation of thermodynamical equilibrium (RETE), which is the condition of stability of the object. The description of energy state of a gas particle was completed with the term corresponding with the potential-type energy. The necessity of such the term is set by the demand of convergence of the relativistic particle-impulse distribution law to its Maxwell-Boltzmann form in the classical limit. The identity of the ES and RETE, both applied to the gas in the object's surface layer, becomes perfect, yielding the stable object, when the object's physical radius is identical to its gravitational radius. In this state, the internal energy of gas particles in a volume of the object's surface layer increases over all limits in the frame of the volume and this opens the question if the horizon of events actually is an insuperable barrier. It seems to be possible that some matter can be temporarily lifted above the surface or, so far, be ejected from the object and can emit a radiation detectable by a distant observer. In our contribution, we demonstrate a general validity of the functional form of the potential-type energy found in our previous work. The consistency of the RETE with its non-relativistic approximation can occur only for this functional form. We also point out some observational consequences of the approximate identity of ES and RETE before the object collapses, in the proper time, to its gravitational radius as well as the possible observational consequences of the infinitely high internal energy in the surface layer of already collapsed object. In general, we propagate the idea that a lot of phenomena observed at the stellar-sized or supermassive black holes (or not-yet black holes) can be not necessarily related to the structures in a vicinity of the black hole, e.g. to an accretion disk, but they can be linked directly to the behaviour of the central, ultra-compact object.", "prompt": "On the basis of general relativity and quantum statistics, it was shown (Neslu\\v{s}an L.: 2009, Phys. Rev. D 80, 024015, arxiv:0808.3484) that the equation of state (ES) of extremely hot Fermi-", "orig": "Dirac gas in the surface layer of an ultra-relativistic compact object converges to the same form as the relativistic equation of thermodynamical equilibrium (RETE), which is the condition of stability of the object. The description of energy state of a gas particle was completed with the term corresponding with the potential-type", "generated": "On the basis of general relativity and quantum statistics, it was shown (Neslu\\v{s}an L.: 2009, Phys. Rev. D 80, 024015, arxiv:0808.3484) that the equation of state (ES) of extremely hot Fermi-g"}
{"full_text": "Linear latent variable models such as principal component analysis (PCA), independent component analysis (ICA), canonical correlation analysis (CCA), and factor analysis (FA) identify latent directions (or loadings) either ordered or unordered. The data is then projected onto the latent directions to obtain their projected representations (or scores). For example, PCA solvers usually rank the principal directions by explaining the most to least variance, while ICA solvers usually return independent directions unordered and often with single sources spread across multiple directions as multiple sub-sources, which is of severe detriment to their usability and interpretability. This paper proposes a general framework to enhance latent space representations for improving the interpretability of linear latent spaces. Although the concepts in this paper are language agnostic, the framework is written in Python. This framework automates the clustering and ranking of latent vectors to enhance the latent information per latent vector, as well as, the interpretation of latent vectors. Several innovative enhancements are incorporated including latent ranking (LR), latent scaling (LS), latent clustering (LC), and latent condensing (LCON). For a specified linear latent variable model, LR ranks latent directions according to a specified metric, LS scales latent directions according to a specified metric, LC automatically clusters latent directions into a specified number of clusters, while, LCON automatically determines an appropriate number of clusters into which to condense the latent directions for a given metric. Additional functionality of the framework includes single-channel and multi-channel data sources, data preprocessing strategies such as Hankelisation to seamlessly expand the applicability of linear latent variable models (LLVMs) to a wider variety of data. The effectiveness of LR, LS, and LCON are showcased on two crafted foundational problems with two applied latent variable models, namely, PCA and ICA.", "prompt": "Linear latent variable models such as principal component analysis (PCA), independent component analysis (ICA), canonical correlation analysis (CCA), and factor analysis (FA) identify latent directions (or loadings) either ordered or unordered. The data is then projected onto the latent directions to obtain their projected representations (or scores). For example", "orig": ", PCA solvers usually rank the principal directions by explaining the most to least variance, while ICA solvers usually return independent directions unordered and often with single sources spread across multiple directions as multiple sub-sources, which is of severe detriment to their usability and interpretability. This paper proposes a general framework to enhance latent space", "generated": "Linear latent variable models such as principal component analysis (PCA), independent component analysis (ICA), canonical correlation analysis (CCA), and factor analysis (FA) identify latent directions (or loadings) either ordered or unordered. The data is then projected onto the latent directions to obtain their projected representations (or scores). \n\nFor"}
{"full_text": "When we consider a proper holomorphic map \\ $\\tilde{f}: X \\to C$ \\ of a complex manifold \\ $X$ \\ on a smooth complex curve \\ $C$ \\ with a critical value at a point \\ $0$ \\ in \\ $C$, the choice of a local coordinate near this point allows to dispose of an holomorphic function \\ $f$. Then we may construct, using this function, an (a,b)-modules structure on the cohomology sheaves of the formal completion (in \\ $f$) \\ of the complex of sheaves \\ $(Ker\\, df^{\\bullet},d^{\\bullet})$. These (a,b)-modules represent a filtered version of the Gauss-Manin connection of \\ $f$. The most simple example of this construction is the Brieskorn module (see [Br.70]) of a function with an isolated singular point. See [B.08] for the case of a 1-dimensional critical locus. But it is clear that this construction depends seriously on the choice of the function \\ $f$ \\ that is to say on the choice of the local coordinate near the critical point \\ $0$ \\ in the complex curve \\ $C$. The aim of the present paper is to study the behaviour of such constructions when we make a change of local coordinate near the origin. We consider the case of \\ $[\\lambda]-$primitive frescos, which are monogenic geometric (a,b)-modules corresponding to a minimal filtered differential equation associated to a relative de Rham cohomology class on \\ $X$ \\ (see [B.09-a] and [B.09-b]). An holomorphic parameter is a function on the set of isomorphism classes of frescos which behave holomorphically in an holomorphic family of frescos. In general, an holomorphic parameter is not invariant by a change of variable, but we prove a theorem of stability of holomorphic families of frescos by a change of variable and it implies that an holomorphic parameter gives again an holomorphic parameter by a change of variable. We construct here two different kinds of holomorphic parameters which are (quasi-)invariant by change of variable. The first kind is associated to Jordan blocks of the monodromy with size at least two. The second kind is associated to the semi-simple part of the monodromy and look like some \"cross ratio\" of eigenvectors. They allow, in the situation describe above, to associate to a given (vanishing) relative de Rham cohomology class some numbers, which will depend holomorphically of our data, and are independant of the choice of the local coordinate near \\ $0$ \\ to study the Gauss-Manin connection of this degeneration of compact complex manifolds.", "prompt": "When we consider a proper holomorphic map \\ $\\tilde{f}: X \\to C$ \\ of a complex manifold \\ $X$ \\ on a smooth complex curve \\ $C$ \\ with a critical value at a point \\ $0$ \\ in \\ $C$, the choice of a local coordinate", "orig": "near this point allows to dispose of an holomorphic function \\ $f$. Then we may construct, using this function, an (a,b)-modules structure on the cohomology sheaves of the formal completion (in \\ $f$) \\ of the complex of sheaves \\ $(Ker\\, df^", "generated": "When we consider a proper holomorphic map \\ $\\tilde{f}: X \\to C$ \\ of a complex manifold \\ $X$ \\ on a smooth complex curve \\ $C$ \\ with a critical value at a point \\ $0$ \\ in \\ $C$, the choice of a local coordinate"}
{"full_text": "We present an attempt to reach realistic turbulent regime in direct numerical simulations of the geodynamo. We rely on a sequence of three convection-driven simulations in a rapidly rotating spherical shell. The most extreme case reaches towards the Earth's core regime by lowering viscosity (magnetic Prandtl number Pm=0.1) while maintaining vigorous convection (magnetic Reynolds number Rm>500) and rapid rotation (Ekman number E=1e-7), at the limit of what is feasible on today's supercomputers. A detailed and comprehensive analysis highlights several key features matching geomagnetic observations or dynamo theory predictions -- all present together in the same simulation -- but it also unveils interesting insights relevant for Earth's core dynamics.In this strong-field, dipole-dominated dynamo simulation, the magnetic energy is one order of magnitude larger than the kinetic energy. The spatial distribution of magnetic intensity is highly heterogeneous, and a stark dynamical contrast exists between the interior and the exterior of the tangent cylinder (the cylinder parallel to the axis of rotation that circumscribes the inner core).In the interior, the magnetic field is strongest, and is associated with a vigorous twisted polar vortex, whose dynamics may occasionally lead to the formation of a reverse polar flux patch at the surface of the shell. Furthermore, the strong magnetic field also allows accumulation of light material within the tangent cylinder, leading to stable stratification there. Torsional Alfv{\\'e}n waves are frequently triggered in the vicinity of the tangent cylinder and propagate towards the equator.Outside the tangent cylinder, the magnetic field inhibits the growth of zonal winds and the kinetic energy is mostly non-zonal. Spatio-temporal analysis indicates that the low-frequency, non-zonal flow is quite geostrophic (columnar) and predominantly large-scale: an m=1 eddy spontaneously emerges in our most extreme simulations, without any heterogeneous boundary forcing.Our spatio-temporal analysis further reveals that (i) the low-frequency, large-scale flow is governed by a balance between Coriolis and buoyancy forces -- magnetic field and flow tend to align, minimizing the Lorentz force; (ii) the high-frequency flow obeys a balance between magnetic and Coriolis forces; (iii) the convective plumes mostly live at an intermediate scale, whose dynamics is driven by a 3-term 1 MAC balance -- involving Coriolis, Lorentz and buoyancy forces. However, small-scale (E^{1/3}) quasi-geostrophic convection is still observed in the regions of low magnetic intensity.", "prompt": "We present an attempt to reach realistic turbulent regime in direct numerical simulations of the geodynamo. We rely on a sequence of three convection-driven simulations in a rapidly rotating spherical shell. The most extreme case reaches towards the Earth's core regime by lowering viscosity (magnetic Prandtl number Pm=0.", "orig": "1) while maintaining vigorous convection (magnetic Reynolds number Rm>500) and rapid rotation (Ekman number E=1e-7), at the limit of what is feasible on today's supercomputers. A detailed and comprehensive analysis highlights several key features matching geomagnetic observations or dynamo theory predictions --", "generated": "We present an attempt to reach realistic turbulent regime in direct numerical simulations of the geodynamo. We rely on a sequence of three convection-driven simulations in a rapidly rotating spherical shell. The most extreme case reaches towards the Earth's core regime by lowering viscosity (magnetic Prandtl number Pm=0."}
{"full_text": "VAEs, or variational autoencoders, are autoencoders that explicitly learn the distribution of the input image space rather than assuming no prior information about the distribution. This allows it to classify similar samples close to each other in the latent space's distribution. VAEs classically assume the latent space is normally distributed, though many distribution priors work, and they encode this assumption through a K-L divergence term in the loss function. While VAEs learn the distribution of the latent space and naturally make each dimension in the latent space as disjoint from the others as possible, they do not group together similar features -- the image space feature represented by one unit of the representation layer does not necessarily have high correlation with the feature represented by a neighboring unit of the representation layer. This makes it difficult to interpret VAEs since the representation layer is not structured in a way that is easy for humans to parse. We aim to make a more interpretable VAE by partitioning the representation layer into disjoint sets of units. Partitioning the representation layer into disjoint sets of interconnected units yields a prior that features of the input space to this new VAE, which we call a partition VAE or PVAE, are grouped together by correlation -- for example, if our image space were the space of all ping ping game images (a somewhat complex image space we use to test our architecture) then we would hope the partitions in the representation layer each learned some large feature of the image like the characteristics of the ping pong table or the characteristics and position of the players or the ball. We also add to the PVAE a cost-saving measure: subresolution. Because we do not have access to GPU training environments for long periods of time and Google Colab Pro costs money, we attempt to decrease the complexity of the PVAE by outputting an image with dimensions scaled down from the input image by a constant factor, thus forcing the model to output a smaller version of the image. We then increase the resolution to calculate loss and train by interpolating through neighboring pixels. We train a tuned PVAE on MNIST and Sports10 to test its effectiveness.", "prompt": "VAEs, or variational autoencoders, are autoencoders that explicitly learn the distribution of the input image space rather than assuming no prior information about the distribution. This allows it to classify similar samples close to each other in the latent space's distribution. VAEs classically assume the latent space is normally", "orig": "distributed, though many distribution priors work, and they encode this assumption through a K-L divergence term in the loss function. While VAEs learn the distribution of the latent space and naturally make each dimension in the latent space as disjoint from the others as possible, they do not group together similar features -- the image space", "generated": "VAEs, or variational autoencoders, are autoencoders that explicitly learn the distribution of the input image space rather than assuming no prior information about the distribution. This allows it to classify similar samples close to each other in the latent space's distribution. VAEs classically assume the latent space is normally"}
{"full_text": "Core helium burning is the dominant source of energy of extreme horizontal branch stars, as the hydrogen envelope is too small to contribute to the nuclear energy output. The evolution of each mass in the HR diagram occurs along vertical tracks that, when the core helium is consumed, evolve to higher Teff and then to the white dwarf stage. The larger is the mass, the smaller is the Teff of the models, so that the zero age horizontal branch (ZAHB) is \"horizontal\". In this paper we show that, if the helium mass fraction (Y) of the envelope is larger than Y~0.5, the shape of the tracks changes completely: the hydrogen burning becomes efficient again also for very small envelope masses, thanks to the higher molecular weight and to the higher temperatures of the hydrogen shell. The larger is Y, the smaller is the envelope mass that provides strong H-shell burning. These tracks have a curled shape, are located at a Teff following the approximate relation Teff=8090+ 32900xY, and become more luminous for larger envelope masses. Consequently, the ZAHB of the very high helium models is \"vertical\" in the HR diagram. Synthetic models based on these tracks nicely reproduce the location and shape of the \"blue hook\" in the globular cluster wCen, best fit by a very high Teff (bluer) sequence with Y=0.80 and a cooler (redder) one with Y=0.65. Although these precise values of Y may depend on the color-Teff conversions, we know that the helium content of the progenitors of the blue hook stars can not be larger than Y~0.38-0.40, if they are descendants of the cluster blue main sequence. Consequently, this interpretation implies that all these objects must in fact be progeny of the blue main sequence, but they have all suffered further deep mixing, that has largely and uniformly increased their surface helium abundance, during the red giant branch evolution. A late helium flash can not be the cause of this deep mixing, as the models we propose have hydrogen rich envelopes much more massive than those required for a late flash. We discuss different models of deep mixing proposed in the literature, and conclude that our interpretation of the blue hook can not be ruled out, but requires a much deeper investigation before it can be accepted.", "prompt": "Core helium burning is the dominant source of energy of extreme horizontal branch stars, as the hydrogen envelope is too small to contribute to the nuclear energy output. The evolution of each mass in the HR diagram occurs along vertical tracks that, when the core helium is consumed, evolve to higher Teff and then to the white dwarf", "orig": "stage. The larger is the mass, the smaller is the Teff of the models, so that the zero age horizontal branch (ZAHB) is \"horizontal\". In this paper we show that, if the helium mass fraction (Y) of the envelope is larger than Y~0.5, the shape of", "generated": "Core helium burning is the dominant source of energy of extreme horizontal branch stars, as the hydrogen envelope is too small to contribute to the nuclear energy output. The evolution of each mass in the HR diagram occurs along vertical tracks that, when the core helium is consumed, evolve to higher Teff and then to the white dwarf"}
{"full_text": "Fingerprinting-based positioning, one of the promising indoor positioning solutions, has been broadly explored owing to the pervasiveness of sensor-rich mobile devices, the prosperity of opportunistically measurable location-relevant signals and the progress of data-driven algorithms. One critical challenge is to controland improve the quality of the reference fingerprint map (RFM), which is built at the offline stage and applied for online positioning. The key concept concerningthe quality control of the RFM is updating the RFM according to the newly measured data. Though varies methods have been proposed for adapting the RFM, they approach the problem by introducing extra-positioning schemes (e.g. PDR orUGV) and directly adjust the RFM without distinguishing whether critical changes have occurred. This paper aims at proposing an extra-positioning-free solution by making full use of the redundancy of measurable features. Loosely inspired by random sampling consensus (RANSAC), arbitrarily sampled subset of features from the online measurement are used for generating multi-resamples, which areused for estimating the intermediate locations. In the way of resampling, it can mitigate the impact of the changed features on positioning and enables to retrieve accurate location estimation. The users location is robustly computed by identifying the candidate locations from these intermediate ones using modified Jaccardindex (MJI) and the feature-wise change belief is calculated according to the world model of the RFM and the estimated variability of features. In order to validate our proposed approach, two levels of experimental analysis have been carried out. On the simulated dataset, the average change detection accuracy is about 90%. Meanwhile, the improvement of positioning accuracy within 2 m is about 20% by dropping out the features that are detected as changed when performing positioning comparing to that of using all measured features for location estimation. On the long-term collected dataset, the average change detection accuracy is about 85%.", "prompt": "Fingerprinting-based positioning, one of the promising indoor positioning solutions, has been broadly explored owing to the pervasiveness of sensor-rich mobile devices, the prosperity of opportunistically measurable location-relevant signals and the progress of data-driven algorithms. One critical challenge is to controland improve the quality of the reference fingerprint", "orig": "map (RFM), which is built at the offline stage and applied for online positioning. The key concept concerningthe quality control of the RFM is updating the RFM according to the newly measured data. Though varies methods have been proposed for adapting the RFM, they approach the problem by introducing extra-positioning schemes (", "generated": "Fingerprinting-based positioning, one of the promising indoor positioning solutions, has been broadly explored owing to the pervasiveness of sensor-rich mobile devices, the prosperity of opportunistically measurable location-relevant signals and the progress of data-driven algorithms. One critical challenge is to control and improve the quality of the reference fingerprint"}
{"full_text": "This paper shortly summarises the experimental results obtained since 2011 by a large European academic consortium for the scientific conceptualisation, the definition of the technical requirements, the generation of experimental data, and the validation of a numerical code, for the Pulsating Heat Pipes (PHP) experiment on the International Space Station (ISS). The PHP is a passive, wickless thermal device, whereby a two-phase fluid, forming liquid plugs and vapour slugs, moves with a pulsating or circulating motion inside a meandering tube or channel. The PHP may have a very broad range of geometries (flat, tubular, 3D structured), it can dissipate heat from large areas, and it can be suitable for high power applications with low/medium heat fluxes. PHP functioning is based on the capillary effect, which provides the existence of liquid plugs completely filling the channel cross-section, in a way that any expansion or contraction of the vapour slugs will naturally generate a movement of the fluid along the channel axis. For this, it is important that the channel has a cross-section size below a given threshold, which depends on the liquid surface tension and (for a static fluid) on the gravity acceleration. In space, when only residual accelerations are acting, such a static size threshold is virtually infinite, while a finite dynamic threshold exists even in the absence of gravity. The concept of a ''Space PHP'' was originally developed in 2014 by the team, and from then 17 Parabolic Flight Campaigns (PFC) and 3 Sounding Rocket (SR) experiments have been carried out to generate the data for the preparation of an experiment targeting a Low Earth Orbit (LEO) mission. Both a tubular and a flat plate PHP have been successfully tested in reduced gravity and on ground, by using different combinations of fluids and building materials. The need for having an experiment on a LEO environment is mainly because, during a PFC, only 22sec of reduced gravity are possible, which is a period below the characteristic time for reaching a steady state condition for almost all of the tested devices. Instead, a steady state was reached using the SR campaigns: in this case however, only one experimental condition was achievable, and long-duration data of the PHP performance still remains beyond reach. Several measurement methodologies have been used to characterise the Space PHP, like infrared analysis, high-speed camera visualisation techniques, with data processed with different techniques, from wavelets to inverse heat transfer problem solution. The results clearly showed that PHPs are very interesting for space applications due to their simplicity of construction, the capacity to transfer heat up to several hundred watts, a high power/weight ratio, their geometrical adaptability, and, in particular, the Space PHP will be a breakthrough technology for space thermal management.", "prompt": "This paper shortly summarises the experimental results obtained since 2011 by a large European academic consortium for the scientific conceptualisation, the definition of the technical requirements, the generation of experimental data, and the validation of a numerical code, for the Pulsating Heat Pipes (PHP) experiment on the International Space Station (", "orig": "ISS). The PHP is a passive, wickless thermal device, whereby a two-phase fluid, forming liquid plugs and vapour slugs, moves with a pulsating or circulating motion inside a meandering tube or channel. The PHP may have a very broad range of geometries (flat, tubular, 3", "generated": "This paper shortly summarises the experimental results obtained since 2011 by a large European academic consortium for the scientific conceptualisation, the definition of the technical requirements, the generation of experimental data, and the validation of a numerical code, for the Pulsating Heat Pipes (PHP) experiment on the International Space Station ("}
{"full_text": "Recently developed spatial capture-recapture (SCR) models represent a major advance over traditional capture-recapture (CR) models because they yield explicit estimates of animal density instead of population size within an unknown area. Furthermore, unlike nonspatial CR methods, SCR models account for heterogeneity in capture probability arising from the juxtaposition of animal activity centers and sample locations. Although the utility of SCR methods is gaining recognition, the requirement that all individuals can be uniquely identified excludes their use in many contexts. In this paper, we develop models for situations in which individual recognition is not possible, thereby allowing SCR concepts to be applied in studies of unmarked or partially marked populations. The data required for our model are spatially referenced counts made on one or more sample occasions at a collection of closely spaced sample units such that individuals can be encountered at multiple locations. Our approach includes a spatial point process for the animal activity centers and uses the spatial correlation in counts as information about the number and location of the activity centers. Camera-traps, hair snares, track plates, sound recordings, and even point counts can yield spatially correlated count data, and thus our model is widely applicable. A simulation study demonstrated that while the posterior mean exhibits frequentist bias on the order of 5-10% in small samples, the posterior mode is an accurate point estimator as long as adequate spatial correlation is present. Marking a subset of the population substantially increases posterior precision and is recommended whenever possible. We applied our model to avian point count data collected on an unmarked population of the northern parula (Parula americana) and obtained a density estimate (posterior mode) of 0.38 (95% CI: 0.19-1.64) birds/ha. Our paper challenges sampling and analytical conventions in ecology by demonstrating that neither spatial independence nor individual recognition is needed to estimate population density - rather, spatial dependence can be informative about individual distribution and density.", "prompt": "Recently developed spatial capture-recapture (SCR) models represent a major advance over traditional capture-recapture (CR) models because they yield explicit estimates of animal density instead of population size within an unknown area. Furthermore, unlike nonspatial CR methods, SCR models account for heterogeneity in capture probability arising from the juxtaposition", "orig": "of animal activity centers and sample locations. Although the utility of SCR methods is gaining recognition, the requirement that all individuals can be uniquely identified excludes their use in many contexts. In this paper, we develop models for situations in which individual recognition is not possible, thereby allowing SCR concepts to be applied in studies of unmarked or", "generated": "Recently developed spatial capture-recapture (SCR) models represent a major advance over traditional capture-recapture (CR) models because they yield explicit estimates of animal density instead of population size within an unknown area. Furthermore, unlike nonspatial CR methods, SCR models account for heterogeneity in capture probability arising from the juxtaposition"}
{"full_text": "Gas behavior in systems at microscale has been receiving significant attention from researchers in the last two decades [1-4]. Today, there is an enhanced emphasis on developing new experimental techniques to capture the local temperature profiles in gases at rarefied conditions. The main underlying reason behind this focus is the interesting physics exhibited by gases at these rarefied conditions, especially in the transition regime. There is the onset of local thermodynamic disequilibrium, which manifests as velocity slip and temperature jump [1-4] at the wall. However, there is limited experimental evidence on understanding these aforementioned phenomena. With the advances in experimental facilities, it is today possible, at least in principle, to map the local temperature profiles in gases at rarefied conditions. Molecular tagging approach is one such technique which has shown the potential to map the temperature profile in low pressure conditions [5]. In molecular tagging approach, a very small percentage of tracer molecules are introduced into the gas of interest, referred as carrier gas. In gas flow studies, the typical tracers employed are acetone and biacetyl. These tracer molecules, assumed to be in equilibrium with the carrier gas, are excited with a source of energy at a specific wavelength, typically a laser. The excited molecules are unstable and tend to de-excite in a radiative and non-radiative manner, which is manifested as fluorescence and phosphorescence. Following the deformation with time of a tagged line permits to obtain the flow velocity. In addition, the dependence of the phosphorescence and fluorescence intensity to the gas temperature could also allow to use this technique for local temperature measurements. The objective of this study is to develop an experimental setup capable of simultaneously mapping the wall and fluid near-wall temperatures with the final goal to measure temperature jump at the wall when rarefied conditions are reached. The originality of this setup shown in Figure 1 is to couple surface temperature measurements using an infrared camera with Molecular Tagging Thermometry (MTT) for gas temperature measurements. The bottom wall of the channel will be made of Sapphire substrate of 650 $\\mu$m thickness coated with a thin film of Indium Tin Oxide (ITO). The average roughness of this ITO layer is about 3 nm. The top wall of the channel will be made of SU8 and bonded with the bottom wall with a layer of PDMS. The channel will be filled in with acetone vapor,", "prompt": "Gas behavior in systems at microscale has been receiving significant attention from researchers in the last two decades [1-4]. Today, there is an enhanced emphasis on developing new experimental techniques to capture the local temperature profiles in gases at rarefied conditions. The main underlying reason behind this focus is the interesting physics exhibited by", "orig": "gases at these rarefied conditions, especially in the transition regime. There is the onset of local thermodynamic disequilibrium, which manifests as velocity slip and temperature jump [1-4] at the wall. However, there is limited experimental evidence on understanding these aforementioned phenomena. With the advances in experimental facilities, it", "generated": "Gas behavior in systems at microscale has been receiving significant attention from researchers in the last two decades [1-4]. Today, there is an enhanced emphasis on developing new experimental techniques to capture the local temperature profiles in gases at rarefied conditions. The main underlying reason behind this focus is the interesting physics exhibited by"}
{"full_text": "The COVID-19 pandemic is causing a major outbreak in more than 150 countries around the world, having a severe impact on the health and life of many people globally. One of the crucial step in fighting COVID-19 is the ability to detect the infected patients early enough, and put them under special care. Detecting this disease from radiography and radiology images is perhaps one of the fastest ways to diagnose the patients. Some of the early studies showed specific abnormalities in the chest radiograms of patients infected with COVID-19. Inspired by earlier works, we study the application of deep learning models to detect COVID-19 patients from their chest radiography images. We first prepare a dataset of 5,000 Chest X-rays from the publicly available datasets. Images exhibiting COVID-19 disease presence were identified by board-certified radiologist. Transfer learning on a subset of 2,000 radiograms was used to train four popular convolutional neural networks, including ResNet18, ResNet50, SqueezeNet, and DenseNet-121, to identify COVID-19 disease in the analyzed chest X-ray images. We evaluated these models on the remaining 3,000 images, and most of these networks achieved a sensitivity rate of 98% ($\\pm$ 3%), while having a specificity rate of around 90%. Besides sensitivity and specificity rates, we also present the receiver operating characteristic (ROC) curve, precision-recall curve, average prediction, and confusion matrix of each model. We also used a technique to generate heatmaps of lung regions potentially infected by COVID-19 and show that the generated heatmaps contain most of the infected areas annotated by our board certified radiologist. While the achieved performance is very encouraging, further analysis is required on a larger set of COVID-19 images, to have a more reliable estimation of accuracy rates. The dataset, model implementations (in PyTorch), and evaluations, are all made publicly available for research community at https://github.com/shervinmin/DeepCovid.git", "prompt": "The COVID-19 pandemic is causing a major outbreak in more than 150 countries around the world, having a severe impact on the health and life of many people globally. One of the crucial step in fighting COVID-19 is the ability to detect the infected patients early enough, and put them under special care. Detect", "orig": "ing this disease from radiography and radiology images is perhaps one of the fastest ways to diagnose the patients. Some of the early studies showed specific abnormalities in the chest radiograms of patients infected with COVID-19. Inspired by earlier works, we study the application of deep learning models to detect COVID-19 patients from their", "generated": "The COVID-19 pandemic is causing a major outbreak in more than 150 countries around the world, having a severe impact on the health and life of many people globally. One of the crucial steps in fighting COVID-19 is the ability to detect the infected patients early enough, and put them under special care. Detect"}
{"full_text": "Context. Recent studies show that developers spend most of their programming time testing, verifying and debugging software. As applications become more and more complex, developers demand more advanced debugging support to ease the software development process. Inquiry. Since the 70's many debugging solutions were introduced. Amongst them, online debuggers provide a good insight on the conditions that led to a bug, allowing inspection and interaction with the variables of the program. However, most of the online debugging solutions introduce \\textit{debugging interference} to the execution of the program, i.e. pauses, latency, and evaluation of code containing side-effects. Approach. This paper investigates a novel debugging technique called \\outofplace debugging. The goal is to minimize the debugging interference characteristic of online debugging while allowing online remote capabilities. An \\outofplace debugger transfers the program execution and application state from the debugged application to the debugger application, both running in different processes. Knowledge. On the one hand, \\outofplace debugging allows developers to debug applications remotely, overcoming the need of physical access to the machine where the debugged application is running. On the other hand, debugging happens locally on the remote machine avoiding latency. That makes it suitable to be deployed on a distributed system and handle the debugging of several processes running in parallel. Grounding. We implemented a concrete out-of-place debugger for the Pharo Smalltalk programming language. We show that our approach is practical by performing several benchmarks, comparing our approach with a classic remote online debugger. We show that our prototype debugger outperforms by a 1000 times a traditional remote debugger in several scenarios. Moreover, we show that the presence of our debugger does not impact the overall performance of an application. Importance. This work combines remote debugging with the debugging experience of a local online debugger. Out-of-place debugging is the first online debugging technique that can minimize debugging interference while debugging a remote application. Yet, it still keeps the benefits of online debugging ( e.g. step-by-step execution). This makes the technique suitable for modern applications which are increasingly parallel, distributed and reactive to streams of data from various sources like sensors, UI, network, etc.", "prompt": "Context. Recent studies show that developers spend most of their programming time testing, verifying and debugging software. As applications become more and more complex, developers demand more advanced debugging support to ease the software development process. Inquiry. Since the 70's many debugging solutions were introduced. Amongst them, online debuggers provide a", "orig": "good insight on the conditions that led to a bug, allowing inspection and interaction with the variables of the program. However, most of the online debugging solutions introduce \\textit{debugging interference} to the execution of the program, i.e. pauses, latency, and evaluation of code containing side-effects. Approach. This", "generated": "Context. Recent studies show that developers spend most of their programming time testing, verifying and debugging software. As applications become more and more complex, developers demand more advanced debugging support to ease the software development process. Inquiry. Since the 70's many debugging solutions were introduced. Amongst them, online debuggers provide a"}
{"full_text": "Context: The identification of long-gamma-ray-bursts (LGRBs) is still uncertain, although the collapsar engine of fast-rotating massive stars is gaining a strong consensus. Aims: We propose that low-metallicity Be and Oe stars, which are massive fast rotators, as potential LGRBs progenitors. Methods: We checked this hypothesis by 1) testing the global specific angular momentum of Oe/Be stars in the ZAMS with the SMC metallicity, 2) comparing the ZAMS ($\\Omega/\\Omega_{\\rm c},M/M_{\\odot}$) parameters of these stars with the area predicted theoretically for progenitors with metallicity $Z=0.002$, and 3) calculating the expected rate of LGRBs/year/galaxy and comparing them with the observed ones. To this end, we determined the ZAMS linear and angular rotational velocities for SMC Be and Oe stars using the observed vsini parameters, corrected from the underestimation induced by the gravitational darkening effect. Results: The angular velocities of SMC Oe/Be stars are on average $<\\Omega/\\Omega_{\\rm c}>=0.95$ in the ZAMS. These velocities are in the area theoretically predicted for the LGRBs progenitors. We estimated the yearly rate per galaxy of LGRBs and the number of LGRBs produced in the local Universe up to z=0.2. We have considered that the mass range of LGRB progenitors corresponds to stars hotter than spectral types B0-B1 and used individual beaming angles from 5 to 15\\degr. We thus obtain $R^{\\rm pred}_{\\rm LGRB}\\sim10^{-7}$ to $\\sim10^{-6}$ LGRBs/year/galaxy, which represents on average 2 to 14 LGRB predicted events in the local Universe during the past 11 years. The predicted rates could widely surpass the observed ones [(0.2-3)$\\times10^{-7}$ LGRBs/year/galaxy; 8 LGRBs observed in the local Universe during the last 11 years] if the stellar counts were made from the spectral type B1-B2, in accordance with the expected apparent spectral types of the appropriate massive fast rotators. Conclusion: We conclude that the massive Be/Oe stars with SMC metallicity could be LGRBs progenitors. Nevertheless, other SMC O/B stars without emission lines, which have high enough specific angular momentum, can enhance the predicted $R_{\\rm LGRB}$ rate.", "prompt": "Context: The identification of long-gamma-ray-bursts (LGRBs) is still uncertain, although the collapsar engine of fast-rotating massive stars is gaining a strong consensus. Aims: We propose that low-metallicity Be and Oe stars, which are massive fast rotators, as potential", "orig": "LGRBs progenitors. Methods: We checked this hypothesis by 1) testing the global specific angular momentum of Oe/Be stars in the ZAMS with the SMC metallicity, 2) comparing the ZAMS ($\\Omega/\\Omega_{\\rm c},M/M_{\\odot}$)", "generated": "The identification of long-gamma-ray-bursts (LGRBs) is still uncertain, although the collapsar engine of fast-rotating massive stars is gaining a strong consensus. Aims: We propose that low-metallicity Be and Oe stars, which are massive fast rotators, as potential sources of"}
{"full_text": "This study is about the properties of the sets of objects associated in a structure resulting from multiple-processes involving chance as are materials whose texture is unordered and random. Being a paper scientist the author refers to the sheet of paper which is a stochastic fibrous set whose porous texture can be considered as an archetype for many natural or artificial human structures. The paper properties are correlated with its texture by taking account the effect of chance occurring during its manufacturing process. The theoretical developments, the formalism and the application methods presented in this study have a general significance beyond the only paper material.A specific property of sets of objects randomly unorderly distributed in space is their interfaces orientation distribution. This distribution is usually obtained by the analysis of images sampled in the object sets. The density of orientation probability of the fibers or of the texture interfaces, weighted by their length or by their area, can be interpreted as the radius of curvature of an outline or of a warped surface which characterizes, from a global and statistical point of view, the texture geometry in two or three dimensions. This figure named by the author the \"equivalent pore\", is with its elliptical shape similar to the one of the mean pore defined by the mean directional chord between the interfaces in the texture. Different methods of \"equivalent pore\" establishing are analyzed : by conformal map of the fiber network or of the texture interfaces, by images stereometric analysis of texture tomographical cuts, by scattering and diffraction of a laser light beam impacting the fibrous texture or the material surface replica, by hard X-ray absorption and phase contrast at the European Synchrotron Radiation Facilities(ESRF), in Grenoble. The \"equivalent pore\" concept allows us to study random unordered sets behavior in strength fields while simplifying this analysis. Thus a phenomenon occurring in a plane set, in two dimensions, can be analyzed on its \"equivalent pore\" linear outline, and a phenomenon which occurs in volume in a three dimensional set can be analyzed on its \"equivalent pore\" warped surface. This concept has been applied for physical, mechanical, optical and ionic conduction properties of materials like papers, boards, felts, nonwoven textiles, polymer foams, metallic alloys with grain joints, geological grounds, and for the surface mapping of natural relief and of materials with different gloss, worn or roughness levels.The ellipse and the ellipso\\\"id, as well as multi-modes compositions of it, are the most appropriate figures to represent the \"equivalent pore\" of materials with a random unordered texture. The fact that a law, which defines the curvature of an elliptic deterministic geometrical configuration, is essential to represent interface orientation allocation of elements whose spatial distribution is probabilistic is a noteworthy fact that makes us wonder. This assertion is corroborated by fluid flow analysis through porous media. The global dissipated energy for fluid flow is distributed along the motion (translation and rotation) and fluid deformation components on the \"equivalent pore\" whose surface is conformal to the texture interfaces tangential space. The porous media being homogenous and the fluid particles indistinguishable each ones from the others, due to permanent stochastic exchanges from one fluid volume element into another, we conclude that their motion quantification is invariant on each point of the \"equivalent pore\" surface. This quantification can be represented by a group of elliptical cylinders.The \"equivalent pore\" and cylinders group surface intersections define the fluid particle probabilistic paths in the porous media. One singular geometrical configuration of the elliptic cylinders group union with the ellipsoidal \"equivalent pore\" fulfills the minimal energy dissipation requirements in the stress field applied to the fluid. The resulting paths are ellipso\\\"ido-cylindrical curves carpeting the ellipso\\\"id by a beam of isoperim\\'etric lacets, in close or open loops depending on the possible pairing off the curves in their nodals and isoclinal tangency points. The flow laminarity and unrotationality are globally established in the porous media at the macroscopic scale, for Reynold's number small values, in agreement with the results found moreover. The so defined ellipso\\\"ido-cylindrical curve is a stationary curve at the average least action meaning, for a punctual mobile or a deformable volume element moving on the surface of the ellipsoid from a nodal spot to the other in an antipodal position. This curve establishes a gauge which characterizes, from a global point of view, the physical space embraced by the fluid during its flow, in dynamical balance conditions compatible with the stress field. The ellipso\\\"ido-cylindrical curve allows to carpet the ellipso\\\"id as a function of one cyclic angular variable, which is a new construction for the ellipso\\\"id. When the set of the objects is isotropic the curve is sphero-cylindrical whose plan mapping permit to decipher the tai-chi figure of the Tao\\\"ist Buddhism philosophy.This study of random unordered object sets, and more specially of fibrous porous media, allows to establish a structural link between their small scale texture stochastic disorder and the harmonious order that emerges from these sets at a larger scale. The largest chance possible, compatible with the stress field which put a strain on these sets, is the necessary and sufficient variable which allows to best globally adjust their behaviors according to the probabilistic and deterministic laws governing their evolution.The vocabulary used in this study is issued from the common language, most of the time adapted to the material class, it is transposable in other fields of interest. The text is completed by notes and a bibliography which refer to the works done following the presented concepts or in relation to this field of studies.", "prompt": "This study is about the properties of the sets of objects associated in a structure resulting from multiple-processes involving chance as are materials whose texture is unordered and random. Being a paper scientist the author refers to the sheet of paper which is a stochastic fibrous set whose porous texture can be considered as an archetype for many natural", "orig": "or artificial human structures. The paper properties are correlated with its texture by taking account the effect of chance occurring during its manufacturing process. The theoretical developments, the formalism and the application methods presented in this study have a general significance beyond the only paper material.A specific property of sets of objects randomly unorderly distributed in space", "generated": "This study is about the properties of the sets of objects associated in a structure resulting from multiple-processes involving chance as are materials whose texture is unordered and random. Being a paper scientist the author refers to the sheet of paper which is a stochastic fibrous set whose porous texture can be considered as an archetype for many natural"}
{"full_text": "The context of this work is specification, detection and ultimately removal of detectable harmful patterns in source code that are associated with defects in design and implementation of software. In particular, we investigate five code smells and four antipatterns previously defined in papers and books. Our inquiry is about detecting those in source code written in Python programming language, which is substantially different from all prior research, most of which concerns Java or C-like languages. Our approach was that of software engineers: we have processed existing research literature on the topic, extracted both the abstract definitions of nine design defects and their concrete implementation specifications, implemented them all in a tool we have programmed and let it loose on a huge test set obtained from open source code from thousands of GitHub projects. When it comes to knowledge, we have found that more than twice as many methods in Python can be considered too long (statistically extremely longer than their neighbours within the same project) than in Java, but long parameter lists are seven times less likely to be found in Python code than in Java code. We have also found that Functional Decomposition, the way it was defined for Java, is not found in the Python code at all, and Spaghetti Code and God Classes are extremely rare there as well. The grounding and the confidence in these results comes from the fact that we have performed our experiments on 32'058'823 lines of Python code, which is by far the largest test set for a freely available Python parser. We have also designed the experiment in such a way that it aligned with prior research on design defect detection in Java in order to ease the comparison if we treat our own actions as a replication. Thus, the importance of the work is both in the unique open Python grammar of highest quality, tested on millions of lines of code, and in the design defect detection tool which works on something else than Java.", "prompt": "The context of this work is specification, detection and ultimately removal of detectable harmful patterns in source code that are associated with defects in design and implementation of software. In particular, we investigate five code smells and four antipatterns previously defined in papers and books. Our inquiry is about detecting those in source code written", "orig": "in Python programming language, which is substantially different from all prior research, most of which concerns Java or C-like languages. Our approach was that of software engineers: we have processed existing research literature on the topic, extracted both the abstract definitions of nine design defects and their concrete implementation specifications, implemented them all in a tool we", "generated": "The context of this work is specification, detection and ultimately removal of detectable harmful patterns in source code that are associated with defects in design and implementation of software. In particular, we investigate five code smells and four antipatterns previously defined in papers and books. Our inquiry is about detecting those in source code written"}
{"full_text": "This article deals with the design of saturated controls in the context of partial differential equations. It focuses on a Korteweg-de Vries equation, which is a nonlinear mathematical model of waves on shallow water surfaces. Two different types of saturated controls are considered. The well-posedness is proven applying a Banach fixed point theorem, using some estimates of this equation and some properties of the saturation function. The proof of the asymptotic stability of the closed-loop system is separated in two cases: i) when the control acts on all the domain, a Lyapunov function together with a sector condition describing the saturating input is used to conclude on the stability, ii) when the control is localized, we argue by contradiction. Some numerical simulations illustrate the stability of the closed-loop nonlinear partial differential equation. 1. Introduction. In recent decades, a great effort has been made to take into account input saturations in control designs (see e.g [39], [15] or more recently [17]). In most applications, actuators are limited due to some physical constraints and the control input has to be bounded. Neglecting the amplitude actuator limitation can be source of undesirable and catastrophic behaviors for the closed-loop system. The standard method to analyze the stability with such nonlinear controls follows a two steps design. First the design is carried out without taking into account the saturation. In a second step, a nonlinear analysis of the closed-loop system is made when adding the saturation. In this way, we often get local stabilization results. Tackling this particular nonlinearity in the case of finite dimensional systems is already a difficult problem. However, nowadays, numerous techniques are available (see e.g. [39, 41, 37]) and such systems can be analyzed with an appropriate Lyapunov function and a sector condition of the saturation map, as introduced in [39]. In the literature, there are few papers studying this topic in the infinite dimensional case. Among them, we can cite [18], [29], where a wave equation equipped with a saturated distributed actuator is studied, and [12], where a coupled PDE/ODE system modeling a switched power converter with a transmission line is considered. Due to some restrictions on the system, a saturated feedback has to be designed in the latter paper. There exist also some papers using the nonlinear semigroup theory and focusing on abstract systems ([20],[34],[36]). Let us note that in [36], [34] and [20], the study of a priori bounded controller is tackled using abstract nonlinear theory. To be more specific, for bounded ([36],[34]) and unbounded ([34]) control operators, some conditions are derived to deduce, from the asymptotic stability of an infinite-dimensional linear system in abstract form, the asymptotic stability when closing the loop with saturating controller. These articles use the nonlinear semigroup theory (see e.g. [24] or [1]). The Korteweg-de Vries equation (KdV for short)", "prompt": "This article deals with the design of saturated controls in the context of partial differential equations. It focuses on a Korteweg-de Vries equation, which is a nonlinear mathematical model of waves on shallow water surfaces. Two different types of saturated controls are considered. The well-posedness is proven applying a Banach", "orig": "fixed point theorem, using some estimates of this equation and some properties of the saturation function. The proof of the asymptotic stability of the closed-loop system is separated in two cases: i) when the control acts on all the domain, a Lyapunov function together with a sector condition describing the saturating input is", "generated": "This article deals with the design of saturated controls in the context of partial differential equations. It focuses on a Korteweg-de Vries equation, which is a nonlinear mathematical model of waves on shallow water surfaces. Two different types of saturated controls are considered. The well-posedness is proven applying a Banach"}
{"full_text": "Recent research has shown that surprisingly rich models of human activity can be learned from GPS (positional) data. However, most effort to date has concentrated on modeling single individuals or statistical properties of groups of people. Moreover, prior work focused solely on modeling actual successful executions (and not failed or attempted executions) of the activities of interest. We, in contrast, take on the task of understanding human interactions, attempted interactions, and intentions from noisy sensor data in a fully relational multi-agent setting. We use a real-world game of capture the flag to illustrate our approach in a well-defined domain that involves many distinct cooperative and competitive joint activities. We model the domain using Markov logic, a statistical-relational language, and learn a theory that jointly denoises the data and infers occurrences of high-level activities, such as a player capturing an enemy. Our unified model combines constraints imposed by the geometry of the game area, the motion model of the players, and by the rules and dynamics of the game in a probabilistically and logically sound fashion. We show that while it may be impossible to directly detect a multi-agent activity due to sensor noise or malfunction, the occurrence of the activity can still be inferred by considering both its impact on the future behaviors of the people involved as well as the events that could have preceded it. Further, we show that given a model of successfully performed multi-agent activities, along with a set of examples of failed attempts at the same activities, our system automatically learns an augmented model that is capable of recognizing success and failure, as well as goals of peoples actions with high accuracy. We compare our approach with other alternatives and show that our unified model, which takes into account not only relationships among individual players, but also relationships among activities over the entire length of a game, although more computationally costly, is significantly more accurate. Finally, we demonstrate that explicitly modeling unsuccessful attempts boosts performance on other important recognition tasks.", "prompt": "Recent research has shown that surprisingly rich models of human activity can be learned from GPS (positional) data. However, most effort to date has concentrated on modeling single individuals or statistical properties of groups of people. Moreover, prior work focused solely on modeling actual successful executions (and not failed or attempted executions) of the", "orig": "activities of interest. We, in contrast, take on the task of understanding human interactions, attempted interactions, and intentions from noisy sensor data in a fully relational multi-agent setting. We use a real-world game of capture the flag to illustrate our approach in a well-defined domain that involves many distinct cooperative and competitive joint activities.", "generated": "Recent research has shown that surprisingly rich models of human activity can be learned from GPS (positional) data. However, most effort to date has concentrated on modeling single individuals or statistical properties of groups of people. Moreover, prior work focused solely on modeling actual successful executions (and not failed or attempted executions) of their"}
{"full_text": "Situations where individuals have to contribute to joint efforts or share scarce resources are ubiquitous. Yet, without proper mechanisms to ensure cooperation, the evolutionary pressure to maximize individual success tends to create a tragedy of the commons (such as over-fishing or the destruction of our environment). This contribution addresses a number of related puzzles of human behavior with an evolutionary game theoretical approach as it has been successfully used to explain the behavior of other biological species many times, from bacteria to vertebrates. Our agent-based model distinguishes individuals applying four different behavioral strategies: non-cooperative individuals (\"defectors\"), cooperative individuals abstaining from punishment efforts (called \"cooperators\" or \"second-order free-riders\"), cooperators who punish non-cooperative behavior (\"moralists\"), and defectors, who punish other defectors despite being non-cooperative themselves (\"immoralists\"). By considering spatial interactions with neighboring individuals, our model reveals several interesting effects: First, moralists can fully eliminate cooperators. This spreading of punishing behavior requires a segregation of behavioral strategies and solves the \"second-order free-rider problem\". Second, the system behavior changes its character significantly even after very long times (\"who laughs last laughs best effect\"). Third, the presence of a number of defectors can largely accelerate the victory of moralists over non-punishing cooperators. Forth, in order to succeed, moralists may profit from immoralists in a way that appears like an \"unholy collaboration\". Our findings suggest that the consideration of punishment strategies allows to understand the establishment and spreading of \"moral behavior\" by means of game-theoretical concepts. This demonstrates that quantitative biological modeling approaches are powerful even in domains that have been addressed with non-mathematical concepts so far. The complex dynamics of certain social behaviors becomes understandable as result of an evolutionary competition between different behavioral strategies.", "prompt": "Situations where individuals have to contribute to joint efforts or share scarce resources are ubiquitous. Yet, without proper mechanisms to ensure cooperation, the evolutionary pressure to maximize individual success tends to create a tragedy of the commons (such as over-fishing or the destruction of our environment). This contribution addresses a number of related puzzles", "orig": "of human behavior with an evolutionary game theoretical approach as it has been successfully used to explain the behavior of other biological species many times, from bacteria to vertebrates. Our agent-based model distinguishes individuals applying four different behavioral strategies: non-cooperative individuals (\"defectors\"), cooperative individuals abstaining from punishment efforts (called \"co", "generated": "Situations where individuals have to contribute to joint efforts or share scarce resources are ubiquitous. Yet, without proper mechanisms to ensure cooperation, the evolutionary pressure to maximize individual success tends to create a tragedy of the commons (such as over-fishing or the destruction of our environment). This contribution addresses a number of related puzzles"}
{"full_text": "We use Small Angle Neutron Scattering (SANS), with an original analysis method, to obtain both the characteristic sizes and the inner composition of lysozyme-pectin complexes depending on the charge density. Lysozyme is a globular protein and pectin a natural anionic semiflexible polysaccharide with a degree of methylation (DM) 0, 43 and 74. For our experimental conditions (buffer ionic strength I = 2.5 10-2 mol/L and pH between 3 and 7), the electrostatic charge of lysozyme is always positive (from 8 to 17 depending on pH). The pectin charge per elementary chain segment is negative and can be varied from almost zero to one through the change of DM and pH. The weight molar ratio of lysozyme on pectin monomers is kept constant. The ratio of negative charge content per volume to positive charge content per volume, -/+, is varied between 10 and 0.007. On a local scale, for all charged pectins, a correlation peak appears at 0.2 {\\AA}-1 due to proteins clustering inside the complexes. On a large scale, the complexes appear as formed of spherical globules with a well defined radius of 10 to 50 nm, containing a few thousands proteins. The volume fraction Phi of organic matter within the globules derived from SANS absolute cross-sections is around 0.1. The protein stacking, which occurs inside the globules, is enhanced when pectin is more charged, due to pH or DM. The linear charge density of the pectin determines the size of the globules for pectin chains of comparable molecular weights whether it is controlled by the pH or the DM. The radius of the globules varies between 10 nm and 50 nm. In conclusion the structure is driven by electrostatic interactions and not by hydrophobic interactions. The molecular weight also has a large influence on the structure of the complexes since long chains tend to form larger globules. This maybe one reason why DM and pH are not completely equivalent in our system since DM 0 has a short mass, but this may not be the only one. For very low pectin charge (-/+ = 0.07), globules do not appear and the scattering signals a gel-like structure. We did not observe any beads-on-a-string structure.", "prompt": "We use Small Angle Neutron Scattering (SANS), with an original analysis method, to obtain both the characteristic sizes and the inner composition of lysozyme-pectin complexes depending on the charge density. Lysozyme is a globular protein and pectin a natural anionic semiflexible polys", "orig": "accharide with a degree of methylation (DM) 0, 43 and 74. For our experimental conditions (buffer ionic strength I = 2.5 10-2 mol/L and pH between 3 and 7), the electrostatic charge of lysozyme is always positive (", "generated": "We use Small Angle Neutron Scattering (SANS), with an original analysis method, to obtain both the characteristic sizes and the inner composition of lysozyme-pectin complexes depending on the charge density. Lysozyme is a globular protein and pectin a natural anionic semiflexible polymer"}
{"full_text": "Many current large-scale multiagent team implementations can be characterized as following the belief-desire-intention (BDI) paradigm, with explicit representation of team plans. Despite their promise, current BDI team approaches lack tools for quantitative performance analysis under uncertainty. Distributed partially observable Markov decision problems (POMDPs) are well suited for such analysis, but the complexity of finding optimal policies in such models is highly intractable. The key contribution of this article is a hybrid BDI-POMDP approach, where BDI team plans are exploited to improve POMDP tractability and POMDP analysis improves BDI team plan performance. Concretely, we focus on role allocation, a fundamental problem in BDI teams: which agents to allocate to the different roles in the team. The article provides three key contributions. First, we describe a role allocation technique that takes into account future uncertainties in the domain; prior work in multiagent role allocation has failed to address such uncertainties. To that end, we introduce RMTDP (Role-based Markov Team Decision Problem), a new distributed POMDP model for analysis of role allocations. Our technique gains in tractability by significantly curtailing RMTDP policy search; in particular, BDI team plans provide incomplete RMTDP policies, and the RMTDP policy search fills the gaps in such incomplete policies by searching for the best role allocation. Our second key contribution is a novel decomposition technique to further improve RMTDP policy search efficiency. Even though limited to searching role allocations, there are still combinatorially many role allocations, and evaluating each in RMTDP to identify the best is extremely difficult. Our decomposition technique exploits the structure in the BDI team plans to significantly prune the search space of role allocations. Our third key contribution is a significantly faster policy evaluation algorithm suited for our BDI-POMDP hybrid approach. Finally, we also present experimental results from two domains: mission rehearsal simulation and RoboCupRescue disaster rescue simulation.", "prompt": "Many current large-scale multiagent team implementations can be characterized as following the belief-desire-intention (BDI) paradigm, with explicit representation of team plans. Despite their promise, current BDI team approaches lack tools for quantitative performance analysis under uncertainty. Distributed partially observable Markov decision problems (POMDPs)", "orig": "are well suited for such analysis, but the complexity of finding optimal policies in such models is highly intractable. The key contribution of this article is a hybrid BDI-POMDP approach, where BDI team plans are exploited to improve POMDP tractability and POMDP analysis improves BDI team plan performance", "generated": "Many current large-scale multiagent team implementations can be characterized as following the belief-desire-intention (BDI) paradigm, with explicit representation of team plans. Despite their promise, current BDI team approaches lack tools for quantitative performance analysis under uncertainty. Distributed partially observable Markov decision problems (POMDPs)"}
{"full_text": "Context: Computational notebooks are a contemporary style of literate programming, in which users can communicate and transfer knowledge by interleaving executable code, output, and prose in a single rich document. A Domain-Specific Language (DSL) is an artificial software language tailored for a particular application domain. Usually, DSL users are domain experts that may not have a software engineering background. As a consequence, they might not be familiar with Integrated Development Environments (IDEs). Thus, the development of tools that offer different interfaces for interacting with a DSL is relevant. Inquiry: However, resources available to DSL designers are limited. We would like to leverage tools used to interact with general purpose languages in the context of DSLs. Computational notebooks are an example of such tools. Then, our main question is: What is an efficient and effective method of designing and implementing notebook interfaces for DSLs? By addressing this question we might be able to speed up the development of DSL tools, and ease the interaction between end-users and DSLs. Approach: In this paper, we present Bacat\\'a, a mechanism for generating notebook interfaces for DSLs in a language parametric fashion. We designed this mechanism in a way in which language engineers can reuse as many language components (e.g., language processors, type checkers, code generators) as possible. Knowledge: Our results show that notebook interfaces generated by Bacat\\'a can be automatically generated with little manual configuration. There are few considerations and caveats that should be addressed by language engineers that rely on language design aspects. The creation of a notebook for a DSL with Bacat\\'a becomes a matter of writing the code that wires existing language components in the Rascal language workbench with the Jupyter platform. Grounding: We evaluate Bacat\\'a by generating functional computational notebook interfaces for three different non-trivial DSLs, namely: a small subset of Halide (a DSL for digital image processing), SweeterJS (an extended version of JavaScript), and QL (a DSL for questionnaires). Additionally, it is relevant to generate notebook implementations rather than implementing them manually. We measured and compared the number of Source Lines of Code (SLOCs) that we reused from existing implementations of those languages. Importance: The adoption of notebooks by novice-programmers and end-users has made them very popular in several domains such as exploratory programming, data science, data journalism, and machine learning. Why are they popular? In (data) science, it is essential to make results reproducible as well as understandable. However, notebooks are only available for GPLs. This paper opens up the notebook metaphor for DSLs to improve the end-user experience when interacting with code and to increase DSLs adoption.", "prompt": "Context: Computational notebooks are a contemporary style of literate programming, in which users can communicate and transfer knowledge by interleaving executable code, output, and prose in a single rich document. A Domain-Specific Language (DSL) is an artificial software language tailored for a particular application domain. Usually, DSL users are domain", "orig": "experts that may not have a software engineering background. As a consequence, they might not be familiar with Integrated Development Environments (IDEs). Thus, the development of tools that offer different interfaces for interacting with a DSL is relevant. Inquiry: However, resources available to DSL designers are limited. We would like to leverage tools", "generated": "Computational notebooks are a contemporary style of literate programming, in which users can communicate and transfer knowledge by interleaving executable code, output, and prose in a single rich document. A Domain-Specific Language (DSL) is an artificial software language tailored for a particular application domain. Usually, DSL users are domain experts"}
{"full_text": "Non-Gaussian outcomes are often modeled using members of the so-called exponential family. Notorious members are the Bernoulli model for binary data, leading to logistic regression, and the Poisson model for count data, leading to Poisson regression. Two of the main reasons for extending this family are (1) the occurrence of overdispersion, meaning that the variability in the data is not adequately described by the models, which often exhibit a prescribed mean--variance link, and (2) the accommodation of hierarchical structure in the data, stemming from clustering in the data which, in turn, may result from repeatedly measuring the outcome, for various members of the same family, etc. The first issue is dealt with through a variety of overdispersion models, such as, for example, the beta-binomial model for grouped binary data and the negative-binomial model for counts. Clustering is often accommodated through the inclusion of random subject-specific effects. Though not always, one conventionally assumes such random effects to be normally distributed. While both of these phenomena may occur simultaneously, models combining them are uncommon. This paper proposes a broad class of generalized linear models accommodating overdispersion and clustering through two separate sets of random effects. We place particular emphasis on so-called conjugate random effects at the level of the mean for the first aspect and normal random effects embedded within the linear predictor for the second aspect, even though our family is more general. The binary, count and time-to-event cases are given particular emphasis. Apart from model formulation, we present an overview of estimation methods, and then settle for maximum likelihood estimation with analytic--numerical integration. Implications for the derivation of marginal correlations functions are discussed. The methodology is applied to data from a study in epileptic seizures, a clinical trial in toenail infection named onychomycosis and survival data in children with asthma.", "prompt": "Non-Gaussian outcomes are often modeled using members of the so-called exponential family. Notorious members are the Bernoulli model for binary data, leading to logistic regression, and the Poisson model for count data, leading to Poisson regression. Two of the main reasons for extending this family are (1) the", "orig": "occurrence of overdispersion, meaning that the variability in the data is not adequately described by the models, which often exhibit a prescribed mean--variance link, and (2) the accommodation of hierarchical structure in the data, stemming from clustering in the data which, in turn, may result from repeatedly measuring the outcome,", "generated": "Non-Gaussian outcomes are often modeled using members of the so-called exponential family. Notorious members are the Bernoulli model for binary data, leading to logistic regression, and the Poisson model for count data, leading to Poisson regression. Two of the main reasons for extending this family are (1) the"}
{"full_text": "Domain-specific languages (DSLs) are touted as both easy to embed in programs and easy to optimize. Yet these goals are often in tension. Embedded or internal DSLs fit naturally with a host language, while inheriting the host's performance characteristics. External DSLs can use external optimizers and languages but sit apart from the host. We present Sham, a toolkit designed to enable internal DSLs with high performance. Sham provides a domain-specific language (embedded in Racket) for implementing other high-performance DSLs, with transparent compilation to assembly code at runtime. Sham is well suited as both a compilation target for other embedded DSLs and for transparently replacing DSL support code with faster versions. Sham provides seamless inter-operation with its host language without requiring any additional effort from its users. Sham also provides a framework for defining language syntax which implements Sham's own language interface as well. We validate Sham's design on a series of case studies, ranging from Krishnamurthi's classic automata DSL to a sound synthesis DSL and a probabilistic programming language. All of these are existing DSLs where we replaced the backend using Sham, resulting in major performance gains. We present an example-driven description of how Sham can smoothly enhance an existing DSL into a high-performance one. When compared to existing approaches for implementing high-performance DSLs, Sham's design aims for both simplicity and programmer control. This makes it easier to port our techniques to other languages and frameworks, or borrow Sham's innovations \"\\`a la carte\" without adopting the whole approach. Sham builds a sophisticated and powerful DSL construction toolkit atop fundamental language features including higher-order functions, data structures, and a foreign-function interface (FFI), all readily available in other languages. Furthermore, Sham's approach allows DSL developers to simply write functions, either using Sham or generating Sham, without needing to work through complex staging or partial evaluation systems.", "prompt": "Domain-specific languages (DSLs) are touted as both easy to embed in programs and easy to optimize. Yet these goals are often in tension. Embedded or internal DSLs fit naturally with a host language, while inheriting the host's performance characteristics. External DSLs can use external optimizers and languages but sit apart", "orig": "from the host. We present Sham, a toolkit designed to enable internal DSLs with high performance. Sham provides a domain-specific language (embedded in Racket) for implementing other high-performance DSLs, with transparent compilation to assembly code at runtime. Sham is well suited as both a compilation target for other embedded DSLs and", "generated": "Domain-specific languages (DSLs) are touted as both easy to embed in programs and easy to optimize. Yet these goals are often in tension. Embedded or internal DSLs fit naturally with a host language, while inheriting the host's performance characteristics. External DSLs can use external optimizers and languages but sit apart"}
{"full_text": "Context: Container data types are ubiquitous in computer programming, enabling developers to efficiently store and process collections of data with an easy-to-use programming interface. Many programming languages offer a variety of container implementations in their standard libraries based on data structures offering different capabilities and performance characteristics. Inquiry: Choosing the *best* container for an application is not always straightforward, as performance characteristics can change drastically in different scenarios, and as real-world performance is not always correlated to theoretical complexity. Approach: We present Primrose, a language-agnostic tool for selecting the best performing valid container implementation from a set of container data types that satisfy *properties* given by application developers. Primrose automatically selects the set of valid container implementations for which the *library specifications*, written by the developers of container libraries, satisfies the specified properties. Finally, Primrose ranks the valid library implementations based on their runtime performance. Knowledge: With Primrose, application developers can specify the expected behaviour of a container as a type refinement with *semantic properties*, e.g., if the container should only contain unique values (such as a `set`) or should satisfy the LIFO property of a `stack`. Semantic properties nicely complement *syntactic properties* (i.e., traits, interfaces, or type classes), together allowing developers to specify a container's programming interface *and* behaviour without committing to a concrete implementation. Grounding: We present our prototype implementation of Primrose that preprocesses annotated Rust code, selects valid container implementations and ranks them on their performance. The design of Primrose is, however, language-agnostic, and is easy to integrate into other programming languages that support container data types and traits, interfaces, or type classes. Our implementation encodes properties and library specifications into verification conditions in Rosette, an interface for SMT solvers, which determines the set of valid container implementations. We evaluate Primrose by specifying several container implementations, and measuring the time taken to select valid implementations for various combinations of properties with the solver. We automatically validate that container implementations conform to their library specifications via property-based testing. Importance: This work provides a novel approach to bring abstract modelling and specification of container types directly into the programmer's workflow. Instead of selecting concrete container implementations, application programmers can now work on the level of specification, merely stating the behaviours they require from their container types, and the best implementation can be selected automatically.", "prompt": "Context: Container data types are ubiquitous in computer programming, enabling developers to efficiently store and process collections of data with an easy-to-use programming interface. Many programming languages offer a variety of container implementations in their standard libraries based on data structures offering different capabilities and performance characteristics. Inquiry: Choosing the *best* container for an", "orig": "application is not always straightforward, as performance characteristics can change drastically in different scenarios, and as real-world performance is not always correlated to theoretical complexity. Approach: We present Primrose, a language-agnostic tool for selecting the best performing valid container implementation from a set of container data types that satisfy *properties* given by application", "generated": "Container data types are ubiquitous in computer programming, enabling developers to efficiently store and process collections of data with an easy-to-use programming interface. Many programming languages offer a variety of container implementations in their standard libraries based on data structures offering different capabilities and performance characteristics. \n\nChoosing the *best* container for a particular application depends"}
{"full_text": "Emerging computer architectures will feature drastically decreased flops/byte (ratio of peak processing rate to memory bandwidth) as highlighted by recent studies on Exascale architectural trends. Further, flops are getting cheaper while the energy cost of data movement is increasingly dominant. The understanding and characterization of data locality properties of computations is critical in order to guide efforts to enhance data locality. Reuse distance analysis of memory address traces is a valuable tool to perform data locality characterization of programs. A single reuse distance analysis can be used to estimate the number of cache misses in a fully associative LRU cache of any size, thereby providing estimates on the minimum bandwidth requirements at different levels of the memory hierarchy to avoid being bandwidth bound. However, such an analysis only holds for the particular execution order that produced the trace. It cannot estimate potential improvement in data locality through dependence preserving transformations that change the execution schedule of the operations in the computation. In this article, we develop a novel dynamic analysis approach to characterize the inherent locality properties of a computation and thereby assess the potential for data locality enhancement via dependence preserving transformations. The execution trace of a code is analyzed to extract a computational directed acyclic graph (CDAG) of the data dependences. The CDAG is then partitioned into convex subsets, and the convex partitioning is used to reorder the operations in the execution trace to enhance data locality. The approach enables us to go beyond reuse distance analysis of a single specific order of execution of the operations of a computation in characterization of its data locality properties. It can serve a valuable role in identifying promising code regions for manual transformation, as well as assessing the effectiveness of compiler transformations for data locality enhancement. We demonstrate the effectiveness of the approach using a number of benchmarks, including case studies where the potential shown by the analysis is exploited to achieve lower data movement costs and better performance.", "prompt": "Emerging computer architectures will feature drastically decreased flops/byte (ratio of peak processing rate to memory bandwidth) as highlighted by recent studies on Exascale architectural trends. Further, flops are getting cheaper while the energy cost of data movement is increasingly dominant. The understanding and characterization of data locality properties of computations is", "orig": "critical in order to guide efforts to enhance data locality. Reuse distance analysis of memory address traces is a valuable tool to perform data locality characterization of programs. A single reuse distance analysis can be used to estimate the number of cache misses in a fully associative LRU cache of any size, thereby providing estimates on the minimum bandwidth", "generated": "Emerging computer architectures will feature drastically decreased flops/byte (ratio of peak processing rate to memory bandwidth) as highlighted by recent studies on Exascale architectural trends. Further, flops are getting cheaper while the energy cost of data movement is increasingly dominant. The understanding and characterization of data locality properties of computations is"}
{"full_text": "Atmospheric composition measurements taken at many high-altitude stations around the world, aim to collect data representative of the free troposphere and of an intercontinental scale. However, the high-altitude environment favours vertical mixing and the transportation of air masses at local or regional scales, which has a potential influence on the composition of the sampled air masses. Mixing processes, source-receptor pathways, and atmospheric chemistry may strongly depend on local and regional weather regimes, and these should be characterized specifically for each station. The Pic du Midi (PDM) isa mountaintop observatory (2850 m a.s.l.) on the north side of the Pyrenees. PDM is associated with the Centre de Recherches Atmosph{\\'e}riques (CRA), a site in the foothills ar 600 m a.s.l. 28 km north-east of the PDM. The two centers make up the Pyrenean Platform for the Observation of the Atmosphere (P2OA). Data measured at PDM and CRA were combined to form a5-year hourly dataset of 23 meteorological variables notably: temperature, humidity, cloud cover, wind at several altitudes. The dataset was classified using hierarchical clustering, with the aim of grouping together the days which had similar meteorological characteristics. To complete the clustering, we computed several diagnostic tools, in order to provide additional information and study specific phenomena (foehn, precipitation, atmospheric vertical structure, and thermally driven circulations). This classification resulted in six clusters: three highly populated clusters which correspond to the most frequent meteorological conditions (fair weather, mixed weather and disturbed weather, respectively); a small cluster evidencing clear characteristics of winter northwesterly windstorms; and two small clusters characteristic of south foehn (south- to southwesterly large-scaleflow, associated with warm and dry downslope flow on the lee side of the chain). The diagnostic tools applied to the six clusters provided results in line with the conclusions tentatively drawn from 23 meteorological variables. This, to some extent,validates the approach of hierarchical clustering of local data to distinguish weather regimes. Then statistics of atmospheric composition at PDM were analysed and discussed for each cluster. Radon measurements, notably, revealed that the regional background in the lower troposphere dominates the influence of diurnal thermal flows when daily averaged concentrations are considered. Differences between clusters were demonstrated by the anomalies of CO, CO$_2$, CH$_4$, O$_3$ and aerosol number concentration, and interpretations in relation with chemical sinks and sources are proposed.", "prompt": "Atmospheric composition measurements taken at many high-altitude stations around the world, aim to collect data representative of the free troposphere and of an intercontinental scale. However, the high-altitude environment favours vertical mixing and the transportation of air masses at local or regional scales, which has a potential influence on the", "orig": "composition of the sampled air masses. Mixing processes, source-receptor pathways, and atmospheric chemistry may strongly depend on local and regional weather regimes, and these should be characterized specifically for each station. The Pic du Midi (PDM) isa mountaintop observatory (2850 m a.s.l.) on the north side", "generated": "Atmospheric composition measurements taken at many high-altitude stations around the world, aim to collect data representative of the free troposphere and of an intercontinental scale. However, the high-altitude environment favours vertical mixing and the transportation of air masses at local or regional scales, which has a potential influence on the"}
{"full_text": "A new Riemann solver is built to address numerical resolution of complex flow models. The research direction is closely linked to a variant of the Baer and Nunziato (1986) model developed in Saurel et al. (2017a). This recent model provides a link between the Marble (1963) model for two-phase dilute suspensions and dense mixtures. As in the Marble model, Saurel et al. system is weakly hyperbolic with the same 4 characteristic waves, while the system involves 7 partial differential equations. It poses serious theoretical and practical issues to built simple and accurate flow solver. To overcome related difficulties the Riemann solver of Linde (2002) is revisited. The method is first examined in the simplified context of compressible Euler equations. Physical considerations are introduced in the solver improving robustness and accuracy of the Linde method. With these modifications the flow solver appears as accurate as the HLLC solver of Toro et al. (1994). Second the two-phase flow model is considered. A locally conservative formulation is built and validated removing issues related to non-conservative terms. However, two extra major issues appear from numerical experiments: The solution appears not self-similar and multiple contact waves appear in the dispersed phase. Building HLLC-type or any other solver appears consequently challenging. The modified Linde (2002) method is thus examined for the considered flow model. Some basic properties of the equations are used, such as shock relations of the dispersed phase and jump conditions across the contact wave. Thanks to these ingredients the new Riemann solver with internal reconstruction (RSIR), modification of the Linde method, handles stationary volume fraction discontinuities, presents low dissipation for transport waves and handles shocks and expansion waves accurately. It is validated on various test problems showing method's accuracy and versatility for complex flow models. Its capabilities are illustrated on a difficult two-phase flow instability problem, unresolved before.", "prompt": "A new Riemann solver is built to address numerical resolution of complex flow models. The research direction is closely linked to a variant of the Baer and Nunziato (1986) model developed in Saurel et al. (2017a). This recent model provides a link between the Marble (196", "orig": "3) model for two-phase dilute suspensions and dense mixtures. As in the Marble model, Saurel et al. system is weakly hyperbolic with the same 4 characteristic waves, while the system involves 7 partial differential equations. It poses serious theoretical and practical issues to built simple and accurate", "generated": "A new Riemann solver is built to address numerical resolution of complex flow models. The research direction is closely linked to a variant of the Baer and Nunziato (1986) model developed in Saurel et al. (2017a). This recent model provides a link between the Marble (196"}
{"full_text": "Purpose. It has been reported clinically that rupture or dissections in thoracic aortic aneurysms (TAA) often occur due to hypertension which may be modelled with sudden increase of peripheral resistance, inducing acute changes of blood volumes in the aorta. There is clinical evidence that more compliant aneurysms are less prone to rupture as they can sustain such changes of volume. The aim of the current paper is to verify this paradigm by evaluating computationally the role played by the variation of peripheral resistance and the impact of aortic stiffness onto peak wall stress in ascending TAA. Methods. Fluid-Structure Interaction (FSI) analyses were performed using patient-specific geometries and boundary conditions derived from 4D MRI datasets acquired on a patient. Blood was assumed incompressible and was treated as a non-Newtonian fluid using the Carreau model while the wall mechanical properties were obtained from the bulge inflation tests carried out in vitro after surgical repair. The Navier Stokes equations were solved in ANSYS Fluent. The Arbitrary Lagrangian Eulerian formulation was used to account for the wall deformations. At the interface between the solid domain and the fluid domain, the fluid pressure was transferred to the wall and the displacement of the wall was transferred to the fluid. The two systems were connected by the System Coupling component which controls the solver execution of fluid and solid simulations in ANSYS. Fluid and solid domains were solved sequentially starting from the fluid simulations. Results. Distributions of blood flow, wall shear stress and wall stress were evaluated in the ascending thoracic aorta using the FSI analyses. We always observed a significant flow eccentricity in the simulations, in very good agreement with velocity profiles measured using 4D MRI. The results also showed significant increase of peak wall stress due to the increase of peripheral resistance and aortic stiffness. In the worst case scenario, the largest peripheral resistance (10 10 kg.s.m-4) and stiffness (10 MPa) resulted in a maximal principal stress equal to 702 kPa, whereas it was only 77 kPa in normal conditions. Conclusions. This is the first time that the risk of rupture of an aTAA is quantified in case of the combined effects of hypertension and aortic stiffness increase. Our findings suggest that a stiffer TAA may have the most altered distribution of wall stress and an acute change of peripheral vascular resistance could significantly increase the risk of rupture for a stiffer aneurysm.", "prompt": "Purpose. It has been reported clinically that rupture or dissections in thoracic aortic aneurysms (TAA) often occur due to hypertension which may be modelled with sudden increase of peripheral resistance, inducing acute changes of blood volumes in the aorta. There is clinical evidence that more compliant aneurys", "orig": "ms are less prone to rupture as they can sustain such changes of volume. The aim of the current paper is to verify this paradigm by evaluating computationally the role played by the variation of peripheral resistance and the impact of aortic stiffness onto peak wall stress in ascending TAA. Methods. Fluid-Structure Interaction (FSI", "generated": "Purpose. It has been reported clinically that rupture or dissections in thoracic aortic aneurysms (TAA) often occur due to hypertension which may be modelled with sudden increase of peripheral resistance, inducing acute changes of blood volumes in the aorta. There is clinical evidence that more compliant aneurys"}
{"full_text": "Genomic complexity is a growing field of evolution, with case studies for comparative evolutionary analyses in model and emerging non-model systems. Understanding complexity and the functional components of the genome is an untapped wealth of knowledge ripe for exploration. With the \"remarkable lack of correspondence\" between genome size and complexity, there needs to be a way to quantify complexity across organisms. In this study we use a set of complexity metrics that allow for evaluation of changes in complexity using TranD. We ascertain if complexity is increasing or decreasing across transcriptomes and at what structural level, as complexity is varied. We define three metrics -- TpG, EpT, and EpG in this study to quantify the complexity of the transcriptome that encapsulate the dynamics of alternative splicing. Here we compare complexity metrics across 1) whole genome annotations, 2) a filtered subset of orthologs, and 3) novel genes to elucidate the impacts of ortholog and novel genes in transcriptome analysis. We also derive a metric from Hong et al., 2006, Effective Exon Number (EEN), to compare the distribution of exon sizes within transcripts against random expectations of uniform exon placement. EEN accounts for differences in exon size, which is important because novel genes differences in complexity for orthologs and whole transcriptome analyses are biased towards low complexity genes with few exons and few alternative transcripts. With our metric analyses, we are able to implement changes in complexity across diverse lineages with greater precision and accuracy than previous cross-species comparisons under ortholog conditioning. These analyses represent a step forward toward whole transcriptome analysis in the emerging field of non-model evolutionary genomics, with key insights for evolutionary inference of complexity changes on deep timescales across the tree of life. We suggest a means to quantify biases generated in ortholog calling and correct complexity analysis for lineage-specific effects. With these metrics, we directly assay the quantitative properties of newly formed lineage-specific genes as they lower complexity in transcriptomes.", "prompt": "Genomic complexity is a growing field of evolution, with case studies for comparative evolutionary analyses in model and emerging non-model systems. Understanding complexity and the functional components of the genome is an untapped wealth of knowledge ripe for exploration. With the \"remarkable lack of correspondence\" between genome size and complexity, there needs to", "orig": "be a way to quantify complexity across organisms. In this study we use a set of complexity metrics that allow for evaluation of changes in complexity using TranD. We ascertain if complexity is increasing or decreasing across transcriptomes and at what structural level, as complexity is varied. We define three metrics -- TpG, EpT", "generated": "Genomic complexity is a growing field of evolution, with case studies for comparative evolutionary analyses in model and emerging non-model systems. Understanding complexity and the functional components of the genome is an untapped wealth of knowledge ripe for exploration. With the \"remarkable lack of correspondence\" between genome size and complexity, there needs to"}
{"full_text": "This paper demonstrates the usefulness of distributed local verification of proofs, as a tool for the design of self-stabilizing algorithms.In particular, it introduces a somewhat generalized notion of distributed local proofs, and utilizes it for improving the time complexity significantly, while maintaining space optimality. As a result, we show that optimizing the memory size carries at most a small cost in terms of time, in the context of Minimum Spanning Tree (MST). That is, we present algorithms that are both time and space efficient for both constructing an MST and for verifying it.This involves several parts that may be considered contributions in themselves.First, we generalize the notion of local proofs, trading off the time complexity for memory efficiency. This adds a dimension to the study of distributed local proofs, which has been gaining attention recently. Specifically, we design a (self-stabilizing) proof labeling scheme which is memory optimal (i.e., $O(\\log n)$ bits per node), and whose time complexity is $O(\\log ^2 n)$ in synchronous networks, or $O(\\Delta \\log ^3 n)$ time in asynchronous ones, where $\\Delta$ is the maximum degree of nodes. This answers an open problem posed by Awerbuch and Varghese (FOCS 1991). We also show that $\\Omega(\\log n)$ time is necessary, even in synchronous networks. Another property is that if $f$ faults occurred, then, within the requireddetection time above, they are detected by some node in the $O(f\\log n)$ locality of each of the faults.Second, we show how to enhance a known transformer that makes input/output algorithms self-stabilizing. It now takes as input an efficient construction algorithm and an efficient self-stabilizing proof labeling scheme, and produces an efficient self-stabilizing algorithm. When used for MST, the transformer produces a memory optimal self-stabilizing algorithm, whose time complexity, namely, $O(n)$, is significantly better even than that of previous algorithms. (The time complexity of previous MST algorithms that used $\\Omega(\\log^2 n)$ memory bits per node was $O(n^2)$, and the time for optimal space algorithms was $O(n|E|)$.) Inherited from our proof labelling scheme, our self-stabilising MST construction algorithm also has the following two properties: (1) if faults occur after the construction ended, then they are detected by some nodes within $O(\\log ^2 n)$ time in synchronous networks, or within $O(\\Delta \\log ^3 n)$ time in asynchronous ones, and (2) if $f$ faults occurred, then, within the required detection time above, they are detected within the $O(f\\log n)$ locality of each of the faults. We also show how to improve the above two properties, at the expense of some increase in the memory.", "prompt": "This paper demonstrates the usefulness of distributed local verification of proofs, as a tool for the design of self-stabilizing algorithms.In particular, it introduces a somewhat generalized notion of distributed local proofs, and utilizes it for improving the time complexity significantly, while maintaining space optimality. As a result, we show that optimizing the", "orig": "memory size carries at most a small cost in terms of time, in the context of Minimum Spanning Tree (MST). That is, we present algorithms that are both time and space efficient for both constructing an MST and for verifying it.This involves several parts that may be considered contributions in themselves.First, we generalize the notion", "generated": "This paper demonstrates the usefulness of distributed local verification of proofs, as a tool for the design of self-stabilizing algorithms. In particular, it introduces a somewhat generalized notion of distributed local proofs, and utilizes it for improving the time complexity significantly, while maintaining space optimality. As a result, we show that optimizing"}
{"full_text": "Context. Circumstellar disks are known to contain a significant mass in dust ranging from micron to centimeter size. Meteorites are evidence that individual grains of those sizes were collected and assembled into planetesimals in the young solar system. Aims. We assess the efficiency of dust collection of a swarm of non-drifting planetesimals {\\rev with radii ranging from 1 to $10^3$\\,km and beyond. Methods. We calculate the collision probability of dust drifting in the disk due to gas drag by planetesimal accounting for several regimes depending on the size of the planetesimal, dust, and orbital distance: the geometric, Safronov, settling, and three-body regimes. We also include a hydrodynamical regime to account for the fact that small grains tend to be carried by the gas flow around planetesimals. Results. We provide expressions for the collision probability of dust by planetesimals and for the filtering efficiency by a swarm of planetesimals. For standard turbulence conditions (i.e., a turbulence parameter $\\alpha=10^{-2}$), filtering is found to be inefficient, meaning that when crossing a minimum-mass solar nebula (MMSN) belt of planetesimals extending between 0.1 AU and 35 AU most dust particles are eventually accreted by the central star rather than colliding with planetesimals. However, if the disk is weakly turbulent ($\\alpha=10^{-4}$) filtering becomes efficient in two regimes: (i) when planetesimals are all smaller than about 10 km in size, in which case collisions mostly take place in the geometric regime; and (ii) when planetary embryos larger than about 1000 km in size dominate the distribution, have a scale height smaller than one tenth of the gas scale height, and dust is of millimeter size or larger in which case most collisions take place in the settling regime. These two regimes have very different properties: we find that the local filtering efficiency $x_{filter,MMSN}$ scales with $r^{-7/4}$ (where $r$ is the orbital distance) in the geometric regime, but with $r^{-1/4}$ to $r^{1/4}$ in the settling regime. This implies that the filtering of dust by small planetesimals should occur close to the central star and with a short spread in orbital distances. On the other hand, the filtering by embryos in the settling regime is expected to be more gradual and determined by the extent of the disk of embryos. Dust particles much smaller than millimeter size tend only to be captured by the smallest planetesimals because they otherwise move on gas streamlines and their collisions take place in the hydrodynamical regime. Conclusions. Our results hint at an inside-out formation of planetesimals in the infant solar system because small planetesimals in the geometrical limit can filter dust much more efficiently close to the central star. However, even a fully-formed belt of planetesimals such as the MMSN only marginally captures inward-drifting dust and this seems to imply that dust in the protosolar disk has been filtered by planetesimals even smaller than 1 km (not included in this study) or that it has been assembled into planetesimals by other mechanisms (e.g., orderly growth, capture into vortexes). Further refinement of our work concerns, among other things: a quantitative description of the transition region between the hydro and settling regimes; an assessment of the role of disk turbulence for collisions, in particular in the hydro regime; and the coupling of our model to a planetesimal formation model.", "prompt": "Context. Circumstellar disks are known to contain a significant mass in dust ranging from micron to centimeter size. Meteorites are evidence that individual grains of those sizes were collected and assembled into planetesimals in the young solar system. Aims. We assess the efficiency of dust collection of a swarm of non-dr", "orig": "ifting planetesimals {\\rev with radii ranging from 1 to $10^3$\\,km and beyond. Methods. We calculate the collision probability of dust drifting in the disk due to gas drag by planetesimal accounting for several regimes depending on the size of the planetesimal, dust, and orbital distance: the", "generated": "Context. Circumstellar disks are known to contain a significant mass in dust ranging from micron to centimeter size. Meteorites are evidence that individual grains of those sizes were collected and assembled into planetesimals in the young solar system. Aims. We assess the efficiency of dust collection of a swarm of non-"}
{"full_text": "We consider here one-parameter semigroups ${\\bf T}=(T(t))_{t>0}$ of bounded operators on a Banach space $X$ which are weakly continuous in the sense of Arveson. For such a semigroup ${\\bf T}$ denote by ${\\mathcal M}_{\\omega_{\\bf T}}$ the convolution algebra consisting in those measures $\\mu$ on $(0,+\\infty)$ such that $\\int_0^{+\\infty}\\Vert T(t)\\Vert d\\vert \\mu \\vert (t)<+\\infty.$ The Pettis integral $\\int_0^{+\\infty}T(t)d\\mu(t)$ defines for $\\mu \\in {\\mathcal M}_{\\omega_{\\bf T}}$ a bounded operator $\\phi_{\\bf T}(\\mu)$ on $X.$ Identifying the space $L^1_{\\omega_{\\bf T}}$ of (classes of) measurable functions $f$ satisfying $\\int_0^{+\\infty}\\vert f(t)\\Vert T(t)\\Vert dt< +\\infty$ to a closed subspace ${\\mathcal M}_{\\omega_{\\bf T}}$ in the usual way, we define the Arveson ideal $\\mathcal{I}_{\\bf T}$ of the semigroup to be the closure in ${\\mathcal B}(X)$ of $\\phi_{\\bf T}(L^1_{\\omega_{\\bf T}}).$ Using a variant of a procedure introduced a long time ago by the author we introduce a dense ideal $\\mathcal{U}_{\\bf T}$ of $\\mathcal{I}_{\\bf T},$ which is a Banach algebra with respect to a suitable norm $\\Vert .\\Vert_{\\mathcal{U}_{\\bf T}},$ such that $\\lim \\sup_{t\\to 0^+}\\Vert T(t)\\Vert_{{\\mathcal B}(\\mathcal{U}_{\\bf T})}<+\\infty.$ The normalized Arveson ideal $\\mathcal{J}_{\\bf T}$ is the closure of $\\mathcal{I}_{\\bf T}$ in ${\\mathcal B}(\\mathcal{U}_{\\bf T}).$ The Banach algebra $\\mathcal{J}_{\\bf T}$ has a sequential approximate identity and is isometrically isomorphic to a closed ideal of its multiplier algebra ${\\mathcal M}(\\mathcal{J}_{\\bf T}).$ The Banach algebras $\\mathcal{U}_{\\bf T},$ $\\mathcal{I}_{\\bf T}$ and $\\mathcal{J}_{\\bf T}$ are \"similar\", and the map $S_{u/v}\\to S_{au/av}$ defines when $a$ generates a dense principal ideal of $\\mathcal{U}_{\\bf T}$ a pseudo bounded isomorphism from the algebre $\\mathcal{QM}(\\mathcal{J}_{\\bf T})$ of quasimultipliers on $\\mathcal{J}_{\\bf T}$ onto the quasimultipliers algebras $\\mathcal{QM}(\\mathcal{U}_{\\bf T})$ and $\\mathcal{QM}(\\mathcal{I}_{\\bf T}).$ We define the generator $A_{\\bf T}$ of the semigroup $\\bf T$ to be a quasimultiplier on $\\mathcal{I}_{\\bf T},$ or ,equivalently, on $\\mathcal{J}_{\\bf T}.$ Every character $\\chi$ on $\\mathcal{I}_{\\bf T}$ has an extension $\\tilde \\chi$ to $\\mathcal{QM}(\\mathcal{I}_{\\bf T}).$ Let $Res_{ar} (A_{\\bf T})$ be the complement of the set $\\{ \\tilde \\chi (A_{\\bf T})\\}_{\\chi \\in \\widehat{\\mathcal{I}_{\\bf T}}}.$ The quasimultiplier $A-\\mu I$ has an inverse belonging to $\\mathcal{J}_{\\bf T}$ for $\\mu \\in Res_{ar} (A_{\\bf T}),$ which allows to consider this inverse as a \"regular\" quasimultiplier on the Arveson ideal $\\mathcal{I}_{\\bf T}.$ The usual resolvent formula holds in this context for $Re(\\mu)>\\lim_{t\\to +\\infty}{log \\Vert T(t)\\Vert\\over t}.$ Set $\\Pi_{\\alpha}^+:=\\{ z \\in \\mathbb{C} \\ | \\ Re(z) >\\alpha\\}.$ We revisit the functional calculus associated to the generator $A_{\\bf T}$ by defining $F(-A_{\\bf T})\\in \\mathcal{J}_{\\bf T}$ by a Cauchy integral when $F$ belongs to the Hardy space $H^1(\\Pi_{\\alpha}^+)$ for some $\\alpha < -\\lim_{t\\to +\\infty} {log\\Vert T(t)\\vert\\over t}.$ We then define $F(-A_{\\bf T})$ as a quasimultiplier on $\\mathcal{J}_{\\bf T}$ and $\\mathcal{I}_{\\bf T}$ when $F$ belongs to the Smirnov class on $\\Pi_{\\alpha}^+,$ and $F(-A_{\\bf T})$ is a regular quasimultiplier on $\\mathcal{J}_{\\bf T}$ and $\\mathcal{I}_{\\bf T}$ if $F$ is bounded on $\\Pi_{\\alpha}^+.$ If $F(z)=e^{-zt}$ for some $t>0,$ then $F(-A_{\\bf T})=T(t),$ and if $F(z)=-z,$ we indeed have $F(-A_{\\bf T})=A_{\\bf T}.$", "prompt": "We consider here one-parameter semigroups ${\\bf T}=(T(t))_{t>0}$ of bounded operators on a Banach space $X$ which are weakly continuous in the sense of Arveson. For such a semigroup ${\\bf T}$ denote by ${\\mathcal M", "orig": "}_{\\omega_{\\bf T}}$ the convolution algebra consisting in those measures $\\mu$ on $(0,+\\infty)$ such that $\\int_0^{+\\infty}\\Vert T(t)\\Vert d\\vert \\mu \\vert (t)<+\\infty.$ The Pettis integral $\\int_", "generated": "We consider here one-parameter semigroups ${\\bf T}=(T(t))_{t>0}$ of bounded operators on a Banach space $X$ which are weakly continuous in the sense of Arveson. For such a semigroup ${\\bf T}$ denote by ${\\mathcal M"}
{"full_text": "A Boolean logic driven Markov process (BDMP) is a dependability analysis model that defines a continuous-time Markov chain (CTMC). This formalism has high expressive power, yet it remains readable because its graphical representation stays close to standard fault trees. The size of a BDMP is roughly speaking proportional to the size of the system it models, whereas the size of the CTMC specified by this BDMP suffers from exponential growth. Thus quantifying large BDMPs can be a challenging task. The most general method to quantify them is Monte Carlo simulation, but this may be intractable for highly reliable systems. On the other hand, some subcategories of BDMPs can be processed with much more efficient methods. For example, BDMPs without repairs can be translated into dynamic fault trees, a formalism accepted as an input of the STORM model checker, that performs numerical calculations on sparse matrices, or they can be processed with the tool FIGSEQ that explores paths going to a failure state and calculates their probabilities. BDMPs with repairs can be quantified by FIGSEQ (BDMPs capturing quickly and completely repairable behaviors are solved by a different algorithm), and by the I&AB (Initiator and All Barriers) method, recently published and implemented in a prototype version of RISKSPECTRUM PSA. This tool, based exclusively on Boolean representations looks for and quantifies minimal cut sets of the system, i.e., minimal combinations of component failures that induce the loss of the system. This allows a quick quantification of large models with repairable components, standby redundancies and some other types of dependencies between omponents. All these quantification methods have been tried on a benchmark whose definition was published at the MARS 2017 workshop: the model of emergency power supplies of a nuclear power plant. In this paper, after a recall of the theoretical principles of the various quantification methods, we compare their performances on that benchmark.", "prompt": "A Boolean logic driven Markov process (BDMP) is a dependability analysis model that defines a continuous-time Markov chain (CTMC). This formalism has high expressive power, yet it remains readable because its graphical representation stays close to standard fault trees. The size of a BDMP is roughly speaking proportional to", "orig": "the size of the system it models, whereas the size of the CTMC specified by this BDMP suffers from exponential growth. Thus quantifying large BDMPs can be a challenging task. The most general method to quantify them is Monte Carlo simulation, but this may be intractable for highly reliable systems. On the", "generated": "A Boolean logic driven Markov process (BDMP) is a dependability analysis model that defines a continuous-time Markov chain (CTMC). This formalism has high expressive power, yet it remains readable because its graphical representation stays close to standard fault trees. The size of a BDMP is roughly speaking proportional to"}
{"full_text": "SUMMARY Geophysical imaging using the inversion procedure is a powerful tool for the exploration of the Earth's subsurface. However, the interpretation of inverted images can sometimes be difficult, due to the inherent limitations of existing inversion algorithms, which produce smoothed sections. In order to improve and automate the processing and interpretation of inverted geophysical models, we propose an approach inspired from data mining. We selected an algorithm known as DBSCAN (Density-Based Spatial Clustering of Applications with Noise) to perform clustering of inverted geophysical sections. The methodology relies on the automatic sorting and clustering of data. DBSCAN detects clusters in the inverted electrical resistivity values, with no prior knowledge of the number of clusters. This algorithm has the advantage of being defined by only two parameters: the neighbourhood of a point in the data space, and the minimum number of data points in this neighbourhood. We propose an objective procedure for the determination of these two parameters. The proof of concept described here is applied to simulated ERT (electrical resistivity tomography) sections, for the following three cases: two layers with a step, two layers with a rebound, and two layers with an anomaly embedded in the upper layer. To validate this approach, sensitivity studies were carried out on both of the above parameters, as well as to assess the influence of noise on the algorithm's performance. Finally, this methodology was tested on real field data. DBSCAN detects clusters in the inverted electrical resistivity models, and the former are then associated with various types of earth materials, thus allowing the structure of the prospected area to be determined. The proposed data-mining algorithm is shown to be effective, and to improve the interpretation of the inverted ERT sections. This new approach has considerable potential, as it can be applied to any geophysical data represented in the form of sections or maps.", "prompt": "SUMMARY Geophysical imaging using the inversion procedure is a powerful tool for the exploration of the Earth's subsurface. However, the interpretation of inverted images can sometimes be difficult, due to the inherent limitations of existing inversion algorithms, which produce smoothed sections. In order to improve and automate the processing and interpretation of inverted ge", "orig": "ophysical models, we propose an approach inspired from data mining. We selected an algorithm known as DBSCAN (Density-Based Spatial Clustering of Applications with Noise) to perform clustering of inverted geophysical sections. The methodology relies on the automatic sorting and clustering of data. DBSCAN detects clusters in the inverted electrical resistivity values,", "generated": "Geophysical imaging using the inversion procedure is a powerful tool for the exploration of the Earth's subsurface. However, the interpretation of inverted images can sometimes be difficult, due to the inherent limitations of existing inversion algorithms, which produce smoothed sections. In order to improve and automate the processing and interpretation of inverted geological models,"}
{"full_text": "The epitaxial growth of anatase (001) films deposited by pulsed laser deposition (PLD) and molecular beam epitaxy (MBE) on SrTiO3 (001) (STO) single crystals has been studied using X-ray diffraction and surface sensitivity UHV techniques. The evolution of the strain represented by the microstrain and the change of the in-plane and out-of-plane lattice parameters with film growth temperature, the effect of the annealing temperature and the influence of the oxygen content of the film have been investigated.The out-of-plane lattice strain shows a compressive (-0.2%) or expansive (+0.3%) behavior, in the range 600-900$^\\circ$C, for temperatures below or above 700$^\\circ$C, respectively. The in-plane lattice parameters, as well as the cell volume of the film, remain under compression over the entire temperature range explored.PLD films grow into square islands that align with the surface lattice directions of the STO substrate. The maximum size of these islands is reached at growth temperatures close to 875-925$^\\circ$C. Film annealing at temperatures of 800$^\\circ$C or higher melts the islands into flat terraces. Larger terraces are reached at high annealing temperatures of 925$^\\circ$C for extended periods of 12 hours. This procedure allows flat surface terrace sizes of up to 650 nm to be achieved.The crystalline quality achieved in anatase films prepared by PLD or MBE growth methods is similar. The two-step anatase growth process used during the synthesis of the films with both methods: film growth and post-annealing treatment in oxygen or air at ambient pressure, using temperature and time as key parameters, allows to control the surface terrace size and stoichiometry of the films, as well as the anatase/rutile intermixing rates at sufficiently high temperatures. This growth process could allow the substitution of their equivalent single crystals. The range of applicability of these films would include their use as structural and electronic model systems, or in harsh experimental conditions due to their low production cost.", "prompt": "The epitaxial growth of anatase (001) films deposited by pulsed laser deposition (PLD) and molecular beam epitaxy (MBE) on SrTiO3 (001) (STO) single crystals has been studied using X-ray diffraction and surface sensitivity UHV techniques. The evolution", "orig": "of the strain represented by the microstrain and the change of the in-plane and out-of-plane lattice parameters with film growth temperature, the effect of the annealing temperature and the influence of the oxygen content of the film have been investigated.The out-of-plane lattice strain shows a compressive (-0.2%) or expansive (+", "generated": "The epitaxial growth of anatase (001) films deposited by pulsed laser deposition (PLD) and molecular beam epitaxy (MBE) on SrTiO3 (001) (STO) single crystals has been studied using X-ray diffraction and surface sensitivity UHV techniques. The evolution"}
{"full_text": "Implementing circular economy (CE) principles is increasingly recommended as a convenient solution to meet the goals of sustainable development. New tools are required to support practitioners, decision-makers and policy-makers towards more CE practices, as well as to monitor the effects of CE adoption. Worldwide, academics, industrialists and politicians all agree on the need to use CE-related measuring instruments to manage this transition at different systemic levels. In this context, a wide range of circularity indicators (C-indicators) has been developed in recent years. Yet, as there is not one single definition of the CE concept, it is of the utmost importance to know what the available indicators measure in order to use them properly. Indeed, through a systematic literature review-considering both academic and grey literature-55 sets of C-indicators, developed by scholars, consulting companies and governmental agencies, have been identified, encompassing different purposes, scopes, and potential usages. Inspired by existing taxonomies of eco-design tools and sustainability indicators, and in line with the CE characteristics, a classification of indicators aiming to assess, improve, monitor and communicate on the CE performance is proposed and discussed. In the developed taxonomy including 10 categories, C-indicators are differentiated regarding criteria such as the levels of CE implementation (e.g. micro, meso, macro), the CE loops (maintain, reuse, remanufacture, recycle), the performance (intrinsic, impacts), the perspective of circularity (actual, potential) they are taking into account, or their degree of transversality (generic, sector-specific). In addition, the database inventorying the 55 sets of C-indicators is linked to an Excel-based query tool to facilitate the selection of appropriate indicators according to the specific user's needs and requirements. This study enriches the literature by giving a first need-driven taxonomy of C-indicators, which is experienced on several use cases. It provides a synthesis and clarification to the emerging and must-needed research theme of C-indicators, and sheds some light on remaining key challenges like their effective uptake by industry. Eventually, limitations, improvement areas, as well as implications of the proposed taxonomy are intently addressed to guide future research on C-indicators and CE implementation.", "prompt": "Implementing circular economy (CE) principles is increasingly recommended as a convenient solution to meet the goals of sustainable development. New tools are required to support practitioners, decision-makers and policy-makers towards more CE practices, as well as to monitor the effects of CE adoption. Worldwide, academics, industrialists and politicians all agree on", "orig": "the need to use CE-related measuring instruments to manage this transition at different systemic levels. In this context, a wide range of circularity indicators (C-indicators) has been developed in recent years. Yet, as there is not one single definition of the CE concept, it is of the utmost importance to know what the", "generated": "Implementing circular economy (CE) principles is increasingly recommended as a convenient solution to meet the goals of sustainable development. New tools are required to support practitioners, decision-makers and policy-makers towards more CE practices, as well as to monitor the effects of CE adoption. Worldwide, academics, industrialists and politicians all agree on"}
{"full_text": "Most seismic risk assessment methods focus on estimating the damages to the built environment and the consequent socioeconomic losses without fully taking into account the social aspect of risk. Yet, human behaviour is a key element in predicting the human impact of an earthquake, therefore, it is important to include it in quantitative risk assessment studies. In this study, an interdisciplinary approach simulating pedestrians' evacuation during earthquakes at the city scale is developed using an agent-based model. The model integrates the seismic hazard, the physical vulnerability as well as individuals' behaviours and mobility. The simulator is applied to the case of Beirut, Lebanon. Lebanon is at the heart of the Levant fault system that has generated several Mw>7 earthquakes, the latest being in 1759. It is one of the countries with the highest seismic risk in the Mediterranean region. This is due to the high seismic vulnerability of the buildings due to the absence of mandatory seismic regulation until 2012, the high level of urbanization, and the lack of adequate spatial planning and risk prevention policies. Beirut as the main residential, economic and institutional hub of Lebanon is densely populated. To accommodate the growing need for urban development, constructions have almost taken over all of the green areas of the city; squares and gardens are disappearing to give place to skyscrapers. However, open spaces are safe places to shelter, away from debris, and therefore play an essential role in earthquake evacuation. Despite the massive urbanization, there are a few open spaces but locked gates and other types of anthropogenic barriers often limit their access. To simulate this complex context, pedestrians' evacuation simulations are run in a highly realistic spatial environment implemented in GAMA [1]. Previous data concerning soil and buildings in Beirut [2, 3] are complemented by new geographic data extracted from high-resolution Pleiades satellite images. The seismic loading is defined as a peak ground acceleration of 0.3g, as stated in Lebanese seismic regulations. Building damages are estimated using an artificial neural network trained to predict the mean damage [4] based on the seismic loading as well as the soil and building vibrational properties [5]. Moreover, the quantity and the footprint of the generated debris around each building are also estimated and included in the model. We simulate how topography, buildings, debris, and access to open spaces, affect individuals' mobility. Two city configurations are implemented: 1. Open spaces are accessible without any barriers; 2. Access to some open spaces is blocked. The first simulation results show that while 52% of the population is able to arrive to an open space within 5 minutes after an earthquake, this number is reduced to 39% when one of the open spaces is locked. These results show that the presence of accessible open spaces in a city and their proximity to the residential buildings is a crucial factor for ensuring people's safety when an earthquake occurs.", "prompt": "Most seismic risk assessment methods focus on estimating the damages to the built environment and the consequent socioeconomic losses without fully taking into account the social aspect of risk. Yet, human behaviour is a key element in predicting the human impact of an earthquake, therefore, it is important to include it in quantitative risk assessment studies. In", "orig": "this study, an interdisciplinary approach simulating pedestrians' evacuation during earthquakes at the city scale is developed using an agent-based model. The model integrates the seismic hazard, the physical vulnerability as well as individuals' behaviours and mobility. The simulator is applied to the case of Beirut, Lebanon. Lebanon is at the heart of the Lev", "generated": "Most seismic risk assessment methods focus on estimating the damages to the built environment and the consequent socioeconomic losses without fully taking into account the social aspect of risk. Yet, human behaviour is a key element in predicting the human impact of an earthquake, therefore, it is important to include it in quantitative risk assessment studies. I"}
{"full_text": "Disaggregated memory is promising for improving memory utilization in computer clusters in which memory demands significantly vary across computer nodes under utilization. It allows applications with high memory demands to use memory in other computer nodes. However, disaggregated memory is not easy to use for implementing data structures in C++ because the C++ standard does not provide an adequate abstraction to use it efficiently in a high-level, modular manner. Because accessing remote memory involves high latency, disaggregated memory is often used as a far-memory system, which forms a kind of swap memory where part of local memory is used as a cache area, while the remaining memory is not subject to swapping. To pursue performance, programmers have to be aware of this nonuniform memory view and place data appropriately to minimize swapping. In this work, we model the address space of memory-disaggregated systems as the far-memory model, present the collective allocator abstraction, which enables us to specify object placement aware of memory address subspaces, and apply it to programming aware of the far-memory model. The far-memory model provides a view of the nonuniform memory space while hiding the details. In the model, the virtual address space is divided into two subspaces; one is subject to swapping and the other is not. The swapping subspace is further divided into even-sized pages, which are units of swapping. The collective allocator abstraction forms an allocator as a collection of sub-allocators, each of which owns a distinct subspace, where every allocation is done via sub-allocators. It enables us to control object placement at allocation time by selecting an appropriate sub-allocator according to different criteria, such as subspace characteristics and object collocation. It greatly facilitates implementing container data structures aware of the far-memory model. We develop an allocator based on the collective allocator abstraction by extending the C++ standard allocator for container data structures on the far-memory model and experimentally demonstrate that it facilitates implementing containers equipped with object placement strategies aware of spatial locality under the far-memory model in a high-level, modular manner. More specifically, we have successfully implemented B-trees and skip lists with the combined use of two placement strategies. The modifications therein for the original implementations are fairly modest: addition is mostly due to specifying object placement; deletion and modification are at most 1.2 % and 3.2 % of lines of the original code, respectively. We have experimentally confirmed that the modified implementations successfully have data layouts suppressing swapping. We forecast that the collective allocator abstraction would be a key to high-level integration with different memory hardware technologies because it straightforwardly accommodates new interfaces for subspaces.", "prompt": "Disaggregated memory is promising for improving memory utilization in computer clusters in which memory demands significantly vary across computer nodes under utilization. It allows applications with high memory demands to use memory in other computer nodes. However, disaggregated memory is not easy to use for implementing data structures in C++ because the C++ standard does", "orig": "not provide an adequate abstraction to use it efficiently in a high-level, modular manner. Because accessing remote memory involves high latency, disaggregated memory is often used as a far-memory system, which forms a kind of swap memory where part of local memory is used as a cache area, while the remaining memory is not subject to", "generated": "Disaggregated memory is promising for improving memory utilization in computer clusters in which memory demands significantly vary across computer nodes under utilization. It allows applications with high memory demands to use memory in other computer nodes. However, disaggregated memory is not easy to use for implementing data structures in C++ because the C++ standard does"}
{"full_text": "We review the importance of recent UV observations of solar system targets and discuss the need for further measurements, instrumentation and laboratory work in the coming decade. In the past decade, numerous important advances have been made in solar system science using ultraviolet (UV) spectroscopic techniques. Formerly used nearly exclusively for studies of giant planet atmospheres, planetary exospheres and cometary emissions, UV imaging spectroscopy has recently been more widely applied. The geyser-like plume at Saturn's moon Enceladus was discovered in part as a result of UV stellar occultation observations, and this technique was used to characterize the plume and jets during the entire Cassini mission. Evidence for a similar style of activity has been found at Jupiter's moon Europa using Hubble Space Telescope (HST) UV emission and absorption imaging. At other moons and small bodies throughout the solar system, UV spectroscopy has been utilized to search for activity, probe surface composition, and delineate space weathering effects; UV photometric studies have been used to uncover regolith structure. Insights from UV imaging spectroscopy of solar system surfaces have been gained largely in the last 1-2 decades, including studies of surface composition, space weathering effects (e.g. radiolytic products) and volatiles on asteroids (e.g. [2][39][48][76][84]), the Moon (e.g. [30][46][49]), comet nuclei (e.g. [85]) and icy satellites (e.g. [38][41-44][45][47][65]). The UV is sensitive to some species, minor contaminants and grain sizes often not detected in other spectral regimes. In the coming decade, HST observations will likely come to an end. New infrastructure to bolster future UV studies is critically needed. These needs include both developmental work to help improve future UV observations and laboratory work to help interpret spacecraft data. UV instrumentation will be a critical tool on missions to a variety of targets in the coming decade, especially for the rapidly expanding application of UV reflectance investigations of atmosphereless bodies.", "prompt": "We review the importance of recent UV observations of solar system targets and discuss the need for further measurements, instrumentation and laboratory work in the coming decade. In the past decade, numerous important advances have been made in solar system science using ultraviolet (UV) spectroscopic techniques. Formerly used nearly exclusively for studies of giant", "orig": "planet atmospheres, planetary exospheres and cometary emissions, UV imaging spectroscopy has recently been more widely applied. The geyser-like plume at Saturn's moon Enceladus was discovered in part as a result of UV stellar occultation observations, and this technique was used to characterize the plume and", "generated": "planets, such as the planets in our own solar system and their magnetospheres, recent years have seen significant increases in observations of smaller bodies like asteroids and comets. The UV spectral signatures from these small bodies have been instrumental in gaining a better understanding of their compositions, orbital properties and interaction with their surroundings."}
{"full_text": "The paper deals with the automatic analysis of real-life telephone conversations between an agent and a customer of a customer care service (ccs). The application domain is the public transportation system in Paris and the purpose is to collect statistics about customer problems in order to monitor the service and decide priorities on the intervention for improving user satisfaction. Of primary importance for the analysis is the detection of themes that are the object of customer problems. Themes are defined in the application requirements and are part of the application ontology that is implicit in the ccs documentation. Due to variety of customer population, the structure of conversations with an agent is unpredictable. A conversation may be about one or more themes. Theme mentions can be interleaved with mentions of facts that are irrelevant for the application purpose. Furthermore, in certain conversations theme mentions are localized in specific conversation segments while in other conversations mentions cannot be localized. As a consequence, approaches to feature extraction with and without mention localization are considered. Application domain relevant themes identified by an automatic procedure are expressed by specific sentences whose words are hypothesized by an automatic speech recognition (asr) system. The asr system is error prone. The word error rates can be very high for many reasons. Among them it is worth mentioning unpredictable background noise, speaker accent, and various types of speech disfluencies. As the application task requires the composition of proportions of theme mentions, a sequential decision strategy is introduced in this paper for performing a survey of the large amount of conversations made available in a given time period. The strategy has to sample the conversations to form a survey containing enough data analyzed with high accuracy so that proportions can be estimated with sufficient accuracy. Due to the unpredictable type of theme mentions, it is appropriate to consider methods for theme hypothesization based on global as well as local feature extraction. Two systems based on each type of feature extraction will be considered by the strategy. One of the four methods is novel. It is based on a new definition of density of theme mentions and on the localization of high density zones whose boundaries do not need to be precisely detected. The sequential decision strategy starts by grouping theme hypotheses into sets of different expected accuracy and coverage levels. For those sets for which accuracy can be improved with a consequent increase of coverage a new system with new features is introduced. Its execution is triggered only when specific preconditions are met on the hypotheses generated by the basic four systems. Experimental results are provided on a corpus collected in the call center of the Paris transportation system known as ratp. The results show that surveys with high accuracy and coverage can be composed with the proposed strategy and systems. This makes it possible to apply a previously published proportion estimation approach that takes into account hypothesization errors .", "prompt": "The paper deals with the automatic analysis of real-life telephone conversations between an agent and a customer of a customer care service (ccs). The application domain is the public transportation system in Paris and the purpose is to collect statistics about customer problems in order to monitor the service and decide priorities on the intervention for improving user satisfaction", "orig": ". Of primary importance for the analysis is the detection of themes that are the object of customer problems. Themes are defined in the application requirements and are part of the application ontology that is implicit in the ccs documentation. Due to variety of customer population, the structure of conversations with an agent is unpredictable. A conversation may be", "generated": "The paper deals with the automatic analysis of real-life telephone conversations between an agent and a customer of a customer care service (ccs). The application domain is the public transportation system in Paris and the purpose is to collect statistics about customer problems in order to monitor the service and decide priorities on the intervention for improving user satisfaction"}
{"full_text": "In this paper, we examine the price dynamics of on-line art auctions of modern Indian art using functional data analysis. The purpose here is not just to understand what determines the final prices of art objects, but also the price movement during the entire auction. We identify several factors, such as artist characteristics (established or emerging artist; prior sales history), art characteristics (size; painting medium--canvas or paper), competition characteristics (current number of bidders; current number of bids) and auction design characteristics (opening bid; position of the lot in the auction), that explain the dynamics of price movement in an on-line art auction. We find that the effects on price vary over the duration of the auction, with some of these effects being stronger at the beginning of the auction (such as the opening bid and historical prices realized). In some cases, the rate of change in prices (velocity) increases at the end of the auction (for canvas paintings and paintings by established artists). Our analysis suggests that the opening bid is positively related to on-line auction price levels of art at the beginning of the auction, but its effect declines toward the end of the auction. The order in which the lots appear in an art auction is negatively related to the current price level, with this relationship decreasing toward the end of the auction. This implies that lots that appear earlier have higher current prices during the early part of the auction, but that effect diminishes by the end of the auction. Established artists show a positive relationship with the price level at the beginning of the auction. Reputation or popularity of the artists and their investment potential as assessed by previous history of sales are positively related to the price levels at the beginning of the auction. The medium (canvas or paper) of the painting does not show any relationship with art auction price levels, but the size of the painting is negatively related to the current price during the early part of the auction. Important implications for auction design are drawn from the analysis.", "prompt": "In this paper, we examine the price dynamics of on-line art auctions of modern Indian art using functional data analysis. The purpose here is not just to understand what determines the final prices of art objects, but also the price movement during the entire auction. We identify several factors, such as artist characteristics (established or", "orig": "emerging artist; prior sales history), art characteristics (size; painting medium--canvas or paper), competition characteristics (current number of bidders; current number of bids) and auction design characteristics (opening bid; position of the lot in the auction), that explain the dynamics of price movement in an on-line art auction. We", "generated": "In this paper, we examine the price dynamics of on-line art auctions of modern Indian art using functional data analysis. The purpose here is not just to understand what determines the final prices of art objects, but also the price movement during the entire auction. We identify several factors, such as artist characteristics (established or"}
{"full_text": "In this paper, we introduce a new learning strategy based on a seminal idea of Mojirsheibani (1999, 2000, 2002a, 2002b), who proposed a smart method for combining several classifiers, relying on a consensus notion. In many aggregation methods, the prediction for a new observation x is computed by building a linear or convex combination over a collection of basic estimators r1(x),. .. , rm(x) previously calibrated using a training data set. Mojirsheibani proposes to compute the prediction associated to a new observation by combining selected outputs of the training examples. The output of a training example is selected if some kind of consensus is observed: the predictions computed for the training example with the different machines have to be \"similar\" to the prediction for the new observation. This approach has been recently extended to the context of regression in Biau et al. (2016). In the original scheme, the agreement condition is actually required to hold for all individual estimators, which appears inadequate if there is one bad initial estimator. In practice, a few disagreements are allowed ; for establishing the theoretical results, the proportion of estimators satisfying the condition is required to tend to 1. In this paper, we propose an alternative procedure, mixing the previous consensus ideas on the predictions with the Euclidean distance computed between entries. This may be seen as an alternative approach allowing to reduce the effect of a possibly bad estimator in the initial list, using a constraint on the inputs. We prove the consistency of our strategy in classification and in regression. We also provide some numerical experiments on simulated and real data to illustrate the benefits of this new aggregation method. On the whole, our practical study shows that our method may perform much better than the original combination technique, and, in particular, exhibit far less variance. We also show on simulated examples that this procedure mixing inputs and outputs is still robust to high dimensional inputs.", "prompt": "In this paper, we introduce a new learning strategy based on a seminal idea of Mojirsheibani (1999, 2000, 2002a, 2002b), who proposed a smart method for combining several classifiers, relying on a consensus notion. In many aggregation methods, the prediction for", "orig": "a new observation x is computed by building a linear or convex combination over a collection of basic estimators r1(x),..., rm(x) previously calibrated using a training data set. Mojirsheibani proposes to compute the prediction associated to a new observation by combining selected outputs of the training examples. The output", "generated": "In this paper, we introduce a new learning strategy based on a seminal idea of Mojirsheibani (1999, 2000, 2002a, 2002b), who proposed a smart method for combining several classifiers, relying on a consensus notion. In many aggregation methods, the prediction from"}
{"full_text": "Homozygous mutations in the progranulin gene (GRN) are associated with neuronal ceroid lipofuscinosis 11 (CLN11), a rare lysosomal-storage disorder characterized by cerebellar ataxia, seizures, retinitis pigmentosa, and cognitive disorders, usually beginning between 13 and 25 years of age. This is a rare condition, previously reported in only four families. In contrast, heterozygous GRN mutations are a major cause of frontotemporal dementia associated with neuronal cytoplasmic TDP-43 inclusions. We identified homozygous GRN mutations in six new patients. The phenotypic spectrum is much broader than previously reported, with two remarkably distinct presentations, depending on the age of onset. A childhood/juvenile form is characterized by classical CLN11 symptoms at an early age at onset. Unexpectedly, other homozygous patients presented a distinct delayed phenotype of frontotemporal dementia and parkinsonism after 50 years; none had epilepsy or cerebellar ataxia. Another major finding of this study is that all GRN mutations may not have the same impact on progranulin protein synthesis. A hypomorphic effect of some mutations is supported by the presence of residual levels of plasma progranulin and low levels of normal transcript detected in one case with a homozygous splice-site mutation and late onset frontotemporal dementia. This is a new critical finding that must be considered in therapeutic trials based on replacement strategies. The first neuropathological study in a homozygous carrier provides new insights into the pathological mechanisms of the disease. Hallmarks of neuronal ceroid lipofuscinosis were present. The absence of TDP-43 cytoplasmic inclusions markedly differs from observations of heterozygous mutations, suggesting a pathological shift between lysosomal and TDP-43 pathologies depending on the mono or bi-allelic status. An intriguing observation was the loss of normal TDP-43 staining in the nucleus of some neurons, which could be the first stage of the TDP-43 pathological process preceding the formation of typical cytoplasmic inclusions. Finally, this study has important implications for genetic counselling and molecular diagnosis. Semi-dominant inheritance of GRN mutations implies that specific genetic counseling should be delivered to children and parents of CLN11 patients, as they are heterozygous carriers with a high risk of developing dementia. More broadly, this study illustrates the fact that genetic variants can lead to different phenotypes according to their mono- or bi-allelic state, which is a challenge for genetic diagnosis.", "prompt": "Homozygous mutations in the progranulin gene (GRN) are associated with neuronal ceroid lipofuscinosis 11 (CLN11), a rare lysosomal-storage disorder characterized by cerebellar ataxia, seizures, retinitis pigmentosa, and cognitive disorders, usually beginning", "orig": "between 13 and 25 years of age. This is a rare condition, previously reported in only four families. In contrast, heterozygous GRN mutations are a major cause of frontotemporal dementia associated with neuronal cytoplasmic TDP-43 inclusions. We identified homozygous GR", "generated": "Homozygous mutations in the progranulin gene (GRN) are associated with neuronal ceroid lipofuscinosis 11 (CLN11), a rare lysosomal-storage disorder characterized by cerebellar ataxia, seizures, retinitis pigmentosa, and cognitive disorders, usually beginning"}
{"full_text": "During the summer of 1989, the Forecast Systems Laboratory of the National Oceanic and Atmospheric Administration sponsored an evaluation of artificial intelligence-based systems that forecast severe convective storms. The evaluation experiment, called Shootout-89, took place in Boulder, and focussed on storms over the northeastern Colorado foothills and plains (Moninger, et al., 1990). Six systems participated in Shootout-89. These included traditional expert systems, an analogy-based system, and a system developed using methods from the cognitive science/judgment analysis tradition. Each day of the exercise, the systems generated 2 to 9 hour forecasts of the probabilities of occurrence of: non significant weather, significant weather, and severe weather, in each of four regions in northeastern Colorado. A verification coordinator working at the Denver Weather Service Forecast Office gathered ground-truth data from a network of observers. Systems were evaluated on the basis of several measures of forecast skill, and on other metrics such as timeliness, ease of learning, and ease of use. Systems were generally easy to operate, however the various systems required substantially different levels of meteorological expertise on the part of their users--reflecting the various operational environments for which the systems had been designed. Systems varied in their statistical behavior, but on this difficult forecast problem, the systems generally showed a skill approximately equal to that of persistence forecasts and climatological (historical frequency) forecasts. The two systems that appeared best able to discriminate significant from non significant weather events were traditional expert systems. Both of these systems required the operator to make relatively sophisticated meteorological judgments. We are unable, based on only one summer's worth of data, to determine the extent to which the greater skill of the two systems was due to the content of their knowledge bases, or to the subjective judgments of the operator. A follow-on experiment, Shootout-91, is currently being planned. Interested potential participants are encouraged to contact the author at the address above.", "prompt": "During the summer of 1989, the Forecast Systems Laboratory of the National Oceanic and Atmospheric Administration sponsored an evaluation of artificial intelligence-based systems that forecast severe convective storms. The evaluation experiment, called Shootout-89, took place in Boulder, and focussed on storms over the northeastern Colorado foothills and plains", "orig": "(Moninger, et al., 1990). Six systems participated in Shootout-89. These included traditional expert systems, an analogy-based system, and a system developed using methods from the cognitive science/judgment analysis tradition. Each day of the exercise, the systems generated 2 to 9 hour forecasts of", "generated": "During the summer of 1989, the Forecast Systems Laboratory of the National Oceanic and Atmospheric Administration sponsored an evaluation of artificial intelligence-based systems that forecast severe convective storms. The evaluation experiment, called Shootout-89, took place in Boulder, and focused on storms over the northeastern Colorado foothills and plains."}
{"full_text": "We study the molecular correlations in a lattice model of a solution of a low-solubility solute, with emphasis on how the thermodynamics is reflected in the correlation functions. The model is treated in Bethe-Guggenheim approximation, which is exact on a Bethe lattice (Cayley tree). The solution properties are obtained in the limit of infinite dilution of the solute. With $h_{11}(r)$, $h_{12}(r)$, and $h_{22}(r)$ the three pair correlation functions as functions of the separation $r$ (subscripts 1 and 2 referring to solvent and solute, respectively), we find for $r \\geq 2$ lattice steps that $h_{22}(r)/h_{12}(r) \\equiv h_{12}(r)/h_{11}(r)$. This illustrates a general theorem that holds in the asymptotic limit of infinite $r$. The three correlation functions share a common exponential decay length (correlation length), but when the solubility of the solute is low the amplitude of the decay of $h_{22}(r)$ is much greater than that of $h_{12}(r)$, which in turn is much greater than that of $h_{11}(r)$. As a consequence the amplitude of the decay of $h_{22}(r)$ is enormously greater than that of $h_{11}(r)$. The effective solute-solute attraction then remains discernible at distances at which the solvent molecules are essentially no longer correlated, as found in similar circumstances in an earlier model. The second osmotic virial coefficient is large and negative, as expected. We find that the solvent-mediated part $W(r)$ of the potential of mean force between solutes, evaluated at contact, $r=1$, is related in this model to the Gibbs free energy of solvation at fixed pressure, $\\Delta G_p^*$, by $(Z/2) W(1) + \\Delta G_p^* \\equiv p v_0$, where $Z$ is the coordination number of the lattice, $p$ the pressure, and $v_0$ the volume of the cell associated with each lattice site. A large, positive $\\Delta G_p^*$ associated with the low solubility is thus reflected in a strong attraction (large negative $W$ at contact), which is the major contributor to the second osmotic virial coefficient. In this model, the low solubility (large positive $\\Delta G_p^*$) is due partly to an unfavorable enthalpy of solvation and partly to an unfavorable solvation entropy, unlike in the hydrophobic effect, where the enthalpy of solvation itself favors high solubility, but is overweighed by the unfavorable solvation entropy.", "prompt": "We study the molecular correlations in a lattice model of a solution of a low-solubility solute, with emphasis on how the thermodynamics is reflected in the correlation functions. The model is treated in Bethe-Guggenheim approximation, which is exact on a Bethe lattice (Cayley tree). The", "orig": "solution properties are obtained in the limit of infinite dilution of the solute. With $h_{11}(r)$, $h_{12}(r)$, and $h_{22}(r)$ the three pair correlation functions as functions of the separation $r$ (subscripts 1 and 2 referring to", "generated": "We study the molecular correlations in a lattice model of a solution of a low-solubility solute, with emphasis on how the thermodynamics is reflected in the correlation functions. The model is treated in Bethe-Guggenheim approximation, which is exact on a Bethe lattice (Cayley tree). This"}
{"full_text": "Bottleneck problems are an important class of optimization problems that have recently gained increasing attention in the domain of machine learning and information theory. They are widely used in generative models, fair machine learning algorithms, design of privacy-assuring mechanisms, and appear as information-theoretic performance bounds in various multi-user communication problems. In this work, we propose a general family of optimization problems, termed as complexity-leakage-utility bottleneck (CLUB) model, which (i) provides a unified theoretical framework that generalizes most of the state-of-the-art literature for the information-theoretic privacy models, (ii) establishes a new interpretation of the popular generative and discriminative models, (iii) constructs new insights to the generative compression models, and (iv) can be used in the fair generative models. We first formulate the CLUB model as a complexity-constrained privacy-utility optimization problem. We then connect it with the closely related bottleneck problems, namely information bottleneck (IB), privacy funnel (PF), deterministic IB (DIB), conditional entropy bottleneck (CEB), and conditional PF (CPF). We show that the CLUB model generalizes all these problems as well as most other information-theoretic privacy models. Then, we construct the deep variational CLUB (DVCLUB) models by employing neural networks to parameterize variational approximations of the associated information quantities. Building upon these information quantities, we present unified objectives of the supervised and unsupervised DVCLUB models. Leveraging the DVCLUB model in an unsupervised setup, we then connect it with state-of-the-art generative models, such as variational auto-encoders (VAEs), generative adversarial networks (GANs), as well as the Wasserstein GAN (WGAN), Wasserstein auto-encoder (WAE), and adversarial auto-encoder (AAE) models through the optimal transport (OT) problem. We then show that the DVCLUB model can also be used in fair representation learning problems, where the goal is to mitigate the undesired bias during the training phase of a machine learning model. We conduct extensive quantitative experiments on colored-MNIST and CelebA datasets, with a public implementation available, to evaluate and analyze the CLUB model.", "prompt": "Bottleneck problems are an important class of optimization problems that have recently gained increasing attention in the domain of machine learning and information theory. They are widely used in generative models, fair machine learning algorithms, design of privacy-assuring mechanisms, and appear as information-theoretic performance bounds in various multi-user communication problems", "orig": ". In this work, we propose a general family of optimization problems, termed as complexity-leakage-utility bottleneck (CLUB) model, which (i) provides a unified theoretical framework that generalizes most of the state-of-the-art literature for the information-theoretic privacy models, (ii) establishes a new", "generated": "Bottleneck problems are an important class of optimization problems that have recently gained increasing attention in the domain of machine learning and information theory. They are widely used in generative models, fair machine learning algorithms, design of privacy-assuring mechanisms, and appear as information-theoretic performance bounds in various multi-user communication problems"}
{"full_text": "This PhD thesis is devoted to deterministic study of the turbulence in the Navier- Stokes equations. The thesis is divided in four independent chapters.The first chapter involves a rigorous discussion about the energy's dissipation law, proposed by theory of the turbulence K41, in the deterministic setting of the homogeneous and incompressible Navier-Stokes equations, with a stationary external force (the force only depends of the spatial variable) and on the whole space R3. The energy's dissipation law, also called the Kolmogorov's dissipation law, characterizes the energy's dissipation rate (in the form of heat) of a turbulent fluid and this law was developed by A.N. Kolmogorov in 1941. However, its deduction (which uses mainly tools of statistics) is not fully understood until our days and then an active research area consists in studying this law in the rigorous framework of the Navier-Stokes equations which describe in a mathematical way the fluids motion and in particular the movement of turbulent fluids. In this setting, the purpose of this chapter is to highlight the fact that if we consider the Navier-Stokes equations on R3 then certain physical quantities, necessary for the study of the Kolmogorov's dissipation law, have no a rigorous definition and then to give a sense to these quantities we suggest to consider the Navier-Stokes equations with an additional damping term. In the framework of these damped equations, we obtain some estimates for the energy's dissipation rate according to the Kolmogorov's dissipation law.In the second chapter we are interested in study the stationary solutions of the damped Navier- Stokes introduced in the previous chapter. These stationary solutions are a particular type of solutions which do not depend of the temporal variable and their study is motivated by the fact that we always consider the Navier-Stokes equations with a stationary external force. In this chapter we study two properties of the stationary solutions : the first property concerns the stability of these solutions where we prove that if we have a control on the external force then all non stationary solution (with depends of both spatial and temporal variables) converges toward a stationary solution. The second property concerns the decay in spatial variable of the stationary solutions. These properties of stationary solutions are a consequence of the damping term introduced in the Navier-Stokes equations.In the third chapter we still study the stationary solutions of Navier-Stokes equations but now we consider the classical equations (without any additional damping term). The purpose of this chapter is to study an other problem related to the deterministic description of the turbulence : the frequency decay of the stationary solutions. Indeed, according to the K41 theory, if the fluid is in a laminar setting then the stationary solutions of the Navier-Stokes equations must exhibit a exponential frequency decay which starts at lows frequencies. But, if the fluid is in a turbulent setting then this exponential frequency decay must be observed only at highs frequencies. In this chapter, using some Fourier analysis tools, we give a precise description of this exponential frequency decay in the laminar and in the turbulent setting.In the fourth and last chapter we return to the stationary solutions of the classical Navier-Stokes equations and we study the uniqueness of these solutions in the particular case without any external force. Following some ideas of G. Seregin, we study the uniqueness of these solutions first in the framework of Lebesgue spaces of and then in the a general framework of Morrey spaces.", "prompt": "This PhD thesis is devoted to deterministic study of the turbulence in the Navier- Stokes equations. The thesis is divided in four independent chapters.The first chapter involves a rigorous discussion about the energy's dissipation law, proposed by theory of the turbulence K41, in the deterministic setting of the homogeneous and incompressible Nav", "orig": "ier-Stokes equations, with a stationary external force (the force only depends of the spatial variable) and on the whole space R3. The energy's dissipation law, also called the Kolmogorov's dissipation law, characterizes the energy's dissipation rate (in the form of heat) of", "generated": "This PhD thesis is devoted to deterministic study of the turbulence in the Navier- Stokes equations. The thesis is divided in four independent chapters. The first chapter involves a rigorous discussion about the energy's dissipation law, proposed by theory of the turbulence K41, in the deterministic setting of the homogeneous and incompressible"}
{"full_text": "Device-to-Device (D2D) communication is expected to be a key feature supported by 5G networks, especially due to the proliferation of Mobile Edge Computing (MEC), which has a prominent role in reducing network stress by shifting computational tasks from the Internet to the mobile edge. Apart from being part of MEC, D2D can extend cellular coverage allowing users to communicate directly when telecommunication infrastructure is highly congested or absent. This significant departure from the typical cellular paradigm imposes the need for decentralised network routing protocols. Moreover, enhanced capabilities of mobile devices and D2D networking will likely result in proliferation of new malware types and epidemics. Although the literature is rich in terms of D2D routing protocols that enhance quality-of-service and energy consumption, they provide only basic security support, e.g., in the form of encryption. Routing decisions can, however, contribute to collaborative detection of mobile malware by leveraging different kinds of anti-malware software installed on mobile devices. Benefiting from the cooperative nature of D2D communications, devices can rely on each other's contributions to detect malware. The impact of our work is geared towards having more malware-free D2D networks. To achieve this, we designed and implemented a novel routing protocol for D2D communications that optimises routing decisions for explicitly improving malware detection. The protocol identifies optimal network paths, in terms of malware mitigation and energy spent for malware detection, based on a game theoretic model. Diverse capabilities of network devices running different types of anti-malware software and their potential for inspecting messages relayed towards an intended destination device are leveraged using game theoretic tools. An optimality analysis of both Nash and Stackelberg security games is undertaken, including both zero and non-zero sum variants, and the Defender's equilibrium strategies. By undertaking network simulations, theoretical results obtained are illustrated through randomly generated network scenarios showing how our protocol outperforms conventional routing protocols, in terms of expected payoff, which consists of: security damage in inflicted by malware and malware detection cost.", "prompt": "Device-to-Device (D2D) communication is expected to be a key feature supported by 5G networks, especially due to the proliferation of Mobile Edge Computing (MEC), which has a prominent role in reducing network stress by shifting computational tasks from the Internet to the mobile edge. Apart from being part of", "orig": "MEC, D2D can extend cellular coverage allowing users to communicate directly when telecommunication infrastructure is highly congested or absent. This significant departure from the typical cellular paradigm imposes the need for decentralised network routing protocols. Moreover, enhanced capabilities of mobile devices and D2D networking will likely result in proliferation of new malware", "generated": "Device-to-Device (D2D) communication is expected to be a key feature supported by 5G networks, especially due to the proliferation of Mobile Edge Computing (MEC), which has a prominent role in reducing network stress by shifting computational tasks from the Internet to the mobile edge. Apart from being part of"}
{"full_text": "This study proposes a methodology using OpenAlex (OA) for tracking Open Access publications in the case of Argentina, a country where a self-archiving mandate has been in effect since 2013 ( Law 26.899, 2013). A sample of 167,240 papers by researchers from the National Council for Scientific and Technical Research (CONICET) was created and analyzed using statistical techniques. We estimate that OA is able to capture between 85-93% of authors for all disciplines, with the exception of Social Sciences and Humanities, where it only reaches an estimated 47%. The availability of papers in Open Access was calculated to be 41% for the period 1953-2021 and 46% when considering exclusively the post-law period (2014-2021). In both periods, gold Open Access made up the most common route. When comparing equal periods post and pre-law, we observed that the upward trend of gold Open Access was pre-existing to the legislation and the availability of closed articles in repositories increased by 5% to what is estimated based on existing trends. However, while the green route has had a positive evolution, it has been the publication in gold journals that has boosted access to Argentine production more rapidly. We concluded that the OA-based methodology, piloted here for the first time, is viable for tracking Open Access in Argentina since it yields percentages similar to other national and international studies. En este estudio se propone una metodolog\\'ia utilizando OpenAlex (OA) para monitorear el acceso abierto (AA) a las publicaciones cient\\'ificas para el caso de Argentina, pa\\'is donde rige el mandato de autoarchivo -Ley 26.899 (2013)-. Se conform\\'o una muestra con 167.240 art\\'iculos de investigadores del Consejo Nacional de Investigaciones Cient\\'ificas y T\\'ecnicas (CONICET) que se analizaron con t\\'ecnicas estad\\'isticas. Se estim\\'o que OA puede representar entre 85-93% de los autores para todas las disciplinas, excepto Ciencias Sociales y Humanidades, donde solo alcanza al 47%. Se calcul\\'o que 41% de los art\\'iculos publicados entre 1953-2021 incluidos en la fuente est\\'an en AA, porcentaje que sube a 46% al considerar exclusivamente el periodo post ley (2014-2021). En ambos periodos es la v\\'ia dorada la que representa mayor proporci\\'on. Al comparar periodos iguales post y pre ley, se observ\\'o que la tendencia en alza de la v\\'ia dorada era preexistente a la legislaci\\'on y la disponibilidad de art\\'iculos cerrados en repositorios aument\\'o un 5% a lo que se estima en base a tendencias existentes. Se concluye que si bien la v\\'ia verde ha tenido una evoluci\\'on positiva, ha sido la publicaci\\'on en revistas doradas lo que ha impulsado m\\'as r\\'apidamente el acceso a la producci\\'on argentina. Asimismo, que la metodolog\\'ia basada en OA, piloteada aqu\\'i por primera vez, es viable para monitorear el AA en Argentina ya que arroja porcentajes similares a otros estudios nacionales e internacionales.", "prompt": "This study proposes a methodology using OpenAlex (OA) for tracking Open Access publications in the case of Argentina, a country where a self-archiving mandate has been in effect since 2013 ( Law 26.899, 2013). A sample of 167,240 papers by researchers from the National", "orig": "Council for Scientific and Technical Research (CONICET) was created and analyzed using statistical techniques. We estimate that OA is able to capture between 85-93% of authors for all disciplines, with the exception of Social Sciences and Humanities, where it only reaches an estimated 47%. The availability of papers in Open Access", "generated": "This study proposes a methodology using OpenAlex (OA) for tracking Open Access publications in the case of Argentina, a country where a self-archiving mandate has been in effect since 2013 (Law 26.899, 2013). A sample of 167,240 papers by researchers from the National"}
{"full_text": "Bromochloro alkanes (BCAs) have been manufactured for use as flame retardants for decades and preliminary environmental risk screening suggests they are likely to behave similarly to polychlorinated alkanes (PCAs), subclasses of which are restricted as Stockholm Convention Persistent Organic Pollutants (POPs). BCAs have rarely been studied in the environment, though some evidence suggests they may migrate from treated-consumer materials into indoor dust, resulting in human exposure via inadvertent ingestion. In this study, BCA-C14 mixture standards were synthesized and used to validate an analytical method. This method relies on chloride-enhanced liquid chromatography-electrospray ionization-Orbitrap-high resolution mass spectrometry (LC-ESI-Orbitrap-HRMS) and a novel CP-Seeker integration software package for homologue detection and integration. Dust sample preparation via ultrasonic extraction, acidified silica clean-up and fractionation on neutral silica cartridges was found to be suitable for BCAs, with absolute recovery of individual homologues averaging 66 to 78% and coefficients of variation $\\le$10% in replicated spiking experiments (n=3). In addition, a total of 59 indoor dust samples from six countries including Australia (n=10), Belgium (n=10), Colombia (n=10), Japan (n=10), Thailand (n=10) and the United States of America (n=9) were analysed for BCAs. BCAs were detected in seven samples from the USA, with carbon chain lengths of C8, C10, C12, C14, C16, C18, C24 to C28, C30 and C31 observed overall, though not detected in samples from any other countries. Bromination of detected homologues in the indoor dust samples ranged from Br1-4 as well as Br7, while chlorine numbers ranged from Cl2-11. BCA-C18 were the most frequently detected, observed in each of the USA samples, while the most prevalent halogenation degrees were homologues of Br2 and Cl4-5. Broad estimations of BCA concentrations in the dust samples indicated that levels may approach those of other flame retardants in at least some instances. These findings suggest that development of quantification strategies and further investigation of environmental occurrence and health implications are needed.", "prompt": "Bromochloro alkanes (BCAs) have been manufactured for use as flame retardants for decades and preliminary environmental risk screening suggests they are likely to behave similarly to polychlorinated alkanes (PCAs), subclasses of which are restricted as Stockholm Convention Persistent Organic Pollutants (POPs", "orig": "). BCAs have rarely been studied in the environment, though some evidence suggests they may migrate from treated-consumer materials into indoor dust, resulting in human exposure via inadvertent ingestion. In this study, BCA-C14 mixture standards were synthesized and used to validate an analytical method. This method relies on chloride-enhanced liquid", "generated": "Bromochloro alkanes (BCAs) have been manufactured for use as flame retardants for decades and preliminary environmental risk screening suggests they are likely to behave similarly to polychlorinated alkanes (PCAs), subclasses of which are restricted as Stockholm Convention Persistent Organic Pollutants (POP)."}
{"full_text": "Context: Database-backed applications often run queries with more authority than necessary. Since programs can access more data than they legitimately need, flaws in security checks at the application level can enable malicious or buggy code to view or modify data in violation of intended access control policies. Inquiry: Although database management systems provide tools to control access to data, these tools are not well-suited for modern applications which often have many users and consist of many different software components. First, databases are unaware of application users, and creating a new database user for each application user is impractical for applications with many users. Second, different components of the same application may require different authority, which would require creating different database users for different software components. Thus, it is difficult to use existing tools to properly limit the authority an application has when executing queries. For this reason, we consider a new, language-based approach to application-specific database security. Approach: Prior work has addressed the difficulty of running applications with least privilege using capability-based security and software contracts, which we adapt to the setting of database-backed applications. Knowledge: This paper's main contribution is the design and implementation of ShillDB, a language for writing secure database-backed applications. ShillDB enables reasoning about database access at the language level through capabilities, which limit which database tables a program can access, and contracts, which limit what operations a program can perform on those tables. ShillDB contracts are expressed as part of function interfaces, making it easy to specify different access control policies for different components. Contracts act as executable security documentation for ShillDB programs and are enforced by the language runtime. Further, ShillDB provides database access control guarantees independent of (and in addition to) the security mechanisms of the underlying database management system. Grounding: We have implemented a prototype of ShillDB and have used it to implement the backend for a lending library reservation system with contracts for each endpoint to evaluate the performance and usability of ShillDB. Further, we benchmark individual database operations in ShillDB to better understand the language's performance overhead. Importance: Our experience indicates that ShillDB is a practical language for enforcing database access control policies in realistic, multi-user applications and has reasonable performance overhead. ShillDB allows developers to reason about security at the component level, safely compose components, and reuse third-party components with their own application-specific database security policies.", "prompt": "Context: Database-backed applications often run queries with more authority than necessary. Since programs can access more data than they legitimately need, flaws in security checks at the application level can enable malicious or buggy code to view or modify data in violation of intended access control policies. Inquiry: Although database management systems provide tools to control access", "orig": "to data, these tools are not well-suited for modern applications which often have many users and consist of many different software components. First, databases are unaware of application users, and creating a new database user for each application user is impractical for applications with many users. Second, different components of the same application may require", "generated": "Database-backed applications often run queries with more authority than necessary. Since programs can access more data than they legitimately need, flaws in security checks at the application level can enable malicious or buggy code to view or modify data in violation of intended access control policies. Although database management systems provide tools to control access to sensitive data,"}
{"full_text": "Jerome H. Friedman was born in Yreka, California, USA, on December 29, 1939. He received his high school education at Yreka High School, then spent two years at Chico State College before transferring to the University of California at Berkeley in 1959. He completed an undergraduate degree in physics in 1962 and a Ph.D. in high-energy particle physics in 1968 and was a post-doctoral research physicist at the Lawrence Berkeley Laboratory during 1968-1972. In 1972, he moved to Stanford Linear Accelerator Center (SLAC) as head of the Computation Research Group, retaining this position until 2006. In 1981, he was appointed half time as Professor in the Department of Statistics, Stanford University, remaining half time with his SLAC appointment. He has held visiting appointments at CSIRO in Sydney, CERN and the Department of Statistics at Berkeley, and has had a very active career as a commercial consultant. Jerry became Professor Emeritus in the Department of Statistics in 2007. Apart from some 30 publications in high-energy physics early in his career, Jerry has published over 70 research articles and books in statistics and computer science, including co-authoring the pioneering books Classification and Regression Trees and The Elements of Statistical Learning. Many of his publications have hundreds if not thousands of citations (e.g., the CART book has over 21,000). Much of his software is incorporated in commercial products, including at least one popular search engine. Many of his methods and algorithms are essential inclusions in modern statistical and data mining packages. Honors include the following: the Rietz Lecture (1999) and the Wald Lectures (2009); election to the American Academy of Arts and Sciences (2005) and the US National Academy of Sciences (2010); a Fellow of the American Statistical Association; Paper of the Year (JASA 1980, 1985; Technometrics 1998, 1992); Statistician of the Year (ASA, Chicago Chapter, 1999); ACM Data Mining Lifetime Innovation Award (2002), Emanuel & Carol Parzen Award for Statistical Innovation (2004); Noether Senior Lecturer (American Statistical Association, 2010); and the IEEE Computer Society Data Mining Research Contribution Award (2012).", "prompt": "Jerome H. Friedman was born in Yreka, California, USA, on December 29, 1939. He received his high school education at Yreka High School, then spent two years at Chico State College before transferring to the University of California at Berkeley in 1959. He completed", "orig": "an undergraduate degree in physics in 1962 and a Ph.D. in high-energy particle physics in 1968 and was a post-doctoral research physicist at the Lawrence Berkeley Laboratory during 1968-1972. In 1972, he moved to Stanford Linear Accelerator Center (SLAC) as head", "generated": "Jerome H. Friedman was born in Yreka, California, USA, on December 29, 1939. He received his high school education at Yreka High School, then spent two years at Chico State College before transferring to the University of California at Berkeley in 1959. He completed"}
{"full_text": "Born in Sydney, Australia, on April 20, 1939, Chris Heyde shifted his interest from sport to mathematics thanks to inspiration from a schoolteacher. After earning an M.Sc. degree from the University of Sydney and a Ph.D. from the Australian National University (ANU), he began his academic career in the United States at Michigan State University, and then in the United Kingdom at the University of Sheffield and the University of Manchester. In 1968, Chris moved back to Australia to teach at ANU until 1975, when he joined CSIRO, where he was Acting Chief of the Division of Mathematics and Statistics. From 1983 to 1986, he was a Professor and Chairman of the Department of Statistics at the University of Melbourne. Chris then returned to ANU to become the Head of the Statistics Department, and later the Foundation Dean of the School of Mathematical Sciences (now the Mathematical Sciences Institute). Since 1993, he has also spent one semester each year teaching at the Department of Statistics, Columbia University, and has been the director of the Center for Applied Probability at Columbia University since its creation in 1993. Chris has been honored worldwide for his contributions in probability, statistics and the history of statistics. He is a Fellow of the International Statistical Institute and the Institute of Mathematical Statistics, and he is one of three people to be a member of both the Australian Academy of Science and the Australian Academy of Social Sciences. In 2003, he received the Order of Australia from the Australian government. He has been awarded the Pitman Medal and the Hannan Medal. Chris was conferred a D.Sc. honoris causa by University of Sydney in 1998. Chris has been very active in serving the statistical community, including as the Vice President of the International Statistical Institute, President of the Bernoulli Society and Vice President of the Australian Mathematical Society. He has served on numerous editorial boards, most notably as Editor of Stochastic Processes and Their Applications from 1983 to 1989, and as Editor-in-Chief of Journal of Applied Probability and Advances in Applied Probability since 1990.", "prompt": "Born in Sydney, Australia, on April 20, 1939, Chris Heyde shifted his interest from sport to mathematics thanks to inspiration from a schoolteacher. After earning an M.Sc. degree from the University of Sydney and a Ph.D. from the Australian National University (ANU), he began his academic", "orig": "career in the United States at Michigan State University, and then in the United Kingdom at the University of Sheffield and the University of Manchester. In 1968, Chris moved back to Australia to teach at ANU until 1975, when he joined CSIRO, where he was Acting Chief of the Division of Mathematics and", "generated": "Born in Sydney, Australia, on April 20, 1939, Chris Heyde shifted his interest from sport to mathematics thanks to inspiration from a schoolteacher. After earning an M.Sc. degree from the University of Sydney and a Ph.D. from the Australian National University (ANU), he began his academic"}
{"full_text": "We present DCL-PC: a logic for reasoning about how the abilities of agents and coalitions of agents are altered by transferring control from one agent to another. The logical foundation of DCL-PC is CL-PC, a logic for reasoning about cooperation in which the abilities of agents and coalitions of agents stem from a distribution of atomic Boolean variables to individual agents -- the choices available to a coalition correspond to assignments to the variables the coalition controls. The basic modal constructs of DCL-PC are of the form coalition C can cooperate to bring about phi. DCL-PC extends CL-PC with dynamic logic modalities in which atomic programs are of the form agent i gives control of variable p to agent j; as usual in dynamic logic, these atomic programs may be combined using sequence, iteration, choice, and test operators to form complex programs. By combining such dynamic transfer programs with cooperation modalities, it becomes possible to reason about how the power of agents and coalitions is affected by the transfer of control. We give two alternative semantics for the logic: a direct semantics, in which we capture the distributions of Boolean variables to agents; and a more conventional Kripke semantics. We prove that these semantics are equivalent, and then present an axiomatization for the logic. We investigate the computational complexity of model checking and satisfiability for DCL-PC, and show that both problems are PSPACE-complete (and hence no worse than the underlying logic CL-PC). Finally, we investigate the characterisation of control in DCL-PC. We distinguish between first-order control -- the ability of an agent or coalition to control some state of affairs through the assignment of values to the variables under the control of the agent or coalition -- and second-order control -- the ability of an agent to exert control over the control that other agents have by transferring variables to other agents. We give a logical characterisation of second-order control.", "prompt": "We present DCL-PC: a logic for reasoning about how the abilities of agents and coalitions of agents are altered by transferring control from one agent to another. The logical foundation of DCL-PC is CL-PC, a logic for reasoning about cooperation in which the abilities of agents and coalitions of agents", "orig": "stem from a distribution of atomic Boolean variables to individual agents -- the choices available to a coalition correspond to assignments to the variables the coalition controls. The basic modal constructs of DCL-PC are of the form coalition C can cooperate to bring about phi. DCL-PC extends CL-PC with dynamic logic modalities in", "generated": "We present DCL-PC: a logic for reasoning about how the abilities of agents and coalitions of agents are altered by transferring control from one agent to another. The logical foundation of DCL-PC is CL-PC, a logic for reasoning about cooperation in which the abilities of agents and coalitions of agents"}
{"full_text": "Yuan Shih Chow was born in Hubei province in China, on September 1, 1924. The eldest child of a local militia and political leader, he grew up in war and turmoil. His hometown was on the front line during most of the Japanese invasion and occupation of China. When he was 16, Y. S. Chow journeyed, mostly on foot, to Chongqing (Chung-King), the wartime Chinese capital, to finish his high school education. When the Communist party gained power in China, Y. S. Chow had already followed his university job to Taiwan. In Taiwan, he taught mathematics as an assistant at National Taiwan University until he came to the United States in 1954. At the University of Illinois, he studied under J. L. Doob and received his Ph.D. in 1958. He served as a staff mathematician and adjunct faculty at the IBM Watson Research Laboratory and Columbia University from 1959 to 1962. He was a member of the Statistics Department at Purdue University from 1962 to 1968. From 1968 until his retirement in 1993, Y. S. Chow served as Professor of Mathematical Statistics at Columbia University. At different times, he was a visiting professor at the University of California at Berkeley, University of Heidelberg (Germany) and the National Central University, Taiwan. He served as Director of the Institute of Mathematics of Academia Sinica, Taiwan, and Director of the Center of Applied Statistics at Nankai University, Tianjin, China. He was instrumental in establishing the Institute of Statistics of Academia Sinica in Taiwan. He is currently Professor Emeritus at Columbia University. Y. S. Chow is a fellow of the Institute of Mathematical Statistics, a member of the International Statistical Institute and a member of Taiwan's Academia Sinica. He has numerous publications, including Great Expectations: The Theory of Optimal Stopping (1971), in collaboration with Herbert Robbins and David Siegmund, and Probability Theory (1978), in collaboration with Henry Teicher. Y. S. Chow has a strong interest in mathematics education. He taught high school mathematics for one year in 1947 and wrote a book on high school algebra in collaboration with J. H. Teng and M. L. Chu. In 1992, Y. S. Chow, together with I. S. Chang and W. C. Ho, established the Chinese Institute of Probability and Statistics in Taiwan. This conversation took place in the fall of 2003 in Dobbs Ferry, New York.", "prompt": "Yuan Shih Chow was born in Hubei province in China, on September 1, 1924. The eldest child of a local militia and political leader, he grew up in war and turmoil. His hometown was on the front line during most of the Japanese invasion and occupation of China. When he", "orig": "was 16, Y. S. Chow journeyed, mostly on foot, to Chongqing (Chung-King), the wartime Chinese capital, to finish his high school education. When the Communist party gained power in China, Y. S. Chow had already followed his university job to Taiwan. In Taiwan,", "generated": "Yuan Shih Chow was born in Hubei province in China, on September 1, 1924. The eldest child of a local militia and political leader, he grew up in war and turmoil. His hometown was on the front line during most of the Japanese invasion and occupation of China. When his"}
{"full_text": "Parametrized families of PDEs arise in various contexts such as inverse problems, control and optimization, risk assessment, and uncertainty quantification. In most of these applications, the number of parameters is large or perhaps even infinite. Thus, the development of numerical methods for these parametric problems is faced with the possible curse of dimensionality. This article is directed at (i) identifying and understanding which properties of parametric equations allow one to avoid this curse and (ii) developing and analyzing effective numerical methodd which fully exploit these properties and, in turn, are immune to the growth in dimensionality. The first part of this article studies the smoothness and approximability of the solution map, that is, the map $a\\mapsto u(a)$ where $a$ is the parameter value and $u(a)$ is the corresponding solution to the PDE. It is shown that for many relevant parametric PDEs, the parametric smoothness of this map is typically holomorphic and also highly anisotropic in that the relevant parameters are of widely varying importance in describing the solution. These two properties are then exploited to establish convergence rates of $n$-term approximations to the solution map for which each term is separable in the parametric and physical variables. These results reveal that, at least on a theoretical level, the solution map can be well approximated by discretizations of moderate complexity, thereby showing how the curse of dimensionality is broken. This theoretical analysis is carried out through concepts of approximation theory such as best $n$-term approximation, sparsity, and $n$-widths. These notions determine a priori the best possible performance of numerical methods and thus serve as a benchmark for concrete algorithms. The second part of this article turns to the development of numerical algorithms based on the theoretically established sparse separable approximations. The numerical methods studied fall into two general categories. The first uses polynomial expansions in terms of the parameters to approximate the solution map. The second one searches for suitable low dimensional spaces for simultaneously approximating all members of the parametric family. The numerical implementation of these approaches is carried out through adaptive and greedy algorithms. An a priori analysis of the performance of these algorithms establishes how well they meet the theoretical benchmarks.", "prompt": "Parametrized families of PDEs arise in various contexts such as inverse problems, control and optimization, risk assessment, and uncertainty quantification. In most of these applications, the number of parameters is large or perhaps even infinite. Thus, the development of numerical methods for these parametric problems is faced with the possible", "orig": "curse of dimensionality. This article is directed at (i) identifying and understanding which properties of parametric equations allow one to avoid this curse and (ii) developing and analyzing effective numerical methodd which fully exploit these properties and, in turn, are immune to the growth in dimensionality. The first part of this article", "generated": "Parametrized families of PDEs arise in various contexts such as inverse problems, control and optimization, risk assessment, and uncertainty quantification. In most of these applications, the number of parameters is large or perhaps even infinite. Thus, the development of numerical methods for these parametric problems is faced with the possibility"}
{"full_text": "We measure the large-scale real-space power spectrum P(k) using luminous red galaxies (LRGs) in the Sloan Digital Sky Survey (SDSS) and use this measurement to sharpen constraints on cosmological parameters from the Wilkinson Microwave Anisotropy Probe (WMAP). We employ a matrix-based power spectrum estimation method using Pseudo-Karhunen-Loeve eigenmodes, producing uncorrelated minimum-variance measurements in 20 k-bands of both the clustering power and its anisotropy due to redshift-space distortions, with narrow and well-behaved window functions in the range 0.01h/Mpc < k < 0.2h/Mpc. Results from the LRG and main galaxy samples are consistent, with the former providing higher signal-to-noise. Our results are robust to omitting angular and radial density fluctuations and are consistent between different parts of the sky. They provide a striking confirmation of the predicted large-scale LCDM power spectrum. Combining only SDSS LRG and WMAP data places robust constraints on many cosmological parameters that complement prior analyses of multiple data sets. The LRGs provide independent cross-checks on Om and the baryon fraction in good agreement with WMAP. Within the context of flat LCDM models, our LRG measurements complement WMAP by sharpening the constraints on the matter density, the neutrino density and the tensor amplitude by about a factor of two, giving Omega_m=0.24+-0.02 (1 sigma), sum m_nu < 0.9 eV (95%) and r<0.3 (95%). Baryon oscillations are clearly detected and provide a robust measurement of the comoving distance to the median survey redshift z=0.35 independent of curvature and dark energy properties. Within the LCDM framework, our power spectrum measurement improves the evidence for spatial flatness, sharpening the curvature constraint Omega_tot=1.05+-0.05 from WMAP alone to Omega_tot=1.003+-0.010. Assuming Omega_tot=1, the equation of state parameter is constrained to w=-0.94+-0.09, indicating the potential for more ambitious future LRG measurements to provide precision tests of the nature of dark energy. All these constraints are essentially independent of scales k>0.1h/Mpc and associated nonlinear complications, yet agree well with more aggressive published analyses where nonlinear modeling is crucial.", "prompt": "We measure the large-scale real-space power spectrum P(k) using luminous red galaxies (LRGs) in the Sloan Digital Sky Survey (SDSS) and use this measurement to sharpen constraints on cosmological parameters from the Wilkinson Microwave Anisotropy Probe (WMAP). We employ a matrix-based power spectrum estimation", "orig": "method using Pseudo-Karhunen-Loeve eigenmodes, producing uncorrelated minimum-variance measurements in 20 k-bands of both the clustering power and its anisotropy due to redshift-space distortions, with narrow and well-behaved window functions in the range 0.01", "generated": "We measure the large-scale real-space power spectrum P(k) using luminous red galaxies (LRGs) in the Sloan Digital Sky Survey (SDSS) and use this measurement to sharpen constraints on cosmological parameters from the Wilkinson Microwave Anisotropy Probe (WMAP). We employ a matrix-based power spectrum estimation"}
{"full_text": "Richard Olshen was born in Portland, Oregon, on May 17, 1942. Richard spent his early years in Chevy Chase, Maryland, but has lived most of his life in California. He received an A.B. in Statistics at the University of California, Berkeley, in 1963, and a Ph.D. in Statistics from Yale University in 1966, writing his dissertation under the direction of Jimmie Savage and Frank Anscombe. He served as Research Staff Statistician and Lecturer at Yale in 1966-1967. Richard accepted a faculty appointment at Stanford University in 1967, and has held tenured faculty positions at the University of Michigan (1972-1975), the University of California, San Diego (1975-1989), and Stanford University (since 1989). At Stanford, he is Professor of Health Research and Policy (Biostatistics), Chief of the Division of Biostatistics (since 1998) and Professor (by courtesy) of Electrical Engineering and of Statistics. At various times, he has had visiting faculty positions at Columbia, Harvard, MIT, Stanford and the Hebrew University. Richard's research interests are in statistics and mathematics and their applications to medicine and biology. Much of his work has concerned binary tree-structured algorithms for classification, regression, survival analysis and clustering. Those for classification and survival analysis have been used with success in computer-aided diagnosis and prognosis, especially in cardiology, oncology and toxicology. He coauthored the 1984 book Classification and Regression Trees (with Leo Brieman, Jerome Friedman and Charles Stone) which gives motivation, algorithms, various examples and mathematical theory for what have come to be known as CART algorithms. The approaches to tree-structured clustering have been applied to problems in digital radiography (with Stanford EE Professor Robert Gray) and to HIV genetics, the latter work including studies on single nucleotide polymorphisms, which has helped to shed light on the presence of hypertension in certain subpopulations of women.", "prompt": "Richard Olshen was born in Portland, Oregon, on May 17, 1942. Richard spent his early years in Chevy Chase, Maryland, but has lived most of his life in California. He received an A.B. in Statistics at the University of California, Berkeley, in 1963, and", "orig": "a Ph.D. in Statistics from Yale University in 1966, writing his dissertation under the direction of Jimmie Savage and Frank Anscombe. He served as Research Staff Statistician and Lecturer at Yale in 1966-1967. Richard accepted a faculty appointment at Stanford University in 1967, and", "generated": "Richard Olshen was born in Portland, Oregon, on May 17, 1942. Richard spent his early years in Chevy Chase, Maryland, but has lived most of his life in California. He received an A.B. in Statistics at the University of California, Berkeley, in 1963, and"}
{"full_text": "This article examines the importance of graphic representations in the social sciences, and particularly in (medieval) history, taking as its starting point a reflection by {\\'E}tienne-Jules Marey, a physiologist and pioneer of 19th-century photography and cinema. Marey believed that the visual should replace language in many fields. Indeed, the twentieth and early twenty-first centuries saw an exponential multiplication of visual media, particularly with the advent of digital technology. However, this ''graphics revolution'' has not affected all disciplines equally. Significant differences remain between scientific fields such as astrophysics, anthropology, chemistry and medieval history, despite their shared commitment to describing dynamic processes and changes of state. Yet, while historians have already digitized a large part of the cultural heritage from Antiquity to the 10th-13th centuries, exploration of this corpus using visualizations remains limited. There is therefore untapped potential in this field.This article begins by outlining a typology and quantification of the past and potential roles of visual representations in medieval history. It examines two distinct intellectual approaches: 1. the use of visuals to support a scientific discourse (majority) and 2. the construction of a historical discourse based on observations made from visual figures with the aim of modeling phenomena invisible to the naked eye. The author thus examines the use of ''images'' in medievalism, focusing on the annual volumes of the Soci{\\'e}t{\\'e} des historiens m{\\'e}di{\\'e}vistes de l'enseignement sup{\\'e}rieur (SHMESP), up to 2006. Two other parts of the text look at the still-rare forms of visual representation in medieval history, particularly those with a ''heuristic vocation'', using iconographic objects, parchments, buildings and digitized texts. The article suggests various visualization techniques, such as network analysis, the creation of ''stemmas 2.0'' and interactive chronologies, which could benefit the discipline. These methods could potentially profoundly change our understanding of ancient societies, by showing the dynamic relationships between different aspects of these societies. One of the most important advances expected from these visual methods is a better understanding of the patterns of development in medieval Europe, which varied from region to region. The hypothesis is that the scarcity of heuristic graphics in medieval history stems from the relationship with ancient documents and the historical method based on narration and exemplarity. The article thus questions the value of ''visual modelling'' in medieval history, and highlights the challenges associated with the widespread adoption of this approach in the humanities and social sciences. Finally, the text invites us to reflect on the nature and functioning of heuristic visual devices, by comparing medieval ''images'' and contemporary scientific visuals. In both cases, the point is to materialize the invisible in order to show something that exists beyond the visual. The author suggests that this way of approaching visuals could play a growing role in the decades to come, particularly in the field of data science.", "prompt": "This article examines the importance of graphic representations in the social sciences, and particularly in (medieval) history, taking as its starting point a reflection by {\\'E}tienne-Jules Marey, a physiologist and pioneer of 19th-century photography and cinema. Marey believed that the visual should", "orig": "replace language in many fields. Indeed, the twentieth and early twenty-first centuries saw an exponential multiplication of visual media, particularly with the advent of digital technology. However, this ''graphics revolution'' has not affected all disciplines equally. Significant differences remain between scientific fields such as astrophysics, anthropology, chemistry and medieval history,", "generated": "This article examines the importance of graphic representations in the social sciences, and particularly in (medieval) history, taking as its starting point a reflection by \u00c9tienne-Jules Marey, a physiologist and pioneer of 19th-century photography and cinema. Marey believed that the visual should be a key"}
{"full_text": "The aim of this paper is to present the results of the land use dynamic simulations in the municipality of Paragominas-PA. The simulation is based on models built from past land use data and spatial variables of the natural environment and infrastructure. Two spatial units were analyzed: the central area of commercial agricultural production and the area of settlements and smallholdings the east. The results show distinct spatial dynamics between the analyzed areas, among which we highlight the role of soil characteristics and, associated with the topography and the occupation history, are part of the context in which is defined rationality producers. Considering the transition from forest to pasture in the commercial farming area most often associated soils are sandy. This raises the following hypothesis: the deforestation that occurred in the period are related to livestock activities. Livestock favors access to water and low fertility sands does not affect production. On the other hand, the soybean expansion occurred preferentially on existing pastures and on clay soils (Belterra clay), reducing the availability of pastures on these soils. The relative importance of types of soil increases with time. In the area of settlements, the transition from forest to pasture and family crops occurred preferentially on the variegated clay. However, it is possible that the prevalence of this transition on this texture has been given due more to the history of occupation of this area. Historically the sandy valleys were the first to be occupied, and the continuity of the deforestation occurred toward the slopes dominated by variegated clay and plateaus with Belterra clay. These associations observed indicate that, within a wider context of social, economic and political factors, natural variable factors in space are important for the choice of managements in the properties, but they are done differently in the territory, and the best knowledge of these relationships are useful for territorial planning.", "prompt": "The aim of this paper is to present the results of the land use dynamic simulations in the municipality of Paragominas-PA. The simulation is based on models built from past land use data and spatial variables of the natural environment and infrastructure. Two spatial units were analyzed: the central area of commercial agricultural production and", "orig": "the area of settlements and smallholdings the east. The results show distinct spatial dynamics between the analyzed areas, among which we highlight the role of soil characteristics and, associated with the topography and the occupation history, are part of the context in which is defined rationality producers. Considering the transition from forest to pasture in", "generated": "The central area of commercial agricultural production and the peripheral area of settlement and urban expansion. The simulations were run using the CLUE-S model, a well-known and widely used land use change model, which takes into account the interactions between demographic, economic, and environmental factors.\n\nThe results of the simulations show that the central"}
{"full_text": "Ingram Olkin was born on July 23, 1924 in Waterbury, Connecticut. His family moved to New York in 1934 and he graduated from DeWitt Clinton High School in 1941. He served three years in the Air Force during World War II and obtained a B.S. in mathematics at the City College of New York in 1947. After receiving an M.A. in mathematical statistics from Columbia in 1949, he completed his graduate studies in the Department of Statistics at the University of North Carolina in 1951. His dissertation was written under the direction of S. N. Roy and Harold Hotelling. He joined the Department of Mathematics at Michigan State University in 1951 as an Assistant Professor, subsequently being promoted to Professor. In 1960, he took a position as Chair of the Department of Statistics at the University of Minnesota. He moved to Stanford University in 1961 to take a joint position as Professor of Statistics and Professor of Education; he was also Chair of the Department of Statistics from 1973--1976. In 2007, Ingram became Professor Emeritus. Ingram was Editor of the Annals of Mathematical Statistics (1971--1972) and served as the first editor of the Annals of Statistics from 1972--1974. He was a primary force in the founding of the Journal of Educational Statistics, for which he was also Associate Editor during 1977--1985. In 1984, he was President of the Institute of Mathematical Statistics. Among his many professional activities, he has served as Chair of the Committee of Presidents of Statistical Societies (COPSS), Chair of the Committee on Applied and Theoretical Statistics of the National Research Council, Chair of the Management Board of the American Education Research Association, and as Trustee for the National Institute of Statistical Sciences. He has been honored by the American Statistical Association (ASA) with a Wilks Medal (1992) and a Founder's Award (1992). The American Psychological Association gave him a Lifetime Contribution Award (1997) and he was elected to the National Academy of Education in 2005. He received the COPSS Elizabeth L. Scott Award in 1998 and delivered the R. A. Fisher Lecture in 2000. In 2003, the City University of New York gave him a Townsend Harris Medal. An author of 5 books, an editor of 10 books, and an author of more than 200 publications, Ingram has made major contributions to statistics and education. His research has focused on multivariate analysis, majorization and inequalities, distribution theory, and meta-analysis. A volume in celebration of Ingram's 65th birthday contains a brief biography and an interview [Gleser, Perlman, Press and Sampson (1989)]. Ingram was chosen in 1997 to participate in the American Statistical Association Distinguished Statistician Video Series and a videotaped conversation and a lecture (Olkin, 1997) are available from the ASA (1997, DS041, DS042).", "prompt": "Ingram Olkin was born on July 23, 1924 in Waterbury, Connecticut. His family moved to New York in 1934 and he graduated from DeWitt Clinton High School in 1941. He served three years in the Air Force during World War II and obtained a B.S.", "orig": "in mathematics at the City College of New York in 1947. After receiving an M.A. in mathematical statistics from Columbia in 1949, he completed his graduate studies in the Department of Statistics at the University of North Carolina in 1951. His dissertation was written under the direction of S. N. Roy", "generated": "Ingram Olkin was born on July 23, 1924 in Waterbury, Connecticut. His family moved to New York in 1934 and he graduated from DeWitt Clinton High School in 1941. He served three years in the Air Force during World War II and obtained a B.S."}
